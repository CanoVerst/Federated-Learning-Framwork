The configuration filename is: async_MNIST_lenet5.yml
 
clients:
# Type
type: simple

# The total number of clients
total_clients: 500

# The number of clients selected in each round
per_round: 20

# Should the clients compute test accuracy locally?
do_test: false
# Whether client heterogeneity should be simulated
speed_simulation: true
sleep_simulation: true
avg_training_time: 10
# The simulation distribution
#sleep_simulation: true
simulation_distribution:
distribution: pareto
alpha: 1

server:
address: 127.0.0.1
port: 7793
do_test: true
random_seed: 1
# What is the staleness bound, beyond which the server should wait for stale clients?
staleness_bound: 10
synchronous: false
simulate_wall_time: true
minimum_clients_aggregated: 10


checkpoint_path: results/test/checkpoint
model_path: results/test/model

data:
# The training and testing dataset
datasource: MNIST

# Where the dataset is located
#data_path: ../../data

# Number of samples in each partition
partition_size: 100

# IID or non-IID?
sampler: noniid

#testset_size: 1000
concentration: 0.1

# The random seed for sampling data
random_seed: 1


trainer:
# The type of the trainer
type: basic

# The maximum number of training rounds
rounds: 100

# The maximum number of clients running concurrently
max_concurrency: 3

# The target accuracy
target_accuracy: 0.99

# Number of epoches for local training in each communication round
epochs: 5
batch_size: 10
optimizer: SGD
learning_rate: 0.01
momentum: 0.9
weight_decay: 0.0

# The machine learning model
model_name: lenet5

algorithm:
# Aggregation algorithm
type: fedavg

results:
# Write the following parameter(s) into a CSV
types: round, accuracy, elapsed_time, comm_time, round_time

result_path: /data/ykang/plato/results/MNIST/async_selection
[INFO][09:40:18]: [Server #1127936] Started training on 500 clients with 20 per round.
[INFO][09:40:18]: [Server #1127936] Configuring the server...
[INFO][09:40:18]: Training: 100 rounds or accuracy above 99.0%

[INFO][09:40:18]: Trainer: basic
[INFO][09:40:18]: Algorithm: fedavg
[INFO][09:40:18]: Data source: MNIST
[INFO][09:40:18]: Starting client #1's process.
[INFO][09:40:18]: Starting client #2's process.
[INFO][09:40:18]: Starting client #3's process.
[INFO][09:40:18]: Setting the random seed for selecting clients: 1
[INFO][09:40:18]: Starting a server at address 127.0.0.1 and port 7793.
[INFO][09:40:20]: Client: simple
[INFO][09:40:20]: Starting a simple client #3.
[INFO][09:40:20]: Trainer: basic
[INFO][09:40:20]: Algorithm: fedavg
[INFO][09:40:20]: Client: simple
[INFO][09:40:20]: Starting a simple client #2.
[INFO][09:40:20]: Trainer: basic
[INFO][09:40:20]: Algorithm: fedavg
[INFO][09:40:20]: Client: simple
[INFO][09:40:20]: Starting a simple client #1.
[INFO][09:40:20]: Trainer: basic
[INFO][09:40:20]: Algorithm: fedavg
[INFO][09:40:25]: [Client #3] Contacting the server.
[INFO][09:40:25]: [Client #3] Connecting to the server at http://127.0.0.1:7793.
[INFO][09:40:25]: 127.0.0.1 [16/Jul/2022:13:40:25 +0000] "GET /socket.io/?transport=polling&EIO=4&t=1657978825.3147278 HTTP/1.1" 200 293 "-" "Python/3.9 aiohttp/3.8.1"
[INFO][09:40:25]: [Client #2] Contacting the server.
[INFO][09:40:25]: [Client #2] Connecting to the server at http://127.0.0.1:7793.
[INFO][09:40:25]: [Server #1127936] A new client just connected.
[INFO][09:40:25]: 127.0.0.1 [16/Jul/2022:13:40:25 +0000] "GET /socket.io/?transport=polling&EIO=4&t=1657978825.3217704 HTTP/1.1" 200 293 "-" "Python/3.9 aiohttp/3.8.1"
[INFO][09:40:25]: [Client #3] Connected to the server.
[INFO][09:40:25]: [Client #3] Waiting to be selected.
[INFO][09:40:25]: [Server #1127936] New client with id #3 arrived.
[INFO][09:40:25]: [Server #1127936] A new client just connected.
[INFO][09:40:25]: [Client #2] Connected to the server.
[INFO][09:40:25]: [Client #2] Waiting to be selected.
[INFO][09:40:25]: [Server #1127936] New client with id #2 arrived.
[INFO][09:40:25]: [Client #1] Contacting the server.
[INFO][09:40:25]: [Client #1] Connecting to the server at http://127.0.0.1:7793.
[INFO][09:40:25]: 127.0.0.1 [16/Jul/2022:13:40:25 +0000] "GET /socket.io/?transport=polling&EIO=4&t=1657978825.3758023 HTTP/1.1" 200 293 "-" "Python/3.9 aiohttp/3.8.1"
[INFO][09:40:25]: [Server #1127936] A new client just connected.
[INFO][09:40:25]: [Client #1] Connected to the server.
[INFO][09:40:25]: [Client #1] Waiting to be selected.
[INFO][09:40:25]: [Server #1127936] New client with id #1 arrived.
[INFO][09:40:25]: [Server #1127936] Starting training.
[INFO][09:40:25]: [93m[1m
[Server #1127936] Starting round 1/100.[0m
[INFO][09:40:25]: [Server #1127936] Selected clients: [ 44 114 158  88 304 207 409  93 351 121 288 175  29 115 333 249 260 286
 499 411]
[INFO][09:40:25]: [Server #1127936] Selecting client #44 for training.
[INFO][09:40:25]: [Server #1127936] Sending the current model to client #44 (simulated).
[INFO][09:40:25]: [Server #1127936] Sending 0.24 MB of payload data to client #44 (simulated).
[INFO][09:40:25]: [Server #1127936] Selecting client #114 for training.
[INFO][09:40:25]: [Server #1127936] Sending the current model to client #114 (simulated).
[INFO][09:40:25]: [Server #1127936] Sending 0.24 MB of payload data to client #114 (simulated).
[INFO][09:40:25]: [Client #44] Selected by the server.
[INFO][09:40:25]: [Client #44] Loading its data source...
[INFO][09:40:25]: Data source: MNIST
[INFO][09:40:25]: [Server #1127936] Selecting client #158 for training.
[INFO][09:40:25]: [Server #1127936] Sending the current model to client #158 (simulated).
[INFO][09:40:25]: [Server #1127936] Sending 0.24 MB of payload data to client #158 (simulated).
[INFO][09:40:25]: [Client #114] Selected by the server.
[INFO][09:40:25]: [Client #158] Selected by the server.
[INFO][09:40:25]: [Client #114] Loading its data source...
[INFO][09:40:25]: Data source: MNIST
[INFO][09:40:25]: [Client #158] Loading its data source...
[INFO][09:40:25]: Data source: MNIST
[INFO][09:40:25]: [Client #44] Dataset size: 60000
[INFO][09:40:25]: [Client #44] Sampler: noniid
[INFO][09:40:25]: [Client #44] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:40:25]: [93m[1m[Client #44] Started training in communication round #1.[0m
[INFO][09:40:25]: [Client #158] Dataset size: 60000
[INFO][09:40:25]: [Client #158] Sampler: noniid
[INFO][09:40:25]: [Client #158] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:40:25]: [Client #114] Dataset size: 60000
[INFO][09:40:25]: [Client #114] Sampler: noniid
[INFO][09:40:25]: [Client #114] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:40:25]: [93m[1m[Client #158] Started training in communication round #1.[0m
[INFO][09:40:25]: [93m[1m[Client #114] Started training in communication round #1.[0m
[INFO][09:40:28]: [Client #158] Loading the dataset.
[INFO][09:40:28]: [Client #44] Loading the dataset.
[INFO][09:40:28]: [Client #114] Loading the dataset.
[INFO][09:40:34]: [Client #158] Epoch: [1/5][0/10]	Loss: 2.399698
[INFO][09:40:34]: [Client #44] Epoch: [1/5][0/10]	Loss: 2.252364
[INFO][09:40:34]: [Client #114] Epoch: [1/5][0/10]	Loss: 2.212076
[INFO][09:40:34]: [Client #158] Epoch: [2/5][0/10]	Loss: 2.103168
[INFO][09:40:34]: [Client #44] Epoch: [2/5][0/10]	Loss: 1.831726
[INFO][09:40:34]: [Client #158] Epoch: [3/5][0/10]	Loss: 1.577554
[INFO][09:40:34]: [Client #114] Epoch: [2/5][0/10]	Loss: 1.734798
[INFO][09:40:34]: [Client #44] Epoch: [3/5][0/10]	Loss: 1.059035
[INFO][09:40:34]: [Client #114] Epoch: [3/5][0/10]	Loss: 1.187581
[INFO][09:40:34]: [Client #158] Epoch: [4/5][0/10]	Loss: 0.652827
[INFO][09:40:34]: [Client #114] Epoch: [4/5][0/10]	Loss: 0.509657
[INFO][09:40:34]: [Client #44] Epoch: [4/5][0/10]	Loss: 0.354147
[INFO][09:40:34]: [Client #114] Epoch: [5/5][0/10]	Loss: 0.200484
[INFO][09:40:34]: [Client #44] Epoch: [5/5][0/10]	Loss: 0.121613
[INFO][09:40:34]: [Client #158] Epoch: [5/5][0/10]	Loss: 0.460866
[INFO][09:40:34]: [Client #114] Model saved to /data/ykang/plato/results/test/model/lenet5_114_1127978.pth.
[INFO][09:40:34]: [Client #44] Model saved to /data/ykang/plato/results/test/model/lenet5_44_1127977.pth.
[INFO][09:40:34]: [Client #158] Model saved to /data/ykang/plato/results/test/model/lenet5_158_1127979.pth.
[INFO][09:40:35]: [Client #114] Loading a model from /data/ykang/plato/results/test/model/lenet5_114_1127978.pth.
[INFO][09:40:35]: [Client #114] Model trained.
[INFO][09:40:35]: [Client #114] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:40:35]: [Server #1127936] Received 0.24 MB of payload data from client #114 (simulated).
[INFO][09:40:35]: [Client #158] Loading a model from /data/ykang/plato/results/test/model/lenet5_158_1127979.pth.
[INFO][09:40:35]: [Client #158] Model trained.
[INFO][09:40:35]: [Client #158] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:40:35]: [Server #1127936] Received 0.24 MB of payload data from client #158 (simulated).
[INFO][09:40:35]: [Client #44] Loading a model from /data/ykang/plato/results/test/model/lenet5_44_1127977.pth.
[INFO][09:40:35]: [Client #44] Model trained.
[INFO][09:40:35]: [Client #44] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:40:35]: [Server #1127936] Received 0.24 MB of payload data from client #44 (simulated).
[INFO][09:40:35]: [Server #1127936] Selecting client #88 for training.
[INFO][09:40:35]: [Server #1127936] Sending the current model to client #88 (simulated).
[INFO][09:40:35]: [Server #1127936] Sending 0.24 MB of payload data to client #88 (simulated).
[INFO][09:40:35]: [Server #1127936] Selecting client #304 for training.
[INFO][09:40:35]: [Server #1127936] Sending the current model to client #304 (simulated).
[INFO][09:40:35]: [Server #1127936] Sending 0.24 MB of payload data to client #304 (simulated).
[INFO][09:40:35]: [Server #1127936] Selecting client #207 for training.
[INFO][09:40:35]: [Server #1127936] Sending the current model to client #207 (simulated).
[INFO][09:40:35]: [Client #88] Selected by the server.
[INFO][09:40:35]: [Client #88] Loading its data source...
[INFO][09:40:35]: [Client #88] Dataset size: 60000
[INFO][09:40:35]: [Client #88] Sampler: noniid
[INFO][09:40:35]: [Server #1127936] Sending 0.24 MB of payload data to client #207 (simulated).
[INFO][09:40:35]: [Client #88] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:40:35]: [Client #304] Selected by the server.
[INFO][09:40:35]: [Client #304] Loading its data source...
[INFO][09:40:35]: [Client #304] Dataset size: 60000
[INFO][09:40:35]: [Client #304] Sampler: noniid
[INFO][09:40:35]: [Client #207] Selected by the server.
[INFO][09:40:35]: [Client #207] Loading its data source...
[INFO][09:40:35]: [Client #207] Dataset size: 60000
[INFO][09:40:35]: [Client #207] Sampler: noniid
[INFO][09:40:35]: [Client #304] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:40:35]: [Client #207] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:40:35]: [93m[1m[Client #304] Started training in communication round #1.[0m
[INFO][09:40:35]: [93m[1m[Client #88] Started training in communication round #1.[0m
[INFO][09:40:35]: [93m[1m[Client #207] Started training in communication round #1.[0m
[INFO][09:40:37]: [Client #207] Loading the dataset.
[INFO][09:40:37]: [Client #88] Loading the dataset.
[INFO][09:40:37]: [Client #304] Loading the dataset.
[INFO][09:40:43]: [Client #207] Epoch: [1/5][0/10]	Loss: 2.392712
[INFO][09:40:43]: [Client #304] Epoch: [1/5][0/10]	Loss: 2.246259
[INFO][09:40:43]: [Client #207] Epoch: [2/5][0/10]	Loss: 1.424479
[INFO][09:40:43]: [Client #88] Epoch: [1/5][0/10]	Loss: 2.373425
[INFO][09:40:43]: [Client #88] Epoch: [2/5][0/10]	Loss: 2.180238
[INFO][09:40:43]: [Client #304] Epoch: [2/5][0/10]	Loss: 1.686286
[INFO][09:40:43]: [Client #207] Epoch: [3/5][0/10]	Loss: 1.304137
[INFO][09:40:43]: [Client #88] Epoch: [3/5][0/10]	Loss: 1.513817
[INFO][09:40:43]: [Client #207] Epoch: [4/5][0/10]	Loss: 0.008753
[INFO][09:40:43]: [Client #304] Epoch: [3/5][0/10]	Loss: 0.454871
[INFO][09:40:43]: [Client #88] Epoch: [4/5][0/10]	Loss: 1.533300
[INFO][09:40:44]: [Client #304] Epoch: [4/5][0/10]	Loss: 0.521734
[INFO][09:40:44]: [Client #207] Epoch: [5/5][0/10]	Loss: 0.421967
[INFO][09:40:44]: [Client #88] Epoch: [5/5][0/10]	Loss: 1.003361
[INFO][09:40:44]: [Client #88] Model saved to /data/ykang/plato/results/test/model/lenet5_88_1127977.pth.
[INFO][09:40:44]: [Client #207] Model saved to /data/ykang/plato/results/test/model/lenet5_207_1127979.pth.
[INFO][09:40:44]: [Client #304] Epoch: [5/5][0/10]	Loss: 0.244470
[INFO][09:40:44]: [Client #304] Model saved to /data/ykang/plato/results/test/model/lenet5_304_1127978.pth.
[INFO][09:40:44]: [Client #88] Loading a model from /data/ykang/plato/results/test/model/lenet5_88_1127977.pth.
[INFO][09:40:44]: [Client #88] Model trained.
[INFO][09:40:44]: [Client #88] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:40:44]: [Server #1127936] Received 0.24 MB of payload data from client #88 (simulated).
[INFO][09:40:44]: [Client #207] Loading a model from /data/ykang/plato/results/test/model/lenet5_207_1127979.pth.
[INFO][09:40:44]: [Client #207] Model trained.
[INFO][09:40:44]: [Client #207] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:40:44]: [Server #1127936] Received 0.24 MB of payload data from client #207 (simulated).
[INFO][09:40:45]: [Client #304] Loading a model from /data/ykang/plato/results/test/model/lenet5_304_1127978.pth.
[INFO][09:40:45]: [Client #304] Model trained.
[INFO][09:40:45]: [Client #304] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:40:45]: [Server #1127936] Received 0.24 MB of payload data from client #304 (simulated).
[INFO][09:40:45]: [Server #1127936] Selecting client #409 for training.
[INFO][09:40:45]: [Server #1127936] Sending the current model to client #409 (simulated).
[INFO][09:40:45]: [Server #1127936] Sending 0.24 MB of payload data to client #409 (simulated).
[INFO][09:40:45]: [Server #1127936] Selecting client #93 for training.
[INFO][09:40:45]: [Server #1127936] Sending the current model to client #93 (simulated).
[INFO][09:40:45]: [Server #1127936] Sending 0.24 MB of payload data to client #93 (simulated).
[INFO][09:40:45]: [Server #1127936] Selecting client #351 for training.
[INFO][09:40:45]: [Server #1127936] Sending the current model to client #351 (simulated).
[INFO][09:40:45]: [Client #409] Selected by the server.
[INFO][09:40:45]: [Client #409] Loading its data source...
[INFO][09:40:45]: [Client #409] Dataset size: 60000
[INFO][09:40:45]: [Client #409] Sampler: noniid
[INFO][09:40:45]: [Server #1127936] Sending 0.24 MB of payload data to client #351 (simulated).
[INFO][09:40:45]: [Client #93] Selected by the server.
[INFO][09:40:45]: [Client #93] Loading its data source...
[INFO][09:40:45]: [Client #93] Dataset size: 60000
[INFO][09:40:45]: [Client #351] Selected by the server.
[INFO][09:40:45]: [Client #93] Sampler: noniid
[INFO][09:40:45]: [Client #351] Loading its data source...
[INFO][09:40:45]: [Client #351] Dataset size: 60000
[INFO][09:40:45]: [Client #351] Sampler: noniid
[INFO][09:40:45]: [Client #409] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:40:45]: [Client #351] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:40:45]: [Client #93] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:40:45]: [93m[1m[Client #351] Started training in communication round #1.[0m
[INFO][09:40:45]: [93m[1m[Client #93] Started training in communication round #1.[0m
[INFO][09:40:45]: [93m[1m[Client #409] Started training in communication round #1.[0m
[INFO][09:40:47]: [Client #351] Loading the dataset.
[INFO][09:40:47]: [Client #409] Loading the dataset.
[INFO][09:40:47]: [Client #93] Loading the dataset.
[INFO][09:40:53]: [Client #351] Epoch: [1/5][0/10]	Loss: 2.343502
[INFO][09:40:53]: [Client #93] Epoch: [1/5][0/10]	Loss: 2.283839
[INFO][09:40:53]: [Client #351] Epoch: [2/5][0/10]	Loss: 0.815551
[INFO][09:40:53]: [Client #409] Epoch: [1/5][0/10]	Loss: 2.344030
[INFO][09:40:53]: [Client #93] Epoch: [2/5][0/10]	Loss: 1.427526
[INFO][09:40:53]: [Client #351] Epoch: [3/5][0/10]	Loss: 0.568946
[INFO][09:40:53]: [Client #409] Epoch: [2/5][0/10]	Loss: 2.149507
[INFO][09:40:53]: [Client #93] Epoch: [3/5][0/10]	Loss: 0.712712
[INFO][09:40:53]: [Client #409] Epoch: [3/5][0/10]	Loss: 1.239707
[INFO][09:40:53]: [Client #351] Epoch: [4/5][0/10]	Loss: 0.006089
[INFO][09:40:53]: [Client #409] Epoch: [4/5][0/10]	Loss: 0.575217
[INFO][09:40:53]: [Client #93] Epoch: [4/5][0/10]	Loss: 0.069663
[INFO][09:40:53]: [Client #351] Epoch: [5/5][0/10]	Loss: 0.151814
[INFO][09:40:53]: [Client #409] Epoch: [5/5][0/10]	Loss: 0.898091
[INFO][09:40:53]: [Client #351] Model saved to /data/ykang/plato/results/test/model/lenet5_351_1127979.pth.
[INFO][09:40:53]: [Client #409] Model saved to /data/ykang/plato/results/test/model/lenet5_409_1127977.pth.
[INFO][09:40:53]: [Client #93] Epoch: [5/5][0/10]	Loss: 0.497594
[INFO][09:40:53]: [Client #93] Model saved to /data/ykang/plato/results/test/model/lenet5_93_1127978.pth.
[INFO][09:40:54]: [Client #351] Loading a model from /data/ykang/plato/results/test/model/lenet5_351_1127979.pth.
[INFO][09:40:54]: [Client #351] Model trained.
[INFO][09:40:54]: [Client #351] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:40:54]: [Server #1127936] Received 0.24 MB of payload data from client #351 (simulated).
[INFO][09:40:54]: [Client #409] Loading a model from /data/ykang/plato/results/test/model/lenet5_409_1127977.pth.
[INFO][09:40:54]: [Client #409] Model trained.
[INFO][09:40:54]: [Client #409] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:40:54]: [Server #1127936] Received 0.24 MB of payload data from client #409 (simulated).
[INFO][09:40:54]: [Client #93] Loading a model from /data/ykang/plato/results/test/model/lenet5_93_1127978.pth.
[INFO][09:40:54]: [Client #93] Model trained.
[INFO][09:40:54]: [Client #93] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:40:54]: [Server #1127936] Received 0.24 MB of payload data from client #93 (simulated).
[INFO][09:40:54]: [Server #1127936] Selecting client #121 for training.
[INFO][09:40:54]: [Server #1127936] Sending the current model to client #121 (simulated).
[INFO][09:40:54]: [Server #1127936] Sending 0.24 MB of payload data to client #121 (simulated).
[INFO][09:40:54]: [Server #1127936] Selecting client #288 for training.
[INFO][09:40:54]: [Server #1127936] Sending the current model to client #288 (simulated).
[INFO][09:40:54]: [Server #1127936] Sending 0.24 MB of payload data to client #288 (simulated).
[INFO][09:40:54]: [Server #1127936] Selecting client #175 for training.
[INFO][09:40:54]: [Server #1127936] Sending the current model to client #175 (simulated).
[INFO][09:40:54]: [Client #121] Selected by the server.
[INFO][09:40:54]: [Client #121] Loading its data source...
[INFO][09:40:54]: [Client #121] Dataset size: 60000
[INFO][09:40:54]: [Client #121] Sampler: noniid
[INFO][09:40:54]: [Server #1127936] Sending 0.24 MB of payload data to client #175 (simulated).
[INFO][09:40:54]: [Client #121] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:40:54]: [Client #288] Selected by the server.
[INFO][09:40:54]: [Client #288] Loading its data source...
[INFO][09:40:54]: [Client #288] Dataset size: 60000
[INFO][09:40:54]: [Client #288] Sampler: noniid
[INFO][09:40:54]: [Client #175] Selected by the server.
[INFO][09:40:54]: [Client #175] Loading its data source...
[INFO][09:40:54]: [Client #175] Dataset size: 60000
[INFO][09:40:54]: [Client #175] Sampler: noniid
[INFO][09:40:54]: [Client #288] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:40:54]: [Client #175] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:40:54]: [93m[1m[Client #288] Started training in communication round #1.[0m
[INFO][09:40:54]: [93m[1m[Client #121] Started training in communication round #1.[0m
[INFO][09:40:54]: [93m[1m[Client #175] Started training in communication round #1.[0m
[INFO][09:40:56]: [Client #175] Loading the dataset.
[INFO][09:40:56]: [Client #121] Loading the dataset.
[INFO][09:40:56]: [Client #288] Loading the dataset.
[INFO][09:41:02]: [Client #121] Epoch: [1/5][0/10]	Loss: 2.283839
[INFO][09:41:02]: [Client #121] Epoch: [2/5][0/10]	Loss: 1.510868
[INFO][09:41:02]: [Client #288] Epoch: [1/5][0/10]	Loss: 2.361531
[INFO][09:41:02]: [Client #175] Epoch: [1/5][0/10]	Loss: 2.238902
[INFO][09:41:02]: [Client #121] Epoch: [3/5][0/10]	Loss: 0.921990
[INFO][09:41:02]: [Client #288] Epoch: [2/5][0/10]	Loss: 2.017782
[INFO][09:41:02]: [Client #175] Epoch: [2/5][0/10]	Loss: 1.929964
[INFO][09:41:02]: [Client #121] Epoch: [4/5][0/10]	Loss: 0.552842
[INFO][09:41:02]: [Client #175] Epoch: [3/5][0/10]	Loss: 0.947835
[INFO][09:41:02]: [Client #288] Epoch: [3/5][0/10]	Loss: 0.038084
[INFO][09:41:02]: [Client #121] Epoch: [5/5][0/10]	Loss: 0.832946
[INFO][09:41:02]: [Client #175] Epoch: [4/5][0/10]	Loss: 0.671750
[INFO][09:41:02]: [Client #288] Epoch: [4/5][0/10]	Loss: 0.729147
[INFO][09:41:02]: [Client #121] Model saved to /data/ykang/plato/results/test/model/lenet5_121_1127977.pth.
[INFO][09:41:02]: [Client #175] Epoch: [5/5][0/10]	Loss: 0.746583
[INFO][09:41:02]: [Client #288] Epoch: [5/5][0/10]	Loss: 0.619487
[INFO][09:41:02]: [Client #175] Model saved to /data/ykang/plato/results/test/model/lenet5_175_1127979.pth.
[INFO][09:41:03]: [Client #288] Model saved to /data/ykang/plato/results/test/model/lenet5_288_1127978.pth.
[INFO][09:41:03]: [Client #121] Loading a model from /data/ykang/plato/results/test/model/lenet5_121_1127977.pth.
[INFO][09:41:03]: [Client #121] Model trained.
[INFO][09:41:03]: [Client #121] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:41:03]: [Server #1127936] Received 0.24 MB of payload data from client #121 (simulated).
[INFO][09:41:03]: [Client #288] Loading a model from /data/ykang/plato/results/test/model/lenet5_288_1127978.pth.
[INFO][09:41:03]: [Client #288] Model trained.
[INFO][09:41:03]: [Client #288] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:41:03]: [Server #1127936] Received 0.24 MB of payload data from client #288 (simulated).
[INFO][09:41:03]: [Client #175] Loading a model from /data/ykang/plato/results/test/model/lenet5_175_1127979.pth.
[INFO][09:41:03]: [Client #175] Model trained.
[INFO][09:41:03]: [Client #175] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:41:03]: [Server #1127936] Received 0.24 MB of payload data from client #175 (simulated).
[INFO][09:41:03]: [Server #1127936] Selecting client #29 for training.
[INFO][09:41:03]: [Server #1127936] Sending the current model to client #29 (simulated).
[INFO][09:41:03]: [Server #1127936] Sending 0.24 MB of payload data to client #29 (simulated).
[INFO][09:41:03]: [Server #1127936] Selecting client #115 for training.
[INFO][09:41:03]: [Server #1127936] Sending the current model to client #115 (simulated).
[INFO][09:41:03]: [Server #1127936] Sending 0.24 MB of payload data to client #115 (simulated).
[INFO][09:41:03]: [Server #1127936] Selecting client #333 for training.
[INFO][09:41:03]: [Server #1127936] Sending the current model to client #333 (simulated).
[INFO][09:41:03]: [Client #29] Selected by the server.
[INFO][09:41:03]: [Client #29] Loading its data source...
[INFO][09:41:03]: [Client #29] Dataset size: 60000
[INFO][09:41:03]: [Client #29] Sampler: noniid
[INFO][09:41:03]: [Client #29] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:41:03]: [93m[1m[Client #29] Started training in communication round #1.[0m
[INFO][09:41:03]: [Server #1127936] Sending 0.24 MB of payload data to client #333 (simulated).
[INFO][09:41:03]: [Client #115] Selected by the server.
[INFO][09:41:03]: [Client #115] Loading its data source...
[INFO][09:41:03]: [Client #115] Dataset size: 60000
[INFO][09:41:03]: [Client #115] Sampler: noniid
[INFO][09:41:03]: [Client #333] Selected by the server.
[INFO][09:41:03]: [Client #333] Loading its data source...
[INFO][09:41:03]: [Client #333] Dataset size: 60000
[INFO][09:41:03]: [Client #333] Sampler: noniid
[INFO][09:41:03]: [Client #115] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:41:03]: [Client #333] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:41:03]: [93m[1m[Client #115] Started training in communication round #1.[0m
[INFO][09:41:03]: [93m[1m[Client #333] Started training in communication round #1.[0m
[INFO][09:41:05]: [Client #115] Loading the dataset.
[INFO][09:41:06]: [Client #29] Loading the dataset.
[INFO][09:41:06]: [Client #333] Loading the dataset.
[INFO][09:41:11]: [Client #29] Epoch: [1/5][0/10]	Loss: 2.278265
[INFO][09:41:11]: [Client #115] Epoch: [1/5][0/10]	Loss: 2.366702
[INFO][09:41:11]: [Client #29] Epoch: [2/5][0/10]	Loss: 2.095977
[INFO][09:41:11]: [Client #333] Epoch: [1/5][0/10]	Loss: 2.377568
[INFO][09:41:11]: [Client #115] Epoch: [2/5][0/10]	Loss: 1.786290
[INFO][09:41:12]: [Client #333] Epoch: [2/5][0/10]	Loss: 1.934045
[INFO][09:41:12]: [Client #115] Epoch: [3/5][0/10]	Loss: 1.199975
[INFO][09:41:12]: [Client #29] Epoch: [3/5][0/10]	Loss: 1.624465
[INFO][09:41:12]: [Client #333] Epoch: [3/5][0/10]	Loss: 0.579091
[INFO][09:41:12]: [Client #29] Epoch: [4/5][0/10]	Loss: 0.984511
[INFO][09:41:12]: [Client #115] Epoch: [4/5][0/10]	Loss: 0.748447
[INFO][09:41:12]: [Client #333] Epoch: [4/5][0/10]	Loss: 1.642786
[INFO][09:41:12]: [Client #29] Epoch: [5/5][0/10]	Loss: 0.565677
[INFO][09:41:12]: [Client #115] Epoch: [5/5][0/10]	Loss: 0.950984
[INFO][09:41:12]: [Client #29] Model saved to /data/ykang/plato/results/test/model/lenet5_29_1127977.pth.
[INFO][09:41:12]: [Client #115] Model saved to /data/ykang/plato/results/test/model/lenet5_115_1127978.pth.
[INFO][09:41:12]: [Client #333] Epoch: [5/5][0/10]	Loss: 1.373167
[INFO][09:41:12]: [Client #333] Model saved to /data/ykang/plato/results/test/model/lenet5_333_1127979.pth.
[INFO][09:41:13]: [Client #29] Loading a model from /data/ykang/plato/results/test/model/lenet5_29_1127977.pth.
[INFO][09:41:13]: [Client #29] Model trained.
[INFO][09:41:13]: [Client #29] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:41:13]: [Server #1127936] Received 0.24 MB of payload data from client #29 (simulated).
[INFO][09:41:13]: [Client #333] Loading a model from /data/ykang/plato/results/test/model/lenet5_333_1127979.pth.
[INFO][09:41:13]: [Client #333] Model trained.
[INFO][09:41:13]: [Client #333] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:41:13]: [Server #1127936] Received 0.24 MB of payload data from client #333 (simulated).
[INFO][09:41:13]: [Client #115] Loading a model from /data/ykang/plato/results/test/model/lenet5_115_1127978.pth.
[INFO][09:41:13]: [Client #115] Model trained.
[INFO][09:41:13]: [Client #115] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:41:13]: [Server #1127936] Received 0.24 MB of payload data from client #115 (simulated).
[INFO][09:41:13]: [Server #1127936] Selecting client #249 for training.
[INFO][09:41:13]: [Server #1127936] Sending the current model to client #249 (simulated).
[INFO][09:41:13]: [Server #1127936] Sending 0.24 MB of payload data to client #249 (simulated).
[INFO][09:41:13]: [Server #1127936] Selecting client #260 for training.
[INFO][09:41:13]: [Server #1127936] Sending the current model to client #260 (simulated).
[INFO][09:41:13]: [Server #1127936] Sending 0.24 MB of payload data to client #260 (simulated).
[INFO][09:41:13]: [Server #1127936] Selecting client #286 for training.
[INFO][09:41:13]: [Server #1127936] Sending the current model to client #286 (simulated).
[INFO][09:41:13]: [Client #249] Selected by the server.
[INFO][09:41:13]: [Client #249] Loading its data source...
[INFO][09:41:13]: [Client #249] Dataset size: 60000
[INFO][09:41:13]: [Client #249] Sampler: noniid
[INFO][09:41:13]: [Server #1127936] Sending 0.24 MB of payload data to client #286 (simulated).
[INFO][09:41:13]: [Client #286] Selected by the server.
[INFO][09:41:13]: [Client #260] Selected by the server.
[INFO][09:41:13]: [Client #286] Loading its data source...
[INFO][09:41:13]: [Client #286] Dataset size: 60000
[INFO][09:41:13]: [Client #260] Loading its data source...
[INFO][09:41:13]: [Client #286] Sampler: noniid
[INFO][09:41:13]: [Client #260] Dataset size: 60000
[INFO][09:41:13]: [Client #260] Sampler: noniid
[INFO][09:41:13]: [Client #249] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:41:13]: [Client #286] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:41:13]: [Client #260] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:41:13]: [93m[1m[Client #286] Started training in communication round #1.[0m
[INFO][09:41:13]: [93m[1m[Client #249] Started training in communication round #1.[0m
[INFO][09:41:13]: [93m[1m[Client #260] Started training in communication round #1.[0m
[INFO][09:41:15]: [Client #249] Loading the dataset.
[INFO][09:41:15]: [Client #286] Loading the dataset.
[INFO][09:41:15]: [Client #260] Loading the dataset.
[INFO][09:41:21]: [Client #249] Epoch: [1/5][0/10]	Loss: 2.232127
[INFO][09:41:21]: [Client #260] Epoch: [1/5][0/10]	Loss: 2.187779
[INFO][09:41:21]: [Client #249] Epoch: [2/5][0/10]	Loss: 1.958567
[INFO][09:41:21]: [Client #286] Epoch: [1/5][0/10]	Loss: 2.294827
[INFO][09:41:21]: [Client #249] Epoch: [3/5][0/10]	Loss: 1.105395
[INFO][09:41:21]: [Client #260] Epoch: [2/5][0/10]	Loss: 1.094946
[INFO][09:41:21]: [Client #286] Epoch: [2/5][0/10]	Loss: 1.714661
[INFO][09:41:21]: [Client #260] Epoch: [3/5][0/10]	Loss: 0.709857
[INFO][09:41:21]: [Client #249] Epoch: [4/5][0/10]	Loss: 0.279878
[INFO][09:41:21]: [Client #286] Epoch: [3/5][0/10]	Loss: 1.388981
[INFO][09:41:21]: [Client #260] Epoch: [4/5][0/10]	Loss: 0.098216
[INFO][09:41:21]: [Client #249] Epoch: [5/5][0/10]	Loss: 0.029817
[INFO][09:41:21]: [Client #286] Epoch: [4/5][0/10]	Loss: 0.578134
[INFO][09:41:21]: [Client #249] Model saved to /data/ykang/plato/results/test/model/lenet5_249_1127977.pth.
[INFO][09:41:21]: [Client #260] Epoch: [5/5][0/10]	Loss: 0.285884
[INFO][09:41:21]: [Client #260] Model saved to /data/ykang/plato/results/test/model/lenet5_260_1127978.pth.
[INFO][09:41:21]: [Client #286] Epoch: [5/5][0/10]	Loss: 0.776865
[INFO][09:41:21]: [Client #286] Model saved to /data/ykang/plato/results/test/model/lenet5_286_1127979.pth.
[INFO][09:41:22]: [Client #249] Loading a model from /data/ykang/plato/results/test/model/lenet5_249_1127977.pth.
[INFO][09:41:22]: [Client #249] Model trained.
[INFO][09:41:22]: [Client #249] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:41:22]: [Server #1127936] Received 0.24 MB of payload data from client #249 (simulated).
[INFO][09:41:22]: [Client #260] Loading a model from /data/ykang/plato/results/test/model/lenet5_260_1127978.pth.
[INFO][09:41:22]: [Client #260] Model trained.
[INFO][09:41:22]: [Client #260] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:41:22]: [Server #1127936] Received 0.24 MB of payload data from client #260 (simulated).
[INFO][09:41:22]: [Client #286] Loading a model from /data/ykang/plato/results/test/model/lenet5_286_1127979.pth.
[INFO][09:41:22]: [Client #286] Model trained.
[INFO][09:41:22]: [Client #286] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:41:22]: [Server #1127936] Received 0.24 MB of payload data from client #286 (simulated).
[INFO][09:41:22]: [Server #1127936] Selecting client #499 for training.
[INFO][09:41:22]: [Server #1127936] Sending the current model to client #499 (simulated).
[INFO][09:41:22]: [Server #1127936] Sending 0.24 MB of payload data to client #499 (simulated).
[INFO][09:41:22]: [Server #1127936] Selecting client #411 for training.
[INFO][09:41:22]: [Server #1127936] Sending the current model to client #411 (simulated).
[INFO][09:41:22]: [Server #1127936] Sending 0.24 MB of payload data to client #411 (simulated).
[INFO][09:41:22]: [Client #499] Selected by the server.
[INFO][09:41:22]: [Client #411] Selected by the server.
[INFO][09:41:22]: [Client #499] Loading its data source...
[INFO][09:41:22]: [Client #499] Dataset size: 60000
[INFO][09:41:22]: [Client #411] Loading its data source...
[INFO][09:41:22]: [Client #499] Sampler: noniid
[INFO][09:41:22]: [Client #411] Dataset size: 60000
[INFO][09:41:22]: [Client #411] Sampler: noniid
[INFO][09:41:22]: [Client #499] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:41:22]: [Client #411] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:41:22]: [93m[1m[Client #499] Started training in communication round #1.[0m
[INFO][09:41:22]: [93m[1m[Client #411] Started training in communication round #1.[0m
[INFO][09:41:24]: [Client #499] Loading the dataset.
[INFO][09:41:24]: [Client #411] Loading the dataset.
[INFO][09:41:30]: [Client #411] Epoch: [1/5][0/10]	Loss: 2.256065
[INFO][09:41:30]: [Client #499] Epoch: [1/5][0/10]	Loss: 2.214489
[INFO][09:41:30]: [Client #411] Epoch: [2/5][0/10]	Loss: 2.114865
[INFO][09:41:30]: [Client #499] Epoch: [2/5][0/10]	Loss: 1.587697
[INFO][09:41:30]: [Client #411] Epoch: [3/5][0/10]	Loss: 1.539686
[INFO][09:41:30]: [Client #499] Epoch: [3/5][0/10]	Loss: 0.793778
[INFO][09:41:30]: [Client #499] Epoch: [4/5][0/10]	Loss: 0.745923
[INFO][09:41:30]: [Client #411] Epoch: [4/5][0/10]	Loss: 1.880246
[INFO][09:41:30]: [Client #499] Epoch: [5/5][0/10]	Loss: 0.034939
[INFO][09:41:30]: [Client #499] Model saved to /data/ykang/plato/results/test/model/lenet5_499_1127977.pth.
[INFO][09:41:30]: [Client #411] Epoch: [5/5][0/10]	Loss: 1.181200
[INFO][09:41:30]: [Client #411] Model saved to /data/ykang/plato/results/test/model/lenet5_411_1127978.pth.
[INFO][09:41:31]: [Client #499] Loading a model from /data/ykang/plato/results/test/model/lenet5_499_1127977.pth.
[INFO][09:41:31]: [Client #499] Model trained.
[INFO][09:41:31]: [Client #499] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:41:31]: [Server #1127936] Received 0.24 MB of payload data from client #499 (simulated).
[INFO][09:41:31]: [Client #411] Loading a model from /data/ykang/plato/results/test/model/lenet5_411_1127978.pth.
[INFO][09:41:31]: [Client #411] Model trained.
[INFO][09:41:31]: [Client #411] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:41:31]: [Server #1127936] Received 0.24 MB of payload data from client #411 (simulated).
[INFO][09:41:31]: [Server #1127936] Adding client #121 to the list of clients for aggregation.
[INFO][09:41:31]: [Server #1127936] Adding client #115 to the list of clients for aggregation.
[INFO][09:41:31]: [Server #1127936] Adding client #409 to the list of clients for aggregation.
[INFO][09:41:31]: [Server #1127936] Adding client #411 to the list of clients for aggregation.
[INFO][09:41:31]: [Server #1127936] Adding client #29 to the list of clients for aggregation.
[INFO][09:41:31]: [Server #1127936] Adding client #333 to the list of clients for aggregation.
[INFO][09:41:31]: [Server #1127936] Adding client #93 to the list of clients for aggregation.
[INFO][09:41:31]: [Server #1127936] Adding client #175 to the list of clients for aggregation.
[INFO][09:41:31]: [Server #1127936] Adding client #304 to the list of clients for aggregation.
[INFO][09:41:31]: [Server #1127936] Adding client #158 to the list of clients for aggregation.
[INFO][09:41:31]: [Server #1127936] Aggregating 10 clients in total.
======== Running on http://127.0.0.1:7793 ========
(Press CTRL+C to quit)
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.9078e+00  5e+02  1e+00  1e+00
 1:  6.8387e+00  5.9098e+00  6e+00  1e-02  1e-02
 2:  6.9074e+00  6.0722e+00  9e-01  6e-05  6e-05
 3:  6.9078e+00  6.8750e+00  3e-02  6e-07  6e-07
 4:  6.9078e+00  6.9074e+00  3e-04  6e-09  6e-09
 5:  6.9078e+00  6.9078e+00  3e-06  6e-11  6e-11
Optimal solution found.
The calculated probability is:  [0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002]
current clients pool:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.28894159 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         1.31672773 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.35350069 0.         0.         0.         0.         0.
 0.88687693 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.69261323 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.26483496 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         1.05305112 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         1.88326088 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.37728596 0.         0.24823134 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
local_gradient_bounds:  [INFO][09:41:38]: [Server #1127936] Global model accuracy: 16.82%

[INFO][09:41:38]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_1.pth.
[INFO][09:41:38]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_1.pth.
[INFO][09:41:38]: [93m[1m
[Server #1127936] Starting round 2/100.[0m
[0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.28894159 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         1.31672773 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.35350069 0.         0.         0.         0.         0.
 0.88687693 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.69261323 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.26483496 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         1.05305112 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         1.88326088 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.37728596 0.         0.24823134 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
[INFO][09:41:39]: [Server #1127936] Selected clients: [300 488 451  16  47  33 226 187 487  85]
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  3e-06  6e-11  6e-11
Optimal solution found.
The calculated probability is:  [0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00203633 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00193294 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.0020336  0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.001991   0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00201054 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00203721 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00197086 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00183054 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00203246 0.00204185 0.00203777
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185 0.00204185
 0.00204185 0.00204185 0.00204185 0.00204185]
current clients pool:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 287, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 500]
type of selected clients:  <class 'numpy.ndarray'>[INFO][09:41:39]: [Server #1127936] Selecting client #300 for training.
[INFO][09:41:39]: [Server #1127936] Sending the current model to client #300 (simulated).
[INFO][09:41:39]: [Server #1127936] Sending 0.24 MB of payload data to client #300 (simulated).
[INFO][09:41:39]: [Server #1127936] Selecting client #488 for training.
[INFO][09:41:39]: [Server #1127936] Sending the current model to client #488 (simulated).
[INFO][09:41:39]: [Server #1127936] Sending 0.24 MB of payload data to client #488 (simulated).
[INFO][09:41:39]: [Server #1127936] Selecting client #451 for training.
[INFO][09:41:39]: [Server #1127936] Sending the current model to client #451 (simulated).
[INFO][09:41:39]: [Client #300] Selected by the server.
[INFO][09:41:39]: [Client #300] Loading its data source...
[INFO][09:41:39]: [Client #300] Dataset size: 60000
[INFO][09:41:39]: [Client #300] Sampler: noniid
[INFO][09:41:39]: [Server #1127936] Sending 0.24 MB of payload data to client #451 (simulated).
[INFO][09:41:39]: [Client #488] Selected by the server.
[INFO][09:41:39]: [Client #488] Loading its data source...
[INFO][09:41:39]: [Client #488] Dataset size: 60000
[INFO][09:41:39]: [Client #451] Selected by the server.
[INFO][09:41:39]: [Client #488] Sampler: noniid
[INFO][09:41:39]: [Client #451] Loading its data source...
[INFO][09:41:39]: [Client #451] Dataset size: 60000
[INFO][09:41:39]: [Client #451] Sampler: noniid
[INFO][09:41:39]: [Client #300] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:41:39]: [Client #488] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:41:39]: [93m[1m[Client #300] Started training in communication round #2.[0m
[INFO][09:41:39]: [93m[1m[Client #488] Started training in communication round #2.[0m
[INFO][09:41:39]: [Client #451] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:41:39]: [93m[1m[Client #451] Started training in communication round #2.[0m
[INFO][09:41:41]: [Client #488] Loading the dataset.
[INFO][09:41:41]: [Client #451] Loading the dataset.
[INFO][09:41:41]: [Client #300] Loading the dataset.
[INFO][09:41:47]: [Client #300] Epoch: [1/5][0/10]	Loss: 2.441434
[INFO][09:41:47]: [Client #451] Epoch: [1/5][0/10]	Loss: 2.519698
[INFO][09:41:47]: [Client #300] Epoch: [2/5][0/10]	Loss: 0.784113
[INFO][09:41:47]: [Client #488] Epoch: [1/5][0/10]	Loss: 1.840220
[INFO][09:41:47]: [Client #451] Epoch: [2/5][0/10]	Loss: 1.541688
[INFO][09:41:47]: [Client #488] Epoch: [2/5][0/10]	Loss: 0.720465
[INFO][09:41:47]: [Client #300] Epoch: [3/5][0/10]	Loss: 0.802073
[INFO][09:41:47]: [Client #488] Epoch: [3/5][0/10]	Loss: 0.631779
[INFO][09:41:47]: [Client #451] Epoch: [3/5][0/10]	Loss: 0.862324
[INFO][09:41:47]: [Client #488] Epoch: [4/5][0/10]	Loss: 0.331464
[INFO][09:41:47]: [Client #300] Epoch: [4/5][0/10]	Loss: 0.184031
[INFO][09:41:47]: [Client #451] Epoch: [4/5][0/10]	Loss: 0.074921
[INFO][09:41:47]: [Client #300] Epoch: [5/5][0/10]	Loss: 0.564516
[INFO][09:41:47]: [Client #488] Epoch: [5/5][0/10]	Loss: 0.317648
[INFO][09:41:47]: [Client #451] Epoch: [5/5][0/10]	Loss: 0.086649
[INFO][09:41:47]: [Client #488] Model saved to /data/ykang/plato/results/test/model/lenet5_488_1127978.pth.
[INFO][09:41:47]: [Client #300] Model saved to /data/ykang/plato/results/test/model/lenet5_300_1127977.pth.
[INFO][09:41:47]: [Client #451] Model saved to /data/ykang/plato/results/test/model/lenet5_451_1127979.pth.
[INFO][09:41:48]: [Client #488] Loading a model from /data/ykang/plato/results/test/model/lenet5_488_1127978.pth.
[INFO][09:41:48]: [Client #488] Model trained.
[INFO][09:41:48]: [Client #488] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:41:48]: [Server #1127936] Received 0.24 MB of payload data from client #488 (simulated).
[INFO][09:41:48]: [Client #300] Loading a model from /data/ykang/plato/results/test/model/lenet5_300_1127977.pth.
[INFO][09:41:48]: [Client #300] Model trained.
[INFO][09:41:48]: [Client #300] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:41:48]: [Server #1127936] Received 0.24 MB of payload data from client #300 (simulated).
[INFO][09:41:48]: [Client #451] Loading a model from /data/ykang/plato/results/test/model/lenet5_451_1127979.pth.
[INFO][09:41:48]: [Client #451] Model trained.
[INFO][09:41:48]: [Client #451] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:41:48]: [Server #1127936] Received 0.24 MB of payload data from client #451 (simulated).
[INFO][09:41:48]: [Server #1127936] Selecting client #16 for training.
[INFO][09:41:48]: [Server #1127936] Sending the current model to client #16 (simulated).
[INFO][09:41:48]: [Server #1127936] Sending 0.24 MB of payload data to client #16 (simulated).
[INFO][09:41:48]: [Server #1127936] Selecting client #47 for training.
[INFO][09:41:48]: [Server #1127936] Sending the current model to client #47 (simulated).
[INFO][09:41:48]: [Server #1127936] Sending 0.24 MB of payload data to client #47 (simulated).
[INFO][09:41:48]: [Server #1127936] Selecting client #33 for training.
[INFO][09:41:48]: [Server #1127936] Sending the current model to client #33 (simulated).
[INFO][09:41:48]: [Client #16] Selected by the server.
[INFO][09:41:48]: [Client #16] Loading its data source...
[INFO][09:41:48]: [Client #16] Dataset size: 60000
[INFO][09:41:48]: [Client #16] Sampler: noniid
[INFO][09:41:48]: [Server #1127936] Sending 0.24 MB of payload data to client #33 (simulated).
[INFO][09:41:48]: [Client #47] Selected by the server.
[INFO][09:41:48]: [Client #47] Loading its data source...
[INFO][09:41:48]: [Client #47] Dataset size: 60000
[INFO][09:41:48]: [Client #47] Sampler: noniid
[INFO][09:41:48]: [Client #33] Selected by the server.
[INFO][09:41:48]: [Client #33] Loading its data source...
[INFO][09:41:48]: [Client #33] Dataset size: 60000
[INFO][09:41:48]: [Client #33] Sampler: noniid
[INFO][09:41:48]: [Client #16] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:41:48]: [Client #47] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:41:48]: [Client #33] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:41:48]: [93m[1m[Client #47] Started training in communication round #2.[0m
[INFO][09:41:48]: [93m[1m[Client #16] Started training in communication round #2.[0m
[INFO][09:41:48]: [93m[1m[Client #33] Started training in communication round #2.[0m
[INFO][09:41:50]: [Client #16] Loading the dataset.
[INFO][09:41:50]: [Client #33] Loading the dataset.
[INFO][09:41:50]: [Client #47] Loading the dataset.
[INFO][09:41:56]: [Client #16] Epoch: [1/5][0/10]	Loss: 2.395581
[INFO][09:41:56]: [Client #47] Epoch: [1/5][0/10]	Loss: 2.380092
[INFO][09:41:56]: [Client #33] Epoch: [1/5][0/10]	Loss: 2.338777
[INFO][09:41:56]: [Client #33] Epoch: [2/5][0/10]	Loss: 1.302961
[INFO][09:41:56]: [Client #16] Epoch: [2/5][0/10]	Loss: 1.452074
[INFO][09:41:56]: [Client #47] Epoch: [2/5][0/10]	Loss: 2.004201
[INFO][09:41:56]: [Client #16] Epoch: [3/5][0/10]	Loss: 0.681404
[INFO][09:41:56]: [Client #33] Epoch: [3/5][0/10]	Loss: 0.849717
[INFO][09:41:56]: [Client #47] Epoch: [3/5][0/10]	Loss: 0.884841
[INFO][09:41:56]: [Client #16] Epoch: [4/5][0/10]	Loss: 0.168981
[INFO][09:41:56]: [Client #33] Epoch: [4/5][0/10]	Loss: 0.165083
[INFO][09:41:56]: [Client #47] Epoch: [4/5][0/10]	Loss: 0.325754
[INFO][09:41:56]: [Client #16] Epoch: [5/5][0/10]	Loss: 0.559134
[INFO][09:41:56]: [Client #33] Epoch: [5/5][0/10]	Loss: 0.032319
[INFO][09:41:56]: [Client #16] Model saved to /data/ykang/plato/results/test/model/lenet5_16_1127977.pth.
[INFO][09:41:56]: [Client #47] Epoch: [5/5][0/10]	Loss: 0.870534
[INFO][09:41:56]: [Client #33] Model saved to /data/ykang/plato/results/test/model/lenet5_33_1127979.pth.
[INFO][09:41:57]: [Client #47] Model saved to /data/ykang/plato/results/test/model/lenet5_47_1127978.pth.
[INFO][09:41:57]: [Client #16] Loading a model from /data/ykang/plato/results/test/model/lenet5_16_1127977.pth.
[INFO][09:41:57]: [Client #16] Model trained.
[INFO][09:41:57]: [Client #16] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:41:57]: [Server #1127936] Received 0.24 MB of payload data from client #16 (simulated).
[INFO][09:41:57]: [Client #33] Loading a model from /data/ykang/plato/results/test/model/lenet5_33_1127979.pth.
[INFO][09:41:57]: [Client #33] Model trained.
[INFO][09:41:57]: [Client #33] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:41:57]: [Server #1127936] Received 0.24 MB of payload data from client #33 (simulated).
[INFO][09:41:57]: [Client #47] Loading a model from /data/ykang/plato/results/test/model/lenet5_47_1127978.pth.
[INFO][09:41:57]: [Client #47] Model trained.
[INFO][09:41:57]: [Client #47] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:41:57]: [Server #1127936] Received 0.24 MB of payload data from client #47 (simulated).
[INFO][09:41:57]: [Server #1127936] Selecting client #226 for training.
[INFO][09:41:57]: [Server #1127936] Sending the current model to client #226 (simulated).
[INFO][09:41:57]: [Server #1127936] Sending 0.24 MB of payload data to client #226 (simulated).
[INFO][09:41:57]: [Server #1127936] Selecting client #187 for training.
[INFO][09:41:57]: [Server #1127936] Sending the current model to client #187 (simulated).
[INFO][09:41:57]: [Server #1127936] Sending 0.24 MB of payload data to client #187 (simulated).
[INFO][09:41:57]: [Server #1127936] Selecting client #487 for training.
[INFO][09:41:57]: [Server #1127936] Sending the current model to client #487 (simulated).
[INFO][09:41:57]: [Client #226] Selected by the server.
[INFO][09:41:57]: [Client #226] Loading its data source...
[INFO][09:41:57]: [Client #226] Dataset size: 60000
[INFO][09:41:57]: [Client #226] Sampler: noniid
[INFO][09:41:57]: [Server #1127936] Sending 0.24 MB of payload data to client #487 (simulated).
[INFO][09:41:57]: [Client #187] Selected by the server.
[INFO][09:41:57]: [Client #187] Loading its data source...
[INFO][09:41:57]: [Client #187] Dataset size: 60000
[INFO][09:41:57]: [Client #226] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:41:57]: [Client #187] Sampler: noniid
[INFO][09:41:57]: [Client #487] Selected by the server.
[INFO][09:41:57]: [Client #487] Loading its data source...
[INFO][09:41:57]: [Client #487] Dataset size: 60000
[INFO][09:41:57]: [Client #487] Sampler: noniid
[INFO][09:41:57]: [Client #187] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:41:57]: [Client #487] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:41:57]: [93m[1m[Client #226] Started training in communication round #2.[0m
[INFO][09:41:57]: [93m[1m[Client #187] Started training in communication round #2.[0m
[INFO][09:41:57]: [93m[1m[Client #487] Started training in communication round #2.[0m
[INFO][09:42:00]: [Client #487] Loading the dataset.
[INFO][09:42:00]: [Client #226] Loading the dataset.
[INFO][09:42:00]: [Client #187] Loading the dataset.
[INFO][09:42:05]: [Client #187] Epoch: [1/5][0/10]	Loss: 1.978660
[INFO][09:42:05]: [Client #487] Epoch: [1/5][0/10]	Loss: 2.455746
[INFO][09:42:05]: [Client #226] Epoch: [1/5][0/10]	Loss: 2.269098
[INFO][09:42:05]: [Client #187] Epoch: [2/5][0/10]	Loss: 1.395235
[INFO][09:42:06]: [Client #487] Epoch: [2/5][0/10]	Loss: 1.798118
[INFO][09:42:06]: [Client #187] Epoch: [3/5][0/10]	Loss: 0.626186
[INFO][09:42:06]: [Client #226] Epoch: [2/5][0/10]	Loss: 1.845794
[INFO][09:42:06]: [Client #226] Epoch: [3/5][0/10]	Loss: 0.143242
[INFO][09:42:06]: [Client #487] Epoch: [3/5][0/10]	Loss: 0.837223
[INFO][09:42:06]: [Client #187] Epoch: [4/5][0/10]	Loss: 0.028455
[INFO][09:42:06]: [Client #187] Epoch: [5/5][0/10]	Loss: 0.169485
[INFO][09:42:06]: [Client #226] Epoch: [4/5][0/10]	Loss: 0.007694
[INFO][09:42:06]: [Client #487] Epoch: [4/5][0/10]	Loss: 0.375418
[INFO][09:42:06]: [Client #187] Model saved to /data/ykang/plato/results/test/model/lenet5_187_1127978.pth.
[INFO][09:42:06]: [Client #487] Epoch: [5/5][0/10]	Loss: 0.568511
[INFO][09:42:06]: [Client #226] Epoch: [5/5][0/10]	Loss: 0.175050
[INFO][09:42:06]: [Client #487] Model saved to /data/ykang/plato/results/test/model/lenet5_487_1127979.pth.
[INFO][09:42:06]: [Client #226] Model saved to /data/ykang/plato/results/test/model/lenet5_226_1127977.pth.
[INFO][09:42:07]: [Client #187] Loading a model from /data/ykang/plato/results/test/model/lenet5_187_1127978.pth.
[INFO][09:42:07]: [Client #187] Model trained.
[INFO][09:42:07]: [Client #187] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:42:07]: [Server #1127936] Received 0.24 MB of payload data from client #187 (simulated).
[INFO][09:42:07]: [Client #487] Loading a model from /data/ykang/plato/results/test/model/lenet5_487_1127979.pth.
[INFO][09:42:07]: [Client #487] Model trained.
[INFO][09:42:07]: [Client #487] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:42:07]: [Server #1127936] Received 0.24 MB of payload data from client #487 (simulated).
[INFO][09:42:07]: [Client #226] Loading a model from /data/ykang/plato/results/test/model/lenet5_226_1127977.pth.
[INFO][09:42:07]: [Client #226] Model trained.
[INFO][09:42:07]: [Client #226] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:42:07]: [Server #1127936] Received 0.24 MB of payload data from client #226 (simulated).
[INFO][09:42:07]: [Server #1127936] Selecting client #85 for training.
[INFO][09:42:07]: [Server #1127936] Sending the current model to client #85 (simulated).
[INFO][09:42:07]: [Server #1127936] Sending 0.24 MB of payload data to client #85 (simulated).
[INFO][09:42:07]: [Client #85] Selected by the server.
[INFO][09:42:07]: [Client #85] Loading its data source...
[INFO][09:42:07]: [Client #85] Dataset size: 60000
[INFO][09:42:07]: [Client #85] Sampler: noniid
[INFO][09:42:07]: [Client #85] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:42:07]: [93m[1m[Client #85] Started training in communication round #2.[0m
[INFO][09:42:09]: [Client #85] Loading the dataset.
[INFO][09:42:14]: [Client #85] Epoch: [1/5][0/10]	Loss: 1.837033
[INFO][09:42:14]: [Client #85] Epoch: [2/5][0/10]	Loss: 0.000328
[INFO][09:42:14]: [Client #85] Epoch: [3/5][0/10]	Loss: 0.690448
[INFO][09:42:14]: [Client #85] Epoch: [4/5][0/10]	Loss: 0.001221
[INFO][09:42:14]: [Client #85] Epoch: [5/5][0/10]	Loss: 0.134831
[INFO][09:42:14]: [Client #85] Model saved to /data/ykang/plato/results/test/model/lenet5_85_1127977.pth.
[INFO][09:42:15]: [Client #85] Loading a model from /data/ykang/plato/results/test/model/lenet5_85_1127977.pth.
[INFO][09:42:15]: [Client #85] Model trained.
[INFO][09:42:15]: [Client #85] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:42:15]: [Server #1127936] Received 0.24 MB of payload data from client #85 (simulated).
[INFO][09:42:15]: [Server #1127936] Adding client #260 to the list of clients for aggregation.
[INFO][09:42:15]: [Server #1127936] Adding client #207 to the list of clients for aggregation.
[INFO][09:42:15]: [Server #1127936] Adding client #249 to the list of clients for aggregation.
[INFO][09:42:15]: [Server #1127936] Adding client #114 to the list of clients for aggregation.
[INFO][09:42:15]: [Server #1127936] Adding client #351 to the list of clients for aggregation.
[INFO][09:42:15]: [Server #1127936] Adding client #44 to the list of clients for aggregation.
[INFO][09:42:15]: [Server #1127936] Adding client #288 to the list of clients for aggregation.
[INFO][09:42:15]: [Server #1127936] Adding client #286 to the list of clients for aggregation.
[INFO][09:42:15]: [Server #1127936] Adding client #488 to the list of clients for aggregation.
[INFO][09:42:15]: [Server #1127936] Adding client #499 to the list of clients for aggregation.
[INFO][09:42:15]: [Server #1127936] Aggregating 10 clients in total.

start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.49386955 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         1.20867002
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.83184806 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.6773321  0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         1.05729392 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.44198906 0.         0.3509049
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         1.01807436 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.11299554 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.74235176 0.        ]
start updating records...
!!!The staleness of this round are:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
local_gradient_bounds:  [INFO][09:42:17]: [Server #1127936] Global model accuracy: 19.60%

[INFO][09:42:17]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_2.pth.
[INFO][09:42:17]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_2.pth.
[INFO][09:42:17]: [93m[1m
[Server #1127936] Starting round 3/100.[0m
[0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.49386955 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         1.20867002
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.83184806 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.6773321  0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         1.05729392 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.44198906 0.         0.3509049
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         1.01807436 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.11299554 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.74235176 0.        ]
!!!!!!The aggregation weights of this round are:  [0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8875e+00  6.8860e+00  2e-03  3e-08  3e-08
 5:  6.8875e+00  6.8863e+00  1e-03  2e-08  2e-08
 6:  6.8873e+00  6.8862e+00  1e-03  6e-07  7e-07
 7:  6.8873e+00  6.8865e+00  8e-04  4e-07  5e-07
 8:  6.8872e+00  6.8865e+00  7e-04  2e-07  2e-07
 9:  6.8871e+00  6.8866e+00  5e-04  2e-07  1e-07
10:  6.8869e+00  6.8866e+00  3e-04  3e-07  2e-07
11:  6.8868e+00  6.8866e+00  2e-04  3e-07  2e-07
12:  6.8867e+00  6.8866e+00  9e-05  6e-08  3e-08
13:  6.8866e+00  6.8866e+00  3e-05  4e-08  2e-08
14:  6.8866e+00  6.8866e+00  4e-06  1e-08  5e-09
Optimal solution found.
The calculated probability is:  [9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 3.09390232e-05 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 3.59048244e-01 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 8.45173696e-02 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 1.33693835e-04 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 2.88100349e-01 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 2.49316985e-05 9.10677435e-06
 1.84822656e-05 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 2.63105227e-01 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10507019e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 9.10677435e-06 9.10677435e-06 9.10677435e-06 9.10677435e-06
 6.40407414e-04 9.10677435e-06]
current clients pool:  [INFO][09:42:18]: [Server #1127936] Selected clients: [351 260 114 207 110 414 131 200 292 428]
[INFO][09:42:18]: [Server #1127936] Selecting client #351 for training.
[INFO][09:42:18]: [Server #1127936] Sending the current model to client #351 (simulated).
[INFO][09:42:18]: [Server #1127936] Sending 0.24 MB of payload data to client #351 (simulated).
[INFO][09:42:18]: [Server #1127936] Selecting client #260 for training.
[INFO][09:42:18]: [Server #1127936] Sending the current model to client #260 (simulated).
[INFO][09:42:18]: [Server #1127936] Sending 0.24 MB of payload data to client #260 (simulated).
[INFO][09:42:18]: [Server #1127936] Selecting client #114 for training.
[INFO][09:42:18]: [Server #1127936] Sending the current model to client #114 (simulated).
[INFO][09:42:18]: [Client #351] Selected by the server.
[INFO][09:42:18]: [Client #351] Loading its data source...
[INFO][09:42:18]: [Client #351] Dataset size: 60000
[INFO][09:42:18]: [Client #351] Sampler: noniid
[INFO][09:42:18]: [Server #1127936] Sending 0.24 MB of payload data to client #114 (simulated).
[INFO][09:42:18]: [Client #260] Selected by the server.
[INFO][09:42:18]: [Client #114] Selected by the server.
[INFO][09:42:18]: [Client #260] Loading its data source...
[INFO][09:42:18]: [Client #351] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:42:18]: [Client #114] Loading its data source...
[INFO][09:42:18]: [Client #260] Dataset size: 60000
[INFO][09:42:18]: [Client #260] Sampler: noniid
[INFO][09:42:18]: [Client #114] Dataset size: 60000
[INFO][09:42:18]: [Client #114] Sampler: noniid
[INFO][09:42:18]: [Client #260] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:42:18]: [Client #114] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:42:18]: [93m[1m[Client #351] Started training in communication round #3.[0m
[INFO][09:42:18]: [93m[1m[Client #260] Started training in communication round #3.[0m
[INFO][09:42:18]: [93m[1m[Client #114] Started training in communication round #3.[0m
[INFO][09:42:20]: [Client #114] Loading the dataset.
[INFO][09:42:20]: [Client #260] Loading the dataset.
[INFO][09:42:20]: [Client #351] Loading the dataset.
[INFO][09:42:26]: [Client #260] Epoch: [1/5][0/10]	Loss: 1.240792
[INFO][09:42:26]: [Client #114] Epoch: [1/5][0/10]	Loss: 1.965427
[INFO][09:42:26]: [Client #351] Epoch: [1/5][0/10]	Loss: 2.483016
[INFO][09:42:26]: [Client #260] Epoch: [2/5][0/10]	Loss: 0.761560
[INFO][09:42:26]: [Client #114] Epoch: [2/5][0/10]	Loss: 0.609914
[INFO][09:42:26]: [Client #260] Epoch: [3/5][0/10]	Loss: 0.233281
[INFO][09:42:26]: [Client #351] Epoch: [2/5][0/10]	Loss: 0.580027
[INFO][09:42:26]: [Client #114] Epoch: [3/5][0/10]	Loss: 0.226115
[INFO][09:42:26]: [Client #351] Epoch: [3/5][0/10]	Loss: 0.282481
[INFO][09:42:26]: [Client #260] Epoch: [4/5][0/10]	Loss: 0.000845
[INFO][09:42:26]: [Client #351] Epoch: [4/5][0/10]	Loss: 0.040307
[INFO][09:42:26]: [Client #114] Epoch: [4/5][0/10]	Loss: 0.185439
[INFO][09:42:26]: [Client #260] Epoch: [5/5][0/10]	Loss: 0.081543
[INFO][09:42:26]: [Client #260] Model saved to /data/ykang/plato/results/test/model/lenet5_260_1127978.pth.
[INFO][09:42:26]: [Client #351] Epoch: [5/5][0/10]	Loss: 0.075206
[INFO][09:42:26]: [Client #114] Epoch: [5/5][0/10]	Loss: 0.069216
[INFO][09:42:26]: [Client #351] Model saved to /data/ykang/plato/results/test/model/lenet5_351_1127977.pth.
[INFO][09:42:26]: [Client #114] Model saved to /data/ykang/plato/results/test/model/lenet5_114_1127979.pth.
[INFO][09:42:27]: [Client #260] Loading a model from /data/ykang/plato/results/test/model/lenet5_260_1127978.pth.
[INFO][09:42:27]: [Client #260] Model trained.
[INFO][09:42:27]: [Client #260] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:42:27]: [Server #1127936] Received 0.24 MB of payload data from client #260 (simulated).
[INFO][09:42:27]: [Client #114] Loading a model from /data/ykang/plato/results/test/model/lenet5_114_1127979.pth.
[INFO][09:42:27]: [Client #114] Model trained.
[INFO][09:42:27]: [Client #114] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:42:27]: [Server #1127936] Received 0.24 MB of payload data from client #114 (simulated).
[INFO][09:42:27]: [Client #351] Loading a model from /data/ykang/plato/results/test/model/lenet5_351_1127977.pth.
[INFO][09:42:27]: [Client #351] Model trained.
[INFO][09:42:27]: [Client #351] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:42:27]: [Server #1127936] Received 0.24 MB of payload data from client #351 (simulated).
[INFO][09:42:27]: [Server #1127936] Selecting client #207 for training.
[INFO][09:42:27]: [Server #1127936] Sending the current model to client #207 (simulated).
[INFO][09:42:27]: [Server #1127936] Sending 0.24 MB of payload data to client #207 (simulated).
[INFO][09:42:27]: [Server #1127936] Selecting client #110 for training.
[INFO][09:42:27]: [Server #1127936] Sending the current model to client #110 (simulated).
[INFO][09:42:27]: [Server #1127936] Sending 0.24 MB of payload data to client #110 (simulated).
[INFO][09:42:27]: [Server #1127936] Selecting client #414 for training.
[INFO][09:42:27]: [Server #1127936] Sending the current model to client #414 (simulated).
[INFO][09:42:27]: [Client #207] Selected by the server.
[INFO][09:42:27]: [Client #207] Loading its data source...
[INFO][09:42:27]: [Client #207] Dataset size: 60000
[INFO][09:42:27]: [Client #207] Sampler: noniid
[INFO][09:42:27]: [Server #1127936] Sending 0.24 MB of payload data to client #414 (simulated).
[INFO][09:42:27]: [Client #110] Selected by the server.
[INFO][09:42:27]: [Client #414] Selected by the server.
[INFO][09:42:27]: [Client #110] Loading its data source...
[INFO][09:42:27]: [Client #110] Dataset size: 60000
[INFO][09:42:27]: [Client #414] Loading its data source...
[INFO][09:42:27]: [Client #110] Sampler: noniid
[INFO][09:42:27]: [Client #414] Dataset size: 60000
[INFO][09:42:27]: [Client #414] Sampler: noniid
[INFO][09:42:27]: [Client #207] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:42:27]: [Client #110] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:42:27]: [Client #414] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:42:27]: [93m[1m[Client #207] Started training in communication round #3.[0m
[INFO][09:42:27]: [93m[1m[Client #110] Started training in communication round #3.[0m
[INFO][09:42:27]: [93m[1m[Client #414] Started training in communication round #3.[0m
[INFO][09:42:29]: [Client #414] Loading the dataset.
[INFO][09:42:29]: [Client #207] Loading the dataset.
[INFO][09:42:29]: [Client #110] Loading the dataset.
[INFO][09:42:35]: [Client #110] Epoch: [1/5][0/10]	Loss: 2.483016
[INFO][09:42:35]: [Client #207] Epoch: [1/5][0/10]	Loss: 2.328096
[INFO][09:42:35]: [Client #110] Epoch: [2/5][0/10]	Loss: 3.015021
[INFO][09:42:35]: [Client #414] Epoch: [1/5][0/10]	Loss: 1.953774
[INFO][09:42:35]: [Client #207] Epoch: [2/5][0/10]	Loss: 1.194358
[INFO][09:42:35]: [Client #110] Epoch: [3/5][0/10]	Loss: 0.634757
[INFO][09:42:35]: [Client #414] Epoch: [2/5][0/10]	Loss: 0.486131
[INFO][09:42:35]: [Client #207] Epoch: [3/5][0/10]	Loss: 0.782397
[INFO][09:42:35]: [Client #110] Epoch: [4/5][0/10]	Loss: 0.019372
[INFO][09:42:35]: [Client #414] Epoch: [3/5][0/10]	Loss: 0.904253
[INFO][09:42:36]: [Client #207] Epoch: [4/5][0/10]	Loss: 0.019160
[INFO][09:42:36]: [Client #414] Epoch: [4/5][0/10]	Loss: 0.169434
[INFO][09:42:36]: [Client #110] Epoch: [5/5][0/10]	Loss: 0.001961
[INFO][09:42:36]: [Client #110] Model saved to /data/ykang/plato/results/test/model/lenet5_110_1127978.pth.
[INFO][09:42:36]: [Client #414] Epoch: [5/5][0/10]	Loss: 0.006501
[INFO][09:42:36]: [Client #207] Epoch: [5/5][0/10]	Loss: 0.230722
[INFO][09:42:36]: [Client #414] Model saved to /data/ykang/plato/results/test/model/lenet5_414_1127979.pth.
[INFO][09:42:36]: [Client #207] Model saved to /data/ykang/plato/results/test/model/lenet5_207_1127977.pth.
[INFO][09:42:36]: [Client #110] Loading a model from /data/ykang/plato/results/test/model/lenet5_110_1127978.pth.
[INFO][09:42:36]: [Client #110] Model trained.
[INFO][09:42:36]: [Client #110] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:42:36]: [Server #1127936] Received 0.24 MB of payload data from client #110 (simulated).
[INFO][09:42:37]: [Client #207] Loading a model from /data/ykang/plato/results/test/model/lenet5_207_1127977.pth.
[INFO][09:42:37]: [Client #207] Model trained.
[INFO][09:42:37]: [Client #207] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:42:37]: [Server #1127936] Received 0.24 MB of payload data from client #207 (simulated).
[INFO][09:42:37]: [Client #414] Loading a model from /data/ykang/plato/results/test/model/lenet5_414_1127979.pth.
[INFO][09:42:37]: [Client #414] Model trained.
[INFO][09:42:37]: [Client #414] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:42:37]: [Server #1127936] Received 0.24 MB of payload data from client #414 (simulated).
[INFO][09:42:37]: [Server #1127936] Selecting client #131 for training.
[INFO][09:42:37]: [Server #1127936] Sending the current model to client #131 (simulated).
[INFO][09:42:37]: [Server #1127936] Sending 0.24 MB of payload data to client #131 (simulated).
[INFO][09:42:37]: [Server #1127936] Selecting client #200 for training.
[INFO][09:42:37]: [Server #1127936] Sending the current model to client #200 (simulated).
[INFO][09:42:37]: [Server #1127936] Sending 0.24 MB of payload data to client #200 (simulated).
[INFO][09:42:37]: [Server #1127936] Selecting client #292 for training.
[INFO][09:42:37]: [Server #1127936] Sending the current model to client #292 (simulated).
[INFO][09:42:37]: [Client #131] Selected by the server.
[INFO][09:42:37]: [Client #131] Loading its data source...
[INFO][09:42:37]: [Client #131] Dataset size: 60000
[INFO][09:42:37]: [Client #131] Sampler: noniid
[INFO][09:42:37]: [Client #131] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:42:37]: [93m[1m[Client #131] Started training in communication round #3.[0m
[INFO][09:42:37]: [Server #1127936] Sending 0.24 MB of payload data to client #292 (simulated).
[INFO][09:42:37]: [Client #200] Selected by the server.
[INFO][09:42:37]: [Client #200] Loading its data source...
[INFO][09:42:37]: [Client #200] Dataset size: 60000
[INFO][09:42:37]: [Client #292] Selected by the server.
[INFO][09:42:37]: [Client #200] Sampler: noniid
[INFO][09:42:37]: [Client #292] Loading its data source...
[INFO][09:42:37]: [Client #292] Dataset size: 60000
[INFO][09:42:37]: [Client #292] Sampler: noniid
[INFO][09:42:37]: [Client #292] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:42:37]: [Client #200] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:42:37]: [93m[1m[Client #292] Started training in communication round #3.[0m
[INFO][09:42:37]: [93m[1m[Client #200] Started training in communication round #3.[0m
[INFO][09:42:39]: [Client #131] Loading the dataset.
[INFO][09:42:39]: [Client #200] Loading the dataset.
[INFO][09:42:39]: [Client #292] Loading the dataset.
[INFO][09:42:45]: [Client #131] Epoch: [1/5][0/10]	Loss: 1.964427
[INFO][09:42:45]: [Client #200] Epoch: [1/5][0/10]	Loss: 2.266447
[INFO][09:42:45]: [Client #131] Epoch: [2/5][0/10]	Loss: 0.968889
[INFO][09:42:45]: [Client #292] Epoch: [1/5][0/10]	Loss: 2.085274
[INFO][09:42:45]: [Client #200] Epoch: [2/5][0/10]	Loss: 1.175160
[INFO][09:42:45]: [Client #200] Epoch: [3/5][0/10]	Loss: 1.275380
[INFO][09:42:45]: [Client #131] Epoch: [3/5][0/10]	Loss: 0.593184
[INFO][09:42:45]: [Client #292] Epoch: [2/5][0/10]	Loss: 0.801836
[INFO][09:42:45]: [Client #131] Epoch: [4/5][0/10]	Loss: 0.280028
[INFO][09:42:45]: [Client #292] Epoch: [3/5][0/10]	Loss: 0.861465
[INFO][09:42:45]: [Client #200] Epoch: [4/5][0/10]	Loss: 0.808097
[INFO][09:42:45]: [Client #131] Epoch: [5/5][0/10]	Loss: 0.677734
[INFO][09:42:45]: [Client #200] Epoch: [5/5][0/10]	Loss: 1.162503
[INFO][09:42:45]: [Client #292] Epoch: [4/5][0/10]	Loss: 0.294301
[INFO][09:42:45]: [Client #131] Model saved to /data/ykang/plato/results/test/model/lenet5_131_1127977.pth.
[INFO][09:42:45]: [Client #200] Model saved to /data/ykang/plato/results/test/model/lenet5_200_1127978.pth.
[INFO][09:42:45]: [Client #292] Epoch: [5/5][0/10]	Loss: 0.491426
[INFO][09:42:45]: [Client #292] Model saved to /data/ykang/plato/results/test/model/lenet5_292_1127979.pth.
[INFO][09:42:46]: [Client #131] Loading a model from /data/ykang/plato/results/test/model/lenet5_131_1127977.pth.
[INFO][09:42:46]: [Client #131] Model trained.
[INFO][09:42:46]: [Client #131] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:42:46]: [Server #1127936] Received 0.24 MB of payload data from client #131 (simulated).
[INFO][09:42:46]: [Client #200] Loading a model from /data/ykang/plato/results/test/model/lenet5_200_1127978.pth.
[INFO][09:42:46]: [Client #200] Model trained.
[INFO][09:42:46]: [Client #200] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:42:46]: [Server #1127936] Received 0.24 MB of payload data from client #200 (simulated).
[INFO][09:42:46]: [Client #292] Loading a model from /data/ykang/plato/results/test/model/lenet5_292_1127979.pth.
[INFO][09:42:46]: [Client #292] Model trained.
[INFO][09:42:46]: [Client #292] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:42:46]: [Server #1127936] Received 0.24 MB of payload data from client #292 (simulated).
[INFO][09:42:46]: [Server #1127936] Selecting client #428 for training.
[INFO][09:42:46]: [Server #1127936] Sending the current model to client #428 (simulated).
[INFO][09:42:46]: [Server #1127936] Sending 0.24 MB of payload data to client #428 (simulated).
[INFO][09:42:46]: [Client #428] Selected by the server.
[INFO][09:42:46]: [Client #428] Loading its data source...
[INFO][09:42:46]: [Client #428] Dataset size: 60000
[INFO][09:42:46]: [Client #428] Sampler: noniid
[INFO][09:42:46]: [Client #428] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:42:46]: [93m[1m[Client #428] Started training in communication round #3.[0m
[INFO][09:42:48]: [Client #428] Loading the dataset.
[INFO][09:42:53]: [Client #428] Epoch: [1/5][0/10]	Loss: 2.389336
[INFO][09:42:53]: [Client #428] Epoch: [2/5][0/10]	Loss: 0.000006
[INFO][09:42:53]: [Client #428] Epoch: [3/5][0/10]	Loss: 0.134660
[INFO][09:42:53]: [Client #428] Epoch: [4/5][0/10]	Loss: 0.242057
[INFO][09:42:53]: [Client #428] Epoch: [5/5][0/10]	Loss: 0.221900
[INFO][09:42:53]: [Client #428] Model saved to /data/ykang/plato/results/test/model/lenet5_428_1127977.pth.
[INFO][09:42:54]: [Client #428] Loading a model from /data/ykang/plato/results/test/model/lenet5_428_1127977.pth.
[INFO][09:42:54]: [Client #428] Model trained.
[INFO][09:42:54]: [Client #428] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:42:54]: [Server #1127936] Received 0.24 MB of payload data from client #428 (simulated).
[INFO][09:42:54]: [Server #1127936] Adding client #300 to the list of clients for aggregation.
[INFO][09:42:54]: [Server #1127936] Adding client #451 to the list of clients for aggregation.
[INFO][09:42:54]: [Server #1127936] Adding client #187 to the list of clients for aggregation.
[INFO][09:42:54]: [Server #1127936] Adding client #85 to the list of clients for aggregation.
[INFO][09:42:54]: [Server #1127936] Adding client #487 to the list of clients for aggregation.
[INFO][09:42:54]: [Server #1127936] Adding client #16 to the list of clients for aggregation.
[INFO][09:42:54]: [Server #1127936] Adding client #47 to the list of clients for aggregation.
[INFO][09:42:54]: [Server #1127936] Adding client #131 to the list of clients for aggregation.
[INFO][09:42:54]: [Server #1127936] Adding client #260 to the list of clients for aggregation.
[INFO][09:42:54]: [Server #1127936] Adding client #207 to the list of clients for aggregation.
[INFO][09:42:54]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.30735938 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.19937224 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.36681434 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.15157631 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.17587509 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.14637703 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.09908489 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.22400366
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.16045102 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.19493307 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.30735938 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.19937224 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.36681434 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.15157631 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.17587509 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.14637703 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.09908489 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.22400366
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.16045102 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.19493307 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][09:42:56]: [Server #1127936] Global model accuracy: 38.82%

[INFO][09:42:56]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_3.pth.
[INFO][09:42:56]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_3.pth.
[INFO][09:42:56]: [93m[1m
[Server #1127936] Starting round 4/100.[0m
[0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8875e+00  6.8869e+00  7e-04  1e-08  1e-08
 5:  6.8875e+00  6.8872e+00  4e-04  5e-09  5e-09
 6:  6.8875e+00  6.8871e+00  4e-04  7e-08  3e-08
 7:  6.8875e+00  6.8872e+00  3e-04  6e-08  2e-08
 8:  6.8874e+00  6.8872e+00  2e-04  2e-07  6e-08
 9:  6.8873e+00  6.8872e+00  1e-04  2e-08  7e-09
10:  6.8873e+00  6.8872e+00  6e-05  5e-08  1e-08
11:  6.8872e+00  6.8872e+00  1e-05  3e-08  9e-09
12:  6.8872e+00  6.8872e+00  5e-07  3e-09  7e-10
Optimal solution found.
The calculated probability is:  [3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 2.82973365e-01
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 1.97342498e-05 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 7.15303995e-01
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36103331e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 1.32775561e-05 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36126914e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36303728e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.56339584e-05 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 1.07955517e-05 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 1.81368650e-05 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06 3.36453408e-06 3.36453408e-06
 3.36453408e-06 3.36453408e-06]
current clients pool:  [INFO][09:42:57]: [Server #1127936] Selected clients: [ 85  16   7 336 344 231 461 322   3 249]
[INFO][09:42:57]: [Server #1127936] Selecting client #85 for training.
[INFO][09:42:57]: [Server #1127936] Sending the current model to client #85 (simulated).
[INFO][09:42:57]: [Server #1127936] Sending 0.24 MB of payload data to client #85 (simulated).
[INFO][09:42:57]: [Server #1127936] Selecting client #16 for training.
[INFO][09:42:57]: [Server #1127936] Sending the current model to client #16 (simulated).
[INFO][09:42:57]: [Server #1127936] Sending 0.24 MB of payload data to client #16 (simulated).
[INFO][09:42:57]: [Client #85] Selected by the server.
[INFO][09:42:57]: [Client #85] Loading its data source...
[INFO][09:42:57]: [Client #85] Dataset size: 60000
[INFO][09:42:57]: [Client #85] Sampler: noniid
[INFO][09:42:57]: [Client #85] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:42:57]: [93m[1m[Client #85] Started training in communication round #4.[0m
[INFO][09:42:57]: [Server #1127936] Selecting client #7 for training.
[INFO][09:42:57]: [Server #1127936] Sending the current model to client #7 (simulated).
[INFO][09:42:57]: [Server #1127936] Sending 0.24 MB of payload data to client #7 (simulated).
[INFO][09:42:57]: [Client #16] Selected by the server.
[INFO][09:42:57]: [Client #16] Loading its data source...
[INFO][09:42:57]: [Client #16] Dataset size: 60000
[INFO][09:42:57]: [Client #16] Sampler: noniid
[INFO][09:42:57]: [Client #7] Selected by the server.
[INFO][09:42:57]: [Client #7] Loading its data source...
[INFO][09:42:57]: [Client #7] Dataset size: 60000
[INFO][09:42:57]: [Client #7] Sampler: noniid
[INFO][09:42:57]: [Client #7] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:42:57]: [Client #16] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:42:57]: [93m[1m[Client #7] Started training in communication round #4.[0m
[INFO][09:42:57]: [93m[1m[Client #16] Started training in communication round #4.[0m
[INFO][09:42:59]: [Client #85] Loading the dataset.
[INFO][09:42:59]: [Client #7] Loading the dataset.
[INFO][09:42:59]: [Client #16] Loading the dataset.
[INFO][09:43:05]: [Client #85] Epoch: [1/5][0/10]	Loss: 1.611695
[INFO][09:43:05]: [Client #85] Epoch: [2/5][0/10]	Loss: 0.000020
[INFO][09:43:05]: [Client #7] Epoch: [1/5][0/10]	Loss: 1.353650
[INFO][09:43:05]: [Client #16] Epoch: [1/5][0/10]	Loss: 1.704896
[INFO][09:43:05]: [Client #85] Epoch: [3/5][0/10]	Loss: 0.339728
[INFO][09:43:05]: [Client #7] Epoch: [2/5][0/10]	Loss: 0.000001
[INFO][09:43:05]: [Client #16] Epoch: [2/5][0/10]	Loss: 0.755845
[INFO][09:43:05]: [Client #85] Epoch: [4/5][0/10]	Loss: 0.000111
[INFO][09:43:05]: [Client #16] Epoch: [3/5][0/10]	Loss: 0.249489
[INFO][09:43:05]: [Client #7] Epoch: [3/5][0/10]	Loss: 0.000938
[INFO][09:43:05]: [Client #85] Epoch: [5/5][0/10]	Loss: 0.038713
[INFO][09:43:05]: [Client #16] Epoch: [4/5][0/10]	Loss: 0.094337
[INFO][09:43:05]: [Client #85] Model saved to /data/ykang/plato/results/test/model/lenet5_85_1127977.pth.
[INFO][09:43:05]: [Client #16] Epoch: [5/5][0/10]	Loss: 0.340393
[INFO][09:43:05]: [Client #7] Epoch: [4/5][0/10]	Loss: 0.055579
[INFO][09:43:05]: [Client #16] Model saved to /data/ykang/plato/results/test/model/lenet5_16_1127978.pth.
[INFO][09:43:05]: [Client #7] Epoch: [5/5][0/10]	Loss: 0.000312
[INFO][09:43:05]: [Client #7] Model saved to /data/ykang/plato/results/test/model/lenet5_7_1127979.pth.
[INFO][09:43:06]: [Client #85] Loading a model from /data/ykang/plato/results/test/model/lenet5_85_1127977.pth.
[INFO][09:43:06]: [Client #85] Model trained.
[INFO][09:43:06]: [Client #85] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:43:06]: [Server #1127936] Received 0.24 MB of payload data from client #85 (simulated).
[INFO][09:43:06]: [Client #16] Loading a model from /data/ykang/plato/results/test/model/lenet5_16_1127978.pth.
[INFO][09:43:06]: [Client #16] Model trained.
[INFO][09:43:06]: [Client #16] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:43:06]: [Server #1127936] Received 0.24 MB of payload data from client #16 (simulated).
[INFO][09:43:06]: [Client #7] Loading a model from /data/ykang/plato/results/test/model/lenet5_7_1127979.pth.
[INFO][09:43:06]: [Client #7] Model trained.
[INFO][09:43:06]: [Client #7] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:43:06]: [Server #1127936] Received 0.24 MB of payload data from client #7 (simulated).
[INFO][09:43:06]: [Server #1127936] Selecting client #336 for training.
[INFO][09:43:06]: [Server #1127936] Sending the current model to client #336 (simulated).
[INFO][09:43:06]: [Server #1127936] Sending 0.24 MB of payload data to client #336 (simulated).
[INFO][09:43:06]: [Server #1127936] Selecting client #344 for training.
[INFO][09:43:06]: [Server #1127936] Sending the current model to client #344 (simulated).
[INFO][09:43:06]: [Server #1127936] Sending 0.24 MB of payload data to client #344 (simulated).
[INFO][09:43:06]: [Server #1127936] Selecting client #231 for training.
[INFO][09:43:06]: [Server #1127936] Sending the current model to client #231 (simulated).
[INFO][09:43:06]: [Client #336] Selected by the server.
[INFO][09:43:06]: [Client #336] Loading its data source...
[INFO][09:43:06]: [Client #336] Dataset size: 60000
[INFO][09:43:06]: [Client #336] Sampler: noniid
[INFO][09:43:06]: [Server #1127936] Sending 0.24 MB of payload data to client #231 (simulated).
[INFO][09:43:06]: [Client #231] Selected by the server.
[INFO][09:43:06]: [Client #344] Selected by the server.
[INFO][09:43:06]: [Client #231] Loading its data source...
[INFO][09:43:06]: [Client #344] Loading its data source...
[INFO][09:43:06]: [Client #231] Dataset size: 60000
[INFO][09:43:06]: [Client #344] Dataset size: 60000
[INFO][09:43:06]: [Client #231] Sampler: noniid
[INFO][09:43:06]: [Client #344] Sampler: noniid
[INFO][09:43:06]: [Client #336] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:43:06]: [Client #344] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:43:06]: [93m[1m[Client #336] Started training in communication round #4.[0m
[INFO][09:43:06]: [93m[1m[Client #344] Started training in communication round #4.[0m
[INFO][09:43:06]: [Client #231] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:43:06]: [93m[1m[Client #231] Started training in communication round #4.[0m
[INFO][09:43:08]: [Client #231] Loading the dataset.
[INFO][09:43:08]: [Client #344] Loading the dataset.
[INFO][09:43:08]: [Client #336] Loading the dataset.
[INFO][09:43:14]: [Client #231] Epoch: [1/5][0/10]	Loss: 2.163067
[INFO][09:43:14]: [Client #336] Epoch: [1/5][0/10]	Loss: 1.849796
[INFO][09:43:14]: [Client #344] Epoch: [1/5][0/10]	Loss: 0.919483
[INFO][09:43:14]: [Client #231] Epoch: [2/5][0/10]	Loss: 0.468806
[INFO][09:43:14]: [Client #344] Epoch: [2/5][0/10]	Loss: 0.000058
[INFO][09:43:14]: [Client #336] Epoch: [2/5][0/10]	Loss: 0.740262
[INFO][09:43:14]: [Client #231] Epoch: [3/5][0/10]	Loss: 0.427574
[INFO][09:43:14]: [Client #344] Epoch: [3/5][0/10]	Loss: 0.007555
[INFO][09:43:14]: [Client #336] Epoch: [3/5][0/10]	Loss: 0.550878
[INFO][09:43:14]: [Client #231] Epoch: [4/5][0/10]	Loss: 0.531281
[INFO][09:43:14]: [Client #344] Epoch: [4/5][0/10]	Loss: 0.024689
[INFO][09:43:14]: [Client #336] Epoch: [4/5][0/10]	Loss: 0.033351
[INFO][09:43:14]: [Client #231] Epoch: [5/5][0/10]	Loss: 0.554390
[INFO][09:43:14]: [Client #344] Epoch: [5/5][0/10]	Loss: 0.000939
[INFO][09:43:14]: [Client #336] Epoch: [5/5][0/10]	Loss: 0.422744
[INFO][09:43:14]: [Client #231] Model saved to /data/ykang/plato/results/test/model/lenet5_231_1127979.pth.
[INFO][09:43:14]: [Client #344] Model saved to /data/ykang/plato/results/test/model/lenet5_344_1127978.pth.
[INFO][09:43:14]: [Client #336] Model saved to /data/ykang/plato/results/test/model/lenet5_336_1127977.pth.
[INFO][09:43:15]: [Client #231] Loading a model from /data/ykang/plato/results/test/model/lenet5_231_1127979.pth.
[INFO][09:43:15]: [Client #231] Model trained.
[INFO][09:43:15]: [Client #231] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:43:15]: [Server #1127936] Received 0.24 MB of payload data from client #231 (simulated).
[INFO][09:43:15]: [Client #344] Loading a model from /data/ykang/plato/results/test/model/lenet5_344_1127978.pth.
[INFO][09:43:15]: [Client #344] Model trained.
[INFO][09:43:15]: [Client #344] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:43:15]: [Server #1127936] Received 0.24 MB of payload data from client #344 (simulated).
[INFO][09:43:15]: [Client #336] Loading a model from /data/ykang/plato/results/test/model/lenet5_336_1127977.pth.
[INFO][09:43:15]: [Client #336] Model trained.
[INFO][09:43:15]: [Client #336] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:43:15]: [Server #1127936] Received 0.24 MB of payload data from client #336 (simulated).
[INFO][09:43:15]: [Server #1127936] Selecting client #461 for training.
[INFO][09:43:15]: [Server #1127936] Sending the current model to client #461 (simulated).
[INFO][09:43:15]: [Server #1127936] Sending 0.24 MB of payload data to client #461 (simulated).
[INFO][09:43:15]: [Server #1127936] Selecting client #322 for training.
[INFO][09:43:15]: [Server #1127936] Sending the current model to client #322 (simulated).
[INFO][09:43:15]: [Server #1127936] Sending 0.24 MB of payload data to client #322 (simulated).
[INFO][09:43:15]: [Server #1127936] Selecting client #3 for training.
[INFO][09:43:15]: [Server #1127936] Sending the current model to client #3 (simulated).
[INFO][09:43:15]: [Client #461] Selected by the server.
[INFO][09:43:15]: [Client #461] Loading its data source...
[INFO][09:43:15]: [Client #461] Dataset size: 60000
[INFO][09:43:15]: [Client #461] Sampler: noniid
[INFO][09:43:15]: [Server #1127936] Sending 0.24 MB of payload data to client #3 (simulated).
[INFO][09:43:15]: [Client #3] Selected by the server.
[INFO][09:43:15]: [Client #3] Loading its data source...
[INFO][09:43:15]: [Client #322] Selected by the server.
[INFO][09:43:15]: [Client #3] Dataset size: 60000
[INFO][09:43:15]: [Client #3] Sampler: noniid
[INFO][09:43:15]: [Client #322] Loading its data source...
[INFO][09:43:15]: [Client #322] Dataset size: 60000
[INFO][09:43:15]: [Client #322] Sampler: noniid
[INFO][09:43:15]: [Client #461] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:43:15]: [Client #3] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:43:15]: [Client #322] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:43:15]: [93m[1m[Client #3] Started training in communication round #4.[0m
[INFO][09:43:15]: [93m[1m[Client #322] Started training in communication round #4.[0m
[INFO][09:43:15]: [93m[1m[Client #461] Started training in communication round #4.[0m
[INFO][09:43:17]: [Client #3] Loading the dataset.
[INFO][09:43:17]: [Client #461] Loading the dataset.
[INFO][09:43:17]: [Client #322] Loading the dataset.
[INFO][09:43:23]: [Client #322] Epoch: [1/5][0/10]	Loss: 1.554101
[INFO][09:43:23]: [Client #3] Epoch: [1/5][0/10]	Loss: 1.353650
[INFO][09:43:23]: [Client #461] Epoch: [1/5][0/10]	Loss: 1.250199
[INFO][09:43:23]: [Client #322] Epoch: [2/5][0/10]	Loss: 0.187835
[INFO][09:43:24]: [Client #3] Epoch: [2/5][0/10]	Loss: 0.211989
[INFO][09:43:24]: [Client #461] Epoch: [2/5][0/10]	Loss: 0.046476
[INFO][09:43:24]: [Client #322] Epoch: [3/5][0/10]	Loss: 0.110171
[INFO][09:43:24]: [Client #322] Epoch: [4/5][0/10]	Loss: 0.031553
[INFO][09:43:24]: [Client #3] Epoch: [3/5][0/10]	Loss: 0.003452
[INFO][09:43:24]: [Client #461] Epoch: [3/5][0/10]	Loss: 0.095292
[INFO][09:43:24]: [Client #3] Epoch: [4/5][0/10]	Loss: 0.050948
[INFO][09:43:24]: [Client #322] Epoch: [5/5][0/10]	Loss: 0.007380
[INFO][09:43:24]: [Client #461] Epoch: [4/5][0/10]	Loss: 0.101694
[INFO][09:43:24]: [Client #3] Epoch: [5/5][0/10]	Loss: 0.032463
[INFO][09:43:24]: [Client #322] Model saved to /data/ykang/plato/results/test/model/lenet5_322_1127978.pth.
[INFO][09:43:24]: [Client #3] Model saved to /data/ykang/plato/results/test/model/lenet5_3_1127979.pth.
[INFO][09:43:24]: [Client #461] Epoch: [5/5][0/10]	Loss: 0.083800
[INFO][09:43:24]: [Client #461] Model saved to /data/ykang/plato/results/test/model/lenet5_461_1127977.pth.
[INFO][09:43:25]: [Client #322] Loading a model from /data/ykang/plato/results/test/model/lenet5_322_1127978.pth.
[INFO][09:43:25]: [Client #322] Model trained.
[INFO][09:43:25]: [Client #322] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:43:25]: [Server #1127936] Received 0.24 MB of payload data from client #322 (simulated).
[INFO][09:43:25]: [Client #3] Loading a model from /data/ykang/plato/results/test/model/lenet5_3_1127979.pth.
[INFO][09:43:25]: [Client #3] Model trained.
[INFO][09:43:25]: [Client #3] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:43:25]: [Server #1127936] Received 0.24 MB of payload data from client #3 (simulated).
[INFO][09:43:25]: [Client #461] Loading a model from /data/ykang/plato/results/test/model/lenet5_461_1127977.pth.
[INFO][09:43:25]: [Client #461] Model trained.
[INFO][09:43:25]: [Client #461] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:43:25]: [Server #1127936] Received 0.24 MB of payload data from client #461 (simulated).
[INFO][09:43:25]: [Server #1127936] Selecting client #249 for training.
[INFO][09:43:25]: [Server #1127936] Sending the current model to client #249 (simulated).
[INFO][09:43:25]: [Server #1127936] Sending 0.24 MB of payload data to client #249 (simulated).
[INFO][09:43:25]: [Client #249] Selected by the server.
[INFO][09:43:25]: [Client #249] Loading its data source...
[INFO][09:43:25]: [Client #249] Dataset size: 60000
[INFO][09:43:25]: [Client #249] Sampler: noniid
[INFO][09:43:25]: [Client #249] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:43:25]: [93m[1m[Client #249] Started training in communication round #4.[0m
[INFO][09:43:27]: [Client #249] Loading the dataset.
[INFO][09:43:32]: [Client #249] Epoch: [1/5][0/10]	Loss: 2.172562
[INFO][09:43:32]: [Client #249] Epoch: [2/5][0/10]	Loss: 0.156021
[INFO][09:43:32]: [Client #249] Epoch: [3/5][0/10]	Loss: 0.389845
[INFO][09:43:32]: [Client #249] Epoch: [4/5][0/10]	Loss: 0.046240
[INFO][09:43:32]: [Client #249] Epoch: [5/5][0/10]	Loss: 0.210711
[INFO][09:43:32]: [Client #249] Model saved to /data/ykang/plato/results/test/model/lenet5_249_1127977.pth.
[INFO][09:43:33]: [Client #249] Loading a model from /data/ykang/plato/results/test/model/lenet5_249_1127977.pth.
[INFO][09:43:33]: [Client #249] Model trained.
[INFO][09:43:33]: [Client #249] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:43:33]: [Server #1127936] Received 0.24 MB of payload data from client #249 (simulated).
[INFO][09:43:33]: [Server #1127936] Adding client #414 to the list of clients for aggregation.
[INFO][09:43:33]: [Server #1127936] Adding client #114 to the list of clients for aggregation.
[INFO][09:43:33]: [Server #1127936] Adding client #428 to the list of clients for aggregation.
[INFO][09:43:33]: [Server #1127936] Adding client #351 to the list of clients for aggregation.
[INFO][09:43:33]: [Server #1127936] Adding client #292 to the list of clients for aggregation.
[INFO][09:43:33]: [Server #1127936] Adding client #226 to the list of clients for aggregation.
[INFO][09:43:33]: [Server #1127936] Adding client #88 to the list of clients for aggregation.
[INFO][09:43:33]: [Server #1127936] Adding client #3 to the list of clients for aggregation.
[INFO][09:43:33]: [Server #1127936] Adding client #336 to the list of clients for aggregation.
[INFO][09:43:33]: [Server #1127936] Adding client #461 to the list of clients for aggregation.
[INFO][09:43:33]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.05994729 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.39702566 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.22934228
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.24572032 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.15236314 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.0850637
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.15793315 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.14683169
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.17209776 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.08179177 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
local_gradient_bounds:  [0.         0.         0.05994729 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.39702566 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.22934228
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.24572032 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.15236314 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.0850637
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.15793315 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.14683169
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.17209776 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.08179177 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][09:43:35]: [Server #1127936] Global model accuracy: 44.32%

[INFO][09:43:35]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_4.pth.
[INFO][09:43:35]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_4.pth.
[INFO][09:43:35]: [93m[1m
[Server #1127936] Starting round 5/100.[0m
[0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8875e+00  6.8860e+00  2e-03  3e-08  3e-08
 5:  6.8875e+00  6.8863e+00  1e-03  2e-08  2e-08
 6:  6.8874e+00  6.8862e+00  1e-03  6e-07  8e-07
 7:  6.8873e+00  6.8866e+00  7e-04  4e-07  6e-07
 8:  6.8872e+00  6.8867e+00  4e-04  8e-07  1e-06
 9:  6.8870e+00  6.8868e+00  2e-04  1e-06  2e-06
10:  6.8870e+00  6.8868e+00  1e-04  1e-08  7e-09
11:  6.8869e+00  6.8868e+00  8e-05  9e-08  1e-07
12:  6.8869e+00  6.8868e+00  2e-05  5e-08  3e-08
13:  6.8868e+00  6.8868e+00  9e-07  8e-09  4e-09
Optimal solution found.
The calculated probability is:  [3.83769367e-06 3.83769367e-06 3.83725261e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 8.16397073e-01
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 1.14723595e-05 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 1.81708544e-01
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 7.12213454e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83680570e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 7.33451833e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 6.92169135e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 7.92786075e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83687269e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06 3.83769367e-06 3.83769367e-06
 3.83769367e-06 3.83769367e-06]
current clients pool:  [INFO][09:43:36]: [Server #1127936] Selected clients: [226  88 102 378 147 391 217 407 179 112]
[INFO][09:43:36]: [Server #1127936] Selecting client #226 for training.
[INFO][09:43:36]: [Server #1127936] Sending the current model to client #226 (simulated).
[INFO][09:43:36]: [Server #1127936] Sending 0.24 MB of payload data to client #226 (simulated).
[INFO][09:43:36]: [Server #1127936] Selecting client #88 for training.
[INFO][09:43:36]: [Server #1127936] Sending the current model to client #88 (simulated).
[INFO][09:43:36]: [Server #1127936] Sending 0.24 MB of payload data to client #88 (simulated).
[INFO][09:43:36]: [Server #1127936] Selecting client #102 for training.
[INFO][09:43:36]: [Server #1127936] Sending the current model to client #102 (simulated).
[INFO][09:43:36]: [Client #226] Selected by the server.
[INFO][09:43:36]: [Client #226] Loading its data source...
[INFO][09:43:36]: [Client #226] Dataset size: 60000
[INFO][09:43:36]: [Client #226] Sampler: noniid
[INFO][09:43:36]: [Server #1127936] Sending 0.24 MB of payload data to client #102 (simulated).
[INFO][09:43:36]: [Client #88] Selected by the server.
[INFO][09:43:36]: [Client #88] Loading its data source...
[INFO][09:43:36]: [Client #88] Dataset size: 60000
[INFO][09:43:36]: [Client #88] Sampler: noniid
[INFO][09:43:36]: [Client #102] Selected by the server.
[INFO][09:43:36]: [Client #226] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:43:36]: [Client #102] Loading its data source...
[INFO][09:43:36]: [Client #102] Dataset size: 60000
[INFO][09:43:36]: [Client #102] Sampler: noniid
[INFO][09:43:36]: [Client #88] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:43:36]: [Client #102] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:43:36]: [93m[1m[Client #226] Started training in communication round #5.[0m
[INFO][09:43:36]: [93m[1m[Client #88] Started training in communication round #5.[0m
[INFO][09:43:36]: [93m[1m[Client #102] Started training in communication round #5.[0m
[INFO][09:43:38]: [Client #102] Loading the dataset.
[INFO][09:43:38]: [Client #226] Loading the dataset.
[INFO][09:43:38]: [Client #88] Loading the dataset.
[INFO][09:43:44]: [Client #102] Epoch: [1/5][0/10]	Loss: 0.721896
[INFO][09:43:44]: [Client #88] Epoch: [1/5][0/10]	Loss: 1.758203
[INFO][09:43:44]: [Client #226] Epoch: [1/5][0/10]	Loss: 1.351692
[INFO][09:43:44]: [Client #102] Epoch: [2/5][0/10]	Loss: 0.000001
[INFO][09:43:44]: [Client #88] Epoch: [2/5][0/10]	Loss: 0.529285
[INFO][09:43:44]: [Client #226] Epoch: [2/5][0/10]	Loss: 0.819228
[INFO][09:43:44]: [Client #102] Epoch: [3/5][0/10]	Loss: 0.013242
[INFO][09:43:44]: [Client #88] Epoch: [3/5][0/10]	Loss: 0.171478
[INFO][09:43:44]: [Client #102] Epoch: [4/5][0/10]	Loss: 0.002698
[INFO][09:43:44]: [Client #226] Epoch: [3/5][0/10]	Loss: 0.018090
[INFO][09:43:44]: [Client #88] Epoch: [4/5][0/10]	Loss: 0.263164
[INFO][09:43:44]: [Client #102] Epoch: [5/5][0/10]	Loss: 0.000102
[INFO][09:43:44]: [Client #88] Epoch: [5/5][0/10]	Loss: 0.442906
[INFO][09:43:44]: [Client #226] Epoch: [4/5][0/10]	Loss: 0.000319
[INFO][09:43:44]: [Client #102] Model saved to /data/ykang/plato/results/test/model/lenet5_102_1127979.pth.
[INFO][09:43:44]: [Client #88] Model saved to /data/ykang/plato/results/test/model/lenet5_88_1127978.pth.
[INFO][09:43:44]: [Client #226] Epoch: [5/5][0/10]	Loss: 0.029771
[INFO][09:43:44]: [Client #226] Model saved to /data/ykang/plato/results/test/model/lenet5_226_1127977.pth.
[INFO][09:43:45]: [Client #102] Loading a model from /data/ykang/plato/results/test/model/lenet5_102_1127979.pth.
[INFO][09:43:45]: [Client #102] Model trained.
[INFO][09:43:45]: [Client #102] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:43:45]: [Server #1127936] Received 0.24 MB of payload data from client #102 (simulated).
[INFO][09:43:45]: [Client #88] Loading a model from /data/ykang/plato/results/test/model/lenet5_88_1127978.pth.
[INFO][09:43:45]: [Client #88] Model trained.
[INFO][09:43:45]: [Client #88] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:43:45]: [Server #1127936] Received 0.24 MB of payload data from client #88 (simulated).
[INFO][09:43:45]: [Client #226] Loading a model from /data/ykang/plato/results/test/model/lenet5_226_1127977.pth.
[INFO][09:43:45]: [Client #226] Model trained.
[INFO][09:43:45]: [Client #226] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:43:45]: [Server #1127936] Received 0.24 MB of payload data from client #226 (simulated).
[INFO][09:43:45]: [Server #1127936] Selecting client #378 for training.
[INFO][09:43:45]: [Server #1127936] Sending the current model to client #378 (simulated).
[INFO][09:43:45]: [Server #1127936] Sending 0.24 MB of payload data to client #378 (simulated).
[INFO][09:43:45]: [Server #1127936] Selecting client #147 for training.
[INFO][09:43:45]: [Server #1127936] Sending the current model to client #147 (simulated).
[INFO][09:43:45]: [Server #1127936] Sending 0.24 MB of payload data to client #147 (simulated).
[INFO][09:43:45]: [Server #1127936] Selecting client #391 for training.
[INFO][09:43:45]: [Server #1127936] Sending the current model to client #391 (simulated).
[INFO][09:43:45]: [Client #378] Selected by the server.
[INFO][09:43:45]: [Client #378] Loading its data source...
[INFO][09:43:45]: [Client #378] Dataset size: 60000
[INFO][09:43:45]: [Client #378] Sampler: noniid
[INFO][09:43:45]: [Server #1127936] Sending 0.24 MB of payload data to client #391 (simulated).
[INFO][09:43:45]: [Client #147] Selected by the server.
[INFO][09:43:45]: [Client #147] Loading its data source...
[INFO][09:43:45]: [Client #391] Selected by the server.
[INFO][09:43:45]: [Client #147] Dataset size: 60000
[INFO][09:43:45]: [Client #147] Sampler: noniid
[INFO][09:43:45]: [Client #391] Loading its data source...
[INFO][09:43:45]: [Client #391] Dataset size: 60000
[INFO][09:43:45]: [Client #391] Sampler: noniid
[INFO][09:43:45]: [Client #378] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:43:45]: [Client #147] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:43:45]: [Client #391] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:43:45]: [93m[1m[Client #378] Started training in communication round #5.[0m
[INFO][09:43:45]: [93m[1m[Client #147] Started training in communication round #5.[0m
[INFO][09:43:45]: [93m[1m[Client #391] Started training in communication round #5.[0m
[INFO][09:43:47]: [Client #147] Loading the dataset.
[INFO][09:43:47]: [Client #378] Loading the dataset.
[INFO][09:43:47]: [Client #391] Loading the dataset.
[INFO][09:43:53]: [Client #147] Epoch: [1/5][0/10]	Loss: 1.022113
[INFO][09:43:53]: [Client #378] Epoch: [1/5][0/10]	Loss: 1.454888
[INFO][09:43:53]: [Client #147] Epoch: [2/5][0/10]	Loss: 0.000004
[INFO][09:43:53]: [Client #391] Epoch: [1/5][0/10]	Loss: 2.485755
[INFO][09:43:53]: [Client #378] Epoch: [2/5][0/10]	Loss: 0.039968
[INFO][09:43:53]: [Client #147] Epoch: [3/5][0/10]	Loss: 0.000531
[INFO][09:43:53]: [Client #391] Epoch: [2/5][0/10]	Loss: 1.039218
[INFO][09:43:53]: [Client #378] Epoch: [3/5][0/10]	Loss: 0.327267
[INFO][09:43:53]: [Client #391] Epoch: [3/5][0/10]	Loss: 0.401310
[INFO][09:43:53]: [Client #147] Epoch: [4/5][0/10]	Loss: 0.058631
[INFO][09:43:53]: [Client #378] Epoch: [4/5][0/10]	Loss: 0.221408
[INFO][09:43:53]: [Client #391] Epoch: [4/5][0/10]	Loss: 0.758771
[INFO][09:43:53]: [Client #378] Epoch: [5/5][0/10]	Loss: 0.005802
[INFO][09:43:53]: [Client #147] Epoch: [5/5][0/10]	Loss: 0.000489
[INFO][09:43:53]: [Client #378] Model saved to /data/ykang/plato/results/test/model/lenet5_378_1127977.pth.
[INFO][09:43:53]: [Client #147] Model saved to /data/ykang/plato/results/test/model/lenet5_147_1127978.pth.
[INFO][09:43:54]: [Client #391] Epoch: [5/5][0/10]	Loss: 0.304362
[INFO][09:43:54]: [Client #391] Model saved to /data/ykang/plato/results/test/model/lenet5_391_1127979.pth.
[INFO][09:43:54]: [Client #147] Loading a model from /data/ykang/plato/results/test/model/lenet5_147_1127978.pth.
[INFO][09:43:54]: [Client #147] Model trained.
[INFO][09:43:54]: [Client #147] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:43:54]: [Server #1127936] Received 0.24 MB of payload data from client #147 (simulated).
[INFO][09:43:54]: [Client #378] Loading a model from /data/ykang/plato/results/test/model/lenet5_378_1127977.pth.
[INFO][09:43:54]: [Client #378] Model trained.
[INFO][09:43:54]: [Client #378] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:43:54]: [Server #1127936] Received 0.24 MB of payload data from client #378 (simulated).
[INFO][09:43:54]: [Client #391] Loading a model from /data/ykang/plato/results/test/model/lenet5_391_1127979.pth.
[INFO][09:43:54]: [Client #391] Model trained.
[INFO][09:43:54]: [Client #391] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:43:54]: [Server #1127936] Received 0.24 MB of payload data from client #391 (simulated).
[INFO][09:43:54]: [Server #1127936] Selecting client #217 for training.
[INFO][09:43:54]: [Server #1127936] Sending the current model to client #217 (simulated).
[INFO][09:43:54]: [Server #1127936] Sending 0.24 MB of payload data to client #217 (simulated).
[INFO][09:43:54]: [Server #1127936] Selecting client #407 for training.
[INFO][09:43:54]: [Server #1127936] Sending the current model to client #407 (simulated).
[INFO][09:43:55]: [Server #1127936] Sending 0.24 MB of payload data to client #407 (simulated).
[INFO][09:43:55]: [Server #1127936] Selecting client #179 for training.
[INFO][09:43:55]: [Server #1127936] Sending the current model to client #179 (simulated).
[INFO][09:43:55]: [Client #217] Selected by the server.
[INFO][09:43:55]: [Client #217] Loading its data source...
[INFO][09:43:55]: [Client #217] Dataset size: 60000
[INFO][09:43:55]: [Client #217] Sampler: noniid
[INFO][09:43:55]: [Server #1127936] Sending 0.24 MB of payload data to client #179 (simulated).
[INFO][09:43:55]: [Client #407] Selected by the server.
[INFO][09:43:55]: [Client #407] Loading its data source...
[INFO][09:43:55]: [Client #407] Dataset size: 60000
[INFO][09:43:55]: [Client #407] Sampler: noniid
[INFO][09:43:55]: [Client #179] Selected by the server.
[INFO][09:43:55]: [Client #217] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:43:55]: [Client #179] Loading its data source...
[INFO][09:43:55]: [Client #179] Dataset size: 60000
[INFO][09:43:55]: [Client #179] Sampler: noniid
[INFO][09:43:55]: [Client #407] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:43:55]: [93m[1m[Client #407] Started training in communication round #5.[0m
[INFO][09:43:55]: [Client #179] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:43:55]: [93m[1m[Client #217] Started training in communication round #5.[0m
[INFO][09:43:55]: [93m[1m[Client #179] Started training in communication round #5.[0m
[INFO][09:43:57]: [Client #179] Loading the dataset.
[INFO][09:43:57]: [Client #407] Loading the dataset.
[INFO][09:43:57]: [Client #217] Loading the dataset.
[INFO][09:44:03]: [Client #179] Epoch: [1/5][0/10]	Loss: 1.624606
[INFO][09:44:03]: [Client #217] Epoch: [1/5][0/10]	Loss: 1.769894
[INFO][09:44:03]: [Client #179] Epoch: [2/5][0/10]	Loss: 0.760478
[INFO][09:44:03]: [Client #407] Epoch: [1/5][0/10]	Loss: 2.210615
[INFO][09:44:03]: [Client #217] Epoch: [2/5][0/10]	Loss: 0.000284
[INFO][09:44:03]: [Client #407] Epoch: [2/5][0/10]	Loss: 0.181530
[INFO][09:44:03]: [Client #179] Epoch: [3/5][0/10]	Loss: 0.416318
[INFO][09:44:03]: [Client #217] Epoch: [3/5][0/10]	Loss: 0.005091
[INFO][09:44:03]: [Client #407] Epoch: [3/5][0/10]	Loss: 0.017650
[INFO][09:44:03]: [Client #179] Epoch: [4/5][0/10]	Loss: 0.468048
[INFO][09:44:03]: [Client #217] Epoch: [4/5][0/10]	Loss: 0.093402
[INFO][09:44:03]: [Client #407] Epoch: [4/5][0/10]	Loss: 0.045201
[INFO][09:44:03]: [Client #179] Epoch: [5/5][0/10]	Loss: 0.425269
[INFO][09:44:03]: [Client #407] Epoch: [5/5][0/10]	Loss: 0.570110
[INFO][09:44:03]: [Client #179] Model saved to /data/ykang/plato/results/test/model/lenet5_179_1127979.pth.
[INFO][09:44:03]: [Client #217] Epoch: [5/5][0/10]	Loss: 0.030219
[INFO][09:44:03]: [Client #407] Model saved to /data/ykang/plato/results/test/model/lenet5_407_1127978.pth.
[INFO][09:44:03]: [Client #217] Model saved to /data/ykang/plato/results/test/model/lenet5_217_1127977.pth.
[INFO][09:44:04]: [Client #179] Loading a model from /data/ykang/plato/results/test/model/lenet5_179_1127979.pth.
[INFO][09:44:04]: [Client #179] Model trained.
[INFO][09:44:04]: [Client #179] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:44:04]: [Server #1127936] Received 0.24 MB of payload data from client #179 (simulated).
[INFO][09:44:04]: [Client #407] Loading a model from /data/ykang/plato/results/test/model/lenet5_407_1127978.pth.
[INFO][09:44:04]: [Client #407] Model trained.
[INFO][09:44:04]: [Client #407] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:44:04]: [Server #1127936] Received 0.24 MB of payload data from client #407 (simulated).
[INFO][09:44:04]: [Client #217] Loading a model from /data/ykang/plato/results/test/model/lenet5_217_1127977.pth.
[INFO][09:44:04]: [Client #217] Model trained.
[INFO][09:44:04]: [Client #217] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:44:04]: [Server #1127936] Received 0.24 MB of payload data from client #217 (simulated).
[INFO][09:44:04]: [Server #1127936] Selecting client #112 for training.
[INFO][09:44:04]: [Server #1127936] Sending the current model to client #112 (simulated).
[INFO][09:44:04]: [Server #1127936] Sending 0.24 MB of payload data to client #112 (simulated).
[INFO][09:44:04]: [Client #112] Selected by the server.
[INFO][09:44:04]: [Client #112] Loading its data source...
[INFO][09:44:04]: [Client #112] Dataset size: 60000
[INFO][09:44:04]: [Client #112] Sampler: noniid
[INFO][09:44:04]: [Client #112] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:44:04]: [93m[1m[Client #112] Started training in communication round #5.[0m
[INFO][09:44:06]: [Client #112] Loading the dataset.
[INFO][09:44:11]: [Client #112] Epoch: [1/5][0/10]	Loss: 1.608145
[INFO][09:44:11]: [Client #112] Epoch: [2/5][0/10]	Loss: 0.222154
[INFO][09:44:11]: [Client #112] Epoch: [3/5][0/10]	Loss: 0.345103
[INFO][09:44:11]: [Client #112] Epoch: [4/5][0/10]	Loss: 0.732166
[INFO][09:44:11]: [Client #112] Epoch: [5/5][0/10]	Loss: 0.170894
[INFO][09:44:11]: [Client #112] Model saved to /data/ykang/plato/results/test/model/lenet5_112_1127977.pth.
[INFO][09:44:12]: [Client #112] Loading a model from /data/ykang/plato/results/test/model/lenet5_112_1127977.pth.
[INFO][09:44:12]: [Client #112] Model trained.
[INFO][09:44:12]: [Client #112] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:44:12]: [Server #1127936] Received 0.24 MB of payload data from client #112 (simulated).
[INFO][09:44:12]: [Server #1127936] Adding client #7 to the list of clients for aggregation.
[INFO][09:44:12]: [Server #1127936] Adding client #85 to the list of clients for aggregation.
[INFO][09:44:12]: [Server #1127936] Adding client #231 to the list of clients for aggregation.
[INFO][09:44:12]: [Server #1127936] Adding client #33 to the list of clients for aggregation.
[INFO][09:44:12]: [Server #1127936] Adding client #322 to the list of clients for aggregation.
[INFO][09:44:12]: [Server #1127936] Adding client #16 to the list of clients for aggregation.
[INFO][09:44:12]: [Server #1127936] Adding client #249 to the list of clients for aggregation.
[INFO][09:44:12]: [Server #1127936] Adding client #344 to the list of clients for aggregation.
[INFO][09:44:12]: [Server #1127936] Adding client #200 to the list of clients for aggregation.
[INFO][09:44:12]: [Server #1127936] Adding client #217 to the list of clients for aggregation.
[INFO][09:44:12]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.11154882 0.         0.         0.         0.         0.
 0.         0.         0.         0.14407982 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.29969129 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.26549446 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.1016887  0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.13092532 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.13707492 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.20362126 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.16432973 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.0688061  0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.11154882 0.         0.         0.         0.         0.
 0.         0.         0.         0.14407982 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.29969129 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.26549446 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.1016887  0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.13092532 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.13707492 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.20362126 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.16432973 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.0688061  0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][09:44:14]: [Server #1127936] Global model accuracy: 54.53%

[INFO][09:44:14]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_5.pth.
[INFO][09:44:14]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_5.pth.
[INFO][09:44:14]: [93m[1m
[Server #1127936] Starting round 6/100.[0m
[0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8875e+00  6.8863e+00  1e-03  2e-08  2e-08
 5:  6.8875e+00  6.8866e+00  9e-04  1e-08  1e-08
 6:  6.8874e+00  6.8865e+00  9e-04  5e-07  4e-07
 7:  6.8873e+00  6.8868e+00  5e-04  3e-07  3e-07
 8:  6.8872e+00  6.8869e+00  3e-04  6e-07  5e-07
 9:  6.8870e+00  6.8869e+00  1e-04  1e-06  1e-06
10:  6.8870e+00  6.8869e+00  3e-05  4e-07  4e-07
11:  6.8869e+00  6.8869e+00  2e-06  6e-08  5e-08
Optimal solution found.
The calculated probability is:  [1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 2.50358565e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 3.19444535e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 9.93380045e-01 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.19770321e-04 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 5.50169764e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29749425e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 3.02316330e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 5.49781255e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 3.78101760e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.89330380e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05 1.29859571e-05 1.29859571e-05
 1.29859571e-05 1.29859571e-05]
current clients pool:  [INFO][09:44:15]: [Server #1127936] Selected clients: [ 33  56 332 397  40 113  98 458 350 272]
[INFO][09:44:15]: [Server #1127936] Selecting client #33 for training.
[INFO][09:44:15]: [Server #1127936] Sending the current model to client #33 (simulated).
[INFO][09:44:15]: [Server #1127936] Sending 0.24 MB of payload data to client #33 (simulated).
[INFO][09:44:15]: [Server #1127936] Selecting client #56 for training.
[INFO][09:44:15]: [Server #1127936] Sending the current model to client #56 (simulated).
[INFO][09:44:15]: [Server #1127936] Sending 0.24 MB of payload data to client #56 (simulated).
[INFO][09:44:15]: [Server #1127936] Selecting client #332 for training.
[INFO][09:44:15]: [Server #1127936] Sending the current model to client #332 (simulated).
[INFO][09:44:15]: [Client #33] Selected by the server.
[INFO][09:44:15]: [Client #33] Loading its data source...
[INFO][09:44:15]: [Client #33] Dataset size: 60000
[INFO][09:44:15]: [Client #33] Sampler: noniid
[INFO][09:44:15]: [Server #1127936] Sending 0.24 MB of payload data to client #332 (simulated).
[INFO][09:44:15]: [Client #56] Selected by the server.
[INFO][09:44:15]: [Client #56] Loading its data source...
[INFO][09:44:15]: [Client #56] Dataset size: 60000
[INFO][09:44:15]: [Client #56] Sampler: noniid
[INFO][09:44:15]: [Client #332] Selected by the server.
[INFO][09:44:15]: [Client #33] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:44:15]: [Client #332] Loading its data source...
[INFO][09:44:15]: [Client #332] Dataset size: 60000
[INFO][09:44:15]: [Client #332] Sampler: noniid
[INFO][09:44:15]: [Client #56] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:44:15]: [Client #332] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:44:15]: [93m[1m[Client #33] Started training in communication round #6.[0m
[INFO][09:44:15]: [93m[1m[Client #56] Started training in communication round #6.[0m
[INFO][09:44:15]: [93m[1m[Client #332] Started training in communication round #6.[0m
[INFO][09:44:17]: [Client #33] Loading the dataset.
[INFO][09:44:17]: [Client #56] Loading the dataset.
[INFO][09:44:17]: [Client #332] Loading the dataset.
[INFO][09:44:23]: [Client #33] Epoch: [1/5][0/10]	Loss: 1.062500
[INFO][09:44:23]: [Client #56] Epoch: [1/5][0/10]	Loss: 1.847037
[INFO][09:44:23]: [Client #33] Epoch: [2/5][0/10]	Loss: 0.005376
[INFO][09:44:23]: [Client #332] Epoch: [1/5][0/10]	Loss: 1.116633
[INFO][09:44:23]: [Client #56] Epoch: [2/5][0/10]	Loss: 0.654291
[INFO][09:44:23]: [Client #33] Epoch: [3/5][0/10]	Loss: 1.170545
[INFO][09:44:23]: [Client #56] Epoch: [3/5][0/10]	Loss: 0.432687
[INFO][09:44:23]: [Client #332] Epoch: [2/5][0/10]	Loss: 0.000634
[INFO][09:44:23]: [Client #33] Epoch: [4/5][0/10]	Loss: 0.162481
[INFO][09:44:23]: [Client #56] Epoch: [4/5][0/10]	Loss: 0.018083
[INFO][09:44:23]: [Client #332] Epoch: [3/5][0/10]	Loss: 0.000086
[INFO][09:44:23]: [Client #33] Epoch: [5/5][0/10]	Loss: 0.077595
[INFO][09:44:23]: [Client #33] Model saved to /data/ykang/plato/results/test/model/lenet5_33_1127977.pth.
[INFO][09:44:23]: [Client #56] Epoch: [5/5][0/10]	Loss: 0.254590
[INFO][09:44:23]: [Client #332] Epoch: [4/5][0/10]	Loss: 0.304680
[INFO][09:44:23]: [Client #56] Model saved to /data/ykang/plato/results/test/model/lenet5_56_1127978.pth.
[INFO][09:44:23]: [Client #332] Epoch: [5/5][0/10]	Loss: 0.000235
[INFO][09:44:23]: [Client #332] Model saved to /data/ykang/plato/results/test/model/lenet5_332_1127979.pth.
[INFO][09:44:24]: [Client #33] Loading a model from /data/ykang/plato/results/test/model/lenet5_33_1127977.pth.
[INFO][09:44:24]: [Client #33] Model trained.
[INFO][09:44:24]: [Client #33] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:44:24]: [Server #1127936] Received 0.24 MB of payload data from client #33 (simulated).
[INFO][09:44:24]: [Client #56] Loading a model from /data/ykang/plato/results/test/model/lenet5_56_1127978.pth.
[INFO][09:44:24]: [Client #56] Model trained.
[INFO][09:44:24]: [Client #56] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:44:24]: [Server #1127936] Received 0.24 MB of payload data from client #56 (simulated).
[INFO][09:44:24]: [Client #332] Loading a model from /data/ykang/plato/results/test/model/lenet5_332_1127979.pth.
[INFO][09:44:24]: [Client #332] Model trained.
[INFO][09:44:24]: [Client #332] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:44:24]: [Server #1127936] Received 0.24 MB of payload data from client #332 (simulated).
[INFO][09:44:24]: [Server #1127936] Selecting client #397 for training.
[INFO][09:44:24]: [Server #1127936] Sending the current model to client #397 (simulated).
[INFO][09:44:24]: [Server #1127936] Sending 0.24 MB of payload data to client #397 (simulated).
[INFO][09:44:24]: [Server #1127936] Selecting client #40 for training.
[INFO][09:44:24]: [Server #1127936] Sending the current model to client #40 (simulated).
[INFO][09:44:24]: [Server #1127936] Sending 0.24 MB of payload data to client #40 (simulated).
[INFO][09:44:24]: [Server #1127936] Selecting client #113 for training.
[INFO][09:44:24]: [Server #1127936] Sending the current model to client #113 (simulated).
[INFO][09:44:24]: [Client #397] Selected by the server.
[INFO][09:44:24]: [Client #397] Loading its data source...
[INFO][09:44:24]: [Client #397] Dataset size: 60000
[INFO][09:44:24]: [Client #397] Sampler: noniid
[INFO][09:44:24]: [Server #1127936] Sending 0.24 MB of payload data to client #113 (simulated).
[INFO][09:44:24]: [Client #40] Selected by the server.
[INFO][09:44:24]: [Client #40] Loading its data source...
[INFO][09:44:24]: [Client #397] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:44:24]: [Client #113] Selected by the server.
[INFO][09:44:24]: [Client #40] Dataset size: 60000
[INFO][09:44:24]: [Client #40] Sampler: noniid
[INFO][09:44:24]: [Client #113] Loading its data source...
[INFO][09:44:24]: [Client #113] Dataset size: 60000
[INFO][09:44:24]: [Client #40] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:44:24]: [Client #113] Sampler: noniid
[INFO][09:44:24]: [Client #113] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:44:24]: [93m[1m[Client #397] Started training in communication round #6.[0m
[INFO][09:44:24]: [93m[1m[Client #113] Started training in communication round #6.[0m
[INFO][09:44:24]: [93m[1m[Client #40] Started training in communication round #6.[0m
[INFO][09:44:26]: [Client #113] Loading the dataset.
[INFO][09:44:26]: [Client #40] Loading the dataset.
[INFO][09:44:26]: [Client #397] Loading the dataset.
[INFO][09:44:32]: [Client #113] Epoch: [1/5][0/10]	Loss: 1.883587
[INFO][09:44:32]: [Client #397] Epoch: [1/5][0/10]	Loss: 1.447525
[INFO][09:44:32]: [Client #40] Epoch: [1/5][0/10]	Loss: 1.339848
[INFO][09:44:32]: [Client #113] Epoch: [2/5][0/10]	Loss: 0.324889
[INFO][09:44:32]: [Client #397] Epoch: [2/5][0/10]	Loss: 0.000000
[INFO][09:44:32]: [Client #40] Epoch: [2/5][0/10]	Loss: 0.385615
[INFO][09:44:32]: [Client #397] Epoch: [3/5][0/10]	Loss: 0.000000
[INFO][09:44:32]: [Client #113] Epoch: [3/5][0/10]	Loss: 0.249045
[INFO][09:44:32]: [Client #40] Epoch: [3/5][0/10]	Loss: 0.306852
[INFO][09:44:32]: [Client #397] Epoch: [4/5][0/10]	Loss: 0.135624
[INFO][09:44:32]: [Client #40] Epoch: [4/5][0/10]	Loss: 0.071505
[INFO][09:44:32]: [Client #113] Epoch: [4/5][0/10]	Loss: 0.459680
[INFO][09:44:32]: [Client #397] Epoch: [5/5][0/10]	Loss: 0.108691
[INFO][09:44:33]: [Client #40] Epoch: [5/5][0/10]	Loss: 0.644956
[INFO][09:44:33]: [Client #113] Epoch: [5/5][0/10]	Loss: 0.354828
[INFO][09:44:33]: [Client #397] Model saved to /data/ykang/plato/results/test/model/lenet5_397_1127977.pth.
[INFO][09:44:33]: [Client #40] Model saved to /data/ykang/plato/results/test/model/lenet5_40_1127978.pth.
[INFO][09:44:33]: [Client #113] Model saved to /data/ykang/plato/results/test/model/lenet5_113_1127979.pth.
[INFO][09:44:33]: [Client #397] Loading a model from /data/ykang/plato/results/test/model/lenet5_397_1127977.pth.
[INFO][09:44:33]: [Client #397] Model trained.
[INFO][09:44:33]: [Client #397] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:44:33]: [Server #1127936] Received 0.24 MB of payload data from client #397 (simulated).
[INFO][09:44:33]: [Client #113] Loading a model from /data/ykang/plato/results/test/model/lenet5_113_1127979.pth.
[INFO][09:44:33]: [Client #113] Model trained.
[INFO][09:44:33]: [Client #113] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:44:33]: [Server #1127936] Received 0.24 MB of payload data from client #113 (simulated).
[INFO][09:44:33]: [Client #40] Loading a model from /data/ykang/plato/results/test/model/lenet5_40_1127978.pth.
[INFO][09:44:33]: [Client #40] Model trained.
[INFO][09:44:33]: [Client #40] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:44:33]: [Server #1127936] Received 0.24 MB of payload data from client #40 (simulated).
[INFO][09:44:33]: [Server #1127936] Selecting client #98 for training.
[INFO][09:44:33]: [Server #1127936] Sending the current model to client #98 (simulated).
[INFO][09:44:33]: [Server #1127936] Sending 0.24 MB of payload data to client #98 (simulated).
[INFO][09:44:33]: [Server #1127936] Selecting client #458 for training.
[INFO][09:44:33]: [Server #1127936] Sending the current model to client #458 (simulated).
[INFO][09:44:33]: [Server #1127936] Sending 0.24 MB of payload data to client #458 (simulated).
[INFO][09:44:33]: [Server #1127936] Selecting client #350 for training.
[INFO][09:44:33]: [Server #1127936] Sending the current model to client #350 (simulated).
[INFO][09:44:33]: [Client #98] Selected by the server.
[INFO][09:44:33]: [Client #98] Loading its data source...
[INFO][09:44:33]: [Client #98] Dataset size: 60000
[INFO][09:44:33]: [Client #98] Sampler: noniid
[INFO][09:44:33]: [Server #1127936] Sending 0.24 MB of payload data to client #350 (simulated).
[INFO][09:44:33]: [Client #458] Selected by the server.
[INFO][09:44:33]: [Client #458] Loading its data source...
[INFO][09:44:33]: [Client #458] Dataset size: 60000
[INFO][09:44:33]: [Client #350] Selected by the server.
[INFO][09:44:33]: [Client #458] Sampler: noniid
[INFO][09:44:33]: [Client #350] Loading its data source...
[INFO][09:44:34]: [Client #350] Dataset size: 60000
[INFO][09:44:34]: [Client #350] Sampler: noniid
[INFO][09:44:34]: [Client #98] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:44:34]: [Client #458] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:44:34]: [Client #350] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:44:34]: [93m[1m[Client #98] Started training in communication round #6.[0m
[INFO][09:44:34]: [93m[1m[Client #350] Started training in communication round #6.[0m
[INFO][09:44:34]: [93m[1m[Client #458] Started training in communication round #6.[0m
[INFO][09:44:36]: [Client #458] Loading the dataset.
[INFO][09:44:36]: [Client #98] Loading the dataset.
[INFO][09:44:36]: [Client #350] Loading the dataset.
[INFO][09:44:41]: [Client #98] Epoch: [1/5][0/10]	Loss: 1.308847
[INFO][09:44:41]: [Client #458] Epoch: [1/5][0/10]	Loss: 1.711584
[INFO][09:44:42]: [Client #350] Epoch: [1/5][0/10]	Loss: 1.671029
[INFO][09:44:42]: [Client #458] Epoch: [2/5][0/10]	Loss: 0.763338
[INFO][09:44:42]: [Client #98] Epoch: [2/5][0/10]	Loss: 1.595480
[INFO][09:44:42]: [Client #350] Epoch: [2/5][0/10]	Loss: 0.000442
[INFO][09:44:42]: [Client #458] Epoch: [3/5][0/10]	Loss: 0.322377
[INFO][09:44:42]: [Client #98] Epoch: [3/5][0/10]	Loss: 0.036731
[INFO][09:44:42]: [Client #350] Epoch: [3/5][0/10]	Loss: 0.103268
[INFO][09:44:42]: [Client #98] Epoch: [4/5][0/10]	Loss: 0.025612
[INFO][09:44:42]: [Client #458] Epoch: [4/5][0/10]	Loss: 0.441500
[INFO][09:44:42]: [Client #350] Epoch: [4/5][0/10]	Loss: 0.430740
[INFO][09:44:42]: [Client #458] Epoch: [5/5][0/10]	Loss: 0.306648
[INFO][09:44:42]: [Client #98] Epoch: [5/5][0/10]	Loss: 0.042957
[INFO][09:44:42]: [Client #98] Model saved to /data/ykang/plato/results/test/model/lenet5_98_1127977.pth.
[INFO][09:44:42]: [Client #350] Epoch: [5/5][0/10]	Loss: 0.297642
[INFO][09:44:42]: [Client #458] Model saved to /data/ykang/plato/results/test/model/lenet5_458_1127978.pth.
[INFO][09:44:42]: [Client #350] Model saved to /data/ykang/plato/results/test/model/lenet5_350_1127979.pth.
[INFO][09:44:43]: [Client #458] Loading a model from /data/ykang/plato/results/test/model/lenet5_458_1127978.pth.
[INFO][09:44:43]: [Client #458] Model trained.
[INFO][09:44:43]: [Client #458] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:44:43]: [Server #1127936] Received 0.24 MB of payload data from client #458 (simulated).
[INFO][09:44:43]: [Client #98] Loading a model from /data/ykang/plato/results/test/model/lenet5_98_1127977.pth.
[INFO][09:44:43]: [Client #98] Model trained.
[INFO][09:44:43]: [Client #98] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:44:43]: [Server #1127936] Received 0.24 MB of payload data from client #98 (simulated).
[INFO][09:44:43]: [Client #350] Loading a model from /data/ykang/plato/results/test/model/lenet5_350_1127979.pth.
[INFO][09:44:43]: [Client #350] Model trained.
[INFO][09:44:43]: [Client #350] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:44:43]: [Server #1127936] Received 0.24 MB of payload data from client #350 (simulated).
[INFO][09:44:43]: [Server #1127936] Selecting client #272 for training.
[INFO][09:44:43]: [Server #1127936] Sending the current model to client #272 (simulated).
[INFO][09:44:43]: [Server #1127936] Sending 0.24 MB of payload data to client #272 (simulated).
[INFO][09:44:43]: [Client #272] Selected by the server.
[INFO][09:44:43]: [Client #272] Loading its data source...
[INFO][09:44:43]: [Client #272] Dataset size: 60000
[INFO][09:44:43]: [Client #272] Sampler: noniid
[INFO][09:44:43]: [Client #272] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:44:43]: [93m[1m[Client #272] Started training in communication round #6.[0m
[INFO][09:44:45]: [Client #272] Loading the dataset.
[INFO][09:44:50]: [Client #272] Epoch: [1/5][0/10]	Loss: 1.929233
[INFO][09:44:50]: [Client #272] Epoch: [2/5][0/10]	Loss: 0.614846
[INFO][09:44:50]: [Client #272] Epoch: [3/5][0/10]	Loss: 0.532275
[INFO][09:44:50]: [Client #272] Epoch: [4/5][0/10]	Loss: 0.979394
[INFO][09:44:50]: [Client #272] Epoch: [5/5][0/10]	Loss: 0.155546
[INFO][09:44:50]: [Client #272] Model saved to /data/ykang/plato/results/test/model/lenet5_272_1127977.pth.
[INFO][09:44:51]: [Client #272] Loading a model from /data/ykang/plato/results/test/model/lenet5_272_1127977.pth.
[INFO][09:44:51]: [Client #272] Model trained.
[INFO][09:44:51]: [Client #272] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:44:51]: [Server #1127936] Received 0.24 MB of payload data from client #272 (simulated).
[INFO][09:44:51]: [Server #1127936] Adding client #179 to the list of clients for aggregation.
[INFO][09:44:51]: [Server #1127936] Adding client #112 to the list of clients for aggregation.
[INFO][09:44:51]: [Server #1127936] Adding client #391 to the list of clients for aggregation.
[INFO][09:44:51]: [Server #1127936] Adding client #407 to the list of clients for aggregation.
[INFO][09:44:51]: [Server #1127936] Adding client #147 to the list of clients for aggregation.
[INFO][09:44:51]: [Server #1127936] Adding client #102 to the list of clients for aggregation.
[INFO][09:44:51]: [Server #1127936] Adding client #378 to the list of clients for aggregation.
[INFO][09:44:51]: [Server #1127936] Adding client #350 to the list of clients for aggregation.
[INFO][09:44:51]: [Server #1127936] Adding client #56 to the list of clients for aggregation.
[INFO][09:44:51]: [Server #1127936] Adding client #272 to the list of clients for aggregation.
[INFO][09:44:51]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 108, 109, 111, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.14241436 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.07350172
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.12141051 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.1076784  0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.06586544 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.12370428 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.21564404 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.05619487
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.18033189 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.18963377 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.14241436 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.07350172
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.12141051 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.1076784  0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.06586544 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.12370428 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.21564404 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.05619487
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.18033189 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.18963377 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][09:44:53]: [Server #1127936] Global model accuracy: 62.05%

[INFO][09:44:53]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_6.pth.
[INFO][09:44:53]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_6.pth.
[INFO][09:44:53]: [93m[1m
[Server #1127936] Starting round 7/100.[0m
[0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8870e+00  5e-04  9e-09  9e-09
 5:  6.8876e+00  6.8874e+00  2e-04  3e-09  3e-09
 6:  6.8875e+00  6.8873e+00  2e-04  8e-09  2e-09
 7:  6.8875e+00  6.8874e+00  1e-04  9e-09  2e-09
 8:  6.8875e+00  6.8874e+00  1e-04  3e-08  5e-09
 9:  6.8874e+00  6.8874e+00  5e-05  3e-08  4e-09
10:  6.8874e+00  6.8874e+00  2e-05  2e-08  3e-09
11:  6.8874e+00  6.8874e+00  3e-06  6e-09  9e-10
Optimal solution found.
The calculated probability is:  [3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49138118e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 6.69568936e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 1.57485665e-04 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 1.14291212e-04 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 6.12008494e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49251938e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.48539939e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 5.51701103e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.68283206e-01 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 6.14376090e-01 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05 3.49602214e-05 3.49602214e-05
 3.49602214e-05 3.49602214e-05]
current clients pool:  [INFO][09:44:54]: [Server #1127936] Selected clients: [391 407 213 200  75 363  97 319 375 108]
[INFO][09:44:54]: [Server #1127936] Selecting client #391 for training.
[INFO][09:44:54]: [Server #1127936] Sending the current model to client #391 (simulated).
[INFO][09:44:54]: [Server #1127936] Sending 0.24 MB of payload data to client #391 (simulated).
[INFO][09:44:54]: [Server #1127936] Selecting client #407 for training.
[INFO][09:44:54]: [Server #1127936] Sending the current model to client #407 (simulated).
[INFO][09:44:54]: [Server #1127936] Sending 0.24 MB of payload data to client #407 (simulated).
[INFO][09:44:54]: [Client #391] Selected by the server.
[INFO][09:44:54]: [Client #391] Loading its data source...
[INFO][09:44:54]: [Client #391] Dataset size: 60000
[INFO][09:44:54]: [Client #391] Sampler: noniid
[INFO][09:44:54]: [Client #391] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:44:54]: [Server #1127936] Selecting client #213 for training.
[INFO][09:44:54]: [Server #1127936] Sending the current model to client #213 (simulated).
[INFO][09:44:54]: [Server #1127936] Sending 0.24 MB of payload data to client #213 (simulated).
[INFO][09:44:54]: [Client #407] Selected by the server.
[INFO][09:44:54]: [Client #407] Loading its data source...
[INFO][09:44:54]: [Client #407] Dataset size: 60000
[INFO][09:44:54]: [Client #407] Sampler: noniid
[INFO][09:44:54]: [Client #407] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:44:54]: [93m[1m[Client #407] Started training in communication round #7.[0m
[INFO][09:44:54]: [Client #213] Selected by the server.
[INFO][09:44:54]: [Client #213] Loading its data source...
[INFO][09:44:54]: [Client #213] Dataset size: 60000
[INFO][09:44:54]: [Client #213] Sampler: noniid
[INFO][09:44:54]: [Client #213] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:44:54]: [93m[1m[Client #213] Started training in communication round #7.[0m
[INFO][09:44:54]: [93m[1m[Client #391] Started training in communication round #7.[0m
[INFO][09:44:56]: [Client #213] Loading the dataset.
[INFO][09:44:56]: [Client #407] Loading the dataset.
[INFO][09:44:56]: [Client #391] Loading the dataset.
[INFO][09:45:02]: [Client #407] Epoch: [1/5][0/10]	Loss: 1.205819
[INFO][09:45:02]: [Client #213] Epoch: [1/5][0/10]	Loss: 0.408115
[INFO][09:45:02]: [Client #391] Epoch: [1/5][0/10]	Loss: 1.699056
[INFO][09:45:02]: [Client #407] Epoch: [2/5][0/10]	Loss: 0.653235
[INFO][09:45:02]: [Client #213] Epoch: [2/5][0/10]	Loss: 0.026742
[INFO][09:45:02]: [Client #391] Epoch: [2/5][0/10]	Loss: 0.254903
[INFO][09:45:02]: [Client #407] Epoch: [3/5][0/10]	Loss: 0.000097
[INFO][09:45:02]: [Client #391] Epoch: [3/5][0/10]	Loss: 0.146908
[INFO][09:45:02]: [Client #213] Epoch: [3/5][0/10]	Loss: 0.020151
[INFO][09:45:02]: [Client #407] Epoch: [4/5][0/10]	Loss: 0.014398
[INFO][09:45:02]: [Client #213] Epoch: [4/5][0/10]	Loss: 0.226347
[INFO][09:45:02]: [Client #391] Epoch: [4/5][0/10]	Loss: 0.538172
[INFO][09:45:02]: [Client #407] Epoch: [5/5][0/10]	Loss: 0.420235
[INFO][09:45:02]: [Client #391] Epoch: [5/5][0/10]	Loss: 1.022483
[INFO][09:45:02]: [Client #213] Epoch: [5/5][0/10]	Loss: 0.019939
[INFO][09:45:02]: [Client #407] Model saved to /data/ykang/plato/results/test/model/lenet5_407_1127978.pth.
[INFO][09:45:02]: [Client #391] Model saved to /data/ykang/plato/results/test/model/lenet5_391_1127977.pth.
[INFO][09:45:02]: [Client #213] Model saved to /data/ykang/plato/results/test/model/lenet5_213_1127979.pth.
[INFO][09:45:03]: [Client #407] Loading a model from /data/ykang/plato/results/test/model/lenet5_407_1127978.pth.
[INFO][09:45:03]: [Client #407] Model trained.
[INFO][09:45:03]: [Client #407] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:45:03]: [Server #1127936] Received 0.24 MB of payload data from client #407 (simulated).
[INFO][09:45:03]: [Client #213] Loading a model from /data/ykang/plato/results/test/model/lenet5_213_1127979.pth.
[INFO][09:45:03]: [Client #213] Model trained.
[INFO][09:45:03]: [Client #213] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:45:03]: [Server #1127936] Received 0.24 MB of payload data from client #213 (simulated).
[INFO][09:45:03]: [Client #391] Loading a model from /data/ykang/plato/results/test/model/lenet5_391_1127977.pth.
[INFO][09:45:03]: [Client #391] Model trained.
[INFO][09:45:03]: [Client #391] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:45:03]: [Server #1127936] Received 0.24 MB of payload data from client #391 (simulated).
[INFO][09:45:03]: [Server #1127936] Selecting client #200 for training.
[INFO][09:45:03]: [Server #1127936] Sending the current model to client #200 (simulated).
[INFO][09:45:03]: [Server #1127936] Sending 0.24 MB of payload data to client #200 (simulated).
[INFO][09:45:03]: [Server #1127936] Selecting client #75 for training.
[INFO][09:45:03]: [Server #1127936] Sending the current model to client #75 (simulated).
[INFO][09:45:03]: [Server #1127936] Sending 0.24 MB of payload data to client #75 (simulated).
[INFO][09:45:03]: [Server #1127936] Selecting client #363 for training.
[INFO][09:45:03]: [Server #1127936] Sending the current model to client #363 (simulated).
[INFO][09:45:03]: [Client #200] Selected by the server.
[INFO][09:45:03]: [Client #200] Loading its data source...
[INFO][09:45:03]: [Client #200] Dataset size: 60000
[INFO][09:45:03]: [Client #200] Sampler: noniid
[INFO][09:45:03]: [Client #200] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:45:03]: [93m[1m[Client #200] Started training in communication round #7.[0m
[INFO][09:45:03]: [Server #1127936] Sending 0.24 MB of payload data to client #363 (simulated).
[INFO][09:45:03]: [Client #363] Selected by the server.
[INFO][09:45:03]: [Client #363] Loading its data source...
[INFO][09:45:03]: [Client #363] Dataset size: 60000
[INFO][09:45:03]: [Client #75] Selected by the server.
[INFO][09:45:03]: [Client #363] Sampler: noniid
[INFO][09:45:03]: [Client #75] Loading its data source...
[INFO][09:45:03]: [Client #75] Dataset size: 60000
[INFO][09:45:03]: [Client #75] Sampler: noniid
[INFO][09:45:03]: [Client #363] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:45:03]: [Client #75] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:45:03]: [93m[1m[Client #363] Started training in communication round #7.[0m
[INFO][09:45:03]: [93m[1m[Client #75] Started training in communication round #7.[0m
[INFO][09:45:05]: [Client #363] Loading the dataset.
[INFO][09:45:05]: [Client #75] Loading the dataset.
[INFO][09:45:05]: [Client #200] Loading the dataset.
[INFO][09:45:11]: [Client #363] Epoch: [1/5][0/10]	Loss: 1.364820
[INFO][09:45:11]: [Client #75] Epoch: [1/5][0/10]	Loss: 0.685199
[INFO][09:45:11]: [Client #363] Epoch: [2/5][0/10]	Loss: 0.283849
[INFO][09:45:11]: [Client #200] Epoch: [1/5][0/10]	Loss: 0.886474
[INFO][09:45:11]: [Client #75] Epoch: [2/5][0/10]	Loss: 0.061531
[INFO][09:45:11]: [Client #363] Epoch: [3/5][0/10]	Loss: 0.024173
[INFO][09:45:11]: [Client #200] Epoch: [2/5][0/10]	Loss: 0.120976
[INFO][09:45:11]: [Client #75] Epoch: [3/5][0/10]	Loss: 0.350613
[INFO][09:45:11]: [Client #363] Epoch: [4/5][0/10]	Loss: 0.000002
[INFO][09:45:11]: [Client #200] Epoch: [3/5][0/10]	Loss: 0.225213
[INFO][09:45:11]: [Client #75] Epoch: [4/5][0/10]	Loss: 0.022891
[INFO][09:45:11]: [Client #363] Epoch: [5/5][0/10]	Loss: 0.050640
[INFO][09:45:11]: [Client #75] Epoch: [5/5][0/10]	Loss: 0.009284
[INFO][09:45:11]: [Client #363] Model saved to /data/ykang/plato/results/test/model/lenet5_363_1127979.pth.
[INFO][09:45:11]: [Client #200] Epoch: [4/5][0/10]	Loss: 0.199471
[INFO][09:45:11]: [Client #75] Model saved to /data/ykang/plato/results/test/model/lenet5_75_1127978.pth.
[INFO][09:45:11]: [Client #200] Epoch: [5/5][0/10]	Loss: 0.654038
[INFO][09:45:12]: [Client #200] Model saved to /data/ykang/plato/results/test/model/lenet5_200_1127977.pth.
[INFO][09:45:12]: [Client #363] Loading a model from /data/ykang/plato/results/test/model/lenet5_363_1127979.pth.
[INFO][09:45:12]: [Client #363] Model trained.
[INFO][09:45:12]: [Client #363] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:45:12]: [Server #1127936] Received 0.24 MB of payload data from client #363 (simulated).
[INFO][09:45:12]: [Client #75] Loading a model from /data/ykang/plato/results/test/model/lenet5_75_1127978.pth.
[INFO][09:45:12]: [Client #75] Model trained.
[INFO][09:45:12]: [Client #75] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:45:12]: [Server #1127936] Received 0.24 MB of payload data from client #75 (simulated).
[INFO][09:45:12]: [Client #200] Loading a model from /data/ykang/plato/results/test/model/lenet5_200_1127977.pth.
[INFO][09:45:12]: [Client #200] Model trained.
[INFO][09:45:12]: [Client #200] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:45:12]: [Server #1127936] Received 0.24 MB of payload data from client #200 (simulated).
[INFO][09:45:12]: [Server #1127936] Selecting client #97 for training.
[INFO][09:45:12]: [Server #1127936] Sending the current model to client #97 (simulated).
[INFO][09:45:12]: [Server #1127936] Sending 0.24 MB of payload data to client #97 (simulated).
[INFO][09:45:12]: [Server #1127936] Selecting client #319 for training.
[INFO][09:45:12]: [Server #1127936] Sending the current model to client #319 (simulated).
[INFO][09:45:12]: [Server #1127936] Sending 0.24 MB of payload data to client #319 (simulated).
[INFO][09:45:12]: [Server #1127936] Selecting client #375 for training.
[INFO][09:45:12]: [Server #1127936] Sending the current model to client #375 (simulated).
[INFO][09:45:12]: [Client #97] Selected by the server.
[INFO][09:45:12]: [Client #97] Loading its data source...
[INFO][09:45:12]: [Client #97] Dataset size: 60000
[INFO][09:45:12]: [Client #97] Sampler: noniid
[INFO][09:45:12]: [Server #1127936] Sending 0.24 MB of payload data to client #375 (simulated).
[INFO][09:45:12]: [Client #319] Selected by the server.
[INFO][09:45:12]: [Client #319] Loading its data source...
[INFO][09:45:12]: [Client #319] Dataset size: 60000
[INFO][09:45:12]: [Client #319] Sampler: noniid
[INFO][09:45:12]: [Client #375] Selected by the server.
[INFO][09:45:12]: [Client #375] Loading its data source...
[INFO][09:45:12]: [Client #375] Dataset size: 60000
[INFO][09:45:12]: [Client #375] Sampler: noniid
[INFO][09:45:12]: [Client #97] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:45:12]: [Client #319] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:45:12]: [Client #375] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:45:12]: [93m[1m[Client #97] Started training in communication round #7.[0m
[INFO][09:45:12]: [93m[1m[Client #319] Started training in communication round #7.[0m
[INFO][09:45:12]: [93m[1m[Client #375] Started training in communication round #7.[0m
[INFO][09:45:14]: [Client #97] Loading the dataset.
[INFO][09:45:14]: [Client #319] Loading the dataset.
[INFO][09:45:14]: [Client #375] Loading the dataset.
[INFO][09:45:20]: [Client #319] Epoch: [1/5][0/10]	Loss: 1.971470
[INFO][09:45:20]: [Client #375] Epoch: [1/5][0/10]	Loss: 1.875759
[INFO][09:45:20]: [Client #97] Epoch: [1/5][0/10]	Loss: 2.610144
[INFO][09:45:20]: [Client #319] Epoch: [2/5][0/10]	Loss: 0.298349
[INFO][09:45:20]: [Client #375] Epoch: [2/5][0/10]	Loss: 0.169214
[INFO][09:45:20]: [Client #319] Epoch: [3/5][0/10]	Loss: 0.011722
[INFO][09:45:20]: [Client #97] Epoch: [2/5][0/10]	Loss: 0.700425
[INFO][09:45:20]: [Client #375] Epoch: [3/5][0/10]	Loss: 0.347921
[INFO][09:45:21]: [Client #319] Epoch: [4/5][0/10]	Loss: 0.232370
[INFO][09:45:21]: [Client #375] Epoch: [4/5][0/10]	Loss: 0.073887
[INFO][09:45:21]: [Client #97] Epoch: [3/5][0/10]	Loss: 0.047540
[INFO][09:45:21]: [Client #319] Epoch: [5/5][0/10]	Loss: 0.355003
[INFO][09:45:21]: [Client #375] Epoch: [5/5][0/10]	Loss: 0.404636
[INFO][09:45:21]: [Client #97] Epoch: [4/5][0/10]	Loss: 0.006294
[INFO][09:45:21]: [Client #319] Model saved to /data/ykang/plato/results/test/model/lenet5_319_1127978.pth.
[INFO][09:45:21]: [Client #375] Model saved to /data/ykang/plato/results/test/model/lenet5_375_1127979.pth.
[INFO][09:45:21]: [Client #97] Epoch: [5/5][0/10]	Loss: 0.063075
[INFO][09:45:21]: [Client #97] Model saved to /data/ykang/plato/results/test/model/lenet5_97_1127977.pth.
[INFO][09:45:22]: [Client #319] Loading a model from /data/ykang/plato/results/test/model/lenet5_319_1127978.pth.
[INFO][09:45:22]: [Client #319] Model trained.
[INFO][09:45:22]: [Client #319] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:45:22]: [Server #1127936] Received 0.24 MB of payload data from client #319 (simulated).
[INFO][09:45:22]: [Client #375] Loading a model from /data/ykang/plato/results/test/model/lenet5_375_1127979.pth.
[INFO][09:45:22]: [Client #375] Model trained.
[INFO][09:45:22]: [Client #375] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:45:22]: [Server #1127936] Received 0.24 MB of payload data from client #375 (simulated).
[INFO][09:45:22]: [Client #97] Loading a model from /data/ykang/plato/results/test/model/lenet5_97_1127977.pth.
[INFO][09:45:22]: [Client #97] Model trained.
[INFO][09:45:22]: [Client #97] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:45:22]: [Server #1127936] Received 0.24 MB of payload data from client #97 (simulated).
[INFO][09:45:22]: [Server #1127936] Selecting client #108 for training.
[INFO][09:45:22]: [Server #1127936] Sending the current model to client #108 (simulated).
[INFO][09:45:22]: [Server #1127936] Sending 0.24 MB of payload data to client #108 (simulated).
[INFO][09:45:22]: [Client #108] Selected by the server.
[INFO][09:45:22]: [Client #108] Loading its data source...
[INFO][09:45:22]: [Client #108] Dataset size: 60000
[INFO][09:45:22]: [Client #108] Sampler: noniid
[INFO][09:45:22]: [Client #108] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:45:22]: [93m[1m[Client #108] Started training in communication round #7.[0m
[INFO][09:45:24]: [Client #108] Loading the dataset.
[INFO][09:45:29]: [Client #108] Epoch: [1/5][0/10]	Loss: 1.310335
[INFO][09:45:29]: [Client #108] Epoch: [2/5][0/10]	Loss: 0.731511
[INFO][09:45:29]: [Client #108] Epoch: [3/5][0/10]	Loss: 0.439220
[INFO][09:45:29]: [Client #108] Epoch: [4/5][0/10]	Loss: 0.000002
[INFO][09:45:29]: [Client #108] Epoch: [5/5][0/10]	Loss: 0.407801
[INFO][09:45:29]: [Client #108] Model saved to /data/ykang/plato/results/test/model/lenet5_108_1127977.pth.
[INFO][09:45:30]: [Client #108] Loading a model from /data/ykang/plato/results/test/model/lenet5_108_1127977.pth.
[INFO][09:45:30]: [Client #108] Model trained.
[INFO][09:45:30]: [Client #108] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:45:30]: [Server #1127936] Received 0.24 MB of payload data from client #108 (simulated).
[INFO][09:45:30]: [Server #1127936] Adding client #397 to the list of clients for aggregation.
[INFO][09:45:30]: [Server #1127936] Adding client #458 to the list of clients for aggregation.
[INFO][09:45:30]: [Server #1127936] Adding client #98 to the list of clients for aggregation.
[INFO][09:45:30]: [Server #1127936] Adding client #40 to the list of clients for aggregation.
[INFO][09:45:30]: [Server #1127936] Adding client #332 to the list of clients for aggregation.
[INFO][09:45:30]: [Server #1127936] Adding client #226 to the list of clients for aggregation.
[INFO][09:45:30]: [Server #1127936] Adding client #375 to the list of clients for aggregation.
[INFO][09:45:30]: [Server #1127936] Adding client #213 to the list of clients for aggregation.
[INFO][09:45:30]: [Server #1127936] Adding client #391 to the list of clients for aggregation.
[INFO][09:45:30]: [Server #1127936] Adding client #407 to the list of clients for aggregation.
[INFO][09:45:30]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 35, 36, 37, 38, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.06208558 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.08867526 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.03966022 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.08619758 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.08551598 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.16082356 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.1076144  0.         0.         0.         0.         0.
 0.20037218 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.09522878 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.13024113 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.06208558 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.08867526 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.03966022 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.08619758 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.08551598 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.16082356 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.1076144  0.         0.         0.         0.         0.
 0.20037218 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.09522878 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.13024113 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][09:45:32]: [Server #1127936] Global model accuracy: 58.21%

[INFO][09:45:32]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_7.pth.
[INFO][09:45:32]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_7.pth.
[INFO][09:45:32]: [93m[1m
[Server #1127936] Starting round 8/100.[0m
[0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8870e+00  5e-04  9e-09  9e-09
 5:  6.8876e+00  6.8873e+00  2e-04  3e-09  3e-09
 6:  6.8875e+00  6.8873e+00  2e-04  1e-08  2e-09
 7:  6.8875e+00  6.8874e+00  2e-04  1e-08  2e-09
 8:  6.8874e+00  6.8874e+00  8e-05  1e-07  2e-08
 9:  6.8874e+00  6.8874e+00  4e-05  8e-08  2e-08
10:  6.8874e+00  6.8874e+00  2e-05  4e-09  6e-10
11:  6.8874e+00  6.8874e+00  7e-06  4e-09  6e-10
12:  6.8874e+00  6.8874e+00  9e-07  9e-10  1e-10
Optimal solution found.
The calculated probability is:  [1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.82585800e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 2.49512831e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12096910e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.36304459e-01 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 2.39117629e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.11927629e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12027105e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 8.58144819e-01 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12044620e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 5.76112456e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05 1.12107889e-05 1.12107889e-05
 1.12107889e-05 1.12107889e-05]
current clients pool:  [INFO][09:45:33]: [Server #1127936] Selected clients: [397 226 423  98 202 154  41 457 189 270]
[INFO][09:45:33]: [Server #1127936] Selecting client #397 for training.
[INFO][09:45:33]: [Server #1127936] Sending the current model to client #397 (simulated).
[INFO][09:45:33]: [Server #1127936] Sending 0.24 MB of payload data to client #397 (simulated).
[INFO][09:45:33]: [Server #1127936] Selecting client #226 for training.
[INFO][09:45:33]: [Server #1127936] Sending the current model to client #226 (simulated).
[INFO][09:45:33]: [Server #1127936] Sending 0.24 MB of payload data to client #226 (simulated).
[INFO][09:45:33]: [Server #1127936] Selecting client #423 for training.
[INFO][09:45:33]: [Server #1127936] Sending the current model to client #423 (simulated).
[INFO][09:45:33]: [Client #397] Selected by the server.
[INFO][09:45:33]: [Client #397] Loading its data source...
[INFO][09:45:33]: [Client #397] Dataset size: 60000
[INFO][09:45:33]: [Client #397] Sampler: noniid
[INFO][09:45:33]: [Server #1127936] Sending 0.24 MB of payload data to client #423 (simulated).
[INFO][09:45:33]: [Client #226] Selected by the server.
[INFO][09:45:33]: [Client #226] Loading its data source...
[INFO][09:45:33]: [Client #226] Dataset size: 60000
[INFO][09:45:33]: [Client #226] Sampler: noniid
[INFO][09:45:33]: [Client #397] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:45:33]: [Client #226] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:45:33]: [93m[1m[Client #397] Started training in communication round #8.[0m
[INFO][09:45:33]: [93m[1m[Client #226] Started training in communication round #8.[0m
[INFO][09:45:33]: [Client #423] Selected by the server.
[INFO][09:45:33]: [Client #423] Loading its data source...
[INFO][09:45:33]: [Client #423] Dataset size: 60000
[INFO][09:45:33]: [Client #423] Sampler: noniid
[INFO][09:45:33]: [Client #423] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:45:33]: [93m[1m[Client #423] Started training in communication round #8.[0m
[INFO][09:45:35]: [Client #226] Loading the dataset.
[INFO][09:45:35]: [Client #397] Loading the dataset.
[INFO][09:45:35]: [Client #423] Loading the dataset.
[INFO][09:45:40]: [Client #397] Epoch: [1/5][0/10]	Loss: 0.544547
[INFO][09:45:41]: [Client #423] Epoch: [1/5][0/10]	Loss: 0.671202
[INFO][09:45:41]: [Client #226] Epoch: [1/5][0/10]	Loss: 0.401051
[INFO][09:45:41]: [Client #397] Epoch: [2/5][0/10]	Loss: 0.000003
[INFO][09:45:41]: [Client #226] Epoch: [2/5][0/10]	Loss: 0.266789
[INFO][09:45:41]: [Client #423] Epoch: [2/5][0/10]	Loss: 0.098744
[INFO][09:45:41]: [Client #397] Epoch: [3/5][0/10]	Loss: 0.000000
[INFO][09:45:41]: [Client #226] Epoch: [3/5][0/10]	Loss: 0.000014
[INFO][09:45:41]: [Client #423] Epoch: [3/5][0/10]	Loss: 0.001397
[INFO][09:45:41]: [Client #397] Epoch: [4/5][0/10]	Loss: 0.001028
[INFO][09:45:41]: [Client #226] Epoch: [4/5][0/10]	Loss: 0.000009
[INFO][09:45:41]: [Client #397] Epoch: [5/5][0/10]	Loss: 0.068622
[INFO][09:45:41]: [Client #423] Epoch: [4/5][0/10]	Loss: 0.017534
[INFO][09:45:41]: [Client #226] Epoch: [5/5][0/10]	Loss: 0.041912
[INFO][09:45:41]: [Client #397] Model saved to /data/ykang/plato/results/test/model/lenet5_397_1127977.pth.
[INFO][09:45:41]: [Client #226] Model saved to /data/ykang/plato/results/test/model/lenet5_226_1127978.pth.
[INFO][09:45:41]: [Client #423] Epoch: [5/5][0/10]	Loss: 0.038549
[INFO][09:45:41]: [Client #423] Model saved to /data/ykang/plato/results/test/model/lenet5_423_1127979.pth.
[INFO][09:45:42]: [Client #397] Loading a model from /data/ykang/plato/results/test/model/lenet5_397_1127977.pth.
[INFO][09:45:42]: [Client #397] Model trained.
[INFO][09:45:42]: [Client #397] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:45:42]: [Server #1127936] Received 0.24 MB of payload data from client #397 (simulated).
[INFO][09:45:42]: [Client #226] Loading a model from /data/ykang/plato/results/test/model/lenet5_226_1127978.pth.
[INFO][09:45:42]: [Client #226] Model trained.
[INFO][09:45:42]: [Client #226] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:45:42]: [Server #1127936] Received 0.24 MB of payload data from client #226 (simulated).
[INFO][09:45:42]: [Client #423] Loading a model from /data/ykang/plato/results/test/model/lenet5_423_1127979.pth.
[INFO][09:45:42]: [Client #423] Model trained.
[INFO][09:45:42]: [Client #423] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:45:42]: [Server #1127936] Received 0.24 MB of payload data from client #423 (simulated).
[INFO][09:45:42]: [Server #1127936] Selecting client #98 for training.
[INFO][09:45:42]: [Server #1127936] Sending the current model to client #98 (simulated).
[INFO][09:45:42]: [Server #1127936] Sending 0.24 MB of payload data to client #98 (simulated).
[INFO][09:45:42]: [Server #1127936] Selecting client #202 for training.
[INFO][09:45:42]: [Server #1127936] Sending the current model to client #202 (simulated).
[INFO][09:45:42]: [Server #1127936] Sending 0.24 MB of payload data to client #202 (simulated).
[INFO][09:45:42]: [Server #1127936] Selecting client #154 for training.
[INFO][09:45:42]: [Server #1127936] Sending the current model to client #154 (simulated).
[INFO][09:45:42]: [Client #98] Selected by the server.
[INFO][09:45:42]: [Client #98] Loading its data source...
[INFO][09:45:42]: [Client #98] Dataset size: 60000
[INFO][09:45:42]: [Client #98] Sampler: noniid
[INFO][09:45:42]: [Server #1127936] Sending 0.24 MB of payload data to client #154 (simulated).
[INFO][09:45:42]: [Client #202] Selected by the server.
[INFO][09:45:42]: [Client #154] Selected by the server.
[INFO][09:45:42]: [Client #202] Loading its data source...
[INFO][09:45:42]: [Client #154] Loading its data source...
[INFO][09:45:42]: [Client #154] Dataset size: 60000
[INFO][09:45:42]: [Client #202] Dataset size: 60000
[INFO][09:45:42]: [Client #154] Sampler: noniid
[INFO][09:45:42]: [Client #202] Sampler: noniid
[INFO][09:45:42]: [Client #98] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:45:42]: [Client #154] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:45:42]: [Client #202] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:45:42]: [93m[1m[Client #98] Started training in communication round #8.[0m
[INFO][09:45:42]: [93m[1m[Client #202] Started training in communication round #8.[0m
[INFO][09:45:42]: [93m[1m[Client #154] Started training in communication round #8.[0m
[INFO][09:45:44]: [Client #98] Loading the dataset.
[INFO][09:45:44]: [Client #202] Loading the dataset.
[INFO][09:45:44]: [Client #154] Loading the dataset.
[INFO][09:45:50]: [Client #202] Epoch: [1/5][0/10]	Loss: 0.584190
[INFO][09:45:50]: [Client #154] Epoch: [1/5][0/10]	Loss: 2.482307
[INFO][09:45:50]: [Client #98] Epoch: [1/5][0/10]	Loss: 0.609388
[INFO][09:45:50]: [Client #202] Epoch: [2/5][0/10]	Loss: 0.000940
[INFO][09:45:50]: [Client #154] Epoch: [2/5][0/10]	Loss: 0.593964
[INFO][09:45:50]: [Client #98] Epoch: [2/5][0/10]	Loss: 0.542804
[INFO][09:45:50]: [Client #202] Epoch: [3/5][0/10]	Loss: 0.000553
[INFO][09:45:50]: [Client #154] Epoch: [3/5][0/10]	Loss: 0.249108
[INFO][09:45:50]: [Client #98] Epoch: [3/5][0/10]	Loss: 0.000376
[INFO][09:45:50]: [Client #202] Epoch: [4/5][0/10]	Loss: 0.483036
[INFO][09:45:50]: [Client #154] Epoch: [4/5][0/10]	Loss: 0.131691
[INFO][09:45:50]: [Client #202] Epoch: [5/5][0/10]	Loss: 0.000012
[INFO][09:45:50]: [Client #98] Epoch: [4/5][0/10]	Loss: 0.053460
[INFO][09:45:50]: [Client #154] Epoch: [5/5][0/10]	Loss: 0.109088
[INFO][09:45:50]: [Client #202] Model saved to /data/ykang/plato/results/test/model/lenet5_202_1127978.pth.
[INFO][09:45:50]: [Client #154] Model saved to /data/ykang/plato/results/test/model/lenet5_154_1127979.pth.
[INFO][09:45:50]: [Client #98] Epoch: [5/5][0/10]	Loss: 0.030317
[INFO][09:45:50]: [Client #98] Model saved to /data/ykang/plato/results/test/model/lenet5_98_1127977.pth.
[INFO][09:45:51]: [Client #202] Loading a model from /data/ykang/plato/results/test/model/lenet5_202_1127978.pth.
[INFO][09:45:51]: [Client #202] Model trained.
[INFO][09:45:51]: [Client #202] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:45:51]: [Server #1127936] Received 0.24 MB of payload data from client #202 (simulated).
[INFO][09:45:51]: [Client #154] Loading a model from /data/ykang/plato/results/test/model/lenet5_154_1127979.pth.
[INFO][09:45:51]: [Client #154] Model trained.
[INFO][09:45:51]: [Client #154] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:45:51]: [Server #1127936] Received 0.24 MB of payload data from client #154 (simulated).
[INFO][09:45:51]: [Client #98] Loading a model from /data/ykang/plato/results/test/model/lenet5_98_1127977.pth.
[INFO][09:45:51]: [Client #98] Model trained.
[INFO][09:45:51]: [Client #98] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:45:51]: [Server #1127936] Received 0.24 MB of payload data from client #98 (simulated).
[INFO][09:45:51]: [Server #1127936] Selecting client #41 for training.
[INFO][09:45:51]: [Server #1127936] Sending the current model to client #41 (simulated).
[INFO][09:45:51]: [Server #1127936] Sending 0.24 MB of payload data to client #41 (simulated).
[INFO][09:45:51]: [Server #1127936] Selecting client #457 for training.
[INFO][09:45:51]: [Server #1127936] Sending the current model to client #457 (simulated).
[INFO][09:45:51]: [Server #1127936] Sending 0.24 MB of payload data to client #457 (simulated).
[INFO][09:45:51]: [Server #1127936] Selecting client #189 for training.
[INFO][09:45:51]: [Server #1127936] Sending the current model to client #189 (simulated).
[INFO][09:45:51]: [Client #41] Selected by the server.
[INFO][09:45:51]: [Client #41] Loading its data source...
[INFO][09:45:51]: [Client #41] Dataset size: 60000
[INFO][09:45:51]: [Client #41] Sampler: noniid
[INFO][09:45:51]: [Server #1127936] Sending 0.24 MB of payload data to client #189 (simulated).
[INFO][09:45:51]: [Client #457] Selected by the server.
[INFO][09:45:51]: [Client #457] Loading its data source...
[INFO][09:45:51]: [Client #189] Selected by the server.
[INFO][09:45:51]: [Client #457] Dataset size: 60000
[INFO][09:45:51]: [Client #189] Loading its data source...
[INFO][09:45:51]: [Client #457] Sampler: noniid
[INFO][09:45:51]: [Client #189] Dataset size: 60000
[INFO][09:45:51]: [Client #189] Sampler: noniid
[INFO][09:45:51]: [Client #41] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:45:51]: [Client #457] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:45:51]: [Client #189] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:45:51]: [93m[1m[Client #457] Started training in communication round #8.[0m
[INFO][09:45:51]: [93m[1m[Client #41] Started training in communication round #8.[0m
[INFO][09:45:51]: [93m[1m[Client #189] Started training in communication round #8.[0m
[INFO][09:45:53]: [Client #189] Loading the dataset.
[INFO][09:45:53]: [Client #41] Loading the dataset.
[INFO][09:45:53]: [Client #457] Loading the dataset.
[INFO][09:45:59]: [Client #189] Epoch: [1/5][0/10]	Loss: 0.521846
[INFO][09:45:59]: [Client #457] Epoch: [1/5][0/10]	Loss: 3.150334
[INFO][09:45:59]: [Client #189] Epoch: [2/5][0/10]	Loss: 0.000029
[INFO][09:45:59]: [Client #41] Epoch: [1/5][0/10]	Loss: 2.937978
[INFO][09:45:59]: [Client #457] Epoch: [2/5][0/10]	Loss: 0.187158
[INFO][09:45:59]: [Client #41] Epoch: [2/5][0/10]	Loss: 0.271122
[INFO][09:45:59]: [Client #189] Epoch: [3/5][0/10]	Loss: 0.025388
[INFO][09:45:59]: [Client #457] Epoch: [3/5][0/10]	Loss: 0.009825
[INFO][09:45:59]: [Client #189] Epoch: [4/5][0/10]	Loss: 0.008777
[INFO][09:45:59]: [Client #41] Epoch: [3/5][0/10]	Loss: 0.375643
[INFO][09:46:00]: [Client #457] Epoch: [4/5][0/10]	Loss: 0.001457
[INFO][09:46:00]: [Client #189] Epoch: [5/5][0/10]	Loss: 0.000127
[INFO][09:46:00]: [Client #41] Epoch: [4/5][0/10]	Loss: 0.178779
[INFO][09:46:00]: [Client #189] Model saved to /data/ykang/plato/results/test/model/lenet5_189_1127979.pth.
[INFO][09:46:00]: [Client #457] Epoch: [5/5][0/10]	Loss: 0.335474
[INFO][09:46:00]: [Client #41] Epoch: [5/5][0/10]	Loss: 0.147546
[INFO][09:46:00]: [Client #457] Model saved to /data/ykang/plato/results/test/model/lenet5_457_1127978.pth.
[INFO][09:46:00]: [Client #41] Model saved to /data/ykang/plato/results/test/model/lenet5_41_1127977.pth.
[INFO][09:46:00]: [Client #189] Loading a model from /data/ykang/plato/results/test/model/lenet5_189_1127979.pth.
[INFO][09:46:00]: [Client #189] Model trained.
[INFO][09:46:00]: [Client #189] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:46:00]: [Server #1127936] Received 0.24 MB of payload data from client #189 (simulated).
[INFO][09:46:01]: [Client #457] Loading a model from /data/ykang/plato/results/test/model/lenet5_457_1127978.pth.
[INFO][09:46:01]: [Client #457] Model trained.
[INFO][09:46:01]: [Client #457] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:46:01]: [Server #1127936] Received 0.24 MB of payload data from client #457 (simulated).
[INFO][09:46:01]: [Client #41] Loading a model from /data/ykang/plato/results/test/model/lenet5_41_1127977.pth.
[INFO][09:46:01]: [Client #41] Model trained.
[INFO][09:46:01]: [Client #41] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:46:01]: [Server #1127936] Received 0.24 MB of payload data from client #41 (simulated).
[INFO][09:46:01]: [Server #1127936] Selecting client #270 for training.
[INFO][09:46:01]: [Server #1127936] Sending the current model to client #270 (simulated).
[INFO][09:46:01]: [Server #1127936] Sending 0.24 MB of payload data to client #270 (simulated).
[INFO][09:46:01]: [Client #270] Selected by the server.
[INFO][09:46:01]: [Client #270] Loading its data source...
[INFO][09:46:01]: [Client #270] Dataset size: 60000
[INFO][09:46:01]: [Client #270] Sampler: noniid
[INFO][09:46:01]: [Client #270] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:46:01]: [93m[1m[Client #270] Started training in communication round #8.[0m
[INFO][09:46:02]: [Client #270] Loading the dataset.
[INFO][09:46:08]: [Client #270] Epoch: [1/5][0/10]	Loss: 0.340000
[INFO][09:46:08]: [Client #270] Epoch: [2/5][0/10]	Loss: 0.000200
[INFO][09:46:08]: [Client #270] Epoch: [3/5][0/10]	Loss: 0.000011
[INFO][09:46:08]: [Client #270] Epoch: [4/5][0/10]	Loss: 0.001728
[INFO][09:46:08]: [Client #270] Epoch: [5/5][0/10]	Loss: 0.003309
[INFO][09:46:08]: [Client #270] Model saved to /data/ykang/plato/results/test/model/lenet5_270_1127977.pth.
[INFO][09:46:09]: [Client #270] Loading a model from /data/ykang/plato/results/test/model/lenet5_270_1127977.pth.
[INFO][09:46:09]: [Client #270] Model trained.
[INFO][09:46:09]: [Client #270] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:46:09]: [Server #1127936] Received 0.24 MB of payload data from client #270 (simulated).
[INFO][09:46:09]: [Server #1127936] Adding client #319 to the list of clients for aggregation.
[INFO][09:46:09]: [Server #1127936] Adding client #75 to the list of clients for aggregation.
[INFO][09:46:09]: [Server #1127936] Adding client #113 to the list of clients for aggregation.
[INFO][09:46:09]: [Server #1127936] Adding client #88 to the list of clients for aggregation.
[INFO][09:46:09]: [Server #1127936] Adding client #363 to the list of clients for aggregation.
[INFO][09:46:09]: [Server #1127936] Adding client #97 to the list of clients for aggregation.
[INFO][09:46:09]: [Server #1127936] Adding client #423 to the list of clients for aggregation.
[INFO][09:46:09]: [Server #1127936] Adding client #154 to the list of clients for aggregation.
[INFO][09:46:09]: [Server #1127936] Adding client #397 to the list of clients for aggregation.
[INFO][09:46:09]: [Server #1127936] Adding client #270 to the list of clients for aggregation.
[INFO][09:46:09]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 111, 112, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.05551476 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.08216481 0.         0.
 0.         0.         0.         0.         0.         0.
 0.16898315 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.08143491 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.11027338 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.0173607
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.13925549 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.09590969 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.03229835 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.05357576 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 2. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.05551476 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.08216481 0.         0.
 0.         0.         0.         0.         0.         0.
 0.16898315 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.08143491 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.11027338 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.0173607
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.13925549 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.09590969 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.03229835 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.05357576 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][09:46:11]: [Server #1127936] Global model accuracy: 66.53%

[INFO][09:46:11]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_8.pth.
[INFO][09:46:11]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_8.pth.
[INFO][09:46:11]: [93m[1m
[Server #1127936] Starting round 9/100.[0m
[0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002
 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8870e+00  6e-04  1e-08  1e-08
 5:  6.8876e+00  6.8873e+00  3e-04  4e-09  4e-09
 6:  6.8875e+00  6.8873e+00  2e-04  2e-08  5e-09
 7:  6.8875e+00  6.8873e+00  2e-04  2e-08  5e-09
 8:  6.8874e+00  6.8873e+00  9e-05  2e-07  4e-08
 9:  6.8874e+00  6.8873e+00  4e-05  1e-07  3e-08
10:  6.8873e+00  6.8873e+00  2e-06  4e-08  1e-08
Optimal solution found.
The calculated probability is:  [2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 4.21482996e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 9.87163320e-01 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 8.29674143e-04 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 6.00337432e-04
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28152689e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28403006e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.29928936e-04 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 7.96122598e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28387336e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28348743e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05 2.28409374e-05 2.28409374e-05
 2.28409374e-05 2.28409374e-05]
current clients pool:  [INFO][09:46:11]: [Server #1127936] Selected clients: [ 88 492  97 271  26 380 246 356  86 275]
[INFO][09:46:11]: [Server #1127936] Selecting client #88 for training.
[INFO][09:46:11]: [Server #1127936] Sending the current model to client #88 (simulated).
[INFO][09:46:11]: [Server #1127936] Sending 0.24 MB of payload data to client #88 (simulated).
[INFO][09:46:11]: [Server #1127936] Selecting client #492 for training.
[INFO][09:46:11]: [Server #1127936] Sending the current model to client #492 (simulated).
[INFO][09:46:11]: [Server #1127936] Sending 0.24 MB of payload data to client #492 (simulated).
[INFO][09:46:11]: [Server #1127936] Selecting client #97 for training.
[INFO][09:46:11]: [Server #1127936] Sending the current model to client #97 (simulated).
[INFO][09:46:11]: [Client #88] Selected by the server.
[INFO][09:46:11]: [Client #88] Loading its data source...
[INFO][09:46:11]: [Client #88] Dataset size: 60000
[INFO][09:46:11]: [Client #88] Sampler: noniid
[INFO][09:46:11]: [Server #1127936] Sending 0.24 MB of payload data to client #97 (simulated).
[INFO][09:46:11]: [Client #492] Selected by the server.
[INFO][09:46:11]: [Client #492] Loading its data source...
[INFO][09:46:11]: [Client #492] Dataset size: 60000
[INFO][09:46:11]: [Client #492] Sampler: noniid
[INFO][09:46:11]: [Client #97] Selected by the server.
[INFO][09:46:11]: [Client #97] Loading its data source...
[INFO][09:46:11]: [Client #97] Dataset size: 60000
[INFO][09:46:11]: [Client #97] Sampler: noniid
[INFO][09:46:11]: [Client #88] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:46:11]: [Client #97] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:46:11]: [Client #492] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:46:11]: [93m[1m[Client #88] Started training in communication round #9.[0m
[INFO][09:46:11]: [93m[1m[Client #492] Started training in communication round #9.[0m
[INFO][09:46:11]: [93m[1m[Client #97] Started training in communication round #9.[0m
[INFO][09:46:13]: [Client #97] Loading the dataset.
[INFO][09:46:13]: [Client #88] Loading the dataset.
[INFO][09:46:13]: [Client #492] Loading the dataset.
[INFO][09:46:19]: [Client #88] Epoch: [1/5][0/10]	Loss: 1.326119
[INFO][09:46:19]: [Client #97] Epoch: [1/5][0/10]	Loss: 0.662983
[INFO][09:46:19]: [Client #88] Epoch: [2/5][0/10]	Loss: 0.121323
[INFO][09:46:19]: [Client #492] Epoch: [1/5][0/10]	Loss: 1.926464
[INFO][09:46:19]: [Client #97] Epoch: [2/5][0/10]	Loss: 0.151079
[INFO][09:46:19]: [Client #97] Epoch: [3/5][0/10]	Loss: 0.019618
[INFO][09:46:19]: [Client #88] Epoch: [3/5][0/10]	Loss: 0.112926
[INFO][09:46:19]: [Client #492] Epoch: [2/5][0/10]	Loss: 0.166895
[INFO][09:46:20]: [Client #97] Epoch: [4/5][0/10]	Loss: 0.000020
[INFO][09:46:20]: [Client #492] Epoch: [3/5][0/10]	Loss: 0.015018
[INFO][09:46:20]: [Client #88] Epoch: [4/5][0/10]	Loss: 0.251431
[INFO][09:46:20]: [Client #97] Epoch: [5/5][0/10]	Loss: 0.001611
[INFO][09:46:20]: [Client #88] Epoch: [5/5][0/10]	Loss: 0.219792
[INFO][09:46:20]: [Client #492] Epoch: [4/5][0/10]	Loss: 0.065247
[INFO][09:46:20]: [Client #97] Model saved to /data/ykang/plato/results/test/model/lenet5_97_1127979.pth.
[INFO][09:46:20]: [Client #88] Model saved to /data/ykang/plato/results/test/model/lenet5_88_1127977.pth.
[INFO][09:46:20]: [Client #492] Epoch: [5/5][0/10]	Loss: 0.171542
[INFO][09:46:20]: [Client #492] Model saved to /data/ykang/plato/results/test/model/lenet5_492_1127978.pth.
[INFO][09:46:21]: [Client #97] Loading a model from /data/ykang/plato/results/test/model/lenet5_97_1127979.pth.
[INFO][09:46:21]: [Client #97] Model trained.
[INFO][09:46:21]: [Client #97] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:46:21]: [Server #1127936] Received 0.24 MB of payload data from client #97 (simulated).
[INFO][09:46:21]: [Client #88] Loading a model from /data/ykang/plato/results/test/model/lenet5_88_1127977.pth.
[INFO][09:46:21]: [Client #88] Model trained.
[INFO][09:46:21]: [Client #88] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:46:21]: [Server #1127936] Received 0.24 MB of payload data from client #88 (simulated).
[INFO][09:46:21]: [Client #492] Loading a model from /data/ykang/plato/results/test/model/lenet5_492_1127978.pth.
[INFO][09:46:21]: [Client #492] Model trained.
[INFO][09:46:21]: [Client #492] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:46:21]: [Server #1127936] Received 0.24 MB of payload data from client #492 (simulated).
[INFO][09:46:21]: [Server #1127936] Selecting client #271 for training.
[INFO][09:46:21]: [Server #1127936] Sending the current model to client #271 (simulated).
[INFO][09:46:21]: [Server #1127936] Sending 0.24 MB of payload data to client #271 (simulated).
[INFO][09:46:21]: [Server #1127936] Selecting client #26 for training.
[INFO][09:46:21]: [Server #1127936] Sending the current model to client #26 (simulated).
[INFO][09:46:21]: [Server #1127936] Sending 0.24 MB of payload data to client #26 (simulated).
[INFO][09:46:21]: [Server #1127936] Selecting client #380 for training.
[INFO][09:46:21]: [Server #1127936] Sending the current model to client #380 (simulated).
[INFO][09:46:21]: [Client #271] Selected by the server.
[INFO][09:46:21]: [Client #271] Loading its data source...
[INFO][09:46:21]: [Client #271] Dataset size: 60000
[INFO][09:46:21]: [Client #271] Sampler: noniid
[INFO][09:46:21]: [Server #1127936] Sending 0.24 MB of payload data to client #380 (simulated).
[INFO][09:46:21]: [Client #26] Selected by the server.
[INFO][09:46:21]: [Client #26] Loading its data source...
[INFO][09:46:21]: [Client #26] Dataset size: 60000
[INFO][09:46:21]: [Client #380] Selected by the server.
[INFO][09:46:21]: [Client #26] Sampler: noniid
[INFO][09:46:21]: [Client #380] Loading its data source...
[INFO][09:46:21]: [Client #380] Dataset size: 60000
[INFO][09:46:21]: [Client #380] Sampler: noniid
[INFO][09:46:21]: [Client #271] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:46:21]: [Client #26] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:46:21]: [93m[1m[Client #271] Started training in communication round #9.[0m
[INFO][09:46:21]: [Client #380] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:46:21]: [93m[1m[Client #380] Started training in communication round #9.[0m
[INFO][09:46:21]: [93m[1m[Client #26] Started training in communication round #9.[0m
[INFO][09:46:23]: [Client #26] Loading the dataset.
[INFO][09:46:23]: [Client #380] Loading the dataset.
[INFO][09:46:23]: [Client #271] Loading the dataset.
[INFO][09:46:29]: [Client #26] Epoch: [1/5][0/10]	Loss: 0.341060
[INFO][09:46:29]: [Client #380] Epoch: [1/5][0/10]	Loss: 0.475243
[INFO][09:46:29]: [Client #26] Epoch: [2/5][0/10]	Loss: 0.145372
[INFO][09:46:29]: [Client #271] Epoch: [1/5][0/10]	Loss: 2.079250
[INFO][09:46:29]: [Client #380] Epoch: [2/5][0/10]	Loss: 0.314531
[INFO][09:46:29]: [Client #26] Epoch: [3/5][0/10]	Loss: 0.093047
[INFO][09:46:29]: [Client #271] Epoch: [2/5][0/10]	Loss: 0.346826
[INFO][09:46:29]: [Client #380] Epoch: [3/5][0/10]	Loss: 0.000438
[INFO][09:46:29]: [Client #26] Epoch: [4/5][0/10]	Loss: 0.056684
[INFO][09:46:29]: [Client #271] Epoch: [3/5][0/10]	Loss: 0.000016
[INFO][09:46:29]: [Client #380] Epoch: [4/5][0/10]	Loss: 0.000549
[INFO][09:46:29]: [Client #26] Epoch: [5/5][0/10]	Loss: 0.004130
[INFO][09:46:29]: [Client #271] Epoch: [4/5][0/10]	Loss: 0.022134
[INFO][09:46:29]: [Client #26] Model saved to /data/ykang/plato/results/test/model/lenet5_26_1127978.pth.
[INFO][09:46:29]: [Client #380] Epoch: [5/5][0/10]	Loss: 0.071522
[INFO][09:46:29]: [Client #271] Epoch: [5/5][0/10]	Loss: 0.001350
[INFO][09:46:29]: [Client #380] Model saved to /data/ykang/plato/results/test/model/lenet5_380_1127979.pth.
[INFO][09:46:29]: [Client #271] Model saved to /data/ykang/plato/results/test/model/lenet5_271_1127977.pth.
[INFO][09:46:30]: [Client #26] Loading a model from /data/ykang/plato/results/test/model/lenet5_26_1127978.pth.
[INFO][09:46:30]: [Client #26] Model trained.
[INFO][09:46:30]: [Client #26] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:46:30]: [Server #1127936] Received 0.24 MB of payload data from client #26 (simulated).
[INFO][09:46:30]: [Client #380] Loading a model from /data/ykang/plato/results/test/model/lenet5_380_1127979.pth.
[INFO][09:46:30]: [Client #380] Model trained.
[INFO][09:46:30]: [Client #380] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:46:30]: [Server #1127936] Received 0.24 MB of payload data from client #380 (simulated).
[INFO][09:46:30]: [Client #271] Loading a model from /data/ykang/plato/results/test/model/lenet5_271_1127977.pth.
[INFO][09:46:30]: [Client #271] Model trained.
[INFO][09:46:30]: [Client #271] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:46:30]: [Server #1127936] Received 0.24 MB of payload data from client #271 (simulated).
[INFO][09:46:30]: [Server #1127936] Selecting client #246 for training.
[INFO][09:46:30]: [Server #1127936] Sending the current model to client #246 (simulated).
[INFO][09:46:30]: [Server #1127936] Sending 0.24 MB of payload data to client #246 (simulated).
[INFO][09:46:30]: [Server #1127936] Selecting client #356 for training.
[INFO][09:46:30]: [Server #1127936] Sending the current model to client #356 (simulated).
[INFO][09:46:30]: [Server #1127936] Sending 0.24 MB of payload data to client #356 (simulated).
[INFO][09:46:30]: [Server #1127936] Selecting client #86 for training.
[INFO][09:46:30]: [Server #1127936] Sending the current model to client #86 (simulated).
[INFO][09:46:30]: [Client #246] Selected by the server.
[INFO][09:46:30]: [Client #246] Loading its data source...
[INFO][09:46:30]: [Client #246] Dataset size: 60000
[INFO][09:46:30]: [Client #246] Sampler: noniid
[INFO][09:46:30]: [Server #1127936] Sending 0.24 MB of payload data to client #86 (simulated).
[INFO][09:46:30]: [Client #246] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:46:30]: [Client #356] Selected by the server.
[INFO][09:46:30]: [Client #356] Loading its data source...
[INFO][09:46:30]: [Client #356] Dataset size: 60000
[INFO][09:46:30]: [Client #356] Sampler: noniid
[INFO][09:46:30]: [Client #86] Selected by the server.
[INFO][09:46:30]: [Client #86] Loading its data source...
[INFO][09:46:30]: [Client #86] Dataset size: 60000
[INFO][09:46:30]: [Client #86] Sampler: noniid
[INFO][09:46:30]: [Client #86] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:46:30]: [Client #356] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:46:30]: [93m[1m[Client #86] Started training in communication round #9.[0m
[INFO][09:46:30]: [93m[1m[Client #246] Started training in communication round #9.[0m
[INFO][09:46:30]: [93m[1m[Client #356] Started training in communication round #9.[0m
[INFO][09:46:32]: [Client #86] Loading the dataset.
[INFO][09:46:32]: [Client #356] Loading the dataset.
[INFO][09:46:32]: [Client #246] Loading the dataset.
[INFO][09:46:38]: [Client #246] Epoch: [1/5][0/10]	Loss: 0.147175
[INFO][09:46:38]: [Client #356] Epoch: [1/5][0/10]	Loss: 0.147175
[INFO][09:46:38]: [Client #86] Epoch: [1/5][0/10]	Loss: 0.845343
[INFO][09:46:38]: [Client #246] Epoch: [2/5][0/10]	Loss: 0.009238
[INFO][09:46:38]: [Client #356] Epoch: [2/5][0/10]	Loss: 0.013843
[INFO][09:46:38]: [Client #86] Epoch: [2/5][0/10]	Loss: 0.001821
[INFO][09:46:38]: [Client #246] Epoch: [3/5][0/10]	Loss: 0.011626
[INFO][09:46:38]: [Client #86] Epoch: [3/5][0/10]	Loss: 0.000344
[INFO][09:46:38]: [Client #356] Epoch: [3/5][0/10]	Loss: 0.003402
[INFO][09:46:38]: [Client #246] Epoch: [4/5][0/10]	Loss: 0.386560
[INFO][09:46:38]: [Client #86] Epoch: [4/5][0/10]	Loss: 0.000002
[INFO][09:46:38]: [Client #356] Epoch: [4/5][0/10]	Loss: 0.011579
[INFO][09:46:38]: [Client #246] Epoch: [5/5][0/10]	Loss: 0.002094
[INFO][09:46:38]: [Client #86] Epoch: [5/5][0/10]	Loss: 0.074030
[INFO][09:46:39]: [Client #246] Model saved to /data/ykang/plato/results/test/model/lenet5_246_1127977.pth.
[INFO][09:46:39]: [Client #356] Epoch: [5/5][0/10]	Loss: 0.001058
[INFO][09:46:39]: [Client #86] Model saved to /data/ykang/plato/results/test/model/lenet5_86_1127979.pth.
[INFO][09:46:39]: [Client #356] Model saved to /data/ykang/plato/results/test/model/lenet5_356_1127978.pth.
[INFO][09:46:39]: [Client #86] Loading a model from /data/ykang/plato/results/test/model/lenet5_86_1127979.pth.
[INFO][09:46:39]: [Client #86] Model trained.
[INFO][09:46:39]: [Client #86] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:46:39]: [Server #1127936] Received 0.24 MB of payload data from client #86 (simulated).
[INFO][09:46:39]: [Client #246] Loading a model from /data/ykang/plato/results/test/model/lenet5_246_1127977.pth.
[INFO][09:46:39]: [Client #246] Model trained.
[INFO][09:46:39]: [Client #246] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:46:39]: [Server #1127936] Received 0.24 MB of payload data from client #246 (simulated).
[INFO][09:46:39]: [Client #356] Loading a model from /data/ykang/plato/results/test/model/lenet5_356_1127978.pth.
[INFO][09:46:39]: [Client #356] Model trained.
[INFO][09:46:40]: [Client #356] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:46:40]: [Server #1127936] Received 0.24 MB of payload data from client #356 (simulated).
[INFO][09:46:40]: [Server #1127936] Selecting client #275 for training.
[INFO][09:46:40]: [Server #1127936] Sending the current model to client #275 (simulated).
[INFO][09:46:40]: [Server #1127936] Sending 0.24 MB of payload data to client #275 (simulated).
[INFO][09:46:40]: [Client #275] Selected by the server.
[INFO][09:46:40]: [Client #275] Loading its data source...
[INFO][09:46:40]: [Client #275] Dataset size: 60000
[INFO][09:46:40]: [Client #275] Sampler: noniid
[INFO][09:46:40]: [Client #275] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:46:40]: [93m[1m[Client #275] Started training in communication round #9.[0m
[INFO][09:46:41]: [Client #275] Loading the dataset.
[INFO][09:46:47]: [Client #275] Epoch: [1/5][0/10]	Loss: 0.249267
[INFO][09:46:47]: [Client #275] Epoch: [2/5][0/10]	Loss: 0.041394
[INFO][09:46:47]: [Client #275] Epoch: [3/5][0/10]	Loss: 0.203922
[INFO][09:46:47]: [Client #275] Epoch: [4/5][0/10]	Loss: 0.076061
[INFO][09:46:47]: [Client #275] Epoch: [5/5][0/10]	Loss: 0.016255
[INFO][09:46:47]: [Client #275] Model saved to /data/ykang/plato/results/test/model/lenet5_275_1127977.pth.
[INFO][09:46:48]: [Client #275] Loading a model from /data/ykang/plato/results/test/model/lenet5_275_1127977.pth.
[INFO][09:46:48]: [Client #275] Model trained.
[INFO][09:46:48]: [Client #275] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:46:48]: [Server #1127936] Received 0.24 MB of payload data from client #275 (simulated).
[INFO][09:46:48]: [Server #1127936] Adding client #189 to the list of clients for aggregation.
[INFO][09:46:48]: [Server #1127936] Adding client #202 to the list of clients for aggregation.
[INFO][09:46:48]: [Server #1127936] Adding client #98 to the list of clients for aggregation.
[INFO][09:46:48]: [Server #1127936] Adding client #33 to the list of clients for aggregation.
[INFO][09:46:48]: [Server #1127936] Adding client #108 to the list of clients for aggregation.
[INFO][09:46:48]: [Server #1127936] Adding client #110 to the list of clients for aggregation.
[INFO][09:46:48]: [Server #1127936] Adding client #200 to the list of clients for aggregation.
[INFO][09:46:48]: [Server #1127936] Adding client #457 to the list of clients for aggregation.
[INFO][09:46:48]: [Server #1127936] Adding client #380 to the list of clients for aggregation.
[INFO][09:46:48]: [Server #1127936] Adding client #356 to the list of clients for aggregation.
[INFO][09:46:48]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 201, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.07604319 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.04954777 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.10388781
 0.         0.24414881 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.04208412 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.03978694 0.         0.0451887  0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.02816003 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.04324525 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.20743679 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 2. 0. 6. 0. 1. 2. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 2. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.07604319 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.04954777 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.10388781
 0.         0.24414881 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.04208412 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.03978694 0.         0.0451887  0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.02816003 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.04324525 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.20743679 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][09:46:50]: [Server #1127936] Global model accuracy: 74.78%

[INFO][09:46:50]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_9.pth.
[INFO][09:46:50]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_9.pth.
[INFO][09:46:50]: [93m[1m
[Server #1127936] Starting round 10/100.[0m
[0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002
 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8875e+00  6.8858e+00  2e-03  3e-08  3e-08
 5:  6.8875e+00  6.8861e+00  1e-03  2e-08  2e-08
 6:  6.8874e+00  6.8859e+00  1e-03  7e-07  1e-06
 7:  6.8873e+00  6.8864e+00  9e-04  5e-07  8e-07
 8:  6.8871e+00  6.8866e+00  5e-04  1e-06  2e-06
 9:  6.8869e+00  6.8867e+00  2e-04  2e-06  3e-06
10:  6.8868e+00  6.8868e+00  7e-05  1e-06  2e-06
11:  6.8868e+00  6.8868e+00  1e-05  3e-07  4e-07
12:  6.8868e+00  6.8868e+00  5e-07  2e-08  3e-08
Optimal solution found.
The calculated probability is:  [2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.27356348e-05
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 3.87282345e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 1.69932299e-05 2.82798856e-06
 9.98564219e-01 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 3.68283203e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 4.80187218e-06 2.82798856e-06
 3.76018377e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82785589e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82767568e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 1.68443317e-05
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06 2.82798856e-06 2.82798856e-06
 2.82798856e-06 2.82798856e-06]
current clients pool:  [INFO][09:46:50]: [Server #1127936] Selected clients: [110 361 494 135   2 466 432 367 257 356]
[INFO][09:46:50]: [Server #1127936] Selecting client #110 for training.
[INFO][09:46:50]: [Server #1127936] Sending the current model to client #110 (simulated).
[INFO][09:46:50]: [Server #1127936] Sending 0.24 MB of payload data to client #110 (simulated).
[INFO][09:46:50]: [Server #1127936] Selecting client #361 for training.
[INFO][09:46:50]: [Server #1127936] Sending the current model to client #361 (simulated).
[INFO][09:46:50]: [Server #1127936] Sending 0.24 MB of payload data to client #361 (simulated).
[INFO][09:46:50]: [Server #1127936] Selecting client #494 for training.
[INFO][09:46:50]: [Server #1127936] Sending the current model to client #494 (simulated).
[INFO][09:46:50]: [Client #110] Selected by the server.
[INFO][09:46:50]: [Client #110] Loading its data source...
[INFO][09:46:50]: [Client #110] Dataset size: 60000
[INFO][09:46:50]: [Client #110] Sampler: noniid
[INFO][09:46:50]: [Server #1127936] Sending 0.24 MB of payload data to client #494 (simulated).
[INFO][09:46:50]: [Client #494] Selected by the server.
[INFO][09:46:50]: [Client #494] Loading its data source...
[INFO][09:46:50]: [Client #494] Dataset size: 60000
[INFO][09:46:50]: [Client #110] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:46:50]: [Client #494] Sampler: noniid
[INFO][09:46:50]: [Client #494] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:46:50]: [93m[1m[Client #110] Started training in communication round #10.[0m
[INFO][09:46:50]: [Client #361] Selected by the server.
[INFO][09:46:50]: [Client #361] Loading its data source...
[INFO][09:46:50]: [Client #361] Dataset size: 60000
[INFO][09:46:50]: [Client #361] Sampler: noniid
[INFO][09:46:50]: [Client #361] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:46:50]: [93m[1m[Client #494] Started training in communication round #10.[0m
[INFO][09:46:50]: [93m[1m[Client #361] Started training in communication round #10.[0m
[INFO][09:46:52]: [Client #110] Loading the dataset.
[INFO][09:46:52]: [Client #494] Loading the dataset.
[INFO][09:46:52]: [Client #361] Loading the dataset.
[INFO][09:46:58]: [Client #110] Epoch: [1/5][0/10]	Loss: 1.286158
[INFO][09:46:58]: [Client #494] Epoch: [1/5][0/10]	Loss: 1.984844
[INFO][09:46:58]: [Client #361] Epoch: [1/5][0/10]	Loss: 0.806555
[INFO][09:46:58]: [Client #110] Epoch: [2/5][0/10]	Loss: 2.655530
[INFO][09:46:58]: [Client #361] Epoch: [2/5][0/10]	Loss: 0.153700
[INFO][09:46:58]: [Client #494] Epoch: [2/5][0/10]	Loss: 1.006584
[INFO][09:46:58]: [Client #110] Epoch: [3/5][0/10]	Loss: 0.306323
[INFO][09:46:59]: [Client #494] Epoch: [3/5][0/10]	Loss: 0.061744
[INFO][09:46:59]: [Client #361] Epoch: [3/5][0/10]	Loss: 0.442480
[INFO][09:46:59]: [Client #110] Epoch: [4/5][0/10]	Loss: 0.027150
[INFO][09:46:59]: [Client #361] Epoch: [4/5][0/10]	Loss: 0.099857
[INFO][09:46:59]: [Client #494] Epoch: [4/5][0/10]	Loss: 0.008960
[INFO][09:46:59]: [Client #110] Epoch: [5/5][0/10]	Loss: 0.000154
[INFO][09:46:59]: [Client #361] Epoch: [5/5][0/10]	Loss: 0.191267
[INFO][09:46:59]: [Client #494] Epoch: [5/5][0/10]	Loss: 0.087218
[INFO][09:46:59]: [Client #110] Model saved to /data/ykang/plato/results/test/model/lenet5_110_1127977.pth.
[INFO][09:46:59]: [Client #361] Model saved to /data/ykang/plato/results/test/model/lenet5_361_1127978.pth.
[INFO][09:46:59]: [Client #494] Model saved to /data/ykang/plato/results/test/model/lenet5_494_1127979.pth.
[INFO][09:47:00]: [Client #110] Loading a model from /data/ykang/plato/results/test/model/lenet5_110_1127977.pth.
[INFO][09:47:00]: [Client #110] Model trained.
[INFO][09:47:00]: [Client #110] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:47:00]: [Server #1127936] Received 0.24 MB of payload data from client #110 (simulated).
[INFO][09:47:00]: [Client #361] Loading a model from /data/ykang/plato/results/test/model/lenet5_361_1127978.pth.
[INFO][09:47:00]: [Client #361] Model trained.
[INFO][09:47:00]: [Client #361] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:47:00]: [Server #1127936] Received 0.24 MB of payload data from client #361 (simulated).
[INFO][09:47:00]: [Client #494] Loading a model from /data/ykang/plato/results/test/model/lenet5_494_1127979.pth.
[INFO][09:47:00]: [Client #494] Model trained.
[INFO][09:47:00]: [Client #494] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:47:00]: [Server #1127936] Received 0.24 MB of payload data from client #494 (simulated).
[INFO][09:47:00]: [Server #1127936] Selecting client #135 for training.
[INFO][09:47:00]: [Server #1127936] Sending the current model to client #135 (simulated).
[INFO][09:47:00]: [Server #1127936] Sending 0.24 MB of payload data to client #135 (simulated).
[INFO][09:47:00]: [Server #1127936] Selecting client #2 for training.
[INFO][09:47:00]: [Server #1127936] Sending the current model to client #2 (simulated).
[INFO][09:47:00]: [Server #1127936] Sending 0.24 MB of payload data to client #2 (simulated).
[INFO][09:47:00]: [Server #1127936] Selecting client #466 for training.
[INFO][09:47:00]: [Server #1127936] Sending the current model to client #466 (simulated).
[INFO][09:47:00]: [Client #135] Selected by the server.
[INFO][09:47:00]: [Client #135] Loading its data source...
[INFO][09:47:00]: [Client #135] Dataset size: 60000
[INFO][09:47:00]: [Client #135] Sampler: noniid
[INFO][09:47:00]: [Server #1127936] Sending 0.24 MB of payload data to client #466 (simulated).
[INFO][09:47:00]: [Client #2] Selected by the server.
[INFO][09:47:00]: [Client #466] Selected by the server.
[INFO][09:47:00]: [Client #2] Loading its data source...
[INFO][09:47:00]: [Client #466] Loading its data source...
[INFO][09:47:00]: [Client #2] Dataset size: 60000
[INFO][09:47:00]: [Client #466] Dataset size: 60000
[INFO][09:47:00]: [Client #2] Sampler: noniid
[INFO][09:47:00]: [Client #466] Sampler: noniid
[INFO][09:47:00]: [Client #135] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:47:00]: [Client #466] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:47:00]: [Client #2] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:47:00]: [93m[1m[Client #135] Started training in communication round #10.[0m
[INFO][09:47:00]: [93m[1m[Client #466] Started training in communication round #10.[0m
[INFO][09:47:00]: [93m[1m[Client #2] Started training in communication round #10.[0m
[INFO][09:47:02]: [Client #466] Loading the dataset.
[INFO][09:47:02]: [Client #2] Loading the dataset.
[INFO][09:47:02]: [Client #135] Loading the dataset.
[INFO][09:47:08]: [Client #466] Epoch: [1/5][0/10]	Loss: 0.465526
[INFO][09:47:08]: [Client #135] Epoch: [1/5][0/10]	Loss: 0.569802
[INFO][09:47:08]: [Client #466] Epoch: [2/5][0/10]	Loss: 0.000074
[INFO][09:47:08]: [Client #2] Epoch: [1/5][0/10]	Loss: 2.787515
[INFO][09:47:08]: [Client #135] Epoch: [2/5][0/10]	Loss: 0.374866
[INFO][09:47:08]: [Client #466] Epoch: [3/5][0/10]	Loss: 0.000340
[INFO][09:47:08]: [Client #2] Epoch: [2/5][0/10]	Loss: 0.000001
[INFO][09:47:08]: [Client #135] Epoch: [3/5][0/10]	Loss: 0.000684
[INFO][09:47:08]: [Client #466] Epoch: [4/5][0/10]	Loss: 0.000004
[INFO][09:47:08]: [Client #2] Epoch: [3/5][0/10]	Loss: 0.743411
[INFO][09:47:08]: [Client #135] Epoch: [4/5][0/10]	Loss: 0.240283
[INFO][09:47:08]: [Client #466] Epoch: [5/5][0/10]	Loss: 0.005628
[INFO][09:47:08]: [Client #2] Epoch: [4/5][0/10]	Loss: 0.000620
[INFO][09:47:08]: [Client #135] Epoch: [5/5][0/10]	Loss: 1.069935
[INFO][09:47:08]: [Client #466] Model saved to /data/ykang/plato/results/test/model/lenet5_466_1127979.pth.
[INFO][09:47:08]: [Client #135] Model saved to /data/ykang/plato/results/test/model/lenet5_135_1127977.pth.
[INFO][09:47:08]: [Client #2] Epoch: [5/5][0/10]	Loss: 0.238549
[INFO][09:47:08]: [Client #2] Model saved to /data/ykang/plato/results/test/model/lenet5_2_1127978.pth.
[INFO][09:47:09]: [Client #466] Loading a model from /data/ykang/plato/results/test/model/lenet5_466_1127979.pth.
[INFO][09:47:09]: [Client #466] Model trained.
[INFO][09:47:09]: [Client #466] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:47:09]: [Server #1127936] Received 0.24 MB of payload data from client #466 (simulated).
[INFO][09:47:09]: [Client #2] Loading a model from /data/ykang/plato/results/test/model/lenet5_2_1127978.pth.
[INFO][09:47:09]: [Client #2] Model trained.
[INFO][09:47:09]: [Client #2] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:47:09]: [Server #1127936] Received 0.24 MB of payload data from client #2 (simulated).
[INFO][09:47:09]: [Client #135] Loading a model from /data/ykang/plato/results/test/model/lenet5_135_1127977.pth.
[INFO][09:47:09]: [Client #135] Model trained.
[INFO][09:47:09]: [Client #135] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:47:09]: [Server #1127936] Received 0.24 MB of payload data from client #135 (simulated).
[INFO][09:47:09]: [Server #1127936] Selecting client #432 for training.
[INFO][09:47:09]: [Server #1127936] Sending the current model to client #432 (simulated).
[INFO][09:47:09]: [Server #1127936] Sending 0.24 MB of payload data to client #432 (simulated).
[INFO][09:47:09]: [Server #1127936] Selecting client #367 for training.
[INFO][09:47:09]: [Server #1127936] Sending the current model to client #367 (simulated).
[INFO][09:47:09]: [Server #1127936] Sending 0.24 MB of payload data to client #367 (simulated).
[INFO][09:47:09]: [Server #1127936] Selecting client #257 for training.
[INFO][09:47:09]: [Server #1127936] Sending the current model to client #257 (simulated).
[INFO][09:47:09]: [Client #432] Selected by the server.
[INFO][09:47:09]: [Client #432] Loading its data source...
[INFO][09:47:09]: [Client #432] Dataset size: 60000
[INFO][09:47:09]: [Client #432] Sampler: noniid
[INFO][09:47:09]: [Server #1127936] Sending 0.24 MB of payload data to client #257 (simulated).
[INFO][09:47:09]: [Client #367] Selected by the server.
[INFO][09:47:09]: [Client #367] Loading its data source...
[INFO][09:47:09]: [Client #367] Dataset size: 60000
[INFO][09:47:09]: [Client #257] Selected by the server.
[INFO][09:47:09]: [Client #367] Sampler: noniid
[INFO][09:47:09]: [Client #257] Loading its data source...
[INFO][09:47:09]: [Client #257] Dataset size: 60000
[INFO][09:47:09]: [Client #257] Sampler: noniid
[INFO][09:47:09]: [Client #432] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:47:09]: [Client #367] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:47:09]: [Client #257] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:47:09]: [93m[1m[Client #257] Started training in communication round #10.[0m
[INFO][09:47:09]: [93m[1m[Client #367] Started training in communication round #10.[0m
[INFO][09:47:09]: [93m[1m[Client #432] Started training in communication round #10.[0m
[INFO][09:47:11]: [Client #367] Loading the dataset.
[INFO][09:47:11]: [Client #432] Loading the dataset.
[INFO][09:47:11]: [Client #257] Loading the dataset.
[INFO][09:47:17]: [Client #367] Epoch: [1/5][0/10]	Loss: 1.286158
[INFO][09:47:17]: [Client #432] Epoch: [1/5][0/10]	Loss: 0.724420
[INFO][09:47:17]: [Client #367] Epoch: [2/5][0/10]	Loss: 0.000001
[INFO][09:47:17]: [Client #432] Epoch: [2/5][0/10]	Loss: 0.852238
[INFO][09:47:17]: [Client #257] Epoch: [1/5][0/10]	Loss: 0.794114
[INFO][09:47:17]: [Client #432] Epoch: [3/5][0/10]	Loss: 0.032856
[INFO][09:47:17]: [Client #367] Epoch: [3/5][0/10]	Loss: 0.000000
[INFO][09:47:17]: [Client #257] Epoch: [2/5][0/10]	Loss: 0.686766
[INFO][09:47:17]: [Client #432] Epoch: [4/5][0/10]	Loss: 0.001572
[INFO][09:47:17]: [Client #367] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][09:47:17]: [Client #257] Epoch: [3/5][0/10]	Loss: 0.008302
[INFO][09:47:17]: [Client #367] Epoch: [5/5][0/10]	Loss: 0.383390
[INFO][09:47:17]: [Client #432] Epoch: [5/5][0/10]	Loss: 0.048638
[INFO][09:47:17]: [Client #257] Epoch: [4/5][0/10]	Loss: 0.071111
[INFO][09:47:17]: [Client #367] Model saved to /data/ykang/plato/results/test/model/lenet5_367_1127978.pth.
[INFO][09:47:17]: [Client #432] Model saved to /data/ykang/plato/results/test/model/lenet5_432_1127977.pth.
[INFO][09:47:17]: [Client #257] Epoch: [5/5][0/10]	Loss: 0.185225
[INFO][09:47:17]: [Client #257] Model saved to /data/ykang/plato/results/test/model/lenet5_257_1127979.pth.
[INFO][09:47:18]: [Client #367] Loading a model from /data/ykang/plato/results/test/model/lenet5_367_1127978.pth.
[INFO][09:47:18]: [Client #367] Model trained.
[INFO][09:47:18]: [Client #367] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:47:18]: [Server #1127936] Received 0.24 MB of payload data from client #367 (simulated).
[INFO][09:47:18]: [Client #432] Loading a model from /data/ykang/plato/results/test/model/lenet5_432_1127977.pth.
[INFO][09:47:18]: [Client #432] Model trained.
[INFO][09:47:18]: [Client #432] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:47:18]: [Server #1127936] Received 0.24 MB of payload data from client #432 (simulated).
[INFO][09:47:18]: [Client #257] Loading a model from /data/ykang/plato/results/test/model/lenet5_257_1127979.pth.
[INFO][09:47:18]: [Client #257] Model trained.
[INFO][09:47:18]: [Client #257] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:47:18]: [Server #1127936] Received 0.24 MB of payload data from client #257 (simulated).
[INFO][09:47:18]: [Server #1127936] Selecting client #356 for training.
[INFO][09:47:18]: [Server #1127936] Sending the current model to client #356 (simulated).
[INFO][09:47:18]: [Server #1127936] Sending 0.24 MB of payload data to client #356 (simulated).
[INFO][09:47:18]: [Client #356] Selected by the server.
[INFO][09:47:18]: [Client #356] Loading its data source...
[INFO][09:47:18]: [Client #356] Dataset size: 60000
[INFO][09:47:18]: [Client #356] Sampler: noniid
[INFO][09:47:18]: [Client #356] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:47:18]: [93m[1m[Client #356] Started training in communication round #10.[0m
[INFO][09:47:20]: [Client #356] Loading the dataset.
[INFO][09:47:25]: [Client #356] Epoch: [1/5][0/10]	Loss: 0.271113
[INFO][09:47:26]: [Client #356] Epoch: [2/5][0/10]	Loss: 0.005580
[INFO][09:47:26]: [Client #356] Epoch: [3/5][0/10]	Loss: 0.002833
[INFO][09:47:26]: [Client #356] Epoch: [4/5][0/10]	Loss: 0.010912
[INFO][09:47:26]: [Client #356] Epoch: [5/5][0/10]	Loss: 0.001370
[INFO][09:47:26]: [Client #356] Model saved to /data/ykang/plato/results/test/model/lenet5_356_1127977.pth.
[INFO][09:47:26]: [Client #356] Loading a model from /data/ykang/plato/results/test/model/lenet5_356_1127977.pth.
[INFO][09:47:26]: [Client #356] Model trained.
[INFO][09:47:26]: [Client #356] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:47:26]: [Server #1127936] Received 0.24 MB of payload data from client #356 (simulated).
[INFO][09:47:26]: [Server #1127936] Adding client #271 to the list of clients for aggregation.
[INFO][09:47:26]: [Server #1127936] Adding client #246 to the list of clients for aggregation.
[INFO][09:47:26]: [Server #1127936] Adding client #226 to the list of clients for aggregation.
[INFO][09:47:26]: [Server #1127936] Adding client #26 to the list of clients for aggregation.
[INFO][09:47:26]: [Server #1127936] Adding client #86 to the list of clients for aggregation.
[INFO][09:47:26]: [Server #1127936] Adding client #97 to the list of clients for aggregation.
[INFO][09:47:26]: [Server #1127936] Adding client #494 to the list of clients for aggregation.
[INFO][09:47:26]: [Server #1127936] Adding client #361 to the list of clients for aggregation.
[INFO][09:47:26]: [Server #1127936] Adding client #492 to the list of clients for aggregation.
[INFO][09:47:26]: [Server #1127936] Adding client #466 to the list of clients for aggregation.
[INFO][09:47:26]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 87, 89, 90, 91, 92, 93, 94, 95, 96, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 272, 273, 274, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.02675425 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.0444356  0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.04638629 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.04396783 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.03322043
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.20049805 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.02915971 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.02984944 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.15813021
 0.         0.1245186  0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 2. 0. 6. 0. 1. 2. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 2. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.02675425 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.0444356  0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.04638629 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.04396783 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.03322043
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.20049805 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.02915971 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.02984944 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.15813021
 0.         0.1245186  0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][09:47:28]: [Server #1127936] Global model accuracy: 83.45%

[INFO][09:47:28]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_10.pth.
[INFO][09:47:28]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_10.pth.
[INFO][09:47:28]: [93m[1m
[Server #1127936] Starting round 11/100.[0m
[0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002
 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8870e+00  5e-04  9e-09  9e-09
 5:  6.8876e+00  6.8873e+00  2e-04  3e-09  3e-09
 6:  6.8875e+00  6.8873e+00  2e-04  1e-08  2e-09
 7:  6.8875e+00  6.8874e+00  2e-04  1e-08  2e-09
 8:  6.8874e+00  6.8874e+00  9e-05  1e-07  2e-08
 9:  6.8874e+00  6.8874e+00  4e-05  8e-08  2e-08
10:  6.8874e+00  6.8874e+00  3e-06  4e-08  8e-09
Optimal solution found.
The calculated probability is:  [3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 4.07958793e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 5.16667054e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 5.31513516e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 1.13538729e-04 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 4.42990096e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 9.81355927e-01 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01907284e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01906029e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.75728586e-03 3.01933505e-05 3.01455959e-05
 3.01933505e-05 3.01933505e-05 3.01933505e-05 3.01933505e-05
 3.01933505e-05 3.01933505e-05]
current clients pool:  [INFO][09:47:29]: [Server #1127936] Selected clients: [271 105 152  77 140 191 492 339 181  87]
[INFO][09:47:29]: [Server #1127936] Selecting client #271 for training.
[INFO][09:47:29]: [Server #1127936] Sending the current model to client #271 (simulated).
[INFO][09:47:29]: [Server #1127936] Sending 0.24 MB of payload data to client #271 (simulated).
[INFO][09:47:29]: [Server #1127936] Selecting client #105 for training.
[INFO][09:47:29]: [Server #1127936] Sending the current model to client #105 (simulated).
[INFO][09:47:29]: [Server #1127936] Sending 0.24 MB of payload data to client #105 (simulated).
[INFO][09:47:29]: [Server #1127936] Selecting client #152 for training.
[INFO][09:47:29]: [Server #1127936] Sending the current model to client #152 (simulated).
[INFO][09:47:29]: [Client #271] Selected by the server.
[INFO][09:47:29]: [Client #271] Loading its data source...
[INFO][09:47:29]: [Client #271] Dataset size: 60000
[INFO][09:47:29]: [Client #271] Sampler: noniid
[INFO][09:47:29]: [Server #1127936] Sending 0.24 MB of payload data to client #152 (simulated).
[INFO][09:47:29]: [Client #105] Selected by the server.
[INFO][09:47:29]: [Client #105] Loading its data source...
[INFO][09:47:29]: [Client #105] Dataset size: 60000
[INFO][09:47:29]: [Client #105] Sampler: noniid
[INFO][09:47:29]: [Client #152] Selected by the server.
[INFO][09:47:29]: [Client #152] Loading its data source...
[INFO][09:47:29]: [Client #152] Dataset size: 60000
[INFO][09:47:29]: [Client #152] Sampler: noniid
[INFO][09:47:29]: [Client #271] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:47:29]: [Client #105] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:47:29]: [Client #152] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:47:29]: [93m[1m[Client #271] Started training in communication round #11.[0m
[INFO][09:47:29]: [93m[1m[Client #105] Started training in communication round #11.[0m
[INFO][09:47:29]: [93m[1m[Client #152] Started training in communication round #11.[0m
[INFO][09:47:31]: [Client #271] Loading the dataset.
[INFO][09:47:31]: [Client #152] Loading the dataset.
[INFO][09:47:31]: [Client #105] Loading the dataset.
[INFO][09:47:37]: [Client #271] Epoch: [1/5][0/10]	Loss: 1.838835
[INFO][09:47:37]: [Client #152] Epoch: [1/5][0/10]	Loss: 0.541431
[INFO][09:47:37]: [Client #105] Epoch: [1/5][0/10]	Loss: 0.464815
[INFO][09:47:37]: [Client #271] Epoch: [2/5][0/10]	Loss: 0.445046
[INFO][09:47:37]: [Client #105] Epoch: [2/5][0/10]	Loss: 0.000032
[INFO][09:47:37]: [Client #152] Epoch: [2/5][0/10]	Loss: 0.010401
[INFO][09:47:37]: [Client #271] Epoch: [3/5][0/10]	Loss: 0.001548
[INFO][09:47:37]: [Client #152] Epoch: [3/5][0/10]	Loss: 0.368915
[INFO][09:47:37]: [Client #105] Epoch: [3/5][0/10]	Loss: 0.313343
[INFO][09:47:37]: [Client #271] Epoch: [4/5][0/10]	Loss: 0.001815
[INFO][09:47:37]: [Client #152] Epoch: [4/5][0/10]	Loss: 0.015767
[INFO][09:47:37]: [Client #105] Epoch: [4/5][0/10]	Loss: 0.001959
[INFO][09:47:37]: [Client #271] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][09:47:37]: [Client #105] Epoch: [5/5][0/10]	Loss: 0.151792
[INFO][09:47:37]: [Client #152] Epoch: [5/5][0/10]	Loss: 0.000007
[INFO][09:47:37]: [Client #271] Model saved to /data/ykang/plato/results/test/model/lenet5_271_1127977.pth.
[INFO][09:47:37]: [Client #105] Model saved to /data/ykang/plato/results/test/model/lenet5_105_1127978.pth.
[INFO][09:47:37]: [Client #152] Model saved to /data/ykang/plato/results/test/model/lenet5_152_1127979.pth.
[INFO][09:47:38]: [Client #271] Loading a model from /data/ykang/plato/results/test/model/lenet5_271_1127977.pth.
[INFO][09:47:38]: [Client #271] Model trained.
[INFO][09:47:38]: [Client #271] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:47:38]: [Server #1127936] Received 0.24 MB of payload data from client #271 (simulated).
[INFO][09:47:38]: [Client #105] Loading a model from /data/ykang/plato/results/test/model/lenet5_105_1127978.pth.
[INFO][09:47:38]: [Client #105] Model trained.
[INFO][09:47:38]: [Client #105] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:47:38]: [Server #1127936] Received 0.24 MB of payload data from client #105 (simulated).
[INFO][09:47:38]: [Client #152] Loading a model from /data/ykang/plato/results/test/model/lenet5_152_1127979.pth.
[INFO][09:47:38]: [Client #152] Model trained.
[INFO][09:47:38]: [Client #152] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:47:38]: [Server #1127936] Received 0.24 MB of payload data from client #152 (simulated).
[INFO][09:47:38]: [Server #1127936] Selecting client #77 for training.
[INFO][09:47:38]: [Server #1127936] Sending the current model to client #77 (simulated).
[INFO][09:47:38]: [Server #1127936] Sending 0.24 MB of payload data to client #77 (simulated).
[INFO][09:47:38]: [Server #1127936] Selecting client #140 for training.
[INFO][09:47:38]: [Server #1127936] Sending the current model to client #140 (simulated).
[INFO][09:47:38]: [Server #1127936] Sending 0.24 MB of payload data to client #140 (simulated).
[INFO][09:47:38]: [Server #1127936] Selecting client #191 for training.
[INFO][09:47:38]: [Server #1127936] Sending the current model to client #191 (simulated).
[INFO][09:47:38]: [Client #77] Selected by the server.
[INFO][09:47:38]: [Client #77] Loading its data source...
[INFO][09:47:38]: [Client #77] Dataset size: 60000
[INFO][09:47:38]: [Client #77] Sampler: noniid
[INFO][09:47:38]: [Server #1127936] Sending 0.24 MB of payload data to client #191 (simulated).
[INFO][09:47:38]: [Client #191] Selected by the server.
[INFO][09:47:38]: [Client #140] Selected by the server.
[INFO][09:47:38]: [Client #191] Loading its data source...
[INFO][09:47:38]: [Client #140] Loading its data source...
[INFO][09:47:38]: [Client #191] Dataset size: 60000
[INFO][09:47:38]: [Client #191] Sampler: noniid
[INFO][09:47:38]: [Client #140] Dataset size: 60000
[INFO][09:47:38]: [Client #77] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:47:38]: [Client #140] Sampler: noniid
[INFO][09:47:38]: [Client #140] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:47:38]: [Client #191] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:47:38]: [93m[1m[Client #77] Started training in communication round #11.[0m
[INFO][09:47:38]: [93m[1m[Client #140] Started training in communication round #11.[0m
[INFO][09:47:38]: [93m[1m[Client #191] Started training in communication round #11.[0m
[INFO][09:47:40]: [Client #191] Loading the dataset.
[INFO][09:47:40]: [Client #77] Loading the dataset.
[INFO][09:47:40]: [Client #140] Loading the dataset.
[INFO][09:47:46]: [Client #191] Epoch: [1/5][0/10]	Loss: 0.657931
[INFO][09:47:46]: [Client #140] Epoch: [1/5][0/10]	Loss: 1.509364
[INFO][09:47:46]: [Client #191] Epoch: [2/5][0/10]	Loss: 0.000007
[INFO][09:47:46]: [Client #77] Epoch: [1/5][0/10]	Loss: 0.289240
[INFO][09:47:46]: [Client #140] Epoch: [2/5][0/10]	Loss: 0.168243
[INFO][09:47:46]: [Client #191] Epoch: [3/5][0/10]	Loss: 0.000000
[INFO][09:47:46]: [Client #77] Epoch: [2/5][0/10]	Loss: 0.191500
[INFO][09:47:46]: [Client #140] Epoch: [3/5][0/10]	Loss: 0.029402
[INFO][09:47:46]: [Client #191] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][09:47:47]: [Client #77] Epoch: [3/5][0/10]	Loss: 0.082818
[INFO][09:47:47]: [Client #140] Epoch: [4/5][0/10]	Loss: 0.062152
[INFO][09:47:47]: [Client #191] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][09:47:47]: [Client #77] Epoch: [4/5][0/10]	Loss: 0.001279
[INFO][09:47:47]: [Client #191] Model saved to /data/ykang/plato/results/test/model/lenet5_191_1127979.pth.
[INFO][09:47:47]: [Client #140] Epoch: [5/5][0/10]	Loss: 0.129892
[INFO][09:47:47]: [Client #77] Epoch: [5/5][0/10]	Loss: 0.784576
[INFO][09:47:47]: [Client #140] Model saved to /data/ykang/plato/results/test/model/lenet5_140_1127978.pth.
[INFO][09:47:47]: [Client #77] Model saved to /data/ykang/plato/results/test/model/lenet5_77_1127977.pth.
[INFO][09:47:47]: [Client #191] Loading a model from /data/ykang/plato/results/test/model/lenet5_191_1127979.pth.
[INFO][09:47:47]: [Client #191] Model trained.
[INFO][09:47:47]: [Client #191] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:47:47]: [Server #1127936] Received 0.24 MB of payload data from client #191 (simulated).
[INFO][09:47:48]: [Client #77] Loading a model from /data/ykang/plato/results/test/model/lenet5_77_1127977.pth.
[INFO][09:47:48]: [Client #77] Model trained.
[INFO][09:47:48]: [Client #77] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:47:48]: [Server #1127936] Received 0.24 MB of payload data from client #77 (simulated).
[INFO][09:47:48]: [Client #140] Loading a model from /data/ykang/plato/results/test/model/lenet5_140_1127978.pth.
[INFO][09:47:48]: [Client #140] Model trained.
[INFO][09:47:48]: [Client #140] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:47:48]: [Server #1127936] Received 0.24 MB of payload data from client #140 (simulated).
[INFO][09:47:48]: [Server #1127936] Selecting client #492 for training.
[INFO][09:47:48]: [Server #1127936] Sending the current model to client #492 (simulated).
[INFO][09:47:48]: [Server #1127936] Sending 0.24 MB of payload data to client #492 (simulated).
[INFO][09:47:48]: [Server #1127936] Selecting client #339 for training.
[INFO][09:47:48]: [Server #1127936] Sending the current model to client #339 (simulated).
[INFO][09:47:48]: [Server #1127936] Sending 0.24 MB of payload data to client #339 (simulated).
[INFO][09:47:48]: [Server #1127936] Selecting client #181 for training.
[INFO][09:47:48]: [Server #1127936] Sending the current model to client #181 (simulated).
[INFO][09:47:48]: [Client #492] Selected by the server.
[INFO][09:47:48]: [Client #492] Loading its data source...
[INFO][09:47:48]: [Client #492] Dataset size: 60000
[INFO][09:47:48]: [Client #492] Sampler: noniid
[INFO][09:47:48]: [Server #1127936] Sending 0.24 MB of payload data to client #181 (simulated).
[INFO][09:47:48]: [Client #339] Selected by the server.
[INFO][09:47:48]: [Client #339] Loading its data source...
[INFO][09:47:48]: [Client #339] Dataset size: 60000
[INFO][09:47:48]: [Client #339] Sampler: noniid
[INFO][09:47:48]: [Client #181] Selected by the server.
[INFO][09:47:48]: [Client #181] Loading its data source...
[INFO][09:47:48]: [Client #181] Dataset size: 60000
[INFO][09:47:48]: [Client #181] Sampler: noniid
[INFO][09:47:48]: [Client #492] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:47:48]: [Client #339] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:47:48]: [Client #181] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:47:48]: [93m[1m[Client #339] Started training in communication round #11.[0m
[INFO][09:47:48]: [93m[1m[Client #181] Started training in communication round #11.[0m
[INFO][09:47:48]: [93m[1m[Client #492] Started training in communication round #11.[0m
[INFO][09:47:50]: [Client #181] Loading the dataset.
[INFO][09:47:50]: [Client #339] Loading the dataset.
[INFO][09:47:50]: [Client #492] Loading the dataset.
[INFO][09:47:55]: [Client #181] Epoch: [1/5][0/10]	Loss: 1.553496
[INFO][09:47:56]: [Client #339] Epoch: [1/5][0/10]	Loss: 0.446920
[INFO][09:47:56]: [Client #492] Epoch: [1/5][0/10]	Loss: 0.657931
[INFO][09:47:56]: [Client #181] Epoch: [2/5][0/10]	Loss: 0.551183
[INFO][09:47:56]: [Client #339] Epoch: [2/5][0/10]	Loss: 0.000607
[INFO][09:47:56]: [Client #181] Epoch: [3/5][0/10]	Loss: 0.020420
[INFO][09:47:56]: [Client #492] Epoch: [2/5][0/10]	Loss: 0.084322
[INFO][09:47:56]: [Client #339] Epoch: [3/5][0/10]	Loss: 0.018389
[INFO][09:47:56]: [Client #181] Epoch: [4/5][0/10]	Loss: 0.033372
[INFO][09:47:56]: [Client #492] Epoch: [3/5][0/10]	Loss: 0.000765
[INFO][09:47:56]: [Client #339] Epoch: [4/5][0/10]	Loss: 0.000057
[INFO][09:47:56]: [Client #492] Epoch: [4/5][0/10]	Loss: 0.059778
[INFO][09:47:56]: [Client #181] Epoch: [5/5][0/10]	Loss: 0.007961
[INFO][09:47:56]: [Client #492] Epoch: [5/5][0/10]	Loss: 0.052126
[INFO][09:47:56]: [Client #181] Model saved to /data/ykang/plato/results/test/model/lenet5_181_1127979.pth.
[INFO][09:47:56]: [Client #339] Epoch: [5/5][0/10]	Loss: 0.003855
[INFO][09:47:56]: [Client #492] Model saved to /data/ykang/plato/results/test/model/lenet5_492_1127977.pth.
[INFO][09:47:56]: [Client #339] Model saved to /data/ykang/plato/results/test/model/lenet5_339_1127978.pth.
[INFO][09:47:57]: [Client #181] Loading a model from /data/ykang/plato/results/test/model/lenet5_181_1127979.pth.
[INFO][09:47:57]: [Client #181] Model trained.
[INFO][09:47:57]: [Client #181] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:47:57]: [Server #1127936] Received 0.24 MB of payload data from client #181 (simulated).
[INFO][09:47:57]: [Client #339] Loading a model from /data/ykang/plato/results/test/model/lenet5_339_1127978.pth.
[INFO][09:47:57]: [Client #339] Model trained.
[INFO][09:47:57]: [Client #339] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:47:57]: [Server #1127936] Received 0.24 MB of payload data from client #339 (simulated).
[INFO][09:47:57]: [Client #492] Loading a model from /data/ykang/plato/results/test/model/lenet5_492_1127977.pth.
[INFO][09:47:57]: [Client #492] Model trained.
[INFO][09:47:57]: [Client #492] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:47:57]: [Server #1127936] Received 0.24 MB of payload data from client #492 (simulated).
[INFO][09:47:57]: [Server #1127936] Selecting client #87 for training.
[INFO][09:47:57]: [Server #1127936] Sending the current model to client #87 (simulated).
[INFO][09:47:57]: [Server #1127936] Sending 0.24 MB of payload data to client #87 (simulated).
[INFO][09:47:57]: [Client #87] Selected by the server.
[INFO][09:47:57]: [Client #87] Loading its data source...
[INFO][09:47:57]: [Client #87] Dataset size: 60000
[INFO][09:47:57]: [Client #87] Sampler: noniid
[INFO][09:47:57]: [Client #87] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:47:57]: [93m[1m[Client #87] Started training in communication round #11.[0m
[INFO][09:47:59]: [Client #87] Loading the dataset.
[INFO][09:48:04]: [Client #87] Epoch: [1/5][0/10]	Loss: 0.240314
[INFO][09:48:04]: [Client #87] Epoch: [2/5][0/10]	Loss: 0.013332
[INFO][09:48:04]: [Client #87] Epoch: [3/5][0/10]	Loss: 0.005301
[INFO][09:48:04]: [Client #87] Epoch: [4/5][0/10]	Loss: 0.017135
[INFO][09:48:04]: [Client #87] Epoch: [5/5][0/10]	Loss: 0.000206
[INFO][09:48:04]: [Client #87] Model saved to /data/ykang/plato/results/test/model/lenet5_87_1127977.pth.
[INFO][09:48:05]: [Client #87] Loading a model from /data/ykang/plato/results/test/model/lenet5_87_1127977.pth.
[INFO][09:48:05]: [Client #87] Model trained.
[INFO][09:48:05]: [Client #87] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:48:05]: [Server #1127936] Received 0.24 MB of payload data from client #87 (simulated).
[INFO][09:48:05]: [Server #1127936] Adding client #356 to the list of clients for aggregation.
[INFO][09:48:05]: [Server #1127936] Adding client #432 to the list of clients for aggregation.
[INFO][09:48:05]: [Server #1127936] Adding client #257 to the list of clients for aggregation.
[INFO][09:48:05]: [Server #1127936] Adding client #2 to the list of clients for aggregation.
[INFO][09:48:05]: [Server #1127936] Adding client #135 to the list of clients for aggregation.
[INFO][09:48:05]: [Server #1127936] Adding client #191 to the list of clients for aggregation.
[INFO][09:48:05]: [Server #1127936] Adding client #271 to the list of clients for aggregation.
[INFO][09:48:05]: [Server #1127936] Adding client #87 to the list of clients for aggregation.
[INFO][09:48:05]: [Server #1127936] Adding client #181 to the list of clients for aggregation.
[INFO][09:48:05]: [Server #1127936] Adding client #140 to the list of clients for aggregation.
[INFO][09:48:05]: [Server #1127936] Aggregating 10 clients in total.
[1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.24930138 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01826274 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.03811206 0.         0.         0.
 0.         0.07856261 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.06966378 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.06205887 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.02242318 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.08338552 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.0204636  0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.0449507
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 2. 0. 6. 0. 1. 2. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 2. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]
local_gradient_bounds:  [0.         0.24930138 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01826274 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.03811206 0.         0.         0.
 0.         0.07856261 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.06966378 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.06205887 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.02242318 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.08338552 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.0204636  0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.0449507
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][09:48:07]: [Server #1127936] Global model accuracy: 76.96%

[INFO][09:48:07]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_11.pth.
[INFO][09:48:07]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_11.pth.
[INFO][09:48:07]: [93m[1m
[Server #1127936] Starting round 12/100.[0m
[0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002
 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8870e+00  6e-04  1e-08  1e-08
 5:  6.8876e+00  6.8873e+00  3e-04  4e-09  4e-09
 6:  6.8875e+00  6.8873e+00  2e-04  2e-08  6e-09
 7:  6.8875e+00  6.8873e+00  2e-04  2e-08  6e-09
 8:  6.8874e+00  6.8873e+00  9e-05  2e-07  4e-08
 9:  6.8874e+00  6.8873e+00  4e-05  1e-07  3e-08
10:  6.8873e+00  6.8873e+00  2e-06  4e-08  1e-08
Optimal solution found.
The calculated probability is:  [2.20005980e-05 9.89207111e-01 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.19999167e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 3.27257209e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.19879948e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.19906872e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.19927324e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.74773312e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.19864007e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.69161887e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 3.55252743e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05 2.20005980e-05 2.20005980e-05
 2.20005980e-05 2.20005980e-05]
current clients pool:  [INFO][09:48:08]: [Server #1127936] Selected clients: [  2 236 161 315 439 227 394 231 330  67]
[INFO][09:48:08]: [Server #1127936] Selecting client #2 for training.
[INFO][09:48:08]: [Server #1127936] Sending the current model to client #2 (simulated).
[INFO][09:48:08]: [Server #1127936] Sending 0.24 MB of payload data to client #2 (simulated).
[INFO][09:48:08]: [Server #1127936] Selecting client #236 for training.
[INFO][09:48:08]: [Server #1127936] Sending the current model to client #236 (simulated).
[INFO][09:48:08]: [Server #1127936] Sending 0.24 MB of payload data to client #236 (simulated).
[INFO][09:48:08]: [Server #1127936] Selecting client #161 for training.
[INFO][09:48:08]: [Server #1127936] Sending the current model to client #161 (simulated).
[INFO][09:48:08]: [Client #2] Selected by the server.
[INFO][09:48:08]: [Client #2] Loading its data source...
[INFO][09:48:08]: [Client #2] Dataset size: 60000
[INFO][09:48:08]: [Client #2] Sampler: noniid
[INFO][09:48:08]: [Server #1127936] Sending 0.24 MB of payload data to client #161 (simulated).
[INFO][09:48:08]: [Client #236] Selected by the server.
[INFO][09:48:08]: [Client #236] Loading its data source...
[INFO][09:48:08]: [Client #161] Selected by the server.
[INFO][09:48:08]: [Client #236] Dataset size: 60000
[INFO][09:48:08]: [Client #2] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:48:08]: [Client #236] Sampler: noniid
[INFO][09:48:08]: [Client #161] Loading its data source...
[INFO][09:48:08]: [Client #161] Dataset size: 60000
[INFO][09:48:08]: [Client #161] Sampler: noniid
[INFO][09:48:08]: [Client #236] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:48:08]: [Client #161] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:48:08]: [93m[1m[Client #161] Started training in communication round #12.[0m
[INFO][09:48:08]: [93m[1m[Client #236] Started training in communication round #12.[0m
[INFO][09:48:08]: [93m[1m[Client #2] Started training in communication round #12.[0m
[INFO][09:48:10]: [Client #2] Loading the dataset.
[INFO][09:48:10]: [Client #161] Loading the dataset.
[INFO][09:48:10]: [Client #236] Loading the dataset.
[INFO][09:48:16]: [Client #236] Epoch: [1/5][0/10]	Loss: 2.766727
[INFO][09:48:16]: [Client #161] Epoch: [1/5][0/10]	Loss: 0.946189
[INFO][09:48:16]: [Client #2] Epoch: [1/5][0/10]	Loss: 0.881828
[INFO][09:48:16]: [Client #236] Epoch: [2/5][0/10]	Loss: 0.000019
[INFO][09:48:16]: [Client #161] Epoch: [2/5][0/10]	Loss: 0.020847
[INFO][09:48:16]: [Client #2] Epoch: [2/5][0/10]	Loss: 0.014076
[INFO][09:48:16]: [Client #236] Epoch: [3/5][0/10]	Loss: 0.025666
[INFO][09:48:16]: [Client #161] Epoch: [3/5][0/10]	Loss: 0.000630
[INFO][09:48:16]: [Client #2] Epoch: [3/5][0/10]	Loss: 0.003780
[INFO][09:48:16]: [Client #236] Epoch: [4/5][0/10]	Loss: 0.003074
[INFO][09:48:16]: [Client #2] Epoch: [4/5][0/10]	Loss: 0.047565
[INFO][09:48:16]: [Client #161] Epoch: [4/5][0/10]	Loss: 0.008006
[INFO][09:48:16]: [Client #236] Epoch: [5/5][0/10]	Loss: 0.000001
[INFO][09:48:16]: [Client #2] Epoch: [5/5][0/10]	Loss: 0.194638
[INFO][09:48:16]: [Client #236] Model saved to /data/ykang/plato/results/test/model/lenet5_236_1127978.pth.
[INFO][09:48:16]: [Client #161] Epoch: [5/5][0/10]	Loss: 0.127382
[INFO][09:48:16]: [Client #2] Model saved to /data/ykang/plato/results/test/model/lenet5_2_1127977.pth.
[INFO][09:48:16]: [Client #161] Model saved to /data/ykang/plato/results/test/model/lenet5_161_1127979.pth.
[INFO][09:48:17]: [Client #236] Loading a model from /data/ykang/plato/results/test/model/lenet5_236_1127978.pth.
[INFO][09:48:17]: [Client #236] Model trained.
[INFO][09:48:17]: [Client #236] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:48:17]: [Server #1127936] Received 0.24 MB of payload data from client #236 (simulated).
[INFO][09:48:17]: [Client #2] Loading a model from /data/ykang/plato/results/test/model/lenet5_2_1127977.pth.
[INFO][09:48:17]: [Client #2] Model trained.
[INFO][09:48:17]: [Client #2] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:48:17]: [Server #1127936] Received 0.24 MB of payload data from client #2 (simulated).
[INFO][09:48:17]: [Client #161] Loading a model from /data/ykang/plato/results/test/model/lenet5_161_1127979.pth.
[INFO][09:48:17]: [Client #161] Model trained.
[INFO][09:48:17]: [Client #161] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:48:17]: [Server #1127936] Received 0.24 MB of payload data from client #161 (simulated).
[INFO][09:48:17]: [Server #1127936] Selecting client #315 for training.
[INFO][09:48:17]: [Server #1127936] Sending the current model to client #315 (simulated).
[INFO][09:48:17]: [Server #1127936] Sending 0.24 MB of payload data to client #315 (simulated).
[INFO][09:48:17]: [Server #1127936] Selecting client #439 for training.
[INFO][09:48:17]: [Server #1127936] Sending the current model to client #439 (simulated).
[INFO][09:48:17]: [Server #1127936] Sending 0.24 MB of payload data to client #439 (simulated).
[INFO][09:48:17]: [Server #1127936] Selecting client #227 for training.
[INFO][09:48:17]: [Server #1127936] Sending the current model to client #227 (simulated).
[INFO][09:48:17]: [Client #315] Selected by the server.
[INFO][09:48:17]: [Client #315] Loading its data source...
[INFO][09:48:17]: [Client #315] Dataset size: 60000
[INFO][09:48:17]: [Client #315] Sampler: noniid
[INFO][09:48:17]: [Server #1127936] Sending 0.24 MB of payload data to client #227 (simulated).
[INFO][09:48:17]: [Client #439] Selected by the server.
[INFO][09:48:17]: [Client #227] Selected by the server.
[INFO][09:48:17]: [Client #439] Loading its data source...
[INFO][09:48:17]: [Client #227] Loading its data source...
[INFO][09:48:17]: [Client #227] Dataset size: 60000
[INFO][09:48:17]: [Client #439] Dataset size: 60000
[INFO][09:48:17]: [Client #227] Sampler: noniid
[INFO][09:48:17]: [Client #439] Sampler: noniid
[INFO][09:48:17]: [Client #315] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:48:17]: [Client #439] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:48:17]: [93m[1m[Client #315] Started training in communication round #12.[0m
[INFO][09:48:17]: [93m[1m[Client #439] Started training in communication round #12.[0m
[INFO][09:48:17]: [Client #227] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:48:17]: [93m[1m[Client #227] Started training in communication round #12.[0m
[INFO][09:48:19]: [Client #315] Loading the dataset.
[INFO][09:48:19]: [Client #439] Loading the dataset.
[INFO][09:48:19]: [Client #227] Loading the dataset.
[INFO][09:48:25]: [Client #439] Epoch: [1/5][0/10]	Loss: 0.620985
[INFO][09:48:25]: [Client #227] Epoch: [1/5][0/10]	Loss: 0.833188
[INFO][09:48:25]: [Client #315] Epoch: [1/5][0/10]	Loss: 0.548768
[INFO][09:48:25]: [Client #315] Epoch: [2/5][0/10]	Loss: 0.108686
[INFO][09:48:25]: [Client #227] Epoch: [2/5][0/10]	Loss: 0.184331
[INFO][09:48:25]: [Client #439] Epoch: [2/5][0/10]	Loss: 0.376769
[INFO][09:48:25]: [Client #315] Epoch: [3/5][0/10]	Loss: 0.002059
[INFO][09:48:25]: [Client #439] Epoch: [3/5][0/10]	Loss: 0.008700
[INFO][09:48:25]: [Client #227] Epoch: [3/5][0/10]	Loss: 0.012098
[INFO][09:48:25]: [Client #227] Epoch: [4/5][0/10]	Loss: 0.069264
[INFO][09:48:25]: [Client #439] Epoch: [4/5][0/10]	Loss: 0.095438
[INFO][09:48:25]: [Client #315] Epoch: [4/5][0/10]	Loss: 0.172890
[INFO][09:48:25]: [Client #227] Epoch: [5/5][0/10]	Loss: 0.043459
[INFO][09:48:25]: [Client #315] Epoch: [5/5][0/10]	Loss: 0.164132
[INFO][09:48:25]: [Client #439] Epoch: [5/5][0/10]	Loss: 0.059040
[INFO][09:48:25]: [Client #227] Model saved to /data/ykang/plato/results/test/model/lenet5_227_1127979.pth.
[INFO][09:48:25]: [Client #315] Model saved to /data/ykang/plato/results/test/model/lenet5_315_1127977.pth.
[INFO][09:48:25]: [Client #439] Model saved to /data/ykang/plato/results/test/model/lenet5_439_1127978.pth.
[INFO][09:48:26]: [Client #227] Loading a model from /data/ykang/plato/results/test/model/lenet5_227_1127979.pth.
[INFO][09:48:26]: [Client #227] Model trained.
[INFO][09:48:26]: [Client #227] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:48:26]: [Server #1127936] Received 0.24 MB of payload data from client #227 (simulated).
[INFO][09:48:26]: [Client #315] Loading a model from /data/ykang/plato/results/test/model/lenet5_315_1127977.pth.
[INFO][09:48:26]: [Client #315] Model trained.
[INFO][09:48:26]: [Client #315] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:48:26]: [Server #1127936] Received 0.24 MB of payload data from client #315 (simulated).
[INFO][09:48:26]: [Client #439] Loading a model from /data/ykang/plato/results/test/model/lenet5_439_1127978.pth.
[INFO][09:48:26]: [Client #439] Model trained.
[INFO][09:48:26]: [Client #439] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:48:26]: [Server #1127936] Received 0.24 MB of payload data from client #439 (simulated).
[INFO][09:48:26]: [Server #1127936] Selecting client #394 for training.
[INFO][09:48:26]: [Server #1127936] Sending the current model to client #394 (simulated).
[INFO][09:48:26]: [Server #1127936] Sending 0.24 MB of payload data to client #394 (simulated).
[INFO][09:48:26]: [Server #1127936] Selecting client #231 for training.
[INFO][09:48:26]: [Server #1127936] Sending the current model to client #231 (simulated).
[INFO][09:48:26]: [Server #1127936] Sending 0.24 MB of payload data to client #231 (simulated).
[INFO][09:48:26]: [Server #1127936] Selecting client #330 for training.
[INFO][09:48:26]: [Server #1127936] Sending the current model to client #330 (simulated).
[INFO][09:48:26]: [Client #394] Selected by the server.
[INFO][09:48:26]: [Client #394] Loading its data source...
[INFO][09:48:26]: [Client #394] Dataset size: 60000
[INFO][09:48:26]: [Client #394] Sampler: noniid
[INFO][09:48:26]: [Client #394] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:48:26]: [93m[1m[Client #394] Started training in communication round #12.[0m
[INFO][09:48:26]: [Server #1127936] Sending 0.24 MB of payload data to client #330 (simulated).
[INFO][09:48:26]: [Client #231] Selected by the server.
[INFO][09:48:26]: [Client #231] Loading its data source...
[INFO][09:48:26]: [Client #231] Dataset size: 60000
[INFO][09:48:26]: [Client #231] Sampler: noniid
[INFO][09:48:26]: [Client #330] Selected by the server.
[INFO][09:48:26]: [Client #330] Loading its data source...
[INFO][09:48:26]: [Client #330] Dataset size: 60000
[INFO][09:48:26]: [Client #330] Sampler: noniid
[INFO][09:48:26]: [Client #330] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:48:26]: [Client #231] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:48:26]: [93m[1m[Client #330] Started training in communication round #12.[0m
[INFO][09:48:26]: [93m[1m[Client #231] Started training in communication round #12.[0m
[INFO][09:48:28]: [Client #394] Loading the dataset.
[INFO][09:48:28]: [Client #231] Loading the dataset.
[INFO][09:48:29]: [Client #330] Loading the dataset.
[INFO][09:48:34]: [Client #394] Epoch: [1/5][0/10]	Loss: 1.986203
[INFO][09:48:34]: [Client #394] Epoch: [2/5][0/10]	Loss: 0.131018
[INFO][09:48:34]: [Client #330] Epoch: [1/5][0/10]	Loss: 0.390110
[INFO][09:48:34]: [Client #394] Epoch: [3/5][0/10]	Loss: 0.424969
[INFO][09:48:34]: [Client #231] Epoch: [1/5][0/10]	Loss: 0.517971
[INFO][09:48:35]: [Client #330] Epoch: [2/5][0/10]	Loss: 0.087308
[INFO][09:48:35]: [Client #231] Epoch: [2/5][0/10]	Loss: 0.071527
[INFO][09:48:35]: [Client #330] Epoch: [3/5][0/10]	Loss: 0.031850
[INFO][09:48:35]: [Client #394] Epoch: [4/5][0/10]	Loss: 0.082607
[INFO][09:48:35]: [Client #231] Epoch: [3/5][0/10]	Loss: 0.001736
[INFO][09:48:35]: [Client #330] Epoch: [4/5][0/10]	Loss: 0.161986
[INFO][09:48:35]: [Client #394] Epoch: [5/5][0/10]	Loss: 0.006729
[INFO][09:48:35]: [Client #394] Model saved to /data/ykang/plato/results/test/model/lenet5_394_1127977.pth.
[INFO][09:48:35]: [Client #231] Epoch: [4/5][0/10]	Loss: 0.383716
[INFO][09:48:35]: [Client #330] Epoch: [5/5][0/10]	Loss: 0.021616
[INFO][09:48:35]: [Client #330] Model saved to /data/ykang/plato/results/test/model/lenet5_330_1127979.pth.
[INFO][09:48:35]: [Client #231] Epoch: [5/5][0/10]	Loss: 0.660774
[INFO][09:48:35]: [Client #231] Model saved to /data/ykang/plato/results/test/model/lenet5_231_1127978.pth.
[INFO][09:48:36]: [Client #394] Loading a model from /data/ykang/plato/results/test/model/lenet5_394_1127977.pth.
[INFO][09:48:36]: [Client #394] Model trained.
[INFO][09:48:36]: [Client #394] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:48:36]: [Server #1127936] Received 0.24 MB of payload data from client #394 (simulated).
[INFO][09:48:36]: [Client #330] Loading a model from /data/ykang/plato/results/test/model/lenet5_330_1127979.pth.
[INFO][09:48:36]: [Client #330] Model trained.
[INFO][09:48:36]: [Client #330] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:48:36]: [Server #1127936] Received 0.24 MB of payload data from client #330 (simulated).
[INFO][09:48:36]: [Client #231] Loading a model from /data/ykang/plato/results/test/model/lenet5_231_1127978.pth.
[INFO][09:48:36]: [Client #231] Model trained.
[INFO][09:48:36]: [Client #231] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:48:36]: [Server #1127936] Received 0.24 MB of payload data from client #231 (simulated).
[INFO][09:48:36]: [Server #1127936] Selecting client #67 for training.
[INFO][09:48:36]: [Server #1127936] Sending the current model to client #67 (simulated).
[INFO][09:48:36]: [Server #1127936] Sending 0.24 MB of payload data to client #67 (simulated).
[INFO][09:48:36]: [Client #67] Selected by the server.
[INFO][09:48:36]: [Client #67] Loading its data source...
[INFO][09:48:36]: [Client #67] Dataset size: 60000
[INFO][09:48:36]: [Client #67] Sampler: noniid
[INFO][09:48:36]: [Client #67] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:48:36]: [93m[1m[Client #67] Started training in communication round #12.[0m
[INFO][09:48:38]: [Client #67] Loading the dataset.
[INFO][09:48:43]: [Client #67] Epoch: [1/5][0/10]	Loss: 0.691788
[INFO][09:48:43]: [Client #67] Epoch: [2/5][0/10]	Loss: 0.131465
[INFO][09:48:44]: [Client #67] Epoch: [3/5][0/10]	Loss: 0.536478
[INFO][09:48:44]: [Client #67] Epoch: [4/5][0/10]	Loss: 0.058945
[INFO][09:48:44]: [Client #67] Epoch: [5/5][0/10]	Loss: 0.059313
[INFO][09:48:44]: [Client #67] Model saved to /data/ykang/plato/results/test/model/lenet5_67_1127977.pth.
[INFO][09:48:44]: [Client #67] Loading a model from /data/ykang/plato/results/test/model/lenet5_67_1127977.pth.
[INFO][09:48:44]: [Client #67] Model trained.
[INFO][09:48:44]: [Client #67] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:48:44]: [Server #1127936] Received 0.24 MB of payload data from client #67 (simulated).
[INFO][09:48:44]: [Server #1127936] Adding client #339 to the list of clients for aggregation.
[INFO][09:48:44]: [Server #1127936] Adding client #88 to the list of clients for aggregation.
[INFO][09:48:44]: [Server #1127936] Adding client #105 to the list of clients for aggregation.
[INFO][09:48:44]: [Server #1127936] Adding client #492 to the list of clients for aggregation.
[INFO][09:48:44]: [Server #1127936] Adding client #77 to the list of clients for aggregation.
[INFO][09:48:44]: [Server #1127936] Adding client #394 to the list of clients for aggregation.
[INFO][09:48:44]: [Server #1127936] Adding client #315 to the list of clients for aggregation.
[INFO][09:48:44]: [Server #1127936] Adding client #161 to the list of clients for aggregation.
[INFO][09:48:44]: [Server #1127936] Adding client #231 to the list of clients for aggregation.
[INFO][09:48:44]: [Server #1127936] Adding client #227 to the list of clients for aggregation.
[INFO][09:48:44]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.02126479 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.06014768 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01759818 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.13851519 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.0358968  0.
 0.         0.         0.04799694 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.03518469 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.02512858 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.18994761 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.05135149
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 2. 0. 6. 0. 1. 2. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 2. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.02126479 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.06014768 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01759818 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.13851519 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.0358968  0.
 0.         0.         0.04799694 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.03518469 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.02512858 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.18994761 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.05135149
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][09:48:46]: [Server #1127936] Global model accuracy: 71.01%

[INFO][09:48:46]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_12.pth.
[INFO][09:48:46]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_12.pth.
[INFO][09:48:46]: [93m[1m
[Server #1127936] Starting round 13/100.[0m
[0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002
 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8871e+00  5e-04  9e-09  9e-09
 5:  6.8876e+00  6.8874e+00  2e-04  3e-09  3e-09
 6:  6.8875e+00  6.8874e+00  2e-04  7e-09  1e-09
 7:  6.8875e+00  6.8874e+00  1e-04  8e-09  2e-09
 8:  6.8875e+00  6.8874e+00  8e-05  8e-08  1e-08
 9:  6.8874e+00  6.8874e+00  4e-05  7e-08  1e-08
10:  6.8874e+00  6.8874e+00  3e-06  4e-08  7e-09
Optimal solution found.
The calculated probability is:  [3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 4.37321328e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 9.83267807e-01 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 4.17640958e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40181229e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40837593e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40800308e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40839452e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 4.59788048e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.39563648e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 6.80186608e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05 3.40884931e-05 3.40884931e-05
 3.40884931e-05 3.40884931e-05]
current clients pool:  [INFO][09:48:47]: [Server #1127936] Selected clients: [ 88 261 100 426 129 353 272 476 314 421]
[INFO][09:48:47]: [Server #1127936] Selecting client #88 for training.
[INFO][09:48:47]: [Server #1127936] Sending the current model to client #88 (simulated).
[INFO][09:48:47]: [Server #1127936] Sending 0.24 MB of payload data to client #88 (simulated).
[INFO][09:48:47]: [Server #1127936] Selecting client #261 for training.
[INFO][09:48:47]: [Server #1127936] Sending the current model to client #261 (simulated).
[INFO][09:48:47]: [Server #1127936] Sending 0.24 MB of payload data to client #261 (simulated).
[INFO][09:48:47]: [Server #1127936] Selecting client #100 for training.
[INFO][09:48:47]: [Server #1127936] Sending the current model to client #100 (simulated).
[INFO][09:48:47]: [Client #88] Selected by the server.
[INFO][09:48:47]: [Client #88] Loading its data source...
[INFO][09:48:47]: [Client #88] Dataset size: 60000
[INFO][09:48:47]: [Client #88] Sampler: noniid
[INFO][09:48:47]: [Server #1127936] Sending 0.24 MB of payload data to client #100 (simulated).
[INFO][09:48:47]: [Client #261] Selected by the server.
[INFO][09:48:47]: [Client #261] Loading its data source...
[INFO][09:48:47]: [Client #261] Dataset size: 60000
[INFO][09:48:47]: [Client #261] Sampler: noniid
[INFO][09:48:47]: [Client #100] Selected by the server.
[INFO][09:48:47]: [Client #100] Loading its data source...
[INFO][09:48:47]: [Client #100] Dataset size: 60000
[INFO][09:48:47]: [Client #100] Sampler: noniid
[INFO][09:48:47]: [Client #88] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:48:47]: [Client #261] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:48:47]: [Client #100] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:48:47]: [93m[1m[Client #100] Started training in communication round #13.[0m
[INFO][09:48:47]: [93m[1m[Client #261] Started training in communication round #13.[0m
[INFO][09:48:47]: [93m[1m[Client #88] Started training in communication round #13.[0m
[INFO][09:48:49]: [Client #100] Loading the dataset.
[INFO][09:48:49]: [Client #261] Loading the dataset.
[INFO][09:48:49]: [Client #88] Loading the dataset.
[INFO][09:48:56]: [Client #261] Epoch: [1/5][0/10]	Loss: 0.283776
[INFO][09:48:56]: [Client #100] Epoch: [1/5][0/10]	Loss: 1.775354
[INFO][09:48:56]: [Client #88] Epoch: [1/5][0/10]	Loss: 0.261131
[INFO][09:48:56]: [Client #261] Epoch: [2/5][0/10]	Loss: 0.091156
[INFO][09:48:56]: [Client #100] Epoch: [2/5][0/10]	Loss: 0.002837
[INFO][09:48:56]: [Client #88] Epoch: [2/5][0/10]	Loss: 0.273197
[INFO][09:48:56]: [Client #261] Epoch: [3/5][0/10]	Loss: 0.000945
[INFO][09:48:56]: [Client #100] Epoch: [3/5][0/10]	Loss: 0.015754
[INFO][09:48:57]: [Client #261] Epoch: [4/5][0/10]	Loss: 0.000905
[INFO][09:48:57]: [Client #88] Epoch: [3/5][0/10]	Loss: 0.026255
[INFO][09:48:57]: [Client #100] Epoch: [4/5][0/10]	Loss: 0.145600
[INFO][09:48:57]: [Client #261] Epoch: [5/5][0/10]	Loss: 0.387961
[INFO][09:48:57]: [Client #88] Epoch: [4/5][0/10]	Loss: 0.484076
[INFO][09:48:57]: [Client #100] Epoch: [5/5][0/10]	Loss: 0.003545
[INFO][09:48:57]: [Client #261] Model saved to /data/ykang/plato/results/test/model/lenet5_261_1127978.pth.
[INFO][09:48:57]: [Client #100] Model saved to /data/ykang/plato/results/test/model/lenet5_100_1127979.pth.
[INFO][09:48:57]: [Client #88] Epoch: [5/5][0/10]	Loss: 0.190002
[INFO][09:48:57]: [Client #88] Model saved to /data/ykang/plato/results/test/model/lenet5_88_1127977.pth.
[INFO][09:48:58]: [Client #261] Loading a model from /data/ykang/plato/results/test/model/lenet5_261_1127978.pth.
[INFO][09:48:58]: [Client #261] Model trained.
[INFO][09:48:58]: [Client #261] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:48:58]: [Server #1127936] Received 0.24 MB of payload data from client #261 (simulated).
[INFO][09:48:58]: [Client #100] Loading a model from /data/ykang/plato/results/test/model/lenet5_100_1127979.pth.
[INFO][09:48:58]: [Client #100] Model trained.
[INFO][09:48:58]: [Client #100] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:48:58]: [Server #1127936] Received 0.24 MB of payload data from client #100 (simulated).
[INFO][09:48:58]: [Client #88] Loading a model from /data/ykang/plato/results/test/model/lenet5_88_1127977.pth.
[INFO][09:48:58]: [Client #88] Model trained.
[INFO][09:48:58]: [Client #88] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:48:58]: [Server #1127936] Received 0.24 MB of payload data from client #88 (simulated).
[INFO][09:48:58]: [Server #1127936] Selecting client #426 for training.
[INFO][09:48:58]: [Server #1127936] Sending the current model to client #426 (simulated).
[INFO][09:48:58]: [Server #1127936] Sending 0.24 MB of payload data to client #426 (simulated).
[INFO][09:48:58]: [Server #1127936] Selecting client #129 for training.
[INFO][09:48:58]: [Server #1127936] Sending the current model to client #129 (simulated).
[INFO][09:48:58]: [Server #1127936] Sending 0.24 MB of payload data to client #129 (simulated).
[INFO][09:48:58]: [Server #1127936] Selecting client #353 for training.
[INFO][09:48:58]: [Server #1127936] Sending the current model to client #353 (simulated).
[INFO][09:48:58]: [Client #426] Selected by the server.
[INFO][09:48:58]: [Client #426] Loading its data source...
[INFO][09:48:58]: [Client #426] Dataset size: 60000
[INFO][09:48:58]: [Client #426] Sampler: noniid
[INFO][09:48:58]: [Server #1127936] Sending 0.24 MB of payload data to client #353 (simulated).
[INFO][09:48:58]: [Client #129] Selected by the server.
[INFO][09:48:58]: [Client #129] Loading its data source...
[INFO][09:48:58]: [Client #129] Dataset size: 60000
[INFO][09:48:58]: [Client #426] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:48:58]: [Client #129] Sampler: noniid
[INFO][09:48:58]: [Client #353] Selected by the server.
[INFO][09:48:58]: [Client #353] Loading its data source...
[INFO][09:48:58]: [Client #353] Dataset size: 60000
[INFO][09:48:58]: [Client #353] Sampler: noniid
[INFO][09:48:58]: [Client #129] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:48:58]: [Client #353] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:48:58]: [93m[1m[Client #129] Started training in communication round #13.[0m
[INFO][09:48:58]: [93m[1m[Client #426] Started training in communication round #13.[0m
[INFO][09:48:58]: [93m[1m[Client #353] Started training in communication round #13.[0m
[INFO][09:49:00]: [Client #129] Loading the dataset.
[INFO][09:49:00]: [Client #353] Loading the dataset.
[INFO][09:49:00]: [Client #426] Loading the dataset.
[INFO][09:49:06]: [Client #426] Epoch: [1/5][0/10]	Loss: 1.860755
[INFO][09:49:06]: [Client #353] Epoch: [1/5][0/10]	Loss: 0.127478
[INFO][09:49:06]: [Client #129] Epoch: [1/5][0/10]	Loss: 0.339111
[INFO][09:49:06]: [Client #426] Epoch: [2/5][0/10]	Loss: 0.059312
[INFO][09:49:06]: [Client #353] Epoch: [2/5][0/10]	Loss: 0.015152
[INFO][09:49:06]: [Client #129] Epoch: [2/5][0/10]	Loss: 0.062992
[INFO][09:49:06]: [Client #426] Epoch: [3/5][0/10]	Loss: 0.241308
[INFO][09:49:06]: [Client #129] Epoch: [3/5][0/10]	Loss: 0.000063
[INFO][09:49:06]: [Client #353] Epoch: [3/5][0/10]	Loss: 0.201124
[INFO][09:49:06]: [Client #426] Epoch: [4/5][0/10]	Loss: 0.102529
[INFO][09:49:06]: [Client #353] Epoch: [4/5][0/10]	Loss: 0.257507
[INFO][09:49:06]: [Client #129] Epoch: [4/5][0/10]	Loss: 0.005915
[INFO][09:49:07]: [Client #426] Epoch: [5/5][0/10]	Loss: 0.125759
[INFO][09:49:07]: [Client #129] Epoch: [5/5][0/10]	Loss: 0.012822
[INFO][09:49:07]: [Client #426] Model saved to /data/ykang/plato/results/test/model/lenet5_426_1127977.pth.
[INFO][09:49:07]: [Client #353] Epoch: [5/5][0/10]	Loss: 0.002998
[INFO][09:49:07]: [Client #353] Model saved to /data/ykang/plato/results/test/model/lenet5_353_1127979.pth.
[INFO][09:49:07]: [Client #129] Model saved to /data/ykang/plato/results/test/model/lenet5_129_1127978.pth.
[INFO][09:49:07]: [Client #426] Loading a model from /data/ykang/plato/results/test/model/lenet5_426_1127977.pth.
[INFO][09:49:07]: [Client #426] Model trained.
[INFO][09:49:07]: [Client #426] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:49:07]: [Server #1127936] Received 0.24 MB of payload data from client #426 (simulated).
[INFO][09:49:08]: [Client #353] Loading a model from /data/ykang/plato/results/test/model/lenet5_353_1127979.pth.
[INFO][09:49:08]: [Client #353] Model trained.
[INFO][09:49:08]: [Client #353] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:49:08]: [Server #1127936] Received 0.24 MB of payload data from client #353 (simulated).
[INFO][09:49:08]: [Client #129] Loading a model from /data/ykang/plato/results/test/model/lenet5_129_1127978.pth.
[INFO][09:49:08]: [Client #129] Model trained.
[INFO][09:49:08]: [Client #129] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:49:08]: [Server #1127936] Received 0.24 MB of payload data from client #129 (simulated).
[INFO][09:49:08]: [Server #1127936] Selecting client #272 for training.
[INFO][09:49:08]: [Server #1127936] Sending the current model to client #272 (simulated).
[INFO][09:49:08]: [Server #1127936] Sending 0.24 MB of payload data to client #272 (simulated).
[INFO][09:49:08]: [Server #1127936] Selecting client #476 for training.
[INFO][09:49:08]: [Server #1127936] Sending the current model to client #476 (simulated).
[INFO][09:49:08]: [Server #1127936] Sending 0.24 MB of payload data to client #476 (simulated).
[INFO][09:49:08]: [Server #1127936] Selecting client #314 for training.
[INFO][09:49:08]: [Server #1127936] Sending the current model to client #314 (simulated).
[INFO][09:49:08]: [Client #272] Selected by the server.
[INFO][09:49:08]: [Client #272] Loading its data source...
[INFO][09:49:08]: [Client #272] Dataset size: 60000
[INFO][09:49:08]: [Client #272] Sampler: noniid
[INFO][09:49:08]: [Server #1127936] Sending 0.24 MB of payload data to client #314 (simulated).
[INFO][09:49:08]: [Client #476] Selected by the server.
[INFO][09:49:08]: [Client #476] Loading its data source...
[INFO][09:49:08]: [Client #272] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:49:08]: [Client #476] Dataset size: 60000
[INFO][09:49:08]: [Client #476] Sampler: noniid
[INFO][09:49:08]: [Client #314] Selected by the server.
[INFO][09:49:08]: [Client #314] Loading its data source...
[INFO][09:49:08]: [Client #314] Dataset size: 60000
[INFO][09:49:08]: [Client #314] Sampler: noniid
[INFO][09:49:08]: [Client #476] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:49:08]: [Client #314] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:49:08]: [93m[1m[Client #272] Started training in communication round #13.[0m
[INFO][09:49:08]: [93m[1m[Client #314] Started training in communication round #13.[0m
[INFO][09:49:08]: [93m[1m[Client #476] Started training in communication round #13.[0m
[INFO][09:49:10]: [Client #476] Loading the dataset.
[INFO][09:49:10]: [Client #314] Loading the dataset.
[INFO][09:49:10]: [Client #272] Loading the dataset.
[INFO][09:49:16]: [Client #476] Epoch: [1/5][0/10]	Loss: 3.463052
[INFO][09:49:16]: [Client #476] Epoch: [2/5][0/10]	Loss: 0.000182
[INFO][09:49:16]: [Client #314] Epoch: [1/5][0/10]	Loss: 1.525191
[INFO][09:49:16]: [Client #272] Epoch: [1/5][0/10]	Loss: 0.982675
[INFO][09:49:16]: [Client #476] Epoch: [3/5][0/10]	Loss: 0.000195
[INFO][09:49:16]: [Client #314] Epoch: [2/5][0/10]	Loss: 0.039195
[INFO][09:49:16]: [Client #476] Epoch: [4/5][0/10]	Loss: 0.004894
[INFO][09:49:16]: [Client #272] Epoch: [2/5][0/10]	Loss: 0.409174
[INFO][09:49:16]: [Client #314] Epoch: [3/5][0/10]	Loss: 0.027522
[INFO][09:49:16]: [Client #272] Epoch: [3/5][0/10]	Loss: 0.069088
[INFO][09:49:16]: [Client #476] Epoch: [5/5][0/10]	Loss: 0.036234
[INFO][09:49:16]: [Client #314] Epoch: [4/5][0/10]	Loss: 0.457822
[INFO][09:49:16]: [Client #476] Model saved to /data/ykang/plato/results/test/model/lenet5_476_1127978.pth.
[INFO][09:49:16]: [Client #272] Epoch: [4/5][0/10]	Loss: 0.469992
[INFO][09:49:16]: [Client #314] Epoch: [5/5][0/10]	Loss: 0.565417
[INFO][09:49:16]: [Client #272] Epoch: [5/5][0/10]	Loss: 0.222776
[INFO][09:49:16]: [Client #314] Model saved to /data/ykang/plato/results/test/model/lenet5_314_1127979.pth.
[INFO][09:49:16]: [Client #272] Model saved to /data/ykang/plato/results/test/model/lenet5_272_1127977.pth.
[INFO][09:49:17]: [Client #476] Loading a model from /data/ykang/plato/results/test/model/lenet5_476_1127978.pth.
[INFO][09:49:17]: [Client #476] Model trained.
[INFO][09:49:17]: [Client #476] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:49:17]: [Server #1127936] Received 0.24 MB of payload data from client #476 (simulated).
[INFO][09:49:17]: [Client #314] Loading a model from /data/ykang/plato/results/test/model/lenet5_314_1127979.pth.
[INFO][09:49:17]: [Client #314] Model trained.
[INFO][09:49:17]: [Client #314] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:49:17]: [Server #1127936] Received 0.24 MB of payload data from client #314 (simulated).
[INFO][09:49:17]: [Client #272] Loading a model from /data/ykang/plato/results/test/model/lenet5_272_1127977.pth.
[INFO][09:49:17]: [Client #272] Model trained.
[INFO][09:49:17]: [Client #272] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:49:17]: [Server #1127936] Received 0.24 MB of payload data from client #272 (simulated).
[INFO][09:49:17]: [Server #1127936] Selecting client #421 for training.
[INFO][09:49:17]: [Server #1127936] Sending the current model to client #421 (simulated).
[INFO][09:49:17]: [Server #1127936] Sending 0.24 MB of payload data to client #421 (simulated).
[INFO][09:49:17]: [Client #421] Selected by the server.
[INFO][09:49:17]: [Client #421] Loading its data source...
[INFO][09:49:17]: [Client #421] Dataset size: 60000
[INFO][09:49:17]: [Client #421] Sampler: noniid
[INFO][09:49:17]: [Client #421] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:49:17]: [93m[1m[Client #421] Started training in communication round #13.[0m
[INFO][09:49:19]: [Client #421] Loading the dataset.
[INFO][09:49:25]: [Client #421] Epoch: [1/5][0/10]	Loss: 1.558101
[INFO][09:49:25]: [Client #421] Epoch: [2/5][0/10]	Loss: 0.131400
[INFO][09:49:25]: [Client #421] Epoch: [3/5][0/10]	Loss: 0.001398
[INFO][09:49:25]: [Client #421] Epoch: [4/5][0/10]	Loss: 0.021237
[INFO][09:49:25]: [Client #421] Epoch: [5/5][0/10]	Loss: 0.001297
[INFO][09:49:25]: [Client #421] Model saved to /data/ykang/plato/results/test/model/lenet5_421_1127977.pth.
[INFO][09:49:26]: [Client #421] Loading a model from /data/ykang/plato/results/test/model/lenet5_421_1127977.pth.
[INFO][09:49:26]: [Client #421] Model trained.
[INFO][09:49:26]: [Client #421] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:49:26]: [Server #1127936] Received 0.24 MB of payload data from client #421 (simulated).
[INFO][09:49:26]: [Server #1127936] Adding client #236 to the list of clients for aggregation.
[INFO][09:49:26]: [Server #1127936] Adding client #330 to the list of clients for aggregation.
[INFO][09:49:26]: [Server #1127936] Adding client #67 to the list of clients for aggregation.
[INFO][09:49:26]: [Server #1127936] Adding client #2 to the list of clients for aggregation.
[INFO][09:49:26]: [Server #1127936] Adding client #275 to the list of clients for aggregation.
[INFO][09:49:26]: [Server #1127936] Adding client #439 to the list of clients for aggregation.
[INFO][09:49:26]: [Server #1127936] Adding client #476 to the list of clients for aggregation.
[INFO][09:49:26]: [Server #1127936] Adding client #314 to the list of clients for aggregation.
[INFO][09:49:26]: [Server #1127936] Adding client #426 to the list of clients for aggregation.
[INFO][09:49:26]: [Server #1127936] Adding client #129 to the list of clients for aggregation.
[INFO][09:49:26]: [Server #1127936] Aggregating 10 clients in total.
[1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.07655339 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.05111516 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.03870242 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.24775869 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.02117485 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.06156765 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.02569404
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.06383101
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.02921625 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.26465881 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 2. 0. 6. 0. 1. 2. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 2. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 4. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]
local_gradient_bounds:  [0.         0.07655339 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.05111516 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.03870242 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.24775869 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.02117485 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.06156765 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.02569404
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.06383101
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.02921625 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.26465881 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][09:49:28]: [Server #1127936] Global model accuracy: 73.72%

[INFO][09:49:28]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_13.pth.
[INFO][09:49:28]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_13.pth.
[INFO][09:49:28]: [93m[1m
[Server #1127936] Starting round 14/100.[0m
[0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002
 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8870e+00  6e-04  1e-08  1e-08
 5:  6.8876e+00  6.8873e+00  3e-04  4e-09  4e-09
 6:  6.8875e+00  6.8873e+00  2e-04  2e-08  6e-09
 7:  6.8875e+00  6.8873e+00  2e-04  2e-08  5e-09
 8:  6.8874e+00  6.8873e+00  9e-05  2e-07  4e-08
 9:  6.8874e+00  6.8873e+00  4e-05  1e-07  3e-08
10:  6.8873e+00  6.8873e+00  2e-06  4e-08  1e-08
Optimal solution found.
The calculated probability is:  [2.23208232e-05 5.60393314e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 3.90045215e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23177134e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 9.88978830e-01 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 6.40998590e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23129547e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.88870843e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23123657e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 3.00244971e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.21761394e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05 2.23208232e-05 2.23208232e-05
 2.23208232e-05 2.23208232e-05]
current clients pool:  [INFO][09:49:29]: [Server #1127936] Selected clients: [236 402 240  66 465 449 247 430 210 342]
[INFO][09:49:29]: [Server #1127936] Selecting client #236 for training.
[INFO][09:49:29]: [Server #1127936] Sending the current model to client #236 (simulated).
[INFO][09:49:29]: [Server #1127936] Sending 0.24 MB of payload data to client #236 (simulated).
[INFO][09:49:29]: [Server #1127936] Selecting client #402 for training.
[INFO][09:49:29]: [Server #1127936] Sending the current model to client #402 (simulated).
[INFO][09:49:29]: [Server #1127936] Sending 0.24 MB of payload data to client #402 (simulated).
[INFO][09:49:29]: [Server #1127936] Selecting client #240 for training.
[INFO][09:49:29]: [Server #1127936] Sending the current model to client #240 (simulated).
[INFO][09:49:29]: [Client #236] Selected by the server.
[INFO][09:49:29]: [Client #236] Loading its data source...
[INFO][09:49:29]: [Client #236] Dataset size: 60000
[INFO][09:49:29]: [Client #236] Sampler: noniid
[INFO][09:49:29]: [Server #1127936] Sending 0.24 MB of payload data to client #240 (simulated).
[INFO][09:49:29]: [Client #402] Selected by the server.
[INFO][09:49:29]: [Client #402] Loading its data source...
[INFO][09:49:29]: [Client #402] Dataset size: 60000
[INFO][09:49:29]: [Client #402] Sampler: noniid
[INFO][09:49:29]: [Client #240] Selected by the server.
[INFO][09:49:29]: [Client #236] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:49:29]: [Client #240] Loading its data source...
[INFO][09:49:29]: [Client #240] Dataset size: 60000
[INFO][09:49:29]: [Client #240] Sampler: noniid
[INFO][09:49:29]: [Client #402] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:49:29]: [Client #240] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:49:29]: [93m[1m[Client #402] Started training in communication round #14.[0m
[INFO][09:49:29]: [93m[1m[Client #236] Started training in communication round #14.[0m
[INFO][09:49:29]: [93m[1m[Client #240] Started training in communication round #14.[0m
[INFO][09:49:31]: [Client #240] Loading the dataset.
[INFO][09:49:31]: [Client #402] Loading the dataset.
[INFO][09:49:31]: [Client #236] Loading the dataset.
[INFO][09:49:37]: [Client #236] Epoch: [1/5][0/10]	Loss: 1.028935
[INFO][09:49:37]: [Client #240] Epoch: [1/5][0/10]	Loss: 0.083440
[INFO][09:49:37]: [Client #402] Epoch: [1/5][0/10]	Loss: 1.393440
[INFO][09:49:37]: [Client #236] Epoch: [2/5][0/10]	Loss: 0.000064
[INFO][09:49:37]: [Client #402] Epoch: [2/5][0/10]	Loss: 0.225389
[INFO][09:49:37]: [Client #240] Epoch: [2/5][0/10]	Loss: 0.206158
[INFO][09:49:37]: [Client #402] Epoch: [3/5][0/10]	Loss: 0.053262
[INFO][09:49:37]: [Client #236] Epoch: [3/5][0/10]	Loss: 0.000452
[INFO][09:49:37]: [Client #240] Epoch: [3/5][0/10]	Loss: 0.033967
[INFO][09:49:37]: [Client #402] Epoch: [4/5][0/10]	Loss: 0.102756
[INFO][09:49:37]: [Client #236] Epoch: [4/5][0/10]	Loss: 0.040740
[INFO][09:49:37]: [Client #402] Epoch: [5/5][0/10]	Loss: 0.107640
[INFO][09:49:37]: [Client #240] Epoch: [4/5][0/10]	Loss: 0.006634
[INFO][09:49:37]: [Client #236] Epoch: [5/5][0/10]	Loss: 0.000984
[INFO][09:49:37]: [Client #402] Model saved to /data/ykang/plato/results/test/model/lenet5_402_1127978.pth.
[INFO][09:49:37]: [Client #236] Model saved to /data/ykang/plato/results/test/model/lenet5_236_1127977.pth.
[INFO][09:49:37]: [Client #240] Epoch: [5/5][0/10]	Loss: 0.002230
[INFO][09:49:37]: [Client #240] Model saved to /data/ykang/plato/results/test/model/lenet5_240_1127979.pth.
[INFO][09:49:38]: [Client #402] Loading a model from /data/ykang/plato/results/test/model/lenet5_402_1127978.pth.
[INFO][09:49:38]: [Client #402] Model trained.
[INFO][09:49:38]: [Client #402] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:49:38]: [Server #1127936] Received 0.24 MB of payload data from client #402 (simulated).
[INFO][09:49:38]: [Client #236] Loading a model from /data/ykang/plato/results/test/model/lenet5_236_1127977.pth.
[INFO][09:49:38]: [Client #236] Model trained.
[INFO][09:49:38]: [Client #236] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:49:38]: [Server #1127936] Received 0.24 MB of payload data from client #236 (simulated).
[INFO][09:49:38]: [Client #240] Loading a model from /data/ykang/plato/results/test/model/lenet5_240_1127979.pth.
[INFO][09:49:38]: [Client #240] Model trained.
[INFO][09:49:38]: [Client #240] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:49:38]: [Server #1127936] Received 0.24 MB of payload data from client #240 (simulated).
[INFO][09:49:38]: [Server #1127936] Selecting client #66 for training.
[INFO][09:49:38]: [Server #1127936] Sending the current model to client #66 (simulated).
[INFO][09:49:38]: [Server #1127936] Sending 0.24 MB of payload data to client #66 (simulated).
[INFO][09:49:38]: [Server #1127936] Selecting client #465 for training.
[INFO][09:49:38]: [Server #1127936] Sending the current model to client #465 (simulated).
[INFO][09:49:38]: [Server #1127936] Sending 0.24 MB of payload data to client #465 (simulated).
[INFO][09:49:38]: [Server #1127936] Selecting client #449 for training.
[INFO][09:49:38]: [Server #1127936] Sending the current model to client #449 (simulated).
[INFO][09:49:38]: [Client #66] Selected by the server.
[INFO][09:49:38]: [Client #66] Loading its data source...
[INFO][09:49:38]: [Client #66] Dataset size: 60000
[INFO][09:49:38]: [Client #66] Sampler: noniid
[INFO][09:49:38]: [Server #1127936] Sending 0.24 MB of payload data to client #449 (simulated).
[INFO][09:49:38]: [Client #465] Selected by the server.
[INFO][09:49:38]: [Client #465] Loading its data source...
[INFO][09:49:38]: [Client #465] Dataset size: 60000
[INFO][09:49:38]: [Client #465] Sampler: noniid
[INFO][09:49:38]: [Client #449] Selected by the server.
[INFO][09:49:38]: [Client #449] Loading its data source...
[INFO][09:49:38]: [Client #449] Dataset size: 60000
[INFO][09:49:38]: [Client #449] Sampler: noniid
[INFO][09:49:38]: [Client #465] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:49:38]: [Client #66] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:49:38]: [93m[1m[Client #465] Started training in communication round #14.[0m
[INFO][09:49:38]: [Client #449] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:49:38]: [93m[1m[Client #66] Started training in communication round #14.[0m
[INFO][09:49:38]: [93m[1m[Client #449] Started training in communication round #14.[0m
[INFO][09:49:40]: [Client #449] Loading the dataset.
[INFO][09:49:40]: [Client #66] Loading the dataset.
[INFO][09:49:41]: [Client #465] Loading the dataset.
[INFO][09:49:47]: [Client #465] Epoch: [1/5][0/10]	Loss: 0.349671
[INFO][09:49:47]: [Client #66] Epoch: [1/5][0/10]	Loss: 1.010836
[INFO][09:49:47]: [Client #449] Epoch: [1/5][0/10]	Loss: 0.857291
[INFO][09:49:47]: [Client #465] Epoch: [2/5][0/10]	Loss: 0.067558
[INFO][09:49:47]: [Client #66] Epoch: [2/5][0/10]	Loss: 0.315774
[INFO][09:49:47]: [Client #449] Epoch: [2/5][0/10]	Loss: 0.295232
[INFO][09:49:47]: [Client #465] Epoch: [3/5][0/10]	Loss: 0.072195
[INFO][09:49:47]: [Client #449] Epoch: [3/5][0/10]	Loss: 0.002311
[INFO][09:49:47]: [Client #66] Epoch: [3/5][0/10]	Loss: 0.002974
[INFO][09:49:47]: [Client #465] Epoch: [4/5][0/10]	Loss: 0.120224
[INFO][09:49:47]: [Client #449] Epoch: [4/5][0/10]	Loss: 0.320537
[INFO][09:49:47]: [Client #66] Epoch: [4/5][0/10]	Loss: 0.646489
[INFO][09:49:47]: [Client #465] Epoch: [5/5][0/10]	Loss: 0.114251
[INFO][09:49:47]: [Client #449] Epoch: [5/5][0/10]	Loss: 0.074213
[INFO][09:49:47]: [Client #66] Epoch: [5/5][0/10]	Loss: 0.298681
[INFO][09:49:47]: [Client #465] Model saved to /data/ykang/plato/results/test/model/lenet5_465_1127978.pth.
[INFO][09:49:47]: [Client #449] Model saved to /data/ykang/plato/results/test/model/lenet5_449_1127979.pth.
[INFO][09:49:47]: [Client #66] Model saved to /data/ykang/plato/results/test/model/lenet5_66_1127977.pth.
[INFO][09:49:48]: [Client #465] Loading a model from /data/ykang/plato/results/test/model/lenet5_465_1127978.pth.
[INFO][09:49:48]: [Client #465] Model trained.
[INFO][09:49:48]: [Client #465] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:49:48]: [Server #1127936] Received 0.24 MB of payload data from client #465 (simulated).
[INFO][09:49:48]: [Client #449] Loading a model from /data/ykang/plato/results/test/model/lenet5_449_1127979.pth.
[INFO][09:49:48]: [Client #449] Model trained.
[INFO][09:49:48]: [Client #449] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:49:48]: [Server #1127936] Received 0.24 MB of payload data from client #449 (simulated).
[INFO][09:49:48]: [Client #66] Loading a model from /data/ykang/plato/results/test/model/lenet5_66_1127977.pth.
[INFO][09:49:48]: [Client #66] Model trained.
[INFO][09:49:48]: [Client #66] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:49:48]: [Server #1127936] Received 0.24 MB of payload data from client #66 (simulated).
[INFO][09:49:48]: [Server #1127936] Selecting client #247 for training.
[INFO][09:49:48]: [Server #1127936] Sending the current model to client #247 (simulated).
[INFO][09:49:48]: [Server #1127936] Sending 0.24 MB of payload data to client #247 (simulated).
[INFO][09:49:48]: [Server #1127936] Selecting client #430 for training.
[INFO][09:49:48]: [Server #1127936] Sending the current model to client #430 (simulated).
[INFO][09:49:48]: [Server #1127936] Sending 0.24 MB of payload data to client #430 (simulated).
[INFO][09:49:48]: [Server #1127936] Selecting client #210 for training.
[INFO][09:49:48]: [Server #1127936] Sending the current model to client #210 (simulated).
[INFO][09:49:48]: [Client #247] Selected by the server.
[INFO][09:49:48]: [Client #247] Loading its data source...
[INFO][09:49:48]: [Client #247] Dataset size: 60000
[INFO][09:49:48]: [Client #247] Sampler: noniid
[INFO][09:49:48]: [Server #1127936] Sending 0.24 MB of payload data to client #210 (simulated).
[INFO][09:49:48]: [Client #247] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:49:48]: [Client #430] Selected by the server.
[INFO][09:49:48]: [Client #430] Loading its data source...
[INFO][09:49:48]: [Client #430] Dataset size: 60000
[INFO][09:49:48]: [Client #430] Sampler: noniid
[INFO][09:49:48]: [Client #210] Selected by the server.
[INFO][09:49:48]: [Client #210] Loading its data source...
[INFO][09:49:48]: [Client #210] Dataset size: 60000
[INFO][09:49:48]: [Client #210] Sampler: noniid
[INFO][09:49:48]: [Client #430] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:49:48]: [Client #210] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:49:48]: [93m[1m[Client #247] Started training in communication round #14.[0m
[INFO][09:49:48]: [93m[1m[Client #210] Started training in communication round #14.[0m
[INFO][09:49:48]: [93m[1m[Client #430] Started training in communication round #14.[0m
[INFO][09:49:50]: [Client #210] Loading the dataset.
[INFO][09:49:50]: [Client #247] Loading the dataset.
[INFO][09:49:50]: [Client #430] Loading the dataset.
[INFO][09:49:56]: [Client #430] Epoch: [1/5][0/10]	Loss: 0.456450
[INFO][09:49:56]: [Client #247] Epoch: [1/5][0/10]	Loss: 0.242069
[INFO][09:49:56]: [Client #430] Epoch: [2/5][0/10]	Loss: 0.025099
[INFO][09:49:56]: [Client #210] Epoch: [1/5][0/10]	Loss: 0.921057
[INFO][09:49:56]: [Client #247] Epoch: [2/5][0/10]	Loss: 0.168862
[INFO][09:49:56]: [Client #430] Epoch: [3/5][0/10]	Loss: 0.102331
[INFO][09:49:56]: [Client #247] Epoch: [3/5][0/10]	Loss: 0.029797
[INFO][09:49:57]: [Client #210] Epoch: [2/5][0/10]	Loss: 0.094932
[INFO][09:49:57]: [Client #430] Epoch: [4/5][0/10]	Loss: 0.008076
[INFO][09:49:57]: [Client #247] Epoch: [4/5][0/10]	Loss: 0.000177
[INFO][09:49:57]: [Client #430] Epoch: [5/5][0/10]	Loss: 0.051453
[INFO][09:49:57]: [Client #210] Epoch: [3/5][0/10]	Loss: 0.366253
[INFO][09:49:57]: [Client #430] Model saved to /data/ykang/plato/results/test/model/lenet5_430_1127978.pth.
[INFO][09:49:57]: [Client #247] Epoch: [5/5][0/10]	Loss: 0.022095
[INFO][09:49:57]: [Client #210] Epoch: [4/5][0/10]	Loss: 0.113726
[INFO][09:49:57]: [Client #247] Model saved to /data/ykang/plato/results/test/model/lenet5_247_1127977.pth.
[INFO][09:49:57]: [Client #210] Epoch: [5/5][0/10]	Loss: 0.021256
[INFO][09:49:57]: [Client #210] Model saved to /data/ykang/plato/results/test/model/lenet5_210_1127979.pth.
[INFO][09:49:58]: [Client #430] Loading a model from /data/ykang/plato/results/test/model/lenet5_430_1127978.pth.
[INFO][09:49:58]: [Client #430] Model trained.
[INFO][09:49:58]: [Client #430] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:49:58]: [Server #1127936] Received 0.24 MB of payload data from client #430 (simulated).
[INFO][09:49:58]: [Client #247] Loading a model from /data/ykang/plato/results/test/model/lenet5_247_1127977.pth.
[INFO][09:49:58]: [Client #247] Model trained.
[INFO][09:49:58]: [Client #247] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:49:58]: [Server #1127936] Received 0.24 MB of payload data from client #247 (simulated).
[INFO][09:49:58]: [Client #210] Loading a model from /data/ykang/plato/results/test/model/lenet5_210_1127979.pth.
[INFO][09:49:58]: [Client #210] Model trained.
[INFO][09:49:58]: [Client #210] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:49:58]: [Server #1127936] Received 0.24 MB of payload data from client #210 (simulated).
[INFO][09:49:58]: [Server #1127936] Selecting client #342 for training.
[INFO][09:49:58]: [Server #1127936] Sending the current model to client #342 (simulated).
[INFO][09:49:58]: [Server #1127936] Sending 0.24 MB of payload data to client #342 (simulated).
[INFO][09:49:58]: [Client #342] Selected by the server.
[INFO][09:49:58]: [Client #342] Loading its data source...
[INFO][09:49:58]: [Client #342] Dataset size: 60000
[INFO][09:49:58]: [Client #342] Sampler: noniid
[INFO][09:49:58]: [Client #342] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:49:58]: [93m[1m[Client #342] Started training in communication round #14.[0m
[INFO][09:50:00]: [Client #342] Loading the dataset.
[INFO][09:50:05]: [Client #342] Epoch: [1/5][0/10]	Loss: 0.147876
[INFO][09:50:05]: [Client #342] Epoch: [2/5][0/10]	Loss: 0.052831
[INFO][09:50:05]: [Client #342] Epoch: [3/5][0/10]	Loss: 0.000295
[INFO][09:50:05]: [Client #342] Epoch: [4/5][0/10]	Loss: 0.001283
[INFO][09:50:06]: [Client #342] Epoch: [5/5][0/10]	Loss: 0.000099
[INFO][09:50:06]: [Client #342] Model saved to /data/ykang/plato/results/test/model/lenet5_342_1127977.pth.
[INFO][09:50:06]: [Client #342] Loading a model from /data/ykang/plato/results/test/model/lenet5_342_1127977.pth.
[INFO][09:50:06]: [Client #342] Model trained.
[INFO][09:50:06]: [Client #342] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:50:06]: [Server #1127936] Received 0.24 MB of payload data from client #342 (simulated).
[INFO][09:50:06]: [Server #1127936] Adding client #421 to the list of clients for aggregation.
[INFO][09:50:06]: [Server #1127936] Adding client #353 to the list of clients for aggregation.
[INFO][09:50:06]: [Server #1127936] Adding client #272 to the list of clients for aggregation.
[INFO][09:50:06]: [Server #1127936] Adding client #100 to the list of clients for aggregation.
[INFO][09:50:06]: [Server #1127936] Adding client #261 to the list of clients for aggregation.
[INFO][09:50:06]: [Server #1127936] Adding client #41 to the list of clients for aggregation.
[INFO][09:50:06]: [Server #1127936] Adding client #152 to the list of clients for aggregation.
[INFO][09:50:06]: [Server #1127936] Adding client #247 to the list of clients for aggregation.
[INFO][09:50:06]: [Server #1127936] Adding client #449 to the list of clients for aggregation.
[INFO][09:50:06]: [Server #1127936] Adding client #66 to the list of clients for aggregation.
[INFO][09:50:06]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.09588456 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.07935217
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.13954214 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.0161583  0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.03463645 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.02808385 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.0900685  0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.03171488 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.11151513 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.08202577 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 1. 6. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 2. 0. 6. 0. 1. 2. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 2. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 4. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.09588456 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.07935217
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.13954214 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.0161583  0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.03463645 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.02808385 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.0900685  0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.03171488 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.11151513 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.08202577 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][09:50:09]: [Server #1127936] Global model accuracy: 88.50%

[INFO][09:50:09]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_14.pth.
[INFO][09:50:09]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_14.pth.
[INFO][09:50:09]: [93m[1m
[Server #1127936] Starting round 15/100.[0m
[0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8867e+00  9e-04  2e-08  2e-08
 5:  6.8875e+00  6.8870e+00  6e-04  8e-09  8e-09
 6:  6.8874e+00  6.8869e+00  6e-04  2e-07  1e-07
 7:  6.8874e+00  6.8870e+00  4e-04  2e-07  1e-07
 8:  6.8873e+00  6.8871e+00  2e-04  3e-07  2e-07
 9:  6.8872e+00  6.8871e+00  6e-05  6e-07  4e-07
10:  6.8871e+00  6.8871e+00  1e-05  2e-07  1e-07
11:  6.8871e+00  6.8871e+00  7e-07  1e-08  8e-09
Optimal solution found.
The calculated probability is:  [4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 9.97773080e-01 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51486405e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 1.18323154e-05 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 5.94004230e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51605624e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 5.26188740e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 7.85601359e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 5.37298724e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 9.26897445e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51476316e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06 4.51633691e-06 4.51633691e-06
 4.51633691e-06 4.51633691e-06]
current clients pool:  [INFO][09:50:09]: [Server #1127936] Selected clients: [ 41 397 255 396 348 389 202 323  91 160]
[INFO][09:50:09]: [Server #1127936] Selecting client #41 for training.
[INFO][09:50:09]: [Server #1127936] Sending the current model to client #41 (simulated).
[INFO][09:50:09]: [Server #1127936] Sending 0.24 MB of payload data to client #41 (simulated).
[INFO][09:50:09]: [Server #1127936] Selecting client #397 for training.
[INFO][09:50:09]: [Server #1127936] Sending the current model to client #397 (simulated).
[INFO][09:50:09]: [Server #1127936] Sending 0.24 MB of payload data to client #397 (simulated).
[INFO][09:50:09]: [Client #41] Selected by the server.
[INFO][09:50:09]: [Client #41] Loading its data source...
[INFO][09:50:09]: [Client #41] Dataset size: 60000
[INFO][09:50:09]: [Client #41] Sampler: noniid
[INFO][09:50:09]: [Client #41] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:50:09]: [93m[1m[Client #41] Started training in communication round #15.[0m
[INFO][09:50:09]: [Server #1127936] Selecting client #255 for training.
[INFO][09:50:09]: [Server #1127936] Sending the current model to client #255 (simulated).
[INFO][09:50:09]: [Server #1127936] Sending 0.24 MB of payload data to client #255 (simulated).
[INFO][09:50:09]: [Client #397] Selected by the server.
[INFO][09:50:09]: [Client #397] Loading its data source...
[INFO][09:50:09]: [Client #397] Dataset size: 60000
[INFO][09:50:09]: [Client #397] Sampler: noniid
[INFO][09:50:09]: [Client #397] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:50:09]: [93m[1m[Client #397] Started training in communication round #15.[0m
[INFO][09:50:09]: [Client #255] Selected by the server.
[INFO][09:50:09]: [Client #255] Loading its data source...
[INFO][09:50:09]: [Client #255] Dataset size: 60000
[INFO][09:50:09]: [Client #255] Sampler: noniid
[INFO][09:50:09]: [Client #255] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:50:09]: [93m[1m[Client #255] Started training in communication round #15.[0m
[INFO][09:50:11]: [Client #397] Loading the dataset.
[INFO][09:50:11]: [Client #255] Loading the dataset.
[INFO][09:50:11]: [Client #41] Loading the dataset.
[INFO][09:50:17]: [Client #397] Epoch: [1/5][0/10]	Loss: 0.559407
[INFO][09:50:17]: [Client #255] Epoch: [1/5][0/10]	Loss: 0.410523
[INFO][09:50:17]: [Client #41] Epoch: [1/5][0/10]	Loss: 0.665440
[INFO][09:50:17]: [Client #397] Epoch: [2/5][0/10]	Loss: 0.000007
[INFO][09:50:17]: [Client #255] Epoch: [2/5][0/10]	Loss: 0.001655
[INFO][09:50:17]: [Client #397] Epoch: [3/5][0/10]	Loss: 0.000000
[INFO][09:50:17]: [Client #41] Epoch: [2/5][0/10]	Loss: 0.065982
[INFO][09:50:18]: [Client #255] Epoch: [3/5][0/10]	Loss: 0.022393
[INFO][09:50:18]: [Client #397] Epoch: [4/5][0/10]	Loss: 0.000011
[INFO][09:50:18]: [Client #41] Epoch: [3/5][0/10]	Loss: 0.010620
[INFO][09:50:18]: [Client #255] Epoch: [4/5][0/10]	Loss: 0.014332
[INFO][09:50:18]: [Client #397] Epoch: [5/5][0/10]	Loss: 0.010052
[INFO][09:50:18]: [Client #41] Epoch: [4/5][0/10]	Loss: 0.024964
[INFO][09:50:18]: [Client #255] Epoch: [5/5][0/10]	Loss: 0.140766
[INFO][09:50:18]: [Client #397] Model saved to /data/ykang/plato/results/test/model/lenet5_397_1127978.pth.
[INFO][09:50:18]: [Client #255] Model saved to /data/ykang/plato/results/test/model/lenet5_255_1127979.pth.
[INFO][09:50:18]: [Client #41] Epoch: [5/5][0/10]	Loss: 0.155929
[INFO][09:50:18]: [Client #41] Model saved to /data/ykang/plato/results/test/model/lenet5_41_1127977.pth.
[INFO][09:50:19]: [Client #397] Loading a model from /data/ykang/plato/results/test/model/lenet5_397_1127978.pth.
[INFO][09:50:19]: [Client #397] Model trained.
[INFO][09:50:19]: [Client #397] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:50:19]: [Server #1127936] Received 0.24 MB of payload data from client #397 (simulated).
[INFO][09:50:19]: [Client #255] Loading a model from /data/ykang/plato/results/test/model/lenet5_255_1127979.pth.
[INFO][09:50:19]: [Client #255] Model trained.
[INFO][09:50:19]: [Client #255] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:50:19]: [Server #1127936] Received 0.24 MB of payload data from client #255 (simulated).
[INFO][09:50:19]: [Client #41] Loading a model from /data/ykang/plato/results/test/model/lenet5_41_1127977.pth.
[INFO][09:50:19]: [Client #41] Model trained.
[INFO][09:50:19]: [Client #41] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:50:19]: [Server #1127936] Received 0.24 MB of payload data from client #41 (simulated).
[INFO][09:50:19]: [Server #1127936] Selecting client #396 for training.
[INFO][09:50:19]: [Server #1127936] Sending the current model to client #396 (simulated).
[INFO][09:50:19]: [Server #1127936] Sending 0.24 MB of payload data to client #396 (simulated).
[INFO][09:50:19]: [Server #1127936] Selecting client #348 for training.
[INFO][09:50:19]: [Server #1127936] Sending the current model to client #348 (simulated).
[INFO][09:50:19]: [Server #1127936] Sending 0.24 MB of payload data to client #348 (simulated).
[INFO][09:50:19]: [Server #1127936] Selecting client #389 for training.
[INFO][09:50:19]: [Server #1127936] Sending the current model to client #389 (simulated).
[INFO][09:50:19]: [Client #396] Selected by the server.
[INFO][09:50:19]: [Client #396] Loading its data source...
[INFO][09:50:19]: [Client #396] Dataset size: 60000
[INFO][09:50:19]: [Client #396] Sampler: noniid
[INFO][09:50:19]: [Client #396] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:50:19]: [93m[1m[Client #396] Started training in communication round #15.[0m
[INFO][09:50:19]: [Server #1127936] Sending 0.24 MB of payload data to client #389 (simulated).
[INFO][09:50:19]: [Client #348] Selected by the server.
[INFO][09:50:19]: [Client #348] Loading its data source...
[INFO][09:50:19]: [Client #348] Dataset size: 60000
[INFO][09:50:19]: [Client #348] Sampler: noniid
[INFO][09:50:19]: [Client #389] Selected by the server.
[INFO][09:50:19]: [Client #389] Loading its data source...
[INFO][09:50:19]: [Client #389] Dataset size: 60000
[INFO][09:50:19]: [Client #389] Sampler: noniid
[INFO][09:50:19]: [Client #348] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:50:19]: [Client #389] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:50:19]: [93m[1m[Client #348] Started training in communication round #15.[0m
[INFO][09:50:19]: [93m[1m[Client #389] Started training in communication round #15.[0m
[INFO][09:50:21]: [Client #389] Loading the dataset.
[INFO][09:50:21]: [Client #348] Loading the dataset.
[INFO][09:50:21]: [Client #396] Loading the dataset.
[INFO][09:50:27]: [Client #396] Epoch: [1/5][0/10]	Loss: 0.220776
[INFO][09:50:27]: [Client #389] Epoch: [1/5][0/10]	Loss: 0.596338
[INFO][09:50:27]: [Client #348] Epoch: [1/5][0/10]	Loss: 0.492189
[INFO][09:50:27]: [Client #396] Epoch: [2/5][0/10]	Loss: 0.040323
[INFO][09:50:27]: [Client #348] Epoch: [2/5][0/10]	Loss: 0.139091
[INFO][09:50:27]: [Client #389] Epoch: [2/5][0/10]	Loss: 0.085117
[INFO][09:50:27]: [Client #348] Epoch: [3/5][0/10]	Loss: 0.009221
[INFO][09:50:27]: [Client #389] Epoch: [3/5][0/10]	Loss: 0.112877
[INFO][09:50:27]: [Client #396] Epoch: [3/5][0/10]	Loss: 0.001324
[INFO][09:50:27]: [Client #348] Epoch: [4/5][0/10]	Loss: 0.011222
[INFO][09:50:27]: [Client #389] Epoch: [4/5][0/10]	Loss: 0.230469
[INFO][09:50:28]: [Client #396] Epoch: [4/5][0/10]	Loss: 0.006054
[INFO][09:50:28]: [Client #348] Epoch: [5/5][0/10]	Loss: 0.042170
[INFO][09:50:28]: [Client #389] Epoch: [5/5][0/10]	Loss: 0.090862
[INFO][09:50:28]: [Client #396] Epoch: [5/5][0/10]	Loss: 0.000580
[INFO][09:50:28]: [Client #348] Model saved to /data/ykang/plato/results/test/model/lenet5_348_1127978.pth.
[INFO][09:50:28]: [Client #389] Model saved to /data/ykang/plato/results/test/model/lenet5_389_1127979.pth.
[INFO][09:50:28]: [Client #396] Model saved to /data/ykang/plato/results/test/model/lenet5_396_1127977.pth.
[INFO][09:50:28]: [Client #389] Loading a model from /data/ykang/plato/results/test/model/lenet5_389_1127979.pth.
[INFO][09:50:28]: [Client #389] Model trained.
[INFO][09:50:28]: [Client #389] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:50:28]: [Server #1127936] Received 0.24 MB of payload data from client #389 (simulated).
[INFO][09:50:29]: [Client #348] Loading a model from /data/ykang/plato/results/test/model/lenet5_348_1127978.pth.
[INFO][09:50:29]: [Client #348] Model trained.
[INFO][09:50:29]: [Client #348] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:50:29]: [Server #1127936] Received 0.24 MB of payload data from client #348 (simulated).
[INFO][09:50:29]: [Client #396] Loading a model from /data/ykang/plato/results/test/model/lenet5_396_1127977.pth.
[INFO][09:50:29]: [Client #396] Model trained.
[INFO][09:50:29]: [Client #396] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:50:29]: [Server #1127936] Received 0.24 MB of payload data from client #396 (simulated).
[INFO][09:50:29]: [Server #1127936] Selecting client #202 for training.
[INFO][09:50:29]: [Server #1127936] Sending the current model to client #202 (simulated).
[INFO][09:50:29]: [Server #1127936] Sending 0.24 MB of payload data to client #202 (simulated).
[INFO][09:50:29]: [Server #1127936] Selecting client #323 for training.
[INFO][09:50:29]: [Server #1127936] Sending the current model to client #323 (simulated).
[INFO][09:50:29]: [Server #1127936] Sending 0.24 MB of payload data to client #323 (simulated).
[INFO][09:50:29]: [Server #1127936] Selecting client #91 for training.
[INFO][09:50:29]: [Server #1127936] Sending the current model to client #91 (simulated).
[INFO][09:50:29]: [Client #202] Selected by the server.
[INFO][09:50:29]: [Client #202] Loading its data source...
[INFO][09:50:29]: [Client #202] Dataset size: 60000
[INFO][09:50:29]: [Client #202] Sampler: noniid
[INFO][09:50:29]: [Server #1127936] Sending 0.24 MB of payload data to client #91 (simulated).
[INFO][09:50:29]: [Client #323] Selected by the server.
[INFO][09:50:29]: [Client #323] Loading its data source...
[INFO][09:50:29]: [Client #91] Selected by the server.
[INFO][09:50:29]: [Client #323] Dataset size: 60000
[INFO][09:50:29]: [Client #91] Loading its data source...
[INFO][09:50:29]: [Client #323] Sampler: noniid
[INFO][09:50:29]: [Client #91] Dataset size: 60000
[INFO][09:50:29]: [Client #91] Sampler: noniid
[INFO][09:50:29]: [Client #202] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:50:29]: [Client #91] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:50:29]: [Client #323] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:50:29]: [93m[1m[Client #91] Started training in communication round #15.[0m
[INFO][09:50:29]: [93m[1m[Client #202] Started training in communication round #15.[0m
[INFO][09:50:29]: [93m[1m[Client #323] Started training in communication round #15.[0m
[INFO][09:50:31]: [Client #91] Loading the dataset.
[INFO][09:50:31]: [Client #202] Loading the dataset.
[INFO][09:50:31]: [Client #323] Loading the dataset.
[INFO][09:50:37]: [Client #91] Epoch: [1/5][0/10]	Loss: 0.660752
[INFO][09:50:37]: [Client #323] Epoch: [1/5][0/10]	Loss: 0.598029
[INFO][09:50:37]: [Client #91] Epoch: [2/5][0/10]	Loss: 0.000106
[INFO][09:50:37]: [Client #202] Epoch: [1/5][0/10]	Loss: 0.010741
[INFO][09:50:37]: [Client #323] Epoch: [2/5][0/10]	Loss: 0.466525
[INFO][09:50:37]: [Client #91] Epoch: [3/5][0/10]	Loss: 0.002216
[INFO][09:50:37]: [Client #323] Epoch: [3/5][0/10]	Loss: 0.069273
[INFO][09:50:37]: [Client #202] Epoch: [2/5][0/10]	Loss: 0.002901
[INFO][09:50:37]: [Client #91] Epoch: [4/5][0/10]	Loss: 0.000118
[INFO][09:50:37]: [Client #323] Epoch: [4/5][0/10]	Loss: 0.006082
[INFO][09:50:37]: [Client #202] Epoch: [3/5][0/10]	Loss: 0.000035
[INFO][09:50:37]: [Client #91] Epoch: [5/5][0/10]	Loss: 0.020522
[INFO][09:50:37]: [Client #323] Epoch: [5/5][0/10]	Loss: 0.024463
[INFO][09:50:37]: [Client #91] Model saved to /data/ykang/plato/results/test/model/lenet5_91_1127979.pth.
[INFO][09:50:37]: [Client #323] Model saved to /data/ykang/plato/results/test/model/lenet5_323_1127978.pth.
[INFO][09:50:37]: [Client #202] Epoch: [4/5][0/10]	Loss: 0.181526
[INFO][09:50:37]: [Client #202] Epoch: [5/5][0/10]	Loss: 0.000627
[INFO][09:50:37]: [Client #202] Model saved to /data/ykang/plato/results/test/model/lenet5_202_1127977.pth.
[INFO][09:50:38]: [Client #91] Loading a model from /data/ykang/plato/results/test/model/lenet5_91_1127979.pth.
[INFO][09:50:38]: [Client #91] Model trained.
[INFO][09:50:38]: [Client #91] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:50:38]: [Server #1127936] Received 0.24 MB of payload data from client #91 (simulated).
[INFO][09:50:38]: [Client #323] Loading a model from /data/ykang/plato/results/test/model/lenet5_323_1127978.pth.
[INFO][09:50:38]: [Client #323] Model trained.
[INFO][09:50:38]: [Client #323] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:50:38]: [Server #1127936] Received 0.24 MB of payload data from client #323 (simulated).
[INFO][09:50:38]: [Client #202] Loading a model from /data/ykang/plato/results/test/model/lenet5_202_1127977.pth.
[INFO][09:50:38]: [Client #202] Model trained.
[INFO][09:50:38]: [Client #202] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:50:38]: [Server #1127936] Received 0.24 MB of payload data from client #202 (simulated).
[INFO][09:50:38]: [Server #1127936] Selecting client #160 for training.
[INFO][09:50:38]: [Server #1127936] Sending the current model to client #160 (simulated).
[INFO][09:50:38]: [Server #1127936] Sending 0.24 MB of payload data to client #160 (simulated).
[INFO][09:50:38]: [Client #160] Selected by the server.
[INFO][09:50:38]: [Client #160] Loading its data source...
[INFO][09:50:38]: [Client #160] Dataset size: 60000
[INFO][09:50:38]: [Client #160] Sampler: noniid
[INFO][09:50:38]: [Client #160] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:50:38]: [93m[1m[Client #160] Started training in communication round #15.[0m
[INFO][09:50:40]: [Client #160] Loading the dataset.
[INFO][09:50:46]: [Client #160] Epoch: [1/5][0/10]	Loss: 1.091897
[INFO][09:50:46]: [Client #160] Epoch: [2/5][0/10]	Loss: 0.002207
[INFO][09:50:46]: [Client #160] Epoch: [3/5][0/10]	Loss: 0.000273
[INFO][09:50:46]: [Client #160] Epoch: [4/5][0/10]	Loss: 0.001265
[INFO][09:50:46]: [Client #160] Epoch: [5/5][0/10]	Loss: 0.002674
[INFO][09:50:46]: [Client #160] Model saved to /data/ykang/plato/results/test/model/lenet5_160_1127977.pth.
[INFO][09:50:47]: [Client #160] Loading a model from /data/ykang/plato/results/test/model/lenet5_160_1127977.pth.
[INFO][09:50:47]: [Client #160] Model trained.
[INFO][09:50:47]: [Client #160] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:50:47]: [Server #1127936] Received 0.24 MB of payload data from client #160 (simulated).
[INFO][09:50:47]: [Server #1127936] Adding client #236 to the list of clients for aggregation.
[INFO][09:50:47]: [Server #1127936] Adding client #430 to the list of clients for aggregation.
[INFO][09:50:47]: [Server #1127936] Adding client #402 to the list of clients for aggregation.
[INFO][09:50:47]: [Server #1127936] Adding client #240 to the list of clients for aggregation.
[INFO][09:50:47]: [Server #1127936] Adding client #465 to the list of clients for aggregation.
[INFO][09:50:47]: [Server #1127936] Adding client #389 to the list of clients for aggregation.
[INFO][09:50:47]: [Server #1127936] Adding client #91 to the list of clients for aggregation.
[INFO][09:50:47]: [Server #1127936] Adding client #210 to the list of clients for aggregation.
[INFO][09:50:47]: [Server #1127936] Adding client #323 to the list of clients for aggregation.
[INFO][09:50:47]: [Server #1127936] Adding client #397 to the list of clients for aggregation.
[INFO][09:50:47]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 237, 238, 239, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.0327215  0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.0350689
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.04825517 0.         0.         0.         0.01872888
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.03103589 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.03785201 0.
 0.         0.         0.         0.         0.         0.
 0.02744515 0.         0.         0.         0.         0.02699416
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.06949723 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.02245313 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 1. 6. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 2. 0. 6. 0. 1. 2. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 2. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 4. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.0327215  0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.0350689
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.04825517 0.         0.         0.         0.01872888
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.03103589 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.03785201 0.
 0.         0.         0.         0.         0.         0.
 0.02744515 0.         0.         0.         0.         0.02699416
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.06949723 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.02245313 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][09:50:49]: [Server #1127936] Global model accuracy: 83.34%

[INFO][09:50:49]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_15.pth.
[INFO][09:50:49]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_15.pth.
[INFO][09:50:49]: [93m[1m
[Server #1127936] Starting round 16/100.[0m
[0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002
 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  4e-04  7e-09  7e-09
 5:  6.8876e+00  6.8875e+00  7e-05  1e-09  1e-09
 6:  6.8876e+00  6.8875e+00  7e-05  6e-10  6e-10
 7:  6.8875e+00  6.8875e+00  6e-05  2e-09  1e-10
 8:  6.8875e+00  6.8875e+00  5e-05  2e-09  2e-10
 9:  6.8875e+00  6.8875e+00  2e-05  2e-08  1e-09
10:  6.8875e+00  6.8875e+00  7e-06  1e-08  7e-10
Optimal solution found.
The calculated probability is:  [2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04071121e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 4.73960065e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 9.14340135e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.93882126e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04074707e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04059039e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04081715e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 3.64120336e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 8.98844215e-01
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 3.21904428e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04 2.04106856e-04 2.04106856e-04
 2.04106856e-04 2.04106856e-04]
current clients pool:  [INFO][09:50:50]: [Server #1127936] Selected clients: [430 308 128 398 219 182 168 349 137 405]
[INFO][09:50:50]: [Server #1127936] Selecting client #430 for training.
[INFO][09:50:50]: [Server #1127936] Sending the current model to client #430 (simulated).
[INFO][09:50:50]: [Server #1127936] Sending 0.24 MB of payload data to client #430 (simulated).
[INFO][09:50:50]: [Server #1127936] Selecting client #308 for training.
[INFO][09:50:50]: [Server #1127936] Sending the current model to client #308 (simulated).
[INFO][09:50:50]: [Server #1127936] Sending 0.24 MB of payload data to client #308 (simulated).
[INFO][09:50:50]: [Server #1127936] Selecting client #128 for training.
[INFO][09:50:50]: [Server #1127936] Sending the current model to client #128 (simulated).
[INFO][09:50:50]: [Client #430] Selected by the server.
[INFO][09:50:50]: [Client #430] Loading its data source...
[INFO][09:50:50]: [Client #430] Dataset size: 60000
[INFO][09:50:50]: [Client #430] Sampler: noniid
[INFO][09:50:50]: [Server #1127936] Sending 0.24 MB of payload data to client #128 (simulated).
[INFO][09:50:50]: [Client #308] Selected by the server.
[INFO][09:50:50]: [Client #308] Loading its data source...
[INFO][09:50:50]: [Client #308] Dataset size: 60000
[INFO][09:50:50]: [Client #308] Sampler: noniid
[INFO][09:50:50]: [Client #128] Selected by the server.
[INFO][09:50:50]: [Client #430] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:50:50]: [Client #128] Loading its data source...
[INFO][09:50:50]: [Client #128] Dataset size: 60000
[INFO][09:50:50]: [Client #128] Sampler: noniid
[INFO][09:50:50]: [Client #128] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:50:50]: [Client #308] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:50:50]: [93m[1m[Client #430] Started training in communication round #16.[0m
[INFO][09:50:50]: [93m[1m[Client #128] Started training in communication round #16.[0m
[INFO][09:50:50]: [93m[1m[Client #308] Started training in communication round #16.[0m
[INFO][09:50:52]: [Client #128] Loading the dataset.
[INFO][09:50:52]: [Client #430] Loading the dataset.
[INFO][09:50:52]: [Client #308] Loading the dataset.
[INFO][09:50:58]: [Client #128] Epoch: [1/5][0/10]	Loss: 0.310832
[INFO][09:50:58]: [Client #308] Epoch: [1/5][0/10]	Loss: 0.559805
[INFO][09:50:58]: [Client #430] Epoch: [1/5][0/10]	Loss: 0.023432
[INFO][09:50:58]: [Client #128] Epoch: [2/5][0/10]	Loss: 0.057063
[INFO][09:50:58]: [Client #308] Epoch: [2/5][0/10]	Loss: 0.047896
[INFO][09:50:58]: [Client #430] Epoch: [2/5][0/10]	Loss: 0.016516
[INFO][09:50:58]: [Client #128] Epoch: [3/5][0/10]	Loss: 0.010447
[INFO][09:50:58]: [Client #308] Epoch: [3/5][0/10]	Loss: 0.002685
[INFO][09:50:58]: [Client #430] Epoch: [3/5][0/10]	Loss: 0.044432
[INFO][09:50:58]: [Client #308] Epoch: [4/5][0/10]	Loss: 0.002775
[INFO][09:50:58]: [Client #128] Epoch: [4/5][0/10]	Loss: 0.038409
[INFO][09:50:58]: [Client #308] Epoch: [5/5][0/10]	Loss: 0.052145
[INFO][09:50:58]: [Client #430] Epoch: [4/5][0/10]	Loss: 0.003265
[INFO][09:50:58]: [Client #308] Model saved to /data/ykang/plato/results/test/model/lenet5_308_1127978.pth.
[INFO][09:50:58]: [Client #128] Epoch: [5/5][0/10]	Loss: 0.321989
[INFO][09:50:58]: [Client #128] Model saved to /data/ykang/plato/results/test/model/lenet5_128_1127979.pth.
[INFO][09:50:58]: [Client #430] Epoch: [5/5][0/10]	Loss: 0.006315
[INFO][09:50:59]: [Client #430] Model saved to /data/ykang/plato/results/test/model/lenet5_430_1127977.pth.
[INFO][09:50:59]: [Client #308] Loading a model from /data/ykang/plato/results/test/model/lenet5_308_1127978.pth.
[INFO][09:50:59]: [Client #308] Model trained.
[INFO][09:50:59]: [Client #308] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:50:59]: [Server #1127936] Received 0.24 MB of payload data from client #308 (simulated).
[INFO][09:50:59]: [Client #430] Loading a model from /data/ykang/plato/results/test/model/lenet5_430_1127977.pth.
[INFO][09:50:59]: [Client #430] Model trained.
[INFO][09:50:59]: [Client #430] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:50:59]: [Server #1127936] Received 0.24 MB of payload data from client #430 (simulated).
[INFO][09:50:59]: [Client #128] Loading a model from /data/ykang/plato/results/test/model/lenet5_128_1127979.pth.
[INFO][09:50:59]: [Client #128] Model trained.
[INFO][09:50:59]: [Client #128] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:50:59]: [Server #1127936] Received 0.24 MB of payload data from client #128 (simulated).
[INFO][09:50:59]: [Server #1127936] Selecting client #398 for training.
[INFO][09:50:59]: [Server #1127936] Sending the current model to client #398 (simulated).
[INFO][09:50:59]: [Server #1127936] Sending 0.24 MB of payload data to client #398 (simulated).
[INFO][09:50:59]: [Server #1127936] Selecting client #219 for training.
[INFO][09:50:59]: [Server #1127936] Sending the current model to client #219 (simulated).
[INFO][09:50:59]: [Server #1127936] Sending 0.24 MB of payload data to client #219 (simulated).
[INFO][09:50:59]: [Server #1127936] Selecting client #182 for training.
[INFO][09:50:59]: [Server #1127936] Sending the current model to client #182 (simulated).
[INFO][09:50:59]: [Client #398] Selected by the server.
[INFO][09:50:59]: [Client #398] Loading its data source...
[INFO][09:50:59]: [Client #398] Dataset size: 60000
[INFO][09:50:59]: [Client #398] Sampler: noniid
[INFO][09:50:59]: [Server #1127936] Sending 0.24 MB of payload data to client #182 (simulated).
[INFO][09:50:59]: [Client #219] Selected by the server.
[INFO][09:50:59]: [Client #219] Loading its data source...
[INFO][09:50:59]: [Client #219] Dataset size: 60000
[INFO][09:50:59]: [Client #219] Sampler: noniid
[INFO][09:50:59]: [Client #182] Selected by the server.
[INFO][09:50:59]: [Client #182] Loading its data source...
[INFO][09:50:59]: [Client #182] Dataset size: 60000
[INFO][09:50:59]: [Client #182] Sampler: noniid
[INFO][09:50:59]: [Client #398] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:50:59]: [Client #219] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:50:59]: [Client #182] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:50:59]: [93m[1m[Client #219] Started training in communication round #16.[0m
[INFO][09:50:59]: [93m[1m[Client #398] Started training in communication round #16.[0m
[INFO][09:50:59]: [93m[1m[Client #182] Started training in communication round #16.[0m
[INFO][09:51:02]: [Client #182] Loading the dataset.
[INFO][09:51:02]: [Client #219] Loading the dataset.
[INFO][09:51:02]: [Client #398] Loading the dataset.
[INFO][09:51:08]: [Client #219] Epoch: [1/5][0/10]	Loss: 0.554279
[INFO][09:51:08]: [Client #398] Epoch: [1/5][0/10]	Loss: 0.045469
[INFO][09:51:08]: [Client #182] Epoch: [1/5][0/10]	Loss: 0.020426
[INFO][09:51:08]: [Client #219] Epoch: [2/5][0/10]	Loss: 0.076462
[INFO][09:51:08]: [Client #182] Epoch: [2/5][0/10]	Loss: 0.003573
[INFO][09:51:08]: [Client #398] Epoch: [2/5][0/10]	Loss: 0.004786
[INFO][09:51:08]: [Client #182] Epoch: [3/5][0/10]	Loss: 0.006176
[INFO][09:51:08]: [Client #219] Epoch: [3/5][0/10]	Loss: 0.001284
[INFO][09:51:08]: [Client #398] Epoch: [3/5][0/10]	Loss: 0.001674
[INFO][09:51:08]: [Client #182] Epoch: [4/5][0/10]	Loss: 0.113254
[INFO][09:51:08]: [Client #219] Epoch: [4/5][0/10]	Loss: 0.000083
[INFO][09:51:08]: [Client #182] Epoch: [5/5][0/10]	Loss: 0.002906
[INFO][09:51:08]: [Client #398] Epoch: [4/5][0/10]	Loss: 0.000706
[INFO][09:51:08]: [Client #182] Model saved to /data/ykang/plato/results/test/model/lenet5_182_1127979.pth.
[INFO][09:51:08]: [Client #219] Epoch: [5/5][0/10]	Loss: 0.080607
[INFO][09:51:08]: [Client #398] Epoch: [5/5][0/10]	Loss: 0.000191
[INFO][09:51:08]: [Client #219] Model saved to /data/ykang/plato/results/test/model/lenet5_219_1127978.pth.
[INFO][09:51:08]: [Client #398] Model saved to /data/ykang/plato/results/test/model/lenet5_398_1127977.pth.
[INFO][09:51:09]: [Client #182] Loading a model from /data/ykang/plato/results/test/model/lenet5_182_1127979.pth.
[INFO][09:51:09]: [Client #182] Model trained.
[INFO][09:51:09]: [Client #182] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:51:09]: [Server #1127936] Received 0.24 MB of payload data from client #182 (simulated).
[INFO][09:51:09]: [Client #219] Loading a model from /data/ykang/plato/results/test/model/lenet5_219_1127978.pth.
[INFO][09:51:09]: [Client #219] Model trained.
[INFO][09:51:09]: [Client #219] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:51:09]: [Server #1127936] Received 0.24 MB of payload data from client #219 (simulated).
[INFO][09:51:09]: [Client #398] Loading a model from /data/ykang/plato/results/test/model/lenet5_398_1127977.pth.
[INFO][09:51:09]: [Client #398] Model trained.
[INFO][09:51:09]: [Client #398] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:51:09]: [Server #1127936] Received 0.24 MB of payload data from client #398 (simulated).
[INFO][09:51:09]: [Server #1127936] Selecting client #168 for training.
[INFO][09:51:09]: [Server #1127936] Sending the current model to client #168 (simulated).
[INFO][09:51:09]: [Server #1127936] Sending 0.24 MB of payload data to client #168 (simulated).
[INFO][09:51:09]: [Server #1127936] Selecting client #349 for training.
[INFO][09:51:09]: [Server #1127936] Sending the current model to client #349 (simulated).
[INFO][09:51:09]: [Server #1127936] Sending 0.24 MB of payload data to client #349 (simulated).
[INFO][09:51:09]: [Server #1127936] Selecting client #137 for training.
[INFO][09:51:09]: [Server #1127936] Sending the current model to client #137 (simulated).
[INFO][09:51:09]: [Client #168] Selected by the server.
[INFO][09:51:09]: [Client #168] Loading its data source...
[INFO][09:51:09]: [Client #168] Dataset size: 60000
[INFO][09:51:09]: [Client #168] Sampler: noniid
[INFO][09:51:09]: [Server #1127936] Sending 0.24 MB of payload data to client #137 (simulated).
[INFO][09:51:09]: [Client #349] Selected by the server.
[INFO][09:51:09]: [Client #349] Loading its data source...
[INFO][09:51:09]: [Client #137] Selected by the server.
[INFO][09:51:09]: [Client #349] Dataset size: 60000
[INFO][09:51:09]: [Client #349] Sampler: noniid
[INFO][09:51:09]: [Client #137] Loading its data source...
[INFO][09:51:09]: [Client #137] Dataset size: 60000
[INFO][09:51:09]: [Client #137] Sampler: noniid
[INFO][09:51:09]: [Client #168] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:51:09]: [Client #349] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:51:09]: [Client #137] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:51:09]: [93m[1m[Client #349] Started training in communication round #16.[0m
[INFO][09:51:09]: [93m[1m[Client #137] Started training in communication round #16.[0m
[INFO][09:51:09]: [93m[1m[Client #168] Started training in communication round #16.[0m
[INFO][09:51:11]: [Client #168] Loading the dataset.
[INFO][09:51:11]: [Client #349] Loading the dataset.
[INFO][09:51:11]: [Client #137] Loading the dataset.
[INFO][09:51:17]: [Client #168] Epoch: [1/5][0/10]	Loss: 1.168560
[INFO][09:51:17]: [Client #349] Epoch: [1/5][0/10]	Loss: 0.521686
[INFO][09:51:17]: [Client #137] Epoch: [1/5][0/10]	Loss: 0.787641
[INFO][09:51:18]: [Client #168] Epoch: [2/5][0/10]	Loss: 0.110924
[INFO][09:51:18]: [Client #349] Epoch: [2/5][0/10]	Loss: 0.108891
[INFO][09:51:18]: [Client #137] Epoch: [2/5][0/10]	Loss: 0.368239
[INFO][09:51:18]: [Client #168] Epoch: [3/5][0/10]	Loss: 0.060969
[INFO][09:51:18]: [Client #349] Epoch: [3/5][0/10]	Loss: 0.084270
[INFO][09:51:18]: [Client #137] Epoch: [3/5][0/10]	Loss: 0.030229
[INFO][09:51:18]: [Client #349] Epoch: [4/5][0/10]	Loss: 0.215939
[INFO][09:51:18]: [Client #168] Epoch: [4/5][0/10]	Loss: 0.490322
[INFO][09:51:18]: [Client #137] Epoch: [4/5][0/10]	Loss: 0.078333
[INFO][09:51:18]: [Client #349] Epoch: [5/5][0/10]	Loss: 0.060905
[INFO][09:51:18]: [Client #168] Epoch: [5/5][0/10]	Loss: 0.186179
[INFO][09:51:18]: [Client #137] Epoch: [5/5][0/10]	Loss: 0.411802
[INFO][09:51:18]: [Client #349] Model saved to /data/ykang/plato/results/test/model/lenet5_349_1127978.pth.
[INFO][09:51:18]: [Client #168] Model saved to /data/ykang/plato/results/test/model/lenet5_168_1127977.pth.
[INFO][09:51:18]: [Client #137] Model saved to /data/ykang/plato/results/test/model/lenet5_137_1127979.pth.
[INFO][09:51:19]: [Client #349] Loading a model from /data/ykang/plato/results/test/model/lenet5_349_1127978.pth.
[INFO][09:51:19]: [Client #349] Model trained.
[INFO][09:51:19]: [Client #349] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:51:19]: [Server #1127936] Received 0.24 MB of payload data from client #349 (simulated).
[INFO][09:51:19]: [Client #168] Loading a model from /data/ykang/plato/results/test/model/lenet5_168_1127977.pth.
[INFO][09:51:19]: [Client #168] Model trained.
[INFO][09:51:19]: [Client #168] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:51:19]: [Server #1127936] Received 0.24 MB of payload data from client #168 (simulated).
[INFO][09:51:19]: [Client #137] Loading a model from /data/ykang/plato/results/test/model/lenet5_137_1127979.pth.
[INFO][09:51:19]: [Client #137] Model trained.
[INFO][09:51:19]: [Client #137] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:51:19]: [Server #1127936] Received 0.24 MB of payload data from client #137 (simulated).
[INFO][09:51:19]: [Server #1127936] Selecting client #405 for training.
[INFO][09:51:19]: [Server #1127936] Sending the current model to client #405 (simulated).
[INFO][09:51:19]: [Server #1127936] Sending 0.24 MB of payload data to client #405 (simulated).
[INFO][09:51:19]: [Client #405] Selected by the server.
[INFO][09:51:19]: [Client #405] Loading its data source...
[INFO][09:51:19]: [Client #405] Dataset size: 60000
[INFO][09:51:19]: [Client #405] Sampler: noniid
[INFO][09:51:19]: [Client #405] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:51:19]: [93m[1m[Client #405] Started training in communication round #16.[0m
[INFO][09:51:21]: [Client #405] Loading the dataset.
[INFO][09:51:27]: [Client #405] Epoch: [1/5][0/10]	Loss: 0.573179
[INFO][09:51:27]: [Client #405] Epoch: [2/5][0/10]	Loss: 0.026707
[INFO][09:51:27]: [Client #405] Epoch: [3/5][0/10]	Loss: 0.000344
[INFO][09:51:27]: [Client #405] Epoch: [4/5][0/10]	Loss: 0.168947
[INFO][09:51:27]: [Client #405] Epoch: [5/5][0/10]	Loss: 0.124253
[INFO][09:51:27]: [Client #405] Model saved to /data/ykang/plato/results/test/model/lenet5_405_1127977.pth.
[INFO][09:51:28]: [Client #405] Loading a model from /data/ykang/plato/results/test/model/lenet5_405_1127977.pth.
[INFO][09:51:28]: [Client #405] Model trained.
[INFO][09:51:28]: [Client #405] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:51:28]: [Server #1127936] Received 0.24 MB of payload data from client #405 (simulated).
[INFO][09:51:28]: [Server #1127936] Adding client #202 to the list of clients for aggregation.
[INFO][09:51:28]: [Server #1127936] Adding client #255 to the list of clients for aggregation.
[INFO][09:51:28]: [Server #1127936] Adding client #348 to the list of clients for aggregation.
[INFO][09:51:28]: [Server #1127936] Adding client #110 to the list of clients for aggregation.
[INFO][09:51:28]: [Server #1127936] Adding client #367 to the list of clients for aggregation.
[INFO][09:51:28]: [Server #1127936] Adding client #396 to the list of clients for aggregation.
[INFO][09:51:28]: [Server #1127936] Adding client #160 to the list of clients for aggregation.
[INFO][09:51:28]: [Server #1127936] Adding client #88 to the list of clients for aggregation.
[INFO][09:51:28]: [Server #1127936] Adding client #342 to the list of clients for aggregation.
[INFO][09:51:28]: [Server #1127936] Adding client #182 to the list of clients for aggregation.
[INFO][09:51:28]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 343, 344, 345, 346, 347, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.03556823 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.0491903  0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.06634964 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01699801 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00952934 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.04637289 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.04519967
 0.         0.         0.         0.         0.         0.0206976
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.21051051 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.02403373
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 1. 6. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 2. 0. 6. 0. 1. 2. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 2. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 4. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 2. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 6. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.03556823 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.0491903  0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.06634964 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01699801 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00952934 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.04637289 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.04519967
 0.         0.         0.         0.         0.         0.0206976
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.21051051 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.02403373
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][09:51:30]: [Server #1127936] Global model accuracy: 83.13%

[INFO][09:51:30]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_16.pth.
[INFO][09:51:30]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_16.pth.
[INFO][09:51:30]: [93m[1m
[Server #1127936] Starting round 17/100.[0m
[0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002
 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1
 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8875e+00  6.8860e+00  2e-03  3e-08  3e-08
 5:  6.8875e+00  6.8863e+00  1e-03  2e-08  2e-08
 6:  6.8874e+00  6.8861e+00  1e-03  6e-07  8e-07
 7:  6.8873e+00  6.8865e+00  8e-04  5e-07  6e-07
 8:  6.8871e+00  6.8867e+00  5e-04  9e-07  1e-06
 9:  6.8869e+00  6.8868e+00  2e-04  2e-06  2e-06
10:  6.8869e+00  6.8868e+00  6e-05  8e-07  1e-06
11:  6.8868e+00  6.8868e+00  6e-06  2e-07  2e-07
12:  6.8868e+00  6.8868e+00  2e-07  7e-09  9e-09
Optimal solution found.
The calculated probability is:  [1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 2.23358546e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 2.86250800e-05 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.64509918e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07317974e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.13574235e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.43417621e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.96427895e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.21594365e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 9.99444296e-01
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.24148238e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06 1.07319794e-06 1.07319794e-06
 1.07319794e-06 1.07319794e-06]
current clients pool:  [INFO][09:51:30]: [Server #1127936] Selected clients: [367 119 293 332 110   8 193 469 154 384]
[INFO][09:51:30]: [Server #1127936] Selecting client #367 for training.
[INFO][09:51:30]: [Server #1127936] Sending the current model to client #367 (simulated).
[INFO][09:51:30]: [Server #1127936] Sending 0.24 MB of payload data to client #367 (simulated).
[INFO][09:51:30]: [Server #1127936] Selecting client #119 for training.
[INFO][09:51:30]: [Server #1127936] Sending the current model to client #119 (simulated).
[INFO][09:51:30]: [Server #1127936] Sending 0.24 MB of payload data to client #119 (simulated).
[INFO][09:51:30]: [Server #1127936] Selecting client #293 for training.
[INFO][09:51:30]: [Server #1127936] Sending the current model to client #293 (simulated).
[INFO][09:51:30]: [Client #367] Selected by the server.
[INFO][09:51:30]: [Client #367] Loading its data source...
[INFO][09:51:30]: [Client #367] Dataset size: 60000
[INFO][09:51:30]: [Client #367] Sampler: noniid
[INFO][09:51:30]: [Server #1127936] Sending 0.24 MB of payload data to client #293 (simulated).
[INFO][09:51:30]: [Client #119] Selected by the server.
[INFO][09:51:30]: [Client #119] Loading its data source...
[INFO][09:51:30]: [Client #119] Dataset size: 60000
[INFO][09:51:30]: [Client #119] Sampler: noniid
[INFO][09:51:30]: [Client #293] Selected by the server.
[INFO][09:51:30]: [Client #293] Loading its data source...
[INFO][09:51:30]: [Client #293] Dataset size: 60000
[INFO][09:51:30]: [Client #293] Sampler: noniid
[INFO][09:51:30]: [Client #367] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:51:31]: [Client #293] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:51:31]: [Client #119] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:51:31]: [93m[1m[Client #119] Started training in communication round #17.[0m
[INFO][09:51:31]: [93m[1m[Client #293] Started training in communication round #17.[0m
[INFO][09:51:31]: [93m[1m[Client #367] Started training in communication round #17.[0m
[INFO][09:51:33]: [Client #293] Loading the dataset.
[INFO][09:51:33]: [Client #367] Loading the dataset.
[INFO][09:51:33]: [Client #119] Loading the dataset.
[INFO][09:51:39]: [Client #293] Epoch: [1/5][0/10]	Loss: 1.281345
[INFO][09:51:39]: [Client #119] Epoch: [1/5][0/10]	Loss: 0.505548
[INFO][09:51:39]: [Client #293] Epoch: [2/5][0/10]	Loss: 0.316488
[INFO][09:51:39]: [Client #367] Epoch: [1/5][0/10]	Loss: 0.484645
[INFO][09:51:39]: [Client #119] Epoch: [2/5][0/10]	Loss: 0.023047
[INFO][09:51:39]: [Client #293] Epoch: [3/5][0/10]	Loss: 0.196693
[INFO][09:51:39]: [Client #367] Epoch: [2/5][0/10]	Loss: 0.001774
[INFO][09:51:39]: [Client #119] Epoch: [3/5][0/10]	Loss: 0.080151
[INFO][09:51:39]: [Client #293] Epoch: [4/5][0/10]	Loss: 0.044784
[INFO][09:51:39]: [Client #119] Epoch: [4/5][0/10]	Loss: 0.003080
[INFO][09:51:39]: [Client #367] Epoch: [3/5][0/10]	Loss: 0.000000
[INFO][09:51:39]: [Client #293] Epoch: [5/5][0/10]	Loss: 0.073153
[INFO][09:51:39]: [Client #119] Epoch: [5/5][0/10]	Loss: 0.000610
[INFO][09:51:39]: [Client #367] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][09:51:39]: [Client #293] Model saved to /data/ykang/plato/results/test/model/lenet5_293_1127979.pth.
[INFO][09:51:39]: [Client #119] Model saved to /data/ykang/plato/results/test/model/lenet5_119_1127978.pth.
[INFO][09:51:39]: [Client #367] Epoch: [5/5][0/10]	Loss: 0.000001
[INFO][09:51:40]: [Client #367] Model saved to /data/ykang/plato/results/test/model/lenet5_367_1127977.pth.
[INFO][09:51:40]: [Client #293] Loading a model from /data/ykang/plato/results/test/model/lenet5_293_1127979.pth.
[INFO][09:51:40]: [Client #293] Model trained.
[INFO][09:51:40]: [Client #293] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:51:40]: [Server #1127936] Received 0.24 MB of payload data from client #293 (simulated).
[INFO][09:51:40]: [Client #119] Loading a model from /data/ykang/plato/results/test/model/lenet5_119_1127978.pth.
[INFO][09:51:40]: [Client #119] Model trained.
[INFO][09:51:40]: [Client #119] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:51:40]: [Server #1127936] Received 0.24 MB of payload data from client #119 (simulated).
[INFO][09:51:40]: [Client #367] Loading a model from /data/ykang/plato/results/test/model/lenet5_367_1127977.pth.
[INFO][09:51:40]: [Client #367] Model trained.
[INFO][09:51:40]: [Client #367] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:51:41]: [Server #1127936] Received 0.24 MB of payload data from client #367 (simulated).
[INFO][09:51:41]: [Server #1127936] Selecting client #332 for training.
[INFO][09:51:41]: [Server #1127936] Sending the current model to client #332 (simulated).
[INFO][09:51:41]: [Server #1127936] Sending 0.24 MB of payload data to client #332 (simulated).
[INFO][09:51:41]: [Server #1127936] Selecting client #110 for training.
[INFO][09:51:41]: [Server #1127936] Sending the current model to client #110 (simulated).
[INFO][09:51:41]: [Server #1127936] Sending 0.24 MB of payload data to client #110 (simulated).
[INFO][09:51:41]: [Server #1127936] Selecting client #8 for training.
[INFO][09:51:41]: [Server #1127936] Sending the current model to client #8 (simulated).
[INFO][09:51:41]: [Client #332] Selected by the server.
[INFO][09:51:41]: [Client #332] Loading its data source...
[INFO][09:51:41]: [Client #332] Dataset size: 60000
[INFO][09:51:41]: [Client #332] Sampler: noniid
[INFO][09:51:41]: [Server #1127936] Sending 0.24 MB of payload data to client #8 (simulated).
[INFO][09:51:41]: [Client #110] Selected by the server.
[INFO][09:51:41]: [Client #110] Loading its data source...
[INFO][09:51:41]: [Client #110] Dataset size: 60000
[INFO][09:51:41]: [Client #110] Sampler: noniid
[INFO][09:51:41]: [Client #8] Selected by the server.
[INFO][09:51:41]: [Client #8] Loading its data source...
[INFO][09:51:41]: [Client #8] Dataset size: 60000
[INFO][09:51:41]: [Client #8] Sampler: noniid
[INFO][09:51:41]: [Client #332] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:51:41]: [Client #110] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:51:41]: [Client #8] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:51:41]: [93m[1m[Client #332] Started training in communication round #17.[0m
[INFO][09:51:41]: [93m[1m[Client #8] Started training in communication round #17.[0m
[INFO][09:51:41]: [93m[1m[Client #110] Started training in communication round #17.[0m
[INFO][09:51:43]: [Client #8] Loading the dataset.
[INFO][09:51:43]: [Client #110] Loading the dataset.
[INFO][09:51:43]: [Client #332] Loading the dataset.
[INFO][09:51:48]: [Client #8] Epoch: [1/5][0/10]	Loss: 0.150168
[INFO][09:51:48]: [Client #8] Epoch: [2/5][0/10]	Loss: 0.011637
[INFO][09:51:48]: [Client #110] Epoch: [1/5][0/10]	Loss: 0.484645
[INFO][09:51:48]: [Client #332] Epoch: [1/5][0/10]	Loss: 0.384899
[INFO][09:51:49]: [Client #8] Epoch: [3/5][0/10]	Loss: 0.006002
[INFO][09:51:49]: [Client #110] Epoch: [2/5][0/10]	Loss: 0.165686
[INFO][09:51:49]: [Client #332] Epoch: [2/5][0/10]	Loss: 0.000051
[INFO][09:51:49]: [Client #8] Epoch: [4/5][0/10]	Loss: 0.015129
[INFO][09:51:49]: [Client #332] Epoch: [3/5][0/10]	Loss: 0.001654
[INFO][09:51:49]: [Client #110] Epoch: [3/5][0/10]	Loss: 0.003409
[INFO][09:51:49]: [Client #8] Epoch: [5/5][0/10]	Loss: 0.018882
[INFO][09:51:49]: [Client #8] Model saved to /data/ykang/plato/results/test/model/lenet5_8_1127979.pth.
[INFO][09:51:49]: [Client #332] Epoch: [4/5][0/10]	Loss: 0.008782
[INFO][09:51:49]: [Client #110] Epoch: [4/5][0/10]	Loss: 0.000532
[INFO][09:51:49]: [Client #332] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][09:51:49]: [Client #110] Epoch: [5/5][0/10]	Loss: 0.000026
[INFO][09:51:49]: [Client #332] Model saved to /data/ykang/plato/results/test/model/lenet5_332_1127977.pth.
[INFO][09:51:49]: [Client #110] Model saved to /data/ykang/plato/results/test/model/lenet5_110_1127978.pth.
[INFO][09:51:50]: [Client #8] Loading a model from /data/ykang/plato/results/test/model/lenet5_8_1127979.pth.
[INFO][09:51:50]: [Client #8] Model trained.
[INFO][09:51:50]: [Client #8] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:51:50]: [Server #1127936] Received 0.24 MB of payload data from client #8 (simulated).
[INFO][09:51:50]: [Client #332] Loading a model from /data/ykang/plato/results/test/model/lenet5_332_1127977.pth.
[INFO][09:51:50]: [Client #332] Model trained.
[INFO][09:51:50]: [Client #332] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:51:50]: [Server #1127936] Received 0.24 MB of payload data from client #332 (simulated).
[INFO][09:51:50]: [Client #110] Loading a model from /data/ykang/plato/results/test/model/lenet5_110_1127978.pth.
[INFO][09:51:50]: [Client #110] Model trained.
[INFO][09:51:50]: [Client #110] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:51:50]: [Server #1127936] Received 0.24 MB of payload data from client #110 (simulated).
[INFO][09:51:50]: [Server #1127936] Selecting client #193 for training.
[INFO][09:51:50]: [Server #1127936] Sending the current model to client #193 (simulated).
[INFO][09:51:50]: [Server #1127936] Sending 0.24 MB of payload data to client #193 (simulated).
[INFO][09:51:50]: [Server #1127936] Selecting client #469 for training.
[INFO][09:51:50]: [Server #1127936] Sending the current model to client #469 (simulated).
[INFO][09:51:50]: [Server #1127936] Sending 0.24 MB of payload data to client #469 (simulated).
[INFO][09:51:50]: [Server #1127936] Selecting client #154 for training.
[INFO][09:51:50]: [Server #1127936] Sending the current model to client #154 (simulated).
[INFO][09:51:50]: [Client #193] Selected by the server.
[INFO][09:51:50]: [Client #193] Loading its data source...
[INFO][09:51:50]: [Client #193] Dataset size: 60000
[INFO][09:51:50]: [Client #193] Sampler: noniid
[INFO][09:51:50]: [Server #1127936] Sending 0.24 MB of payload data to client #154 (simulated).
[INFO][09:51:50]: [Client #469] Selected by the server.
[INFO][09:51:50]: [Client #469] Loading its data source...
[INFO][09:51:50]: [Client #469] Dataset size: 60000
[INFO][09:51:50]: [Client #154] Selected by the server.
[INFO][09:51:50]: [Client #469] Sampler: noniid
[INFO][09:51:50]: [Client #154] Loading its data source...
[INFO][09:51:50]: [Client #154] Dataset size: 60000
[INFO][09:51:50]: [Client #154] Sampler: noniid
[INFO][09:51:50]: [Client #193] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:51:50]: [Client #154] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:51:50]: [93m[1m[Client #154] Started training in communication round #17.[0m
[INFO][09:51:50]: [93m[1m[Client #193] Started training in communication round #17.[0m
[INFO][09:51:50]: [Client #469] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:51:50]: [93m[1m[Client #469] Started training in communication round #17.[0m
[INFO][09:51:52]: [Client #193] Loading the dataset.
[INFO][09:51:52]: [Client #154] Loading the dataset.
[INFO][09:51:52]: [Client #469] Loading the dataset.
[INFO][09:51:58]: [Client #193] Epoch: [1/5][0/10]	Loss: 0.264957
[INFO][09:51:58]: [Client #154] Epoch: [1/5][0/10]	Loss: 0.763444
[INFO][09:51:58]: [Client #193] Epoch: [2/5][0/10]	Loss: 0.069071
[INFO][09:51:58]: [Client #469] Epoch: [1/5][0/10]	Loss: 0.945119
[INFO][09:51:58]: [Client #154] Epoch: [2/5][0/10]	Loss: 0.300385
[INFO][09:51:58]: [Client #193] Epoch: [3/5][0/10]	Loss: 0.000023
[INFO][09:51:58]: [Client #469] Epoch: [2/5][0/10]	Loss: 0.497701
[INFO][09:51:58]: [Client #154] Epoch: [3/5][0/10]	Loss: 0.009404
[INFO][09:51:58]: [Client #469] Epoch: [3/5][0/10]	Loss: 0.004689
[INFO][09:51:58]: [Client #193] Epoch: [4/5][0/10]	Loss: 0.000083
[INFO][09:51:58]: [Client #469] Epoch: [4/5][0/10]	Loss: 0.068159
[INFO][09:51:58]: [Client #154] Epoch: [4/5][0/10]	Loss: 0.136159
[INFO][09:51:58]: [Client #193] Epoch: [5/5][0/10]	Loss: 0.001453
[INFO][09:51:58]: [Client #469] Epoch: [5/5][0/10]	Loss: 0.054443
[INFO][09:51:58]: [Client #193] Model saved to /data/ykang/plato/results/test/model/lenet5_193_1127977.pth.
[INFO][09:51:58]: [Client #154] Epoch: [5/5][0/10]	Loss: 0.105187
[INFO][09:51:58]: [Client #469] Model saved to /data/ykang/plato/results/test/model/lenet5_469_1127978.pth.
[INFO][09:51:58]: [Client #154] Model saved to /data/ykang/plato/results/test/model/lenet5_154_1127979.pth.
[INFO][09:51:59]: [Client #193] Loading a model from /data/ykang/plato/results/test/model/lenet5_193_1127977.pth.
[INFO][09:51:59]: [Client #193] Model trained.
[INFO][09:51:59]: [Client #193] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:51:59]: [Server #1127936] Received 0.24 MB of payload data from client #193 (simulated).
[INFO][09:51:59]: [Client #154] Loading a model from /data/ykang/plato/results/test/model/lenet5_154_1127979.pth.
[INFO][09:51:59]: [Client #154] Model trained.
[INFO][09:51:59]: [Client #154] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:51:59]: [Server #1127936] Received 0.24 MB of payload data from client #154 (simulated).
[INFO][09:51:59]: [Client #469] Loading a model from /data/ykang/plato/results/test/model/lenet5_469_1127978.pth.
[INFO][09:51:59]: [Client #469] Model trained.
[INFO][09:51:59]: [Client #469] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:51:59]: [Server #1127936] Received 0.24 MB of payload data from client #469 (simulated).
[INFO][09:51:59]: [Server #1127936] Selecting client #384 for training.
[INFO][09:51:59]: [Server #1127936] Sending the current model to client #384 (simulated).
[INFO][09:51:59]: [Server #1127936] Sending 0.24 MB of payload data to client #384 (simulated).
[INFO][09:51:59]: [Client #384] Selected by the server.
[INFO][09:51:59]: [Client #384] Loading its data source...
[INFO][09:51:59]: [Client #384] Dataset size: 60000
[INFO][09:51:59]: [Client #384] Sampler: noniid
[INFO][09:51:59]: [Client #384] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:51:59]: [93m[1m[Client #384] Started training in communication round #17.[0m
[INFO][09:52:01]: [Client #384] Loading the dataset.
[INFO][09:52:06]: [Client #384] Epoch: [1/5][0/10]	Loss: 0.966093
[INFO][09:52:06]: [Client #384] Epoch: [2/5][0/10]	Loss: 0.273505
[INFO][09:52:07]: [Client #384] Epoch: [3/5][0/10]	Loss: 0.045509
[INFO][09:52:07]: [Client #384] Epoch: [4/5][0/10]	Loss: 0.075075
[INFO][09:52:07]: [Client #384] Epoch: [5/5][0/10]	Loss: 0.425624
[INFO][09:52:07]: [Client #384] Model saved to /data/ykang/plato/results/test/model/lenet5_384_1127977.pth.
[INFO][09:52:07]: [Client #384] Loading a model from /data/ykang/plato/results/test/model/lenet5_384_1127977.pth.
[INFO][09:52:07]: [Client #384] Model trained.
[INFO][09:52:07]: [Client #384] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:52:07]: [Server #1127936] Received 0.24 MB of payload data from client #384 (simulated).
[INFO][09:52:07]: [Server #1127936] Adding client #308 to the list of clients for aggregation.
[INFO][09:52:07]: [Server #1127936] Adding client #405 to the list of clients for aggregation.
[INFO][09:52:07]: [Server #1127936] Adding client #168 to the list of clients for aggregation.
[INFO][09:52:07]: [Server #1127936] Adding client #430 to the list of clients for aggregation.
[INFO][09:52:07]: [Server #1127936] Adding client #398 to the list of clients for aggregation.
[INFO][09:52:07]: [Server #1127936] Adding client #349 to the list of clients for aggregation.
[INFO][09:52:07]: [Server #1127936] Adding client #137 to the list of clients for aggregation.
[INFO][09:52:07]: [Server #1127936] Adding client #219 to the list of clients for aggregation.
[INFO][09:52:07]: [Server #1127936] Adding client #128 to the list of clients for aggregation.
[INFO][09:52:07]: [Server #1127936] Adding client #154 to the list of clients for aggregation.
[INFO][09:52:07]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 399, 400, 401, 402, 403, 404, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.02103001 0.         0.         0.         0.
 0.         0.         0.         0.         0.03874457 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.04887784 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.05785525
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.04259382 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.03341974 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.02508535 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.02982979 0.         0.         0.         0.
 0.         0.         0.02865581 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.03870824 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 1. 6. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 2. 0. 6. 0. 1. 2. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 2. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 4. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 2. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 6. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.02103001 0.         0.         0.         0.
 0.         0.         0.         0.         0.03874457 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.04887784 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.05785525
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.04259382 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.03341974 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.02508535 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.02982979 0.         0.         0.         0.
 0.         0.         0.02865581 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.03870824 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][09:52:09]: [Server #1127936] Global model accuracy: 85.64%

[INFO][09:52:09]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_17.pth.
[INFO][09:52:09]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_17.pth.
[INFO][09:52:09]: [93m[1m
[Server #1127936] Starting round 18/100.[0m
[0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002
 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1
 0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  4e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  6e-05  1e-09  1e-09
 6:  6.8876e+00  6.8875e+00  6e-05  6e-10  5e-10
 7:  6.8875e+00  6.8875e+00  5e-05  1e-09  9e-11
 8:  6.8875e+00  6.8875e+00  4e-05  2e-09  1e-10
 9:  6.8875e+00  6.8875e+00  2e-05  1e-08  8e-10
10:  6.8875e+00  6.8875e+00  6e-06  7e-09  4e-10
Optimal solution found.
The calculated probability is:  [2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 3.44686599e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 7.84977411e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05495763e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 8.96225531e-01
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 1.07304203e-03 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 5.69102598e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 3.96048353e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 4.79227687e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 4.55603606e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 7.82970695e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04 2.05590239e-04 2.05590239e-04
 2.05590239e-04 2.05590239e-04]
current clients pool:  [INFO][09:52:10]: [Server #1127936] Selected clients: [168 190 181  31 317 355 308 325  88  78]
[INFO][09:52:10]: [Server #1127936] Selecting client #168 for training.
[INFO][09:52:10]: [Server #1127936] Sending the current model to client #168 (simulated).
[INFO][09:52:10]: [Server #1127936] Sending 0.24 MB of payload data to client #168 (simulated).
[INFO][09:52:10]: [Server #1127936] Selecting client #190 for training.
[INFO][09:52:10]: [Server #1127936] Sending the current model to client #190 (simulated).
[INFO][09:52:10]: [Server #1127936] Sending 0.24 MB of payload data to client #190 (simulated).
[INFO][09:52:10]: [Server #1127936] Selecting client #181 for training.
[INFO][09:52:10]: [Server #1127936] Sending the current model to client #181 (simulated).
[INFO][09:52:10]: [Client #168] Selected by the server.
[INFO][09:52:10]: [Client #168] Loading its data source...
[INFO][09:52:10]: [Client #168] Dataset size: 60000
[INFO][09:52:10]: [Client #168] Sampler: noniid
[INFO][09:52:10]: [Server #1127936] Sending 0.24 MB of payload data to client #181 (simulated).
[INFO][09:52:10]: [Client #190] Selected by the server.
[INFO][09:52:10]: [Client #190] Loading its data source...
[INFO][09:52:10]: [Client #190] Dataset size: 60000
[INFO][09:52:10]: [Client #190] Sampler: noniid
[INFO][09:52:10]: [Client #168] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:52:10]: [Client #190] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:52:10]: [Client #181] Selected by the server.
[INFO][09:52:10]: [Client #181] Loading its data source...
[INFO][09:52:10]: [Client #181] Dataset size: 60000
[INFO][09:52:10]: [Client #181] Sampler: noniid
[INFO][09:52:10]: [93m[1m[Client #190] Started training in communication round #18.[0m
[INFO][09:52:10]: [Client #181] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:52:10]: [93m[1m[Client #181] Started training in communication round #18.[0m
[INFO][09:52:10]: [93m[1m[Client #168] Started training in communication round #18.[0m
[INFO][09:52:12]: [Client #168] Loading the dataset.
[INFO][09:52:12]: [Client #190] Loading the dataset.
[INFO][09:52:12]: [Client #181] Loading the dataset.
[INFO][09:52:18]: [Client #190] Epoch: [1/5][0/10]	Loss: 0.290881
[INFO][09:52:18]: [Client #181] Epoch: [1/5][0/10]	Loss: 0.595369
[INFO][09:52:18]: [Client #168] Epoch: [1/5][0/10]	Loss: 0.496803
[INFO][09:52:18]: [Client #181] Epoch: [2/5][0/10]	Loss: 0.090342
[INFO][09:52:18]: [Client #190] Epoch: [2/5][0/10]	Loss: 0.000009
[INFO][09:52:18]: [Client #190] Epoch: [3/5][0/10]	Loss: 0.000092
[INFO][09:52:18]: [Client #168] Epoch: [2/5][0/10]	Loss: 0.117267
[INFO][09:52:18]: [Client #181] Epoch: [3/5][0/10]	Loss: 0.006869
[INFO][09:52:18]: [Client #168] Epoch: [3/5][0/10]	Loss: 0.004106
[INFO][09:52:18]: [Client #190] Epoch: [4/5][0/10]	Loss: 0.002635
[INFO][09:52:18]: [Client #181] Epoch: [4/5][0/10]	Loss: 0.000714
[INFO][09:52:18]: [Client #168] Epoch: [4/5][0/10]	Loss: 0.173272
[INFO][09:52:18]: [Client #190] Epoch: [5/5][0/10]	Loss: 0.003994
[INFO][09:52:18]: [Client #181] Epoch: [5/5][0/10]	Loss: 0.000803
[INFO][09:52:18]: [Client #190] Model saved to /data/ykang/plato/results/test/model/lenet5_190_1127978.pth.
[INFO][09:52:18]: [Client #168] Epoch: [5/5][0/10]	Loss: 0.120085
[INFO][09:52:18]: [Client #181] Model saved to /data/ykang/plato/results/test/model/lenet5_181_1127979.pth.
[INFO][09:52:18]: [Client #168] Model saved to /data/ykang/plato/results/test/model/lenet5_168_1127977.pth.
[INFO][09:52:19]: [Client #190] Loading a model from /data/ykang/plato/results/test/model/lenet5_190_1127978.pth.
[INFO][09:52:19]: [Client #190] Model trained.
[INFO][09:52:19]: [Client #190] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:52:19]: [Server #1127936] Received 0.24 MB of payload data from client #190 (simulated).
[INFO][09:52:19]: [Client #181] Loading a model from /data/ykang/plato/results/test/model/lenet5_181_1127979.pth.
[INFO][09:52:19]: [Client #181] Model trained.
[INFO][09:52:19]: [Client #181] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:52:19]: [Server #1127936] Received 0.24 MB of payload data from client #181 (simulated).
[INFO][09:52:19]: [Client #168] Loading a model from /data/ykang/plato/results/test/model/lenet5_168_1127977.pth.
[INFO][09:52:19]: [Client #168] Model trained.
[INFO][09:52:19]: [Client #168] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:52:19]: [Server #1127936] Received 0.24 MB of payload data from client #168 (simulated).
[INFO][09:52:19]: [Server #1127936] Selecting client #31 for training.
[INFO][09:52:19]: [Server #1127936] Sending the current model to client #31 (simulated).
[INFO][09:52:19]: [Server #1127936] Sending 0.24 MB of payload data to client #31 (simulated).
[INFO][09:52:19]: [Server #1127936] Selecting client #317 for training.
[INFO][09:52:19]: [Server #1127936] Sending the current model to client #317 (simulated).
[INFO][09:52:19]: [Server #1127936] Sending 0.24 MB of payload data to client #317 (simulated).
[INFO][09:52:19]: [Server #1127936] Selecting client #355 for training.
[INFO][09:52:19]: [Server #1127936] Sending the current model to client #355 (simulated).
[INFO][09:52:19]: [Client #31] Selected by the server.
[INFO][09:52:19]: [Client #31] Loading its data source...
[INFO][09:52:19]: [Client #31] Dataset size: 60000
[INFO][09:52:19]: [Client #31] Sampler: noniid
[INFO][09:52:19]: [Server #1127936] Sending 0.24 MB of payload data to client #355 (simulated).
[INFO][09:52:19]: [Client #31] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:52:19]: [Client #317] Selected by the server.
[INFO][09:52:19]: [Client #317] Loading its data source...
[INFO][09:52:19]: [Client #317] Dataset size: 60000
[INFO][09:52:19]: [Client #317] Sampler: noniid
[INFO][09:52:19]: [Client #355] Selected by the server.
[INFO][09:52:19]: [Client #355] Loading its data source...
[INFO][09:52:19]: [Client #355] Dataset size: 60000
[INFO][09:52:19]: [Client #355] Sampler: noniid
[INFO][09:52:19]: [Client #317] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:52:19]: [Client #355] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:52:19]: [93m[1m[Client #317] Started training in communication round #18.[0m
[INFO][09:52:19]: [93m[1m[Client #355] Started training in communication round #18.[0m
[INFO][09:52:19]: [93m[1m[Client #31] Started training in communication round #18.[0m
[INFO][09:52:21]: [Client #31] Loading the dataset.
[INFO][09:52:21]: [Client #355] Loading the dataset.
[INFO][09:52:21]: [Client #317] Loading the dataset.
[INFO][09:52:27]: [Client #317] Epoch: [1/5][0/10]	Loss: 0.008165
[INFO][09:52:27]: [Client #355] Epoch: [1/5][0/10]	Loss: 0.008165
[INFO][09:52:27]: [Client #317] Epoch: [2/5][0/10]	Loss: 0.001043
[INFO][09:52:27]: [Client #31] Epoch: [1/5][0/10]	Loss: 0.045436
[INFO][09:52:27]: [Client #355] Epoch: [2/5][0/10]	Loss: 0.001913
[INFO][09:52:27]: [Client #317] Epoch: [3/5][0/10]	Loss: 0.000000
[INFO][09:52:28]: [Client #31] Epoch: [2/5][0/10]	Loss: 0.217717
[INFO][09:52:28]: [Client #317] Epoch: [4/5][0/10]	Loss: 0.000029
[INFO][09:52:28]: [Client #355] Epoch: [3/5][0/10]	Loss: 0.009647
[INFO][09:52:28]: [Client #31] Epoch: [3/5][0/10]	Loss: 0.001842
[INFO][09:52:28]: [Client #355] Epoch: [4/5][0/10]	Loss: 0.069294
[INFO][09:52:28]: [Client #317] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][09:52:28]: [Client #317] Model saved to /data/ykang/plato/results/test/model/lenet5_317_1127978.pth.
[INFO][09:52:28]: [Client #31] Epoch: [4/5][0/10]	Loss: 0.018470
[INFO][09:52:28]: [Client #355] Epoch: [5/5][0/10]	Loss: 0.000202
[INFO][09:52:28]: [Client #355] Model saved to /data/ykang/plato/results/test/model/lenet5_355_1127979.pth.
[INFO][09:52:28]: [Client #31] Epoch: [5/5][0/10]	Loss: 0.000017
[INFO][09:52:28]: [Client #31] Model saved to /data/ykang/plato/results/test/model/lenet5_31_1127977.pth.
[INFO][09:52:29]: [Client #317] Loading a model from /data/ykang/plato/results/test/model/lenet5_317_1127978.pth.
[INFO][09:52:29]: [Client #317] Model trained.
[INFO][09:52:29]: [Client #317] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:52:29]: [Server #1127936] Received 0.24 MB of payload data from client #317 (simulated).
[INFO][09:52:29]: [Client #355] Loading a model from /data/ykang/plato/results/test/model/lenet5_355_1127979.pth.
[INFO][09:52:29]: [Client #355] Model trained.
[INFO][09:52:29]: [Client #355] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:52:29]: [Server #1127936] Received 0.24 MB of payload data from client #355 (simulated).
[INFO][09:52:29]: [Client #31] Loading a model from /data/ykang/plato/results/test/model/lenet5_31_1127977.pth.
[INFO][09:52:29]: [Client #31] Model trained.
[INFO][09:52:29]: [Client #31] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:52:29]: [Server #1127936] Received 0.24 MB of payload data from client #31 (simulated).
[INFO][09:52:29]: [Server #1127936] Selecting client #308 for training.
[INFO][09:52:29]: [Server #1127936] Sending the current model to client #308 (simulated).
[INFO][09:52:29]: [Server #1127936] Sending 0.24 MB of payload data to client #308 (simulated).
[INFO][09:52:29]: [Server #1127936] Selecting client #325 for training.
[INFO][09:52:29]: [Server #1127936] Sending the current model to client #325 (simulated).
[INFO][09:52:29]: [Server #1127936] Sending 0.24 MB of payload data to client #325 (simulated).
[INFO][09:52:29]: [Server #1127936] Selecting client #88 for training.
[INFO][09:52:29]: [Server #1127936] Sending the current model to client #88 (simulated).
[INFO][09:52:29]: [Client #308] Selected by the server.
[INFO][09:52:29]: [Client #308] Loading its data source...
[INFO][09:52:29]: [Client #308] Dataset size: 60000
[INFO][09:52:29]: [Client #308] Sampler: noniid
[INFO][09:52:29]: [Server #1127936] Sending 0.24 MB of payload data to client #88 (simulated).
[INFO][09:52:29]: [Client #325] Selected by the server.
[INFO][09:52:29]: [Client #325] Loading its data source...
[INFO][09:52:29]: [Client #325] Dataset size: 60000
[INFO][09:52:29]: [Client #325] Sampler: noniid
[INFO][09:52:29]: [Client #88] Selected by the server.
[INFO][09:52:29]: [Client #88] Loading its data source...
[INFO][09:52:29]: [Client #88] Dataset size: 60000
[INFO][09:52:29]: [Client #88] Sampler: noniid
[INFO][09:52:29]: [Client #308] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:52:29]: [Client #325] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:52:29]: [93m[1m[Client #325] Started training in communication round #18.[0m
[INFO][09:52:29]: [93m[1m[Client #308] Started training in communication round #18.[0m
[INFO][09:52:29]: [Client #88] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:52:29]: [93m[1m[Client #88] Started training in communication round #18.[0m
[INFO][09:52:31]: [Client #88] Loading the dataset.
[INFO][09:52:31]: [Client #325] Loading the dataset.
[INFO][09:52:31]: [Client #308] Loading the dataset.
[INFO][09:52:37]: [Client #88] Epoch: [1/5][0/10]	Loss: 0.865449
[INFO][09:52:37]: [Client #308] Epoch: [1/5][0/10]	Loss: 0.045603
[INFO][09:52:37]: [Client #325] Epoch: [1/5][0/10]	Loss: 0.175860
[INFO][09:52:37]: [Client #88] Epoch: [2/5][0/10]	Loss: 0.026182
[INFO][09:52:37]: [Client #308] Epoch: [2/5][0/10]	Loss: 0.032916
[INFO][09:52:37]: [Client #325] Epoch: [2/5][0/10]	Loss: 0.071104
[INFO][09:52:37]: [Client #308] Epoch: [3/5][0/10]	Loss: 0.009827
[INFO][09:52:37]: [Client #88] Epoch: [3/5][0/10]	Loss: 0.053521
[INFO][09:52:37]: [Client #325] Epoch: [3/5][0/10]	Loss: 0.086452
[INFO][09:52:37]: [Client #308] Epoch: [4/5][0/10]	Loss: 0.001691
[INFO][09:52:37]: [Client #88] Epoch: [4/5][0/10]	Loss: 0.011650
[INFO][09:52:37]: [Client #325] Epoch: [4/5][0/10]	Loss: 0.054653
[INFO][09:52:37]: [Client #88] Epoch: [5/5][0/10]	Loss: 0.269699
[INFO][09:52:37]: [Client #88] Model saved to /data/ykang/plato/results/test/model/lenet5_88_1127979.pth.
[INFO][09:52:37]: [Client #308] Epoch: [5/5][0/10]	Loss: 0.002203
[INFO][09:52:37]: [Client #325] Epoch: [5/5][0/10]	Loss: 0.497969
[INFO][09:52:37]: [Client #308] Model saved to /data/ykang/plato/results/test/model/lenet5_308_1127977.pth.
[INFO][09:52:37]: [Client #325] Model saved to /data/ykang/plato/results/test/model/lenet5_325_1127978.pth.
[INFO][09:52:38]: [Client #88] Loading a model from /data/ykang/plato/results/test/model/lenet5_88_1127979.pth.
[INFO][09:52:38]: [Client #88] Model trained.
[INFO][09:52:38]: [Client #88] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:52:38]: [Server #1127936] Received 0.24 MB of payload data from client #88 (simulated).
[INFO][09:52:38]: [Client #308] Loading a model from /data/ykang/plato/results/test/model/lenet5_308_1127977.pth.
[INFO][09:52:38]: [Client #308] Model trained.
[INFO][09:52:38]: [Client #308] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:52:38]: [Server #1127936] Received 0.24 MB of payload data from client #308 (simulated).
[INFO][09:52:38]: [Client #325] Loading a model from /data/ykang/plato/results/test/model/lenet5_325_1127978.pth.
[INFO][09:52:38]: [Client #325] Model trained.
[INFO][09:52:38]: [Client #325] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:52:38]: [Server #1127936] Received 0.24 MB of payload data from client #325 (simulated).
[INFO][09:52:38]: [Server #1127936] Selecting client #78 for training.
[INFO][09:52:38]: [Server #1127936] Sending the current model to client #78 (simulated).
[INFO][09:52:38]: [Server #1127936] Sending 0.24 MB of payload data to client #78 (simulated).
[INFO][09:52:38]: [Client #78] Selected by the server.
[INFO][09:52:38]: [Client #78] Loading its data source...
[INFO][09:52:38]: [Client #78] Dataset size: 60000
[INFO][09:52:38]: [Client #78] Sampler: noniid
[INFO][09:52:38]: [Client #78] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:52:38]: [93m[1m[Client #78] Started training in communication round #18.[0m
[INFO][09:52:40]: [Client #78] Loading the dataset.
[INFO][09:52:45]: [Client #78] Epoch: [1/5][0/10]	Loss: 0.191453
[INFO][09:52:45]: [Client #78] Epoch: [2/5][0/10]	Loss: 0.073475
[INFO][09:52:45]: [Client #78] Epoch: [3/5][0/10]	Loss: 0.000079
[INFO][09:52:45]: [Client #78] Epoch: [4/5][0/10]	Loss: 0.004872
[INFO][09:52:45]: [Client #78] Epoch: [5/5][0/10]	Loss: 0.068186
[INFO][09:52:45]: [Client #78] Model saved to /data/ykang/plato/results/test/model/lenet5_78_1127977.pth.
[INFO][09:52:46]: [Client #78] Loading a model from /data/ykang/plato/results/test/model/lenet5_78_1127977.pth.
[INFO][09:52:46]: [Client #78] Model trained.
[INFO][09:52:46]: [Client #78] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:52:46]: [Server #1127936] Received 0.24 MB of payload data from client #78 (simulated).
[INFO][09:52:46]: [Server #1127936] Adding client #384 to the list of clients for aggregation.
[INFO][09:52:46]: [Server #1127936] Adding client #8 to the list of clients for aggregation.
[INFO][09:52:46]: [Server #1127936] Adding client #469 to the list of clients for aggregation.
[INFO][09:52:46]: [Server #1127936] Adding client #293 to the list of clients for aggregation.
[INFO][09:52:46]: [Server #1127936] Adding client #119 to the list of clients for aggregation.
[INFO][09:52:46]: [Server #1127936] Adding client #193 to the list of clients for aggregation.
[INFO][09:52:46]: [Server #1127936] Adding client #332 to the list of clients for aggregation.
[INFO][09:52:46]: [Server #1127936] Adding client #31 to the list of clients for aggregation.
[INFO][09:52:46]: [Server #1127936] Adding client #308 to the list of clients for aggregation.
[INFO][09:52:46]: [Server #1127936] Adding client #78 to the list of clients for aggregation.
[INFO][09:52:46]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.01067873 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.02214516 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.03368153
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01983279 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01607708 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.07466842 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00987498 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.02337158 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.05115881
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.02355152 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 1. 6. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 2. 0. 6. 0. 1. 2. 1. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 2. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 4. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 2. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 6. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.01067873 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.02214516 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.03368153
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01983279 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01607708 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.07466842 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00987498 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.02337158 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.05115881
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.02355152 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][09:52:48]: [Server #1127936] Global model accuracy: 89.12%

[INFO][09:52:48]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_18.pth.
[INFO][09:52:48]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_18.pth.
[INFO][09:52:48]: [93m[1m
[Server #1127936] Starting round 19/100.[0m
[0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002
 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.1   0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1
 0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  4e-04  7e-09  7e-09
 5:  6.8876e+00  6.8875e+00  8e-05  1e-09  1e-09
 6:  6.8876e+00  6.8875e+00  7e-05  6e-10  6e-10
 7:  6.8875e+00  6.8875e+00  7e-05  2e-09  2e-10
 8:  6.8875e+00  6.8875e+00  5e-05  3e-09  2e-10
 9:  6.8875e+00  6.8875e+00  2e-05  2e-08  2e-09
10:  6.8875e+00  6.8875e+00  7e-06  1e-08  9e-10
Optimal solution found.
The calculated probability is:  [1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 2.32101536e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.93986152e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.93966921e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 2.78878252e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 2.57605022e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 9.04066536e-01 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.93997885e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 3.02355205e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 8.56536753e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 3.03653369e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04 1.94000797e-04 1.94000797e-04
 1.94000797e-04 1.94000797e-04]
current clients pool:  [INFO][09:52:49]: [Server #1127936] Selected clients: [293 402 217 307  73 258 150 432 340 321]
[INFO][09:52:49]: [Server #1127936] Selecting client #293 for training.
[INFO][09:52:49]: [Server #1127936] Sending the current model to client #293 (simulated).
[INFO][09:52:49]: [Server #1127936] Sending 0.24 MB of payload data to client #293 (simulated).
[INFO][09:52:49]: [Server #1127936] Selecting client #402 for training.
[INFO][09:52:49]: [Server #1127936] Sending the current model to client #402 (simulated).
[INFO][09:52:49]: [Server #1127936] Sending 0.24 MB of payload data to client #402 (simulated).
[INFO][09:52:49]: [Server #1127936] Selecting client #217 for training.
[INFO][09:52:49]: [Server #1127936] Sending the current model to client #217 (simulated).
[INFO][09:52:49]: [Client #293] Selected by the server.
[INFO][09:52:49]: [Client #293] Loading its data source...
[INFO][09:52:49]: [Client #293] Dataset size: 60000
[INFO][09:52:49]: [Client #293] Sampler: noniid
[INFO][09:52:49]: [Server #1127936] Sending 0.24 MB of payload data to client #217 (simulated).
[INFO][09:52:49]: [Client #402] Selected by the server.
[INFO][09:52:49]: [Client #402] Loading its data source...
[INFO][09:52:49]: [Client #402] Dataset size: 60000
[INFO][09:52:49]: [Client #402] Sampler: noniid
[INFO][09:52:49]: [Client #293] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:52:49]: [Client #217] Selected by the server.
[INFO][09:52:49]: [Client #217] Loading its data source...
[INFO][09:52:49]: [Client #217] Dataset size: 60000
[INFO][09:52:49]: [Client #217] Sampler: noniid
[INFO][09:52:49]: [Client #402] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:52:49]: [Client #217] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:52:49]: [93m[1m[Client #293] Started training in communication round #19.[0m
[INFO][09:52:49]: [93m[1m[Client #217] Started training in communication round #19.[0m
[INFO][09:52:49]: [93m[1m[Client #402] Started training in communication round #19.[0m
[INFO][09:52:51]: [Client #217] Loading the dataset.
[INFO][09:52:51]: [Client #402] Loading the dataset.
[INFO][09:52:51]: [Client #293] Loading the dataset.
[INFO][09:52:57]: [Client #293] Epoch: [1/5][0/10]	Loss: 0.432716
[INFO][09:52:57]: [Client #402] Epoch: [1/5][0/10]	Loss: 0.327946
[INFO][09:52:57]: [Client #217] Epoch: [1/5][0/10]	Loss: 0.468350
[INFO][09:52:57]: [Client #293] Epoch: [2/5][0/10]	Loss: 0.172395
[INFO][09:52:57]: [Client #217] Epoch: [2/5][0/10]	Loss: 0.000425
[INFO][09:52:57]: [Client #402] Epoch: [2/5][0/10]	Loss: 0.112864
[INFO][09:52:57]: [Client #217] Epoch: [3/5][0/10]	Loss: 0.041528
[INFO][09:52:57]: [Client #293] Epoch: [3/5][0/10]	Loss: 0.007227
[INFO][09:52:57]: [Client #402] Epoch: [3/5][0/10]	Loss: 0.012732
[INFO][09:52:57]: [Client #217] Epoch: [4/5][0/10]	Loss: 0.002598
[INFO][09:52:57]: [Client #293] Epoch: [4/5][0/10]	Loss: 0.070882
[INFO][09:52:57]: [Client #217] Epoch: [5/5][0/10]	Loss: 0.046758
[INFO][09:52:57]: [Client #402] Epoch: [4/5][0/10]	Loss: 0.029361
[INFO][09:52:57]: [Client #217] Model saved to /data/ykang/plato/results/test/model/lenet5_217_1127979.pth.
[INFO][09:52:57]: [Client #293] Epoch: [5/5][0/10]	Loss: 0.077696
[INFO][09:52:57]: [Client #293] Model saved to /data/ykang/plato/results/test/model/lenet5_293_1127977.pth.
[INFO][09:52:57]: [Client #402] Epoch: [5/5][0/10]	Loss: 0.082133
[INFO][09:52:57]: [Client #402] Model saved to /data/ykang/plato/results/test/model/lenet5_402_1127978.pth.
[INFO][09:52:58]: [Client #217] Loading a model from /data/ykang/plato/results/test/model/lenet5_217_1127979.pth.
[INFO][09:52:58]: [Client #217] Model trained.
[INFO][09:52:58]: [Client #217] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:52:58]: [Server #1127936] Received 0.24 MB of payload data from client #217 (simulated).
[INFO][09:52:58]: [Client #293] Loading a model from /data/ykang/plato/results/test/model/lenet5_293_1127977.pth.
[INFO][09:52:58]: [Client #293] Model trained.
[INFO][09:52:58]: [Client #293] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:52:58]: [Server #1127936] Received 0.24 MB of payload data from client #293 (simulated).
[INFO][09:52:58]: [Client #402] Loading a model from /data/ykang/plato/results/test/model/lenet5_402_1127978.pth.
[INFO][09:52:58]: [Client #402] Model trained.
[INFO][09:52:58]: [Client #402] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:52:58]: [Server #1127936] Received 0.24 MB of payload data from client #402 (simulated).
[INFO][09:52:58]: [Server #1127936] Selecting client #307 for training.
[INFO][09:52:58]: [Server #1127936] Sending the current model to client #307 (simulated).
[INFO][09:52:58]: [Server #1127936] Sending 0.24 MB of payload data to client #307 (simulated).
[INFO][09:52:58]: [Server #1127936] Selecting client #73 for training.
[INFO][09:52:58]: [Server #1127936] Sending the current model to client #73 (simulated).
[INFO][09:52:58]: [Server #1127936] Sending 0.24 MB of payload data to client #73 (simulated).
[INFO][09:52:58]: [Server #1127936] Selecting client #258 for training.
[INFO][09:52:58]: [Server #1127936] Sending the current model to client #258 (simulated).
[INFO][09:52:58]: [Client #307] Selected by the server.
[INFO][09:52:58]: [Client #307] Loading its data source...
[INFO][09:52:58]: [Client #307] Dataset size: 60000
[INFO][09:52:58]: [Client #307] Sampler: noniid
[INFO][09:52:58]: [Server #1127936] Sending 0.24 MB of payload data to client #258 (simulated).
[INFO][09:52:58]: [Client #73] Selected by the server.
[INFO][09:52:58]: [Client #73] Loading its data source...
[INFO][09:52:58]: [Client #73] Dataset size: 60000
[INFO][09:52:58]: [Client #73] Sampler: noniid
[INFO][09:52:58]: [Client #258] Selected by the server.
[INFO][09:52:58]: [Client #258] Loading its data source...
[INFO][09:52:58]: [Client #258] Dataset size: 60000
[INFO][09:52:58]: [Client #258] Sampler: noniid
[INFO][09:52:58]: [Client #73] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:52:58]: [Client #307] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:52:58]: [93m[1m[Client #73] Started training in communication round #19.[0m
[INFO][09:52:58]: [93m[1m[Client #307] Started training in communication round #19.[0m
[INFO][09:52:58]: [Client #258] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:52:58]: [93m[1m[Client #258] Started training in communication round #19.[0m
[INFO][09:53:00]: [Client #258] Loading the dataset.
[INFO][09:53:00]: [Client #307] Loading the dataset.
[INFO][09:53:00]: [Client #73] Loading the dataset.
[INFO][09:53:06]: [Client #258] Epoch: [1/5][0/10]	Loss: 0.610463
[INFO][09:53:06]: [Client #73] Epoch: [1/5][0/10]	Loss: 0.543596
[INFO][09:53:06]: [Client #258] Epoch: [2/5][0/10]	Loss: 0.094765
[INFO][09:53:06]: [Client #307] Epoch: [1/5][0/10]	Loss: 0.535589
[INFO][09:53:06]: [Client #307] Epoch: [2/5][0/10]	Loss: 0.421276
[INFO][09:53:06]: [Client #73] Epoch: [2/5][0/10]	Loss: 0.001881
[INFO][09:53:06]: [Client #258] Epoch: [3/5][0/10]	Loss: 0.000898
[INFO][09:53:06]: [Client #307] Epoch: [3/5][0/10]	Loss: 0.016479
[INFO][09:53:06]: [Client #258] Epoch: [4/5][0/10]	Loss: 0.263525
[INFO][09:53:06]: [Client #73] Epoch: [3/5][0/10]	Loss: 0.182391
[INFO][09:53:06]: [Client #307] Epoch: [4/5][0/10]	Loss: 0.239048
[INFO][09:53:06]: [Client #258] Epoch: [5/5][0/10]	Loss: 0.427764
[INFO][09:53:06]: [Client #258] Model saved to /data/ykang/plato/results/test/model/lenet5_258_1127979.pth.
[INFO][09:53:06]: [Client #73] Epoch: [4/5][0/10]	Loss: 0.065739
[INFO][09:53:07]: [Client #307] Epoch: [5/5][0/10]	Loss: 0.190911
[INFO][09:53:07]: [Client #73] Epoch: [5/5][0/10]	Loss: 0.037639
[INFO][09:53:07]: [Client #307] Model saved to /data/ykang/plato/results/test/model/lenet5_307_1127977.pth.
[INFO][09:53:07]: [Client #73] Model saved to /data/ykang/plato/results/test/model/lenet5_73_1127978.pth.
[INFO][09:53:07]: [Client #258] Loading a model from /data/ykang/plato/results/test/model/lenet5_258_1127979.pth.
[INFO][09:53:07]: [Client #258] Model trained.
[INFO][09:53:07]: [Client #258] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:53:07]: [Server #1127936] Received 0.24 MB of payload data from client #258 (simulated).
[INFO][09:53:07]: [Client #307] Loading a model from /data/ykang/plato/results/test/model/lenet5_307_1127977.pth.
[INFO][09:53:07]: [Client #307] Model trained.
[INFO][09:53:07]: [Client #307] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:53:07]: [Server #1127936] Received 0.24 MB of payload data from client #307 (simulated).
[INFO][09:53:07]: [Client #73] Loading a model from /data/ykang/plato/results/test/model/lenet5_73_1127978.pth.
[INFO][09:53:07]: [Client #73] Model trained.
[INFO][09:53:07]: [Client #73] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:53:07]: [Server #1127936] Received 0.24 MB of payload data from client #73 (simulated).
[INFO][09:53:07]: [Server #1127936] Selecting client #150 for training.
[INFO][09:53:07]: [Server #1127936] Sending the current model to client #150 (simulated).
[INFO][09:53:07]: [Server #1127936] Sending 0.24 MB of payload data to client #150 (simulated).
[INFO][09:53:07]: [Server #1127936] Selecting client #432 for training.
[INFO][09:53:07]: [Server #1127936] Sending the current model to client #432 (simulated).
[INFO][09:53:07]: [Server #1127936] Sending 0.24 MB of payload data to client #432 (simulated).
[INFO][09:53:07]: [Server #1127936] Selecting client #340 for training.
[INFO][09:53:07]: [Server #1127936] Sending the current model to client #340 (simulated).
[INFO][09:53:07]: [Client #150] Selected by the server.
[INFO][09:53:07]: [Client #150] Loading its data source...
[INFO][09:53:07]: [Client #150] Dataset size: 60000
[INFO][09:53:07]: [Client #150] Sampler: noniid
[INFO][09:53:07]: [Server #1127936] Sending 0.24 MB of payload data to client #340 (simulated).
[INFO][09:53:07]: [Client #150] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:53:07]: [Client #432] Selected by the server.
[INFO][09:53:07]: [Client #432] Loading its data source...
[INFO][09:53:07]: [Client #432] Dataset size: 60000
[INFO][09:53:07]: [Client #432] Sampler: noniid
[INFO][09:53:07]: [Client #340] Selected by the server.
[INFO][09:53:07]: [Client #340] Loading its data source...
[INFO][09:53:07]: [Client #340] Dataset size: 60000
[INFO][09:53:07]: [Client #340] Sampler: noniid
[INFO][09:53:07]: [Client #432] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:53:07]: [Client #340] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:53:07]: [93m[1m[Client #150] Started training in communication round #19.[0m
[INFO][09:53:07]: [93m[1m[Client #340] Started training in communication round #19.[0m
[INFO][09:53:07]: [93m[1m[Client #432] Started training in communication round #19.[0m
[INFO][09:53:10]: [Client #150] Loading the dataset.
[INFO][09:53:10]: [Client #340] Loading the dataset.
[INFO][09:53:10]: [Client #432] Loading the dataset.
[INFO][09:53:15]: [Client #150] Epoch: [1/5][0/10]	Loss: 0.336048
[INFO][09:53:15]: [Client #432] Epoch: [1/5][0/10]	Loss: 0.378093
[INFO][09:53:16]: [Client #150] Epoch: [2/5][0/10]	Loss: 0.024371
[INFO][09:53:16]: [Client #340] Epoch: [1/5][0/10]	Loss: 0.195570
[INFO][09:53:16]: [Client #432] Epoch: [2/5][0/10]	Loss: 0.001029
[INFO][09:53:16]: [Client #150] Epoch: [3/5][0/10]	Loss: 0.044776
[INFO][09:53:16]: [Client #340] Epoch: [2/5][0/10]	Loss: 0.007453
[INFO][09:53:16]: [Client #432] Epoch: [3/5][0/10]	Loss: 0.034681
[INFO][09:53:16]: [Client #150] Epoch: [4/5][0/10]	Loss: 0.000956
[INFO][09:53:16]: [Client #340] Epoch: [3/5][0/10]	Loss: 0.007483
[INFO][09:53:16]: [Client #432] Epoch: [4/5][0/10]	Loss: 0.000184
[INFO][09:53:16]: [Client #150] Epoch: [5/5][0/10]	Loss: 0.186108
[INFO][09:53:16]: [Client #340] Epoch: [4/5][0/10]	Loss: 0.192272
[INFO][09:53:16]: [Client #150] Model saved to /data/ykang/plato/results/test/model/lenet5_150_1127977.pth.
[INFO][09:53:16]: [Client #432] Epoch: [5/5][0/10]	Loss: 0.000114
[INFO][09:53:16]: [Client #340] Epoch: [5/5][0/10]	Loss: 0.002338
[INFO][09:53:16]: [Client #432] Model saved to /data/ykang/plato/results/test/model/lenet5_432_1127978.pth.
[INFO][09:53:16]: [Client #340] Model saved to /data/ykang/plato/results/test/model/lenet5_340_1127979.pth.
[INFO][09:53:17]: [Client #150] Loading a model from /data/ykang/plato/results/test/model/lenet5_150_1127977.pth.
[INFO][09:53:17]: [Client #150] Model trained.
[INFO][09:53:17]: [Client #150] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:53:17]: [Server #1127936] Received 0.24 MB of payload data from client #150 (simulated).
[INFO][09:53:17]: [Client #432] Loading a model from /data/ykang/plato/results/test/model/lenet5_432_1127978.pth.
[INFO][09:53:17]: [Client #432] Model trained.
[INFO][09:53:17]: [Client #432] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:53:17]: [Server #1127936] Received 0.24 MB of payload data from client #432 (simulated).
[INFO][09:53:17]: [Client #340] Loading a model from /data/ykang/plato/results/test/model/lenet5_340_1127979.pth.
[INFO][09:53:17]: [Client #340] Model trained.
[INFO][09:53:17]: [Client #340] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:53:17]: [Server #1127936] Received 0.24 MB of payload data from client #340 (simulated).
[INFO][09:53:17]: [Server #1127936] Selecting client #321 for training.
[INFO][09:53:17]: [Server #1127936] Sending the current model to client #321 (simulated).
[INFO][09:53:17]: [Server #1127936] Sending 0.24 MB of payload data to client #321 (simulated).
[INFO][09:53:17]: [Client #321] Selected by the server.
[INFO][09:53:17]: [Client #321] Loading its data source...
[INFO][09:53:17]: [Client #321] Dataset size: 60000
[INFO][09:53:17]: [Client #321] Sampler: noniid
[INFO][09:53:17]: [Client #321] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:53:17]: [93m[1m[Client #321] Started training in communication round #19.[0m
[INFO][09:53:19]: [Client #321] Loading the dataset.
[INFO][09:53:24]: [Client #321] Epoch: [1/5][0/10]	Loss: 0.035246
[INFO][09:53:24]: [Client #321] Epoch: [2/5][0/10]	Loss: 0.041408
[INFO][09:53:24]: [Client #321] Epoch: [3/5][0/10]	Loss: 0.000033
[INFO][09:53:24]: [Client #321] Epoch: [4/5][0/10]	Loss: 0.000862
[INFO][09:53:24]: [Client #321] Epoch: [5/5][0/10]	Loss: 0.039075
[INFO][09:53:24]: [Client #321] Model saved to /data/ykang/plato/results/test/model/lenet5_321_1127977.pth.
[INFO][09:53:25]: [Client #321] Loading a model from /data/ykang/plato/results/test/model/lenet5_321_1127977.pth.
[INFO][09:53:25]: [Client #321] Model trained.
[INFO][09:53:25]: [Client #321] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:53:25]: [Server #1127936] Received 0.24 MB of payload data from client #321 (simulated).
[INFO][09:53:25]: [Server #1127936] Adding client #168 to the list of clients for aggregation.
[INFO][09:53:25]: [Server #1127936] Adding client #325 to the list of clients for aggregation.
[INFO][09:53:25]: [Server #1127936] Adding client #181 to the list of clients for aggregation.
[INFO][09:53:25]: [Server #1127936] Adding client #355 to the list of clients for aggregation.
[INFO][09:53:25]: [Server #1127936] Adding client #317 to the list of clients for aggregation.
[INFO][09:53:25]: [Server #1127936] Adding client #150 to the list of clients for aggregation.
[INFO][09:53:25]: [Server #1127936] Adding client #217 to the list of clients for aggregation.
[INFO][09:53:25]: [Server #1127936] Adding client #73 to the list of clients for aggregation.
[INFO][09:53:25]: [Server #1127936] Adding client #307 to the list of clients for aggregation.
[INFO][09:53:25]: [Server #1127936] Adding client #321 to the list of clients for aggregation.
[INFO][09:53:25]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 182, 183, 184, 185, 186, 187, 188, 189, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 318, 319, 320, 321, 322, 323, 324, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.02590881 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01542266
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.02379368
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.02943576 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.02284406 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.03593797 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.02232139 0.
 0.         0.         0.00781728 0.         0.         0.
 0.02960327 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01071279 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 1. 6. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 2. 0. 6. 0. 1. 2. 1. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 2. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 4. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 2. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 6. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.02590881 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01542266
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.02379368
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.02943576 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.02284406 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.03593797 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.02232139 0.
 0.         0.         0.00781728 0.         0.         0.
 0.02960327 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01071279 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][09:53:27]: [Server #1127936] Global model accuracy: 90.58%

[INFO][09:53:27]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_19.pth.
[INFO][09:53:27]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_19.pth.
[INFO][09:53:27]: [93m[1m
[Server #1127936] Starting round 20/100.[0m
[0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002
 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.1   0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1
 0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  3e-05  6e-10  6e-10
 6:  6.8876e+00  6.8875e+00  3e-05  3e-10  3e-10
 7:  6.8875e+00  6.8875e+00  3e-05  6e-10  2e-11
 8:  6.8875e+00  6.8875e+00  2e-05  5e-10  2e-11
 9:  6.8875e+00  6.8875e+00  1e-05  1e-09  3e-11
10:  6.8875e+00  6.8875e+00  3e-06  8e-10  2e-11
Optimal solution found.
The calculated probability is:  [2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38563877e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38599847e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 1.37401569e-03 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 3.52412345e-01 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38576276e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38512417e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 1.06334988e-03 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38614519e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 5.29039665e-01 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 3.80353396e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04 2.38619592e-04 2.38619592e-04
 2.38619592e-04 2.38619592e-04]
current clients pool:  [INFO][09:53:28]: [Server #1127936] Selected clients: [181 325 444 317 297 425 375 286 464  32]
[INFO][09:53:28]: [Server #1127936] Selecting client #181 for training.
[INFO][09:53:28]: [Server #1127936] Sending the current model to client #181 (simulated).
[INFO][09:53:28]: [Server #1127936] Sending 0.24 MB of payload data to client #181 (simulated).
[INFO][09:53:28]: [Server #1127936] Selecting client #325 for training.
[INFO][09:53:28]: [Server #1127936] Sending the current model to client #325 (simulated).
[INFO][09:53:28]: [Server #1127936] Sending 0.24 MB of payload data to client #325 (simulated).
[INFO][09:53:28]: [Server #1127936] Selecting client #444 for training.
[INFO][09:53:28]: [Server #1127936] Sending the current model to client #444 (simulated).
[INFO][09:53:28]: [Client #181] Selected by the server.
[INFO][09:53:28]: [Client #181] Loading its data source...
[INFO][09:53:28]: [Client #181] Dataset size: 60000
[INFO][09:53:28]: [Client #181] Sampler: noniid
[INFO][09:53:28]: [Server #1127936] Sending 0.24 MB of payload data to client #444 (simulated).
[INFO][09:53:28]: [Client #325] Selected by the server.
[INFO][09:53:28]: [Client #325] Loading its data source...
[INFO][09:53:28]: [Client #325] Dataset size: 60000
[INFO][09:53:28]: [Client #325] Sampler: noniid
[INFO][09:53:28]: [Client #444] Selected by the server.
[INFO][09:53:28]: [Client #444] Loading its data source...
[INFO][09:53:28]: [Client #181] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:53:28]: [Client #444] Dataset size: 60000
[INFO][09:53:28]: [Client #444] Sampler: noniid
[INFO][09:53:28]: [Client #444] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:53:28]: [93m[1m[Client #181] Started training in communication round #20.[0m
[INFO][09:53:28]: [Client #325] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:53:28]: [93m[1m[Client #444] Started training in communication round #20.[0m
[INFO][09:53:28]: [93m[1m[Client #325] Started training in communication round #20.[0m
[INFO][09:53:30]: [Client #181] Loading the dataset.
[INFO][09:53:30]: [Client #444] Loading the dataset.
[INFO][09:53:30]: [Client #325] Loading the dataset.
[INFO][09:53:36]: [Client #444] Epoch: [1/5][0/10]	Loss: 0.272354
[INFO][09:53:36]: [Client #325] Epoch: [1/5][0/10]	Loss: 0.056661
[INFO][09:53:36]: [Client #444] Epoch: [2/5][0/10]	Loss: 0.004568
[INFO][09:53:36]: [Client #181] Epoch: [1/5][0/10]	Loss: 0.187525
[INFO][09:53:36]: [Client #325] Epoch: [2/5][0/10]	Loss: 0.011937
[INFO][09:53:36]: [Client #181] Epoch: [2/5][0/10]	Loss: 0.017860
[INFO][09:53:36]: [Client #444] Epoch: [3/5][0/10]	Loss: 0.001235
[INFO][09:53:36]: [Client #325] Epoch: [3/5][0/10]	Loss: 0.208603
[INFO][09:53:36]: [Client #181] Epoch: [3/5][0/10]	Loss: 0.000519
[INFO][09:53:36]: [Client #444] Epoch: [4/5][0/10]	Loss: 0.148271
[INFO][09:53:36]: [Client #181] Epoch: [4/5][0/10]	Loss: 0.000174
[INFO][09:53:36]: [Client #325] Epoch: [4/5][0/10]	Loss: 0.032373
[INFO][09:53:36]: [Client #444] Epoch: [5/5][0/10]	Loss: 0.135243
[INFO][09:53:36]: [Client #181] Epoch: [5/5][0/10]	Loss: 0.000188
[INFO][09:53:36]: [Client #444] Model saved to /data/ykang/plato/results/test/model/lenet5_444_1127979.pth.
[INFO][09:53:36]: [Client #325] Epoch: [5/5][0/10]	Loss: 0.064355
[INFO][09:53:36]: [Client #181] Model saved to /data/ykang/plato/results/test/model/lenet5_181_1127977.pth.
[INFO][09:53:36]: [Client #325] Model saved to /data/ykang/plato/results/test/model/lenet5_325_1127978.pth.
[INFO][09:53:37]: [Client #444] Loading a model from /data/ykang/plato/results/test/model/lenet5_444_1127979.pth.
[INFO][09:53:37]: [Client #444] Model trained.
[INFO][09:53:37]: [Client #444] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:53:37]: [Server #1127936] Received 0.24 MB of payload data from client #444 (simulated).
[INFO][09:53:37]: [Client #181] Loading a model from /data/ykang/plato/results/test/model/lenet5_181_1127977.pth.
[INFO][09:53:37]: [Client #181] Model trained.
[INFO][09:53:37]: [Client #181] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:53:37]: [Server #1127936] Received 0.24 MB of payload data from client #181 (simulated).
[INFO][09:53:37]: [Client #325] Loading a model from /data/ykang/plato/results/test/model/lenet5_325_1127978.pth.
[INFO][09:53:37]: [Client #325] Model trained.
[INFO][09:53:37]: [Client #325] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:53:37]: [Server #1127936] Received 0.24 MB of payload data from client #325 (simulated).
[INFO][09:53:37]: [Server #1127936] Selecting client #317 for training.
[INFO][09:53:37]: [Server #1127936] Sending the current model to client #317 (simulated).
[INFO][09:53:37]: [Server #1127936] Sending 0.24 MB of payload data to client #317 (simulated).
[INFO][09:53:37]: [Server #1127936] Selecting client #297 for training.
[INFO][09:53:37]: [Server #1127936] Sending the current model to client #297 (simulated).
[INFO][09:53:37]: [Server #1127936] Sending 0.24 MB of payload data to client #297 (simulated).
[INFO][09:53:37]: [Server #1127936] Selecting client #425 for training.
[INFO][09:53:37]: [Server #1127936] Sending the current model to client #425 (simulated).
[INFO][09:53:37]: [Client #317] Selected by the server.
[INFO][09:53:37]: [Client #317] Loading its data source...
[INFO][09:53:37]: [Client #317] Dataset size: 60000
[INFO][09:53:37]: [Client #317] Sampler: noniid
[INFO][09:53:37]: [Server #1127936] Sending 0.24 MB of payload data to client #425 (simulated).
[INFO][09:53:37]: [Client #297] Selected by the server.
[INFO][09:53:37]: [Client #297] Loading its data source...
[INFO][09:53:37]: [Client #297] Dataset size: 60000
[INFO][09:53:37]: [Client #297] Sampler: noniid
[INFO][09:53:37]: [Client #425] Selected by the server.
[INFO][09:53:37]: [Client #425] Loading its data source...
[INFO][09:53:37]: [Client #425] Dataset size: 60000
[INFO][09:53:37]: [Client #425] Sampler: noniid
[INFO][09:53:37]: [Client #317] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:53:37]: [Client #425] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:53:37]: [Client #297] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:53:37]: [93m[1m[Client #425] Started training in communication round #20.[0m
[INFO][09:53:37]: [93m[1m[Client #317] Started training in communication round #20.[0m
[INFO][09:53:37]: [93m[1m[Client #297] Started training in communication round #20.[0m
[INFO][09:53:39]: [Client #297] Loading the dataset.
[INFO][09:53:39]: [Client #317] Loading the dataset.
[INFO][09:53:39]: [Client #425] Loading the dataset.
[INFO][09:53:45]: [Client #317] Epoch: [1/5][0/10]	Loss: 0.003767
[INFO][09:53:45]: [Client #297] Epoch: [1/5][0/10]	Loss: 0.166548
[INFO][09:53:45]: [Client #317] Epoch: [2/5][0/10]	Loss: 0.001025
[INFO][09:53:45]: [Client #425] Epoch: [1/5][0/10]	Loss: 0.245960
[INFO][09:53:45]: [Client #297] Epoch: [2/5][0/10]	Loss: 0.003066
[INFO][09:53:45]: [Client #317] Epoch: [3/5][0/10]	Loss: 0.000001
[INFO][09:53:45]: [Client #425] Epoch: [2/5][0/10]	Loss: 0.103201
[INFO][09:53:45]: [Client #297] Epoch: [3/5][0/10]	Loss: 0.002002
[INFO][09:53:45]: [Client #317] Epoch: [4/5][0/10]	Loss: 0.000024
[INFO][09:53:45]: [Client #425] Epoch: [3/5][0/10]	Loss: 0.000606
[INFO][09:53:45]: [Client #297] Epoch: [4/5][0/10]	Loss: 0.000298
[INFO][09:53:45]: [Client #317] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][09:53:45]: [Client #317] Model saved to /data/ykang/plato/results/test/model/lenet5_317_1127977.pth.
[INFO][09:53:45]: [Client #425] Epoch: [4/5][0/10]	Loss: 0.002419
[INFO][09:53:45]: [Client #297] Epoch: [5/5][0/10]	Loss: 0.311235
[INFO][09:53:46]: [Client #425] Epoch: [5/5][0/10]	Loss: 0.050960
[INFO][09:53:46]: [Client #297] Model saved to /data/ykang/plato/results/test/model/lenet5_297_1127978.pth.
[INFO][09:53:46]: [Client #425] Model saved to /data/ykang/plato/results/test/model/lenet5_425_1127979.pth.
[INFO][09:53:46]: [Client #317] Loading a model from /data/ykang/plato/results/test/model/lenet5_317_1127977.pth.
[INFO][09:53:46]: [Client #317] Model trained.
[INFO][09:53:46]: [Client #317] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:53:46]: [Server #1127936] Received 0.24 MB of payload data from client #317 (simulated).
[INFO][09:53:46]: [Client #297] Loading a model from /data/ykang/plato/results/test/model/lenet5_297_1127978.pth.
[INFO][09:53:46]: [Client #297] Model trained.
[INFO][09:53:46]: [Client #297] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:53:46]: [Server #1127936] Received 0.24 MB of payload data from client #297 (simulated).
[INFO][09:53:46]: [Client #425] Loading a model from /data/ykang/plato/results/test/model/lenet5_425_1127979.pth.
[INFO][09:53:46]: [Client #425] Model trained.
[INFO][09:53:46]: [Client #425] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:53:46]: [Server #1127936] Received 0.24 MB of payload data from client #425 (simulated).
[INFO][09:53:46]: [Server #1127936] Selecting client #375 for training.
[INFO][09:53:46]: [Server #1127936] Sending the current model to client #375 (simulated).
[INFO][09:53:46]: [Server #1127936] Sending 0.24 MB of payload data to client #375 (simulated).
[INFO][09:53:46]: [Server #1127936] Selecting client #286 for training.
[INFO][09:53:46]: [Server #1127936] Sending the current model to client #286 (simulated).
[INFO][09:53:46]: [Server #1127936] Sending 0.24 MB of payload data to client #286 (simulated).
[INFO][09:53:46]: [Server #1127936] Selecting client #464 for training.
[INFO][09:53:46]: [Server #1127936] Sending the current model to client #464 (simulated).
[INFO][09:53:46]: [Client #375] Selected by the server.
[INFO][09:53:46]: [Client #375] Loading its data source...
[INFO][09:53:46]: [Client #375] Dataset size: 60000
[INFO][09:53:46]: [Client #375] Sampler: noniid
[INFO][09:53:46]: [Server #1127936] Sending 0.24 MB of payload data to client #464 (simulated).
[INFO][09:53:46]: [Client #286] Selected by the server.
[INFO][09:53:46]: [Client #464] Selected by the server.
[INFO][09:53:46]: [Client #286] Loading its data source...
[INFO][09:53:46]: [Client #464] Loading its data source...
[INFO][09:53:46]: [Client #286] Dataset size: 60000
[INFO][09:53:46]: [Client #464] Dataset size: 60000
[INFO][09:53:46]: [Client #286] Sampler: noniid
[INFO][09:53:46]: [Client #464] Sampler: noniid
[INFO][09:53:46]: [Client #375] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:53:46]: [Client #464] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:53:46]: [Client #286] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:53:46]: [93m[1m[Client #375] Started training in communication round #20.[0m
[INFO][09:53:46]: [93m[1m[Client #286] Started training in communication round #20.[0m
[INFO][09:53:46]: [93m[1m[Client #464] Started training in communication round #20.[0m
[INFO][09:53:48]: [Client #286] Loading the dataset.
[INFO][09:53:48]: [Client #464] Loading the dataset.
[INFO][09:53:48]: [Client #375] Loading the dataset.
[INFO][09:53:54]: [Client #464] Epoch: [1/5][0/10]	Loss: 0.053425
[INFO][09:53:54]: [Client #375] Epoch: [1/5][0/10]	Loss: 0.543518
[INFO][09:53:54]: [Client #286] Epoch: [1/5][0/10]	Loss: 0.373480
[INFO][09:53:55]: [Client #286] Epoch: [2/5][0/10]	Loss: 0.006402
[INFO][09:53:55]: [Client #464] Epoch: [2/5][0/10]	Loss: 0.032294
[INFO][09:53:55]: [Client #375] Epoch: [2/5][0/10]	Loss: 0.004478
[INFO][09:53:55]: [Client #286] Epoch: [3/5][0/10]	Loss: 0.001242
[INFO][09:53:55]: [Client #464] Epoch: [3/5][0/10]	Loss: 0.000178
[INFO][09:53:55]: [Client #286] Epoch: [4/5][0/10]	Loss: 0.000001
[INFO][09:53:55]: [Client #375] Epoch: [3/5][0/10]	Loss: 0.000382
[INFO][09:53:55]: [Client #286] Epoch: [5/5][0/10]	Loss: 0.004382
[INFO][09:53:55]: [Client #286] Model saved to /data/ykang/plato/results/test/model/lenet5_286_1127978.pth.
[INFO][09:53:55]: [Client #375] Epoch: [4/5][0/10]	Loss: 0.005025
[INFO][09:53:55]: [Client #464] Epoch: [4/5][0/10]	Loss: 0.000005
[INFO][09:53:55]: [Client #375] Epoch: [5/5][0/10]	Loss: 1.028519
[INFO][09:53:55]: [Client #464] Epoch: [5/5][0/10]	Loss: 0.003444
[INFO][09:53:55]: [Client #375] Model saved to /data/ykang/plato/results/test/model/lenet5_375_1127977.pth.
[INFO][09:53:55]: [Client #464] Model saved to /data/ykang/plato/results/test/model/lenet5_464_1127979.pth.
[INFO][09:53:55]: [Client #286] Loading a model from /data/ykang/plato/results/test/model/lenet5_286_1127978.pth.
[INFO][09:53:55]: [Client #286] Model trained.
[INFO][09:53:55]: [Client #286] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:53:55]: [Server #1127936] Received 0.24 MB of payload data from client #286 (simulated).
[INFO][09:53:56]: [Client #375] Loading a model from /data/ykang/plato/results/test/model/lenet5_375_1127977.pth.
[INFO][09:53:56]: [Client #375] Model trained.
[INFO][09:53:56]: [Client #375] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:53:56]: [Server #1127936] Received 0.24 MB of payload data from client #375 (simulated).
[INFO][09:53:56]: [Client #464] Loading a model from /data/ykang/plato/results/test/model/lenet5_464_1127979.pth.
[INFO][09:53:56]: [Client #464] Model trained.
[INFO][09:53:56]: [Client #464] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:53:56]: [Server #1127936] Received 0.24 MB of payload data from client #464 (simulated).
[INFO][09:53:56]: [Server #1127936] Selecting client #32 for training.
[INFO][09:53:56]: [Server #1127936] Sending the current model to client #32 (simulated).
[INFO][09:53:56]: [Server #1127936] Sending 0.24 MB of payload data to client #32 (simulated).
[INFO][09:53:56]: [Client #32] Selected by the server.
[INFO][09:53:56]: [Client #32] Loading its data source...
[INFO][09:53:56]: [Client #32] Dataset size: 60000
[INFO][09:53:56]: [Client #32] Sampler: noniid
[INFO][09:53:56]: [Client #32] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:53:56]: [93m[1m[Client #32] Started training in communication round #20.[0m
[INFO][09:53:58]: [Client #32] Loading the dataset.
[INFO][09:54:03]: [Client #32] Epoch: [1/5][0/10]	Loss: 0.173927
[INFO][09:54:03]: [Client #32] Epoch: [2/5][0/10]	Loss: 0.025718
[INFO][09:54:03]: [Client #32] Epoch: [3/5][0/10]	Loss: 0.041006
[INFO][09:54:03]: [Client #32] Epoch: [4/5][0/10]	Loss: 0.003840
[INFO][09:54:03]: [Client #32] Epoch: [5/5][0/10]	Loss: 0.005610
[INFO][09:54:03]: [Client #32] Model saved to /data/ykang/plato/results/test/model/lenet5_32_1127977.pth.
[INFO][09:54:04]: [Client #32] Loading a model from /data/ykang/plato/results/test/model/lenet5_32_1127977.pth.
[INFO][09:54:04]: [Client #32] Model trained.
[INFO][09:54:04]: [Client #32] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:54:04]: [Server #1127936] Received 0.24 MB of payload data from client #32 (simulated).
[INFO][09:54:04]: [Server #1127936] Adding client #432 to the list of clients for aggregation.
[INFO][09:54:04]: [Server #1127936] Adding client #340 to the list of clients for aggregation.
[INFO][09:54:04]: [Server #1127936] Adding client #293 to the list of clients for aggregation.
[INFO][09:54:04]: [Server #1127936] Adding client #258 to the list of clients for aggregation.
[INFO][09:54:04]: [Server #1127936] Adding client #190 to the list of clients for aggregation.
[INFO][09:54:04]: [Server #1127936] Adding client #402 to the list of clients for aggregation.
[INFO][09:54:04]: [Server #1127936] Adding client #444 to the list of clients for aggregation.
[INFO][09:54:04]: [Server #1127936] Adding client #375 to the list of clients for aggregation.
[INFO][09:54:04]: [Server #1127936] Adding client #464 to the list of clients for aggregation.
[INFO][09:54:04]: [Server #1127936] Adding client #297 to the list of clients for aggregation.
[INFO][09:54:04]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01283376 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.02010363
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01913037 0.
 0.         0.         0.01699656 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01511856 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.03437319 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01381308
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01397599
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.02536733
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.0160742  0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 1. 6. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 2. 0. 6. 0. 1. 2. 1. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 2. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 2. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 4. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.
 0. 0. 1. 1. 0. 2. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 6. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01283376 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.02010363
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01913037 0.
 0.         0.         0.01699656 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01511856 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.03437319 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01381308
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01397599
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.02536733
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.0160742  0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][09:54:06]: [Server #1127936] Global model accuracy: 91.87%

[INFO][09:54:06]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_20.pth.
[INFO][09:54:06]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_20.pth.
[INFO][09:54:06]: [93m[1m
[Server #1127936] Starting round 21/100.[0m
[0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002
 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1
 0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  3e-05  5e-10  5e-10
 6:  6.8876e+00  6.8875e+00  3e-05  3e-10  3e-10
 7:  6.8875e+00  6.8875e+00  2e-05  7e-10  2e-11
 8:  6.8875e+00  6.8875e+00  2e-05  6e-10  2e-11
 9:  6.8875e+00  6.8875e+00  1e-05  1e-09  3e-11
10:  6.8875e+00  6.8875e+00  8e-07  2e-09  5e-11
Optimal solution found.
The calculated probability is:  [6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 9.68875649e-01 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 4.49605223e-04 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 3.54422088e-04 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16049423e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 1.82849030e-04
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.15804264e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 1.57036627e-04 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 1.59869011e-04 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.15952005e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16057804e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05 6.16128806e-05 6.16128806e-05
 6.16128806e-05 6.16128806e-05]
current clients pool:  [INFO][09:54:06]: [Server #1127936] Selected clients: [361 190 335 197  98 375 176 399 252 265]
[INFO][09:54:06]: [Server #1127936] Selecting client #361 for training.
[INFO][09:54:06]: [Server #1127936] Sending the current model to client #361 (simulated).
[INFO][09:54:06]: [Server #1127936] Sending 0.24 MB of payload data to client #361 (simulated).
[INFO][09:54:06]: [Server #1127936] Selecting client #190 for training.
[INFO][09:54:06]: [Server #1127936] Sending the current model to client #190 (simulated).
[INFO][09:54:06]: [Server #1127936] Sending 0.24 MB of payload data to client #190 (simulated).
[INFO][09:54:06]: [Server #1127936] Selecting client #335 for training.
[INFO][09:54:06]: [Server #1127936] Sending the current model to client #335 (simulated).
[INFO][09:54:06]: [Client #361] Selected by the server.
[INFO][09:54:06]: [Client #361] Loading its data source...
[INFO][09:54:06]: [Client #361] Dataset size: 60000
[INFO][09:54:06]: [Client #361] Sampler: noniid
[INFO][09:54:06]: [Server #1127936] Sending 0.24 MB of payload data to client #335 (simulated).
[INFO][09:54:06]: [Client #190] Selected by the server.
[INFO][09:54:06]: [Client #190] Loading its data source...
[INFO][09:54:06]: [Client #190] Dataset size: 60000
[INFO][09:54:06]: [Client #190] Sampler: noniid
[INFO][09:54:06]: [Client #361] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:54:06]: [Client #335] Selected by the server.
[INFO][09:54:06]: [Client #335] Loading its data source...
[INFO][09:54:06]: [Client #335] Dataset size: 60000
[INFO][09:54:06]: [Client #335] Sampler: noniid
[INFO][09:54:06]: [Client #335] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:54:06]: [Client #190] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:54:07]: [93m[1m[Client #335] Started training in communication round #21.[0m
[INFO][09:54:07]: [93m[1m[Client #190] Started training in communication round #21.[0m
[INFO][09:54:07]: [93m[1m[Client #361] Started training in communication round #21.[0m
[INFO][09:54:09]: [Client #190] Loading the dataset.
[INFO][09:54:09]: [Client #361] Loading the dataset.
[INFO][09:54:09]: [Client #335] Loading the dataset.
[INFO][09:54:14]: [Client #190] Epoch: [1/5][0/10]	Loss: 0.096899
[INFO][09:54:14]: [Client #361] Epoch: [1/5][0/10]	Loss: 0.162609
[INFO][09:54:14]: [Client #335] Epoch: [1/5][0/10]	Loss: 0.429118
[INFO][09:54:14]: [Client #190] Epoch: [2/5][0/10]	Loss: 0.000004
[INFO][09:54:15]: [Client #361] Epoch: [2/5][0/10]	Loss: 0.061940
[INFO][09:54:15]: [Client #190] Epoch: [3/5][0/10]	Loss: 0.000034
[INFO][09:54:15]: [Client #335] Epoch: [2/5][0/10]	Loss: 0.000540
[INFO][09:54:15]: [Client #361] Epoch: [3/5][0/10]	Loss: 0.025976
[INFO][09:54:15]: [Client #190] Epoch: [4/5][0/10]	Loss: 0.001255
[INFO][09:54:15]: [Client #361] Epoch: [4/5][0/10]	Loss: 0.039488
[INFO][09:54:15]: [Client #335] Epoch: [3/5][0/10]	Loss: 0.004672
[INFO][09:54:15]: [Client #190] Epoch: [5/5][0/10]	Loss: 0.002243
[INFO][09:54:15]: [Client #361] Epoch: [5/5][0/10]	Loss: 0.228734
[INFO][09:54:15]: [Client #190] Model saved to /data/ykang/plato/results/test/model/lenet5_190_1127978.pth.
[INFO][09:54:15]: [Client #335] Epoch: [4/5][0/10]	Loss: 0.005869
[INFO][09:54:15]: [Client #361] Model saved to /data/ykang/plato/results/test/model/lenet5_361_1127977.pth.
[INFO][09:54:15]: [Client #335] Epoch: [5/5][0/10]	Loss: 0.000217
[INFO][09:54:15]: [Client #335] Model saved to /data/ykang/plato/results/test/model/lenet5_335_1127979.pth.
[INFO][09:54:16]: [Client #361] Loading a model from /data/ykang/plato/results/test/model/lenet5_361_1127977.pth.
[INFO][09:54:16]: [Client #361] Model trained.
[INFO][09:54:16]: [Client #361] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:54:16]: [Server #1127936] Received 0.24 MB of payload data from client #361 (simulated).
[INFO][09:54:16]: [Client #190] Loading a model from /data/ykang/plato/results/test/model/lenet5_190_1127978.pth.
[INFO][09:54:16]: [Client #190] Model trained.
[INFO][09:54:16]: [Client #190] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:54:16]: [Server #1127936] Received 0.24 MB of payload data from client #190 (simulated).
[INFO][09:54:16]: [Client #335] Loading a model from /data/ykang/plato/results/test/model/lenet5_335_1127979.pth.
[INFO][09:54:16]: [Client #335] Model trained.
[INFO][09:54:16]: [Client #335] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:54:16]: [Server #1127936] Received 0.24 MB of payload data from client #335 (simulated).
[INFO][09:54:16]: [Server #1127936] Selecting client #197 for training.
[INFO][09:54:16]: [Server #1127936] Sending the current model to client #197 (simulated).
[INFO][09:54:16]: [Server #1127936] Sending 0.24 MB of payload data to client #197 (simulated).
[INFO][09:54:16]: [Server #1127936] Selecting client #98 for training.
[INFO][09:54:16]: [Server #1127936] Sending the current model to client #98 (simulated).
[INFO][09:54:16]: [Server #1127936] Sending 0.24 MB of payload data to client #98 (simulated).
[INFO][09:54:16]: [Server #1127936] Selecting client #375 for training.
[INFO][09:54:16]: [Server #1127936] Sending the current model to client #375 (simulated).
[INFO][09:54:16]: [Client #197] Selected by the server.
[INFO][09:54:16]: [Client #197] Loading its data source...
[INFO][09:54:16]: [Client #197] Dataset size: 60000
[INFO][09:54:16]: [Client #197] Sampler: noniid
[INFO][09:54:16]: [Server #1127936] Sending 0.24 MB of payload data to client #375 (simulated).
[INFO][09:54:16]: [Client #98] Selected by the server.
[INFO][09:54:16]: [Client #98] Loading its data source...
[INFO][09:54:16]: [Client #98] Dataset size: 60000
[INFO][09:54:16]: [Client #375] Selected by the server.
[INFO][09:54:16]: [Client #98] Sampler: noniid
[INFO][09:54:16]: [Client #375] Loading its data source...
[INFO][09:54:16]: [Client #375] Dataset size: 60000
[INFO][09:54:16]: [Client #375] Sampler: noniid
[INFO][09:54:16]: [Client #197] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:54:16]: [Client #98] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:54:16]: [Client #375] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:54:16]: [93m[1m[Client #197] Started training in communication round #21.[0m
[INFO][09:54:16]: [93m[1m[Client #98] Started training in communication round #21.[0m
[INFO][09:54:16]: [93m[1m[Client #375] Started training in communication round #21.[0m
[INFO][09:54:18]: [Client #197] Loading the dataset.
[INFO][09:54:18]: [Client #98] Loading the dataset.
[INFO][09:54:18]: [Client #375] Loading the dataset.
[INFO][09:54:24]: [Client #375] Epoch: [1/5][0/10]	Loss: 0.286357
[INFO][09:54:24]: [Client #197] Epoch: [1/5][0/10]	Loss: 0.003763
[INFO][09:54:24]: [Client #98] Epoch: [1/5][0/10]	Loss: 0.348747
[INFO][09:54:24]: [Client #197] Epoch: [2/5][0/10]	Loss: 0.003085
[INFO][09:54:24]: [Client #375] Epoch: [2/5][0/10]	Loss: 0.006313
[INFO][09:54:24]: [Client #98] Epoch: [2/5][0/10]	Loss: 0.558749
[INFO][09:54:24]: [Client #197] Epoch: [3/5][0/10]	Loss: 0.039596
[INFO][09:54:24]: [Client #375] Epoch: [3/5][0/10]	Loss: 0.000179
[INFO][09:54:24]: [Client #98] Epoch: [3/5][0/10]	Loss: 0.000194
[INFO][09:54:24]: [Client #375] Epoch: [4/5][0/10]	Loss: 0.006571
[INFO][09:54:24]: [Client #197] Epoch: [4/5][0/10]	Loss: 0.011132
[INFO][09:54:24]: [Client #98] Epoch: [4/5][0/10]	Loss: 0.011693
[INFO][09:54:24]: [Client #375] Epoch: [5/5][0/10]	Loss: 1.375035
[INFO][09:54:24]: [Client #197] Epoch: [5/5][0/10]	Loss: 0.013917
[INFO][09:54:24]: [Client #375] Model saved to /data/ykang/plato/results/test/model/lenet5_375_1127979.pth.
[INFO][09:54:24]: [Client #98] Epoch: [5/5][0/10]	Loss: 0.034268
[INFO][09:54:24]: [Client #197] Model saved to /data/ykang/plato/results/test/model/lenet5_197_1127977.pth.
[INFO][09:54:24]: [Client #98] Model saved to /data/ykang/plato/results/test/model/lenet5_98_1127978.pth.
[INFO][09:54:25]: [Client #375] Loading a model from /data/ykang/plato/results/test/model/lenet5_375_1127979.pth.
[INFO][09:54:25]: [Client #375] Model trained.
[INFO][09:54:25]: [Client #375] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:54:25]: [Server #1127936] Received 0.24 MB of payload data from client #375 (simulated).
[INFO][09:54:25]: [Client #197] Loading a model from /data/ykang/plato/results/test/model/lenet5_197_1127977.pth.
[INFO][09:54:25]: [Client #197] Model trained.
[INFO][09:54:25]: [Client #197] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:54:25]: [Server #1127936] Received 0.24 MB of payload data from client #197 (simulated).
[INFO][09:54:25]: [Client #98] Loading a model from /data/ykang/plato/results/test/model/lenet5_98_1127978.pth.
[INFO][09:54:25]: [Client #98] Model trained.
[INFO][09:54:25]: [Client #98] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:54:25]: [Server #1127936] Received 0.24 MB of payload data from client #98 (simulated).
[INFO][09:54:25]: [Server #1127936] Selecting client #176 for training.
[INFO][09:54:25]: [Server #1127936] Sending the current model to client #176 (simulated).
[INFO][09:54:25]: [Server #1127936] Sending 0.24 MB of payload data to client #176 (simulated).
[INFO][09:54:25]: [Server #1127936] Selecting client #399 for training.
[INFO][09:54:25]: [Server #1127936] Sending the current model to client #399 (simulated).
[INFO][09:54:25]: [Server #1127936] Sending 0.24 MB of payload data to client #399 (simulated).
[INFO][09:54:25]: [Server #1127936] Selecting client #252 for training.
[INFO][09:54:25]: [Server #1127936] Sending the current model to client #252 (simulated).
[INFO][09:54:25]: [Client #176] Selected by the server.
[INFO][09:54:25]: [Client #176] Loading its data source...
[INFO][09:54:25]: [Client #176] Dataset size: 60000
[INFO][09:54:25]: [Client #176] Sampler: noniid
[INFO][09:54:25]: [Server #1127936] Sending 0.24 MB of payload data to client #252 (simulated).
[INFO][09:54:25]: [Client #399] Selected by the server.
[INFO][09:54:25]: [Client #399] Loading its data source...
[INFO][09:54:25]: [Client #399] Dataset size: 60000
[INFO][09:54:25]: [Client #399] Sampler: noniid
[INFO][09:54:25]: [Client #252] Selected by the server.
[INFO][09:54:25]: [Client #252] Loading its data source...
[INFO][09:54:25]: [Client #252] Dataset size: 60000
[INFO][09:54:25]: [Client #252] Sampler: noniid
[INFO][09:54:25]: [Client #176] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:54:25]: [Client #399] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:54:25]: [Client #252] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:54:25]: [93m[1m[Client #252] Started training in communication round #21.[0m
[INFO][09:54:25]: [93m[1m[Client #176] Started training in communication round #21.[0m
[INFO][09:54:25]: [93m[1m[Client #399] Started training in communication round #21.[0m
[INFO][09:54:27]: [Client #399] Loading the dataset.
[INFO][09:54:27]: [Client #252] Loading the dataset.
[INFO][09:54:27]: [Client #176] Loading the dataset.
[INFO][09:54:33]: [Client #176] Epoch: [1/5][0/10]	Loss: 0.249189
[INFO][09:54:33]: [Client #252] Epoch: [1/5][0/10]	Loss: 0.141159
[INFO][09:54:33]: [Client #399] Epoch: [1/5][0/10]	Loss: 0.447069
[INFO][09:54:33]: [Client #176] Epoch: [2/5][0/10]	Loss: 0.030915
[INFO][09:54:33]: [Client #252] Epoch: [2/5][0/10]	Loss: 0.018489
[INFO][09:54:33]: [Client #399] Epoch: [2/5][0/10]	Loss: 0.008180
[INFO][09:54:33]: [Client #176] Epoch: [3/5][0/10]	Loss: 0.041286
[INFO][09:54:33]: [Client #399] Epoch: [3/5][0/10]	Loss: 0.004119
[INFO][09:54:33]: [Client #252] Epoch: [3/5][0/10]	Loss: 0.000059
[INFO][09:54:33]: [Client #176] Epoch: [4/5][0/10]	Loss: 0.106453
[INFO][09:54:34]: [Client #252] Epoch: [4/5][0/10]	Loss: 0.099244
[INFO][09:54:34]: [Client #399] Epoch: [4/5][0/10]	Loss: 0.000654
[INFO][09:54:34]: [Client #176] Epoch: [5/5][0/10]	Loss: 0.008949
[INFO][09:54:34]: [Client #252] Epoch: [5/5][0/10]	Loss: 0.000855
[INFO][09:54:34]: [Client #176] Model saved to /data/ykang/plato/results/test/model/lenet5_176_1127977.pth.
[INFO][09:54:34]: [Client #252] Model saved to /data/ykang/plato/results/test/model/lenet5_252_1127979.pth.
[INFO][09:54:34]: [Client #399] Epoch: [5/5][0/10]	Loss: 0.163104
[INFO][09:54:34]: [Client #399] Model saved to /data/ykang/plato/results/test/model/lenet5_399_1127978.pth.
[INFO][09:54:34]: [Client #176] Loading a model from /data/ykang/plato/results/test/model/lenet5_176_1127977.pth.
[INFO][09:54:34]: [Client #176] Model trained.
[INFO][09:54:34]: [Client #176] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:54:34]: [Server #1127936] Received 0.24 MB of payload data from client #176 (simulated).
[INFO][09:54:35]: [Client #252] Loading a model from /data/ykang/plato/results/test/model/lenet5_252_1127979.pth.
[INFO][09:54:35]: [Client #252] Model trained.
[INFO][09:54:35]: [Client #252] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:54:35]: [Server #1127936] Received 0.24 MB of payload data from client #252 (simulated).
[INFO][09:54:35]: [Client #399] Loading a model from /data/ykang/plato/results/test/model/lenet5_399_1127978.pth.
[INFO][09:54:35]: [Client #399] Model trained.
[INFO][09:54:35]: [Client #399] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:54:35]: [Server #1127936] Received 0.24 MB of payload data from client #399 (simulated).
[INFO][09:54:35]: [Server #1127936] Selecting client #265 for training.
[INFO][09:54:35]: [Server #1127936] Sending the current model to client #265 (simulated).
[INFO][09:54:35]: [Server #1127936] Sending 0.24 MB of payload data to client #265 (simulated).
[INFO][09:54:35]: [Client #265] Selected by the server.
[INFO][09:54:35]: [Client #265] Loading its data source...
[INFO][09:54:35]: [Client #265] Dataset size: 60000
[INFO][09:54:35]: [Client #265] Sampler: noniid
[INFO][09:54:35]: [Client #265] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:54:35]: [93m[1m[Client #265] Started training in communication round #21.[0m
[INFO][09:54:36]: [Client #265] Loading the dataset.
[INFO][09:54:42]: [Client #265] Epoch: [1/5][0/10]	Loss: 0.227264
[INFO][09:54:42]: [Client #265] Epoch: [2/5][0/10]	Loss: 0.002183
[INFO][09:54:42]: [Client #265] Epoch: [3/5][0/10]	Loss: 0.000034
[INFO][09:54:42]: [Client #265] Epoch: [4/5][0/10]	Loss: 0.000048
[INFO][09:54:42]: [Client #265] Epoch: [5/5][0/10]	Loss: 0.685270
[INFO][09:54:42]: [Client #265] Model saved to /data/ykang/plato/results/test/model/lenet5_265_1127977.pth.
[INFO][09:54:43]: [Client #265] Loading a model from /data/ykang/plato/results/test/model/lenet5_265_1127977.pth.
[INFO][09:54:43]: [Client #265] Model trained.
[INFO][09:54:43]: [Client #265] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:54:43]: [Server #1127936] Received 0.24 MB of payload data from client #265 (simulated).
[INFO][09:54:43]: [Server #1127936] Adding client #32 to the list of clients for aggregation.
[INFO][09:54:43]: [Server #1127936] Adding client #425 to the list of clients for aggregation.
[INFO][09:54:43]: [Server #1127936] Adding client #325 to the list of clients for aggregation.
[INFO][09:54:43]: [Server #1127936] Adding client #181 to the list of clients for aggregation.
[INFO][09:54:43]: [Server #1127936] Adding client #317 to the list of clients for aggregation.
[INFO][09:54:43]: [Server #1127936] Adding client #286 to the list of clients for aggregation.
[INFO][09:54:43]: [Server #1127936] Adding client #88 to the list of clients for aggregation.
[INFO][09:54:43]: [Server #1127936] Adding client #41 to the list of clients for aggregation.
[INFO][09:54:43]: [Server #1127936] Adding client #197 to the list of clients for aggregation.
[INFO][09:54:43]: [Server #1127936] Adding client #375 to the list of clients for aggregation.
[INFO][09:54:43]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 318, 319, 320, 321, 322, 323, 324, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01033655 0.         0.         0.         0.
 0.         0.         0.         0.         0.02855344 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.0582714  0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.02118635 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00542829 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.06879824 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.02263826 0.
 0.         0.         0.         0.         0.         0.
 0.01589752 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.03198439 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01765286 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 1. 3. 0. 0. 0. 0. 0. 0. 1. 6. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 2. 0. 6. 0. 1. 2. 1. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 2. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 2. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 4. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.
 0. 0. 1. 1. 0. 2. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 6. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01033655 0.         0.         0.         0.
 0.         0.         0.         0.         0.02855344 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.0582714  0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.02118635 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00542829 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.06879824 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.02263826 0.
 0.         0.         0.         0.         0.         0.
 0.01589752 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.03198439 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01765286 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][09:54:45]: [Server #1127936] Global model accuracy: 89.43%

[INFO][09:54:45]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_21.pth.
[INFO][09:54:45]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_21.pth.
[INFO][09:54:45]: [93m[1m
[Server #1127936] Starting round 22/100.[0m
[0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002
 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1
 0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8871e+00  5e-04  9e-09  9e-09
 5:  6.8876e+00  6.8874e+00  2e-04  3e-09  3e-09
 6:  6.8875e+00  6.8874e+00  2e-04  7e-09  1e-09
 7:  6.8875e+00  6.8874e+00  1e-04  7e-09  1e-09
 8:  6.8875e+00  6.8874e+00  9e-05  2e-08  3e-09
 9:  6.8874e+00  6.8874e+00  4e-05  3e-08  4e-09
10:  6.8874e+00  6.8874e+00  9e-06  2e-08  2e-09
11:  6.8874e+00  6.8874e+00  6e-07  2e-09  2e-10
Optimal solution found.
The calculated probability is:  [7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 8.04461191e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 4.45354528e-01 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 5.51000094e-01
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 8.78632666e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44414194e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 1.46276714e-05
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 8.89587027e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 8.40872685e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44360723e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 8.53046987e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06 7.44415780e-06 7.44415780e-06
 7.44415780e-06 7.44415780e-06]
current clients pool:  [INFO][09:54:45]: [Server #1127936] Selected clients: [ 41  88 279 446 179 125 457  22 476 281]
[INFO][09:54:45]: [Server #1127936] Selecting client #41 for training.
[INFO][09:54:45]: [Server #1127936] Sending the current model to client #41 (simulated).
[INFO][09:54:45]: [Server #1127936] Sending 0.24 MB of payload data to client #41 (simulated).
[INFO][09:54:45]: [Server #1127936] Selecting client #88 for training.
[INFO][09:54:45]: [Server #1127936] Sending the current model to client #88 (simulated).
[INFO][09:54:45]: [Server #1127936] Sending 0.24 MB of payload data to client #88 (simulated).
[INFO][09:54:45]: [Server #1127936] Selecting client #279 for training.
[INFO][09:54:45]: [Server #1127936] Sending the current model to client #279 (simulated).
[INFO][09:54:45]: [Client #41] Selected by the server.
[INFO][09:54:45]: [Client #41] Loading its data source...
[INFO][09:54:45]: [Client #41] Dataset size: 60000
[INFO][09:54:45]: [Client #41] Sampler: noniid
[INFO][09:54:45]: [Server #1127936] Sending 0.24 MB of payload data to client #279 (simulated).
[INFO][09:54:45]: [Client #88] Selected by the server.
[INFO][09:54:45]: [Client #88] Loading its data source...
[INFO][09:54:45]: [Client #88] Dataset size: 60000
[INFO][09:54:45]: [Client #88] Sampler: noniid
[INFO][09:54:45]: [Client #41] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:54:45]: [Client #88] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:54:45]: [93m[1m[Client #41] Started training in communication round #22.[0m
[INFO][09:54:45]: [93m[1m[Client #88] Started training in communication round #22.[0m
[INFO][09:54:45]: [Client #279] Selected by the server.
[INFO][09:54:45]: [Client #279] Loading its data source...
[INFO][09:54:45]: [Client #279] Dataset size: 60000
[INFO][09:54:45]: [Client #279] Sampler: noniid
[INFO][09:54:45]: [Client #279] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:54:45]: [93m[1m[Client #279] Started training in communication round #22.[0m
[INFO][09:54:47]: [Client #41] Loading the dataset.
[INFO][09:54:47]: [Client #279] Loading the dataset.
[INFO][09:54:48]: [Client #88] Loading the dataset.
[INFO][09:54:53]: [Client #41] Epoch: [1/5][0/10]	Loss: 0.944814
[INFO][09:54:53]: [Client #88] Epoch: [1/5][0/10]	Loss: 0.114758
[INFO][09:54:53]: [Client #279] Epoch: [1/5][0/10]	Loss: 0.298657
[INFO][09:54:53]: [Client #41] Epoch: [2/5][0/10]	Loss: 0.069986
[INFO][09:54:54]: [Client #88] Epoch: [2/5][0/10]	Loss: 0.031628
[INFO][09:54:54]: [Client #279] Epoch: [2/5][0/10]	Loss: 0.036358
[INFO][09:54:54]: [Client #41] Epoch: [3/5][0/10]	Loss: 0.005002
[INFO][09:54:54]: [Client #279] Epoch: [3/5][0/10]	Loss: 0.000426
[INFO][09:54:54]: [Client #88] Epoch: [3/5][0/10]	Loss: 0.000970
[INFO][09:54:54]: [Client #41] Epoch: [4/5][0/10]	Loss: 0.293531
[INFO][09:54:54]: [Client #279] Epoch: [4/5][0/10]	Loss: 0.000694
[INFO][09:54:54]: [Client #88] Epoch: [4/5][0/10]	Loss: 0.334073
[INFO][09:54:54]: [Client #41] Epoch: [5/5][0/10]	Loss: 0.044485
[INFO][09:54:54]: [Client #279] Epoch: [5/5][0/10]	Loss: 0.147782
[INFO][09:54:54]: [Client #279] Model saved to /data/ykang/plato/results/test/model/lenet5_279_1127979.pth.
[INFO][09:54:54]: [Client #88] Epoch: [5/5][0/10]	Loss: 0.371868
[INFO][09:54:54]: [Client #41] Model saved to /data/ykang/plato/results/test/model/lenet5_41_1127977.pth.
[INFO][09:54:54]: [Client #88] Model saved to /data/ykang/plato/results/test/model/lenet5_88_1127978.pth.
[INFO][09:54:55]: [Client #279] Loading a model from /data/ykang/plato/results/test/model/lenet5_279_1127979.pth.
[INFO][09:54:55]: [Client #279] Model trained.
[INFO][09:54:55]: [Client #279] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:54:55]: [Server #1127936] Received 0.24 MB of payload data from client #279 (simulated).
[INFO][09:54:55]: [Client #41] Loading a model from /data/ykang/plato/results/test/model/lenet5_41_1127977.pth.
[INFO][09:54:55]: [Client #41] Model trained.
[INFO][09:54:55]: [Client #41] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:54:55]: [Server #1127936] Received 0.24 MB of payload data from client #41 (simulated).
[INFO][09:54:55]: [Client #88] Loading a model from /data/ykang/plato/results/test/model/lenet5_88_1127978.pth.
[INFO][09:54:55]: [Client #88] Model trained.
[INFO][09:54:55]: [Client #88] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:54:55]: [Server #1127936] Received 0.24 MB of payload data from client #88 (simulated).
[INFO][09:54:55]: [Server #1127936] Selecting client #446 for training.
[INFO][09:54:55]: [Server #1127936] Sending the current model to client #446 (simulated).
[INFO][09:54:55]: [Server #1127936] Sending 0.24 MB of payload data to client #446 (simulated).
[INFO][09:54:55]: [Server #1127936] Selecting client #179 for training.
[INFO][09:54:55]: [Server #1127936] Sending the current model to client #179 (simulated).
[INFO][09:54:55]: [Server #1127936] Sending 0.24 MB of payload data to client #179 (simulated).
[INFO][09:54:55]: [Server #1127936] Selecting client #125 for training.
[INFO][09:54:55]: [Server #1127936] Sending the current model to client #125 (simulated).
[INFO][09:54:55]: [Client #446] Selected by the server.
[INFO][09:54:55]: [Client #446] Loading its data source...
[INFO][09:54:55]: [Client #446] Dataset size: 60000
[INFO][09:54:55]: [Client #446] Sampler: noniid
[INFO][09:54:55]: [Server #1127936] Sending 0.24 MB of payload data to client #125 (simulated).
[INFO][09:54:55]: [Client #446] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:54:55]: [Client #179] Selected by the server.
[INFO][09:54:55]: [Client #179] Loading its data source...
[INFO][09:54:55]: [Client #179] Dataset size: 60000
[INFO][09:54:55]: [Client #179] Sampler: noniid
[INFO][09:54:55]: [Client #125] Selected by the server.
[INFO][09:54:55]: [Client #125] Loading its data source...
[INFO][09:54:55]: [Client #125] Dataset size: 60000
[INFO][09:54:55]: [Client #125] Sampler: noniid
[INFO][09:54:55]: [Client #179] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:54:55]: [Client #125] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:54:55]: [93m[1m[Client #446] Started training in communication round #22.[0m
[INFO][09:54:55]: [93m[1m[Client #125] Started training in communication round #22.[0m
[INFO][09:54:55]: [93m[1m[Client #179] Started training in communication round #22.[0m
[INFO][09:54:57]: [Client #125] Loading the dataset.
[INFO][09:54:57]: [Client #446] Loading the dataset.
[INFO][09:54:57]: [Client #179] Loading the dataset.
[INFO][09:55:03]: [Client #125] Epoch: [1/5][0/10]	Loss: 0.068905
[INFO][09:55:03]: [Client #179] Epoch: [1/5][0/10]	Loss: 0.223536
[INFO][09:55:03]: [Client #125] Epoch: [2/5][0/10]	Loss: 0.010314
[INFO][09:55:03]: [Client #446] Epoch: [1/5][0/10]	Loss: 0.254948
[INFO][09:55:03]: [Client #179] Epoch: [2/5][0/10]	Loss: 0.014819
[INFO][09:55:03]: [Client #446] Epoch: [2/5][0/10]	Loss: 0.129071
[INFO][09:55:03]: [Client #125] Epoch: [3/5][0/10]	Loss: 0.001759
[INFO][09:55:03]: [Client #179] Epoch: [3/5][0/10]	Loss: 0.068756
[INFO][09:55:03]: [Client #446] Epoch: [3/5][0/10]	Loss: 0.080231
[INFO][09:55:03]: [Client #125] Epoch: [4/5][0/10]	Loss: 0.007443
[INFO][09:55:03]: [Client #446] Epoch: [4/5][0/10]	Loss: 0.030875
[INFO][09:55:03]: [Client #179] Epoch: [4/5][0/10]	Loss: 0.045393
[INFO][09:55:03]: [Client #125] Epoch: [5/5][0/10]	Loss: 0.131653
[INFO][09:55:03]: [Client #125] Model saved to /data/ykang/plato/results/test/model/lenet5_125_1127979.pth.
[INFO][09:55:03]: [Client #446] Epoch: [5/5][0/10]	Loss: 0.292641
[INFO][09:55:03]: [Client #179] Epoch: [5/5][0/10]	Loss: 0.106460
[INFO][09:55:03]: [Client #446] Model saved to /data/ykang/plato/results/test/model/lenet5_446_1127977.pth.
[INFO][09:55:03]: [Client #179] Model saved to /data/ykang/plato/results/test/model/lenet5_179_1127978.pth.
[INFO][09:55:04]: [Client #125] Loading a model from /data/ykang/plato/results/test/model/lenet5_125_1127979.pth.
[INFO][09:55:04]: [Client #125] Model trained.
[INFO][09:55:04]: [Client #125] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:55:04]: [Server #1127936] Received 0.24 MB of payload data from client #125 (simulated).
[INFO][09:55:04]: [Client #446] Loading a model from /data/ykang/plato/results/test/model/lenet5_446_1127977.pth.
[INFO][09:55:04]: [Client #446] Model trained.
[INFO][09:55:04]: [Client #446] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:55:04]: [Server #1127936] Received 0.24 MB of payload data from client #446 (simulated).
[INFO][09:55:04]: [Client #179] Loading a model from /data/ykang/plato/results/test/model/lenet5_179_1127978.pth.
[INFO][09:55:04]: [Client #179] Model trained.
[INFO][09:55:04]: [Client #179] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:55:04]: [Server #1127936] Received 0.24 MB of payload data from client #179 (simulated).
[INFO][09:55:04]: [Server #1127936] Selecting client #457 for training.
[INFO][09:55:04]: [Server #1127936] Sending the current model to client #457 (simulated).
[INFO][09:55:04]: [Server #1127936] Sending 0.24 MB of payload data to client #457 (simulated).
[INFO][09:55:04]: [Server #1127936] Selecting client #22 for training.
[INFO][09:55:04]: [Server #1127936] Sending the current model to client #22 (simulated).
[INFO][09:55:04]: [Server #1127936] Sending 0.24 MB of payload data to client #22 (simulated).
[INFO][09:55:04]: [Server #1127936] Selecting client #476 for training.
[INFO][09:55:04]: [Server #1127936] Sending the current model to client #476 (simulated).
[INFO][09:55:04]: [Client #457] Selected by the server.
[INFO][09:55:04]: [Client #457] Loading its data source...
[INFO][09:55:04]: [Client #457] Dataset size: 60000
[INFO][09:55:04]: [Client #457] Sampler: noniid
[INFO][09:55:04]: [Server #1127936] Sending 0.24 MB of payload data to client #476 (simulated).
[INFO][09:55:04]: [Client #22] Selected by the server.
[INFO][09:55:04]: [Client #22] Loading its data source...
[INFO][09:55:04]: [Client #22] Dataset size: 60000
[INFO][09:55:04]: [Client #22] Sampler: noniid
[INFO][09:55:04]: [Client #476] Selected by the server.
[INFO][09:55:04]: [Client #476] Loading its data source...
[INFO][09:55:04]: [Client #476] Dataset size: 60000
[INFO][09:55:04]: [Client #476] Sampler: noniid
[INFO][09:55:04]: [Client #457] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:55:04]: [Client #22] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:55:04]: [Client #476] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:55:04]: [93m[1m[Client #22] Started training in communication round #22.[0m
[INFO][09:55:04]: [93m[1m[Client #457] Started training in communication round #22.[0m
[INFO][09:55:04]: [93m[1m[Client #476] Started training in communication round #22.[0m
[INFO][09:55:06]: [Client #457] Loading the dataset.
[INFO][09:55:06]: [Client #476] Loading the dataset.
[INFO][09:55:06]: [Client #22] Loading the dataset.
[INFO][09:55:12]: [Client #22] Epoch: [1/5][0/10]	Loss: 0.256634
[INFO][09:55:12]: [Client #457] Epoch: [1/5][0/10]	Loss: 0.947703
[INFO][09:55:12]: [Client #476] Epoch: [1/5][0/10]	Loss: 0.947703
[INFO][09:55:13]: [Client #457] Epoch: [2/5][0/10]	Loss: 0.000114
[INFO][09:55:13]: [Client #22] Epoch: [2/5][0/10]	Loss: 0.008501
[INFO][09:55:13]: [Client #476] Epoch: [2/5][0/10]	Loss: 0.000015
[INFO][09:55:13]: [Client #457] Epoch: [3/5][0/10]	Loss: 0.000130
[INFO][09:55:13]: [Client #476] Epoch: [3/5][0/10]	Loss: 0.000009
[INFO][09:55:13]: [Client #22] Epoch: [3/5][0/10]	Loss: 0.000370
[INFO][09:55:13]: [Client #457] Epoch: [4/5][0/10]	Loss: 0.000282
[INFO][09:55:13]: [Client #476] Epoch: [4/5][0/10]	Loss: 0.001306
[INFO][09:55:13]: [Client #22] Epoch: [4/5][0/10]	Loss: 0.001008
[INFO][09:55:13]: [Client #457] Epoch: [5/5][0/10]	Loss: 0.141160
[INFO][09:55:13]: [Client #476] Epoch: [5/5][0/10]	Loss: 0.005652
[INFO][09:55:13]: [Client #457] Model saved to /data/ykang/plato/results/test/model/lenet5_457_1127977.pth.
[INFO][09:55:13]: [Client #476] Model saved to /data/ykang/plato/results/test/model/lenet5_476_1127979.pth.
[INFO][09:55:13]: [Client #22] Epoch: [5/5][0/10]	Loss: 0.366714
[INFO][09:55:13]: [Client #22] Model saved to /data/ykang/plato/results/test/model/lenet5_22_1127978.pth.
[INFO][09:55:14]: [Client #457] Loading a model from /data/ykang/plato/results/test/model/lenet5_457_1127977.pth.
[INFO][09:55:14]: [Client #457] Model trained.
[INFO][09:55:14]: [Client #457] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:55:14]: [Server #1127936] Received 0.24 MB of payload data from client #457 (simulated).
[INFO][09:55:14]: [Client #476] Loading a model from /data/ykang/plato/results/test/model/lenet5_476_1127979.pth.
[INFO][09:55:14]: [Client #476] Model trained.
[INFO][09:55:14]: [Client #476] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:55:14]: [Server #1127936] Received 0.24 MB of payload data from client #476 (simulated).
[INFO][09:55:14]: [Client #22] Loading a model from /data/ykang/plato/results/test/model/lenet5_22_1127978.pth.
[INFO][09:55:14]: [Client #22] Model trained.
[INFO][09:55:14]: [Client #22] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:55:14]: [Server #1127936] Received 0.24 MB of payload data from client #22 (simulated).
[INFO][09:55:14]: [Server #1127936] Selecting client #281 for training.
[INFO][09:55:14]: [Server #1127936] Sending the current model to client #281 (simulated).
[INFO][09:55:14]: [Server #1127936] Sending 0.24 MB of payload data to client #281 (simulated).
[INFO][09:55:14]: [Client #281] Selected by the server.
[INFO][09:55:14]: [Client #281] Loading its data source...
[INFO][09:55:14]: [Client #281] Dataset size: 60000
[INFO][09:55:14]: [Client #281] Sampler: noniid
[INFO][09:55:14]: [Client #281] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:55:14]: [93m[1m[Client #281] Started training in communication round #22.[0m
[INFO][09:55:16]: [Client #281] Loading the dataset.
[INFO][09:55:21]: [Client #281] Epoch: [1/5][0/10]	Loss: 0.947703
[INFO][09:55:21]: [Client #281] Epoch: [2/5][0/10]	Loss: 0.000032
[INFO][09:55:21]: [Client #281] Epoch: [3/5][0/10]	Loss: 0.103513
[INFO][09:55:21]: [Client #281] Epoch: [4/5][0/10]	Loss: 0.486988
[INFO][09:55:21]: [Client #281] Epoch: [5/5][0/10]	Loss: 0.002294
[INFO][09:55:21]: [Client #281] Model saved to /data/ykang/plato/results/test/model/lenet5_281_1127977.pth.
[INFO][09:55:22]: [Client #281] Loading a model from /data/ykang/plato/results/test/model/lenet5_281_1127977.pth.
[INFO][09:55:22]: [Client #281] Model trained.
[INFO][09:55:22]: [Client #281] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:55:22]: [Server #1127936] Received 0.24 MB of payload data from client #281 (simulated).
[INFO][09:55:22]: [Server #1127936] Adding client #361 to the list of clients for aggregation.
[INFO][09:55:22]: [Server #1127936] Adding client #399 to the list of clients for aggregation.
[INFO][09:55:22]: [Server #1127936] Adding client #98 to the list of clients for aggregation.
[INFO][09:55:22]: [Server #1127936] Adding client #176 to the list of clients for aggregation.
[INFO][09:55:22]: [Server #1127936] Adding client #335 to the list of clients for aggregation.
[INFO][09:55:22]: [Server #1127936] Adding client #265 to the list of clients for aggregation.
[INFO][09:55:22]: [Server #1127936] Adding client #476 to the list of clients for aggregation.
[INFO][09:55:22]: [Server #1127936] Adding client #179 to the list of clients for aggregation.
[INFO][09:55:22]: [Server #1127936] Adding client #279 to the list of clients for aggregation.
[INFO][09:55:22]: [Server #1127936] Adding client #190 to the list of clients for aggregation.
[INFO][09:55:22]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.03566697 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.02441646 0.         0.         0.01367124 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01048202 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.0243167  0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01254972 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01323792 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.02231137 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01661446 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.03476306 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 1. 3. 0. 0. 0. 0. 0. 0. 1. 6. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 2. 0. 6. 0. 1. 2. 1. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 2. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 4. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 1. 1. 0. 2. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 0. 1. 0. 0. 0. 6. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.03566697 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.02441646 0.         0.         0.01367124 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01048202 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.0243167  0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01254972 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01323792 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.02231137 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01661446 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.03476306 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][09:55:24]: [Server #1127936] Global model accuracy: 93.08%

[INFO][09:55:24]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_22.pth.
[INFO][09:55:24]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_22.pth.
[INFO][09:55:24]: [93m[1m
[Server #1127936] Starting round 23/100.[0m
[0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002
 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  4e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  4e-05  7e-10  7e-10
 6:  6.8876e+00  6.8875e+00  4e-05  4e-10  4e-10
 7:  6.8875e+00  6.8875e+00  3e-05  9e-10  4e-11
 8:  6.8875e+00  6.8875e+00  2e-05  9e-10  3e-11
 9:  6.8875e+00  6.8875e+00  1e-05  3e-09  1e-10
10:  6.8875e+00  6.8875e+00  1e-06  4e-09  1e-10
Optimal solution found.
The calculated probability is:  [6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 9.68280941e-01 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 2.95611220e-04 6.32576468e-05
 6.32576468e-05 6.32536869e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 9.70325134e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 2.91686258e-04 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32543099e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 1.12571305e-04
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 2.29164174e-04 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 1.39583274e-04 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32320513e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05 6.32576468e-05 6.32576468e-05
 6.32576468e-05 6.32576468e-05]
current clients pool:  [INFO][09:55:25]: [Server #1127936] Selected clients: [ 98 420 202 229 355 388 250 265  78 187]
[INFO][09:55:25]: [Server #1127936] Selecting client #98 for training.
[INFO][09:55:25]: [Server #1127936] Sending the current model to client #98 (simulated).
[INFO][09:55:25]: [Server #1127936] Sending 0.24 MB of payload data to client #98 (simulated).
[INFO][09:55:25]: [Server #1127936] Selecting client #420 for training.
[INFO][09:55:25]: [Server #1127936] Sending the current model to client #420 (simulated).
[INFO][09:55:25]: [Server #1127936] Sending 0.24 MB of payload data to client #420 (simulated).
[INFO][09:55:25]: [Server #1127936] Selecting client #202 for training.
[INFO][09:55:25]: [Server #1127936] Sending the current model to client #202 (simulated).
[INFO][09:55:25]: [Client #98] Selected by the server.
[INFO][09:55:25]: [Client #98] Loading its data source...
[INFO][09:55:25]: [Client #98] Dataset size: 60000
[INFO][09:55:25]: [Client #98] Sampler: noniid
[INFO][09:55:25]: [Server #1127936] Sending 0.24 MB of payload data to client #202 (simulated).
[INFO][09:55:25]: [Client #420] Selected by the server.
[INFO][09:55:25]: [Client #420] Loading its data source...
[INFO][09:55:25]: [Client #420] Dataset size: 60000
[INFO][09:55:25]: [Client #420] Sampler: noniid
[INFO][09:55:25]: [Client #98] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:55:25]: [Client #420] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:55:25]: [Client #202] Selected by the server.
[INFO][09:55:25]: [Client #202] Loading its data source...
[INFO][09:55:25]: [Client #202] Dataset size: 60000
[INFO][09:55:25]: [Client #202] Sampler: noniid
[INFO][09:55:25]: [Client #202] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:55:25]: [93m[1m[Client #98] Started training in communication round #23.[0m
[INFO][09:55:25]: [93m[1m[Client #420] Started training in communication round #23.[0m
[INFO][09:55:25]: [93m[1m[Client #202] Started training in communication round #23.[0m
[INFO][09:55:27]: [Client #98] Loading the dataset.
[INFO][09:55:27]: [Client #202] Loading the dataset.
[INFO][09:55:27]: [Client #420] Loading the dataset.
[INFO][09:55:33]: [Client #202] Epoch: [1/5][0/10]	Loss: 0.011363
[INFO][09:55:33]: [Client #420] Epoch: [1/5][0/10]	Loss: 0.156288
[INFO][09:55:33]: [Client #98] Epoch: [1/5][0/10]	Loss: 0.204952
[INFO][09:55:33]: [Client #420] Epoch: [2/5][0/10]	Loss: 0.011028
[INFO][09:55:33]: [Client #98] Epoch: [2/5][0/10]	Loss: 0.271400
[INFO][09:55:33]: [Client #202] Epoch: [2/5][0/10]	Loss: 0.000907
[INFO][09:55:33]: [Client #420] Epoch: [3/5][0/10]	Loss: 0.003877
[INFO][09:55:33]: [Client #98] Epoch: [3/5][0/10]	Loss: 0.000038
[INFO][09:55:33]: [Client #202] Epoch: [3/5][0/10]	Loss: 0.000011
[INFO][09:55:33]: [Client #420] Epoch: [4/5][0/10]	Loss: 0.002676
[INFO][09:55:33]: [Client #98] Epoch: [4/5][0/10]	Loss: 0.000762
[INFO][09:55:33]: [Client #202] Epoch: [4/5][0/10]	Loss: 0.078261
[INFO][09:55:33]: [Client #98] Epoch: [5/5][0/10]	Loss: 0.034001
[INFO][09:55:33]: [Client #420] Epoch: [5/5][0/10]	Loss: 0.005158
[INFO][09:55:33]: [Client #202] Epoch: [5/5][0/10]	Loss: 0.001535
[INFO][09:55:33]: [Client #98] Model saved to /data/ykang/plato/results/test/model/lenet5_98_1127977.pth.
[INFO][09:55:33]: [Client #420] Model saved to /data/ykang/plato/results/test/model/lenet5_420_1127978.pth.
[INFO][09:55:33]: [Client #202] Model saved to /data/ykang/plato/results/test/model/lenet5_202_1127979.pth.
[INFO][09:55:34]: [Client #420] Loading a model from /data/ykang/plato/results/test/model/lenet5_420_1127978.pth.
[INFO][09:55:34]: [Client #420] Model trained.
[INFO][09:55:34]: [Client #420] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:55:34]: [Server #1127936] Received 0.24 MB of payload data from client #420 (simulated).
[INFO][09:55:34]: [Client #98] Loading a model from /data/ykang/plato/results/test/model/lenet5_98_1127977.pth.
[INFO][09:55:34]: [Client #98] Model trained.
[INFO][09:55:34]: [Client #98] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:55:34]: [Server #1127936] Received 0.24 MB of payload data from client #98 (simulated).
[INFO][09:55:34]: [Client #202] Loading a model from /data/ykang/plato/results/test/model/lenet5_202_1127979.pth.
[INFO][09:55:34]: [Client #202] Model trained.
[INFO][09:55:34]: [Client #202] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:55:34]: [Server #1127936] Received 0.24 MB of payload data from client #202 (simulated).
[INFO][09:55:34]: [Server #1127936] Selecting client #229 for training.
[INFO][09:55:34]: [Server #1127936] Sending the current model to client #229 (simulated).
[INFO][09:55:34]: [Server #1127936] Sending 0.24 MB of payload data to client #229 (simulated).
[INFO][09:55:34]: [Server #1127936] Selecting client #355 for training.
[INFO][09:55:34]: [Server #1127936] Sending the current model to client #355 (simulated).
[INFO][09:55:34]: [Server #1127936] Sending 0.24 MB of payload data to client #355 (simulated).
[INFO][09:55:34]: [Server #1127936] Selecting client #388 for training.
[INFO][09:55:34]: [Server #1127936] Sending the current model to client #388 (simulated).
[INFO][09:55:34]: [Client #229] Selected by the server.
[INFO][09:55:34]: [Client #229] Loading its data source...
[INFO][09:55:34]: [Client #229] Dataset size: 60000
[INFO][09:55:34]: [Client #229] Sampler: noniid
[INFO][09:55:34]: [Server #1127936] Sending 0.24 MB of payload data to client #388 (simulated).
[INFO][09:55:34]: [Client #355] Selected by the server.
[INFO][09:55:34]: [Client #388] Selected by the server.
[INFO][09:55:34]: [Client #355] Loading its data source...
[INFO][09:55:34]: [Client #388] Loading its data source...
[INFO][09:55:34]: [Client #355] Dataset size: 60000
[INFO][09:55:34]: [Client #388] Dataset size: 60000
[INFO][09:55:34]: [Client #355] Sampler: noniid
[INFO][09:55:34]: [Client #388] Sampler: noniid
[INFO][09:55:34]: [Client #229] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:55:34]: [Client #388] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:55:34]: [Client #355] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:55:34]: [93m[1m[Client #355] Started training in communication round #23.[0m
[INFO][09:55:34]: [93m[1m[Client #388] Started training in communication round #23.[0m
[INFO][09:55:34]: [93m[1m[Client #229] Started training in communication round #23.[0m
[INFO][09:55:36]: [Client #355] Loading the dataset.
[INFO][09:55:36]: [Client #388] Loading the dataset.
[INFO][09:55:36]: [Client #229] Loading the dataset.
[INFO][09:55:42]: [Client #388] Epoch: [1/5][0/10]	Loss: 0.092584
[INFO][09:55:42]: [Client #229] Epoch: [1/5][0/10]	Loss: 0.043125
[INFO][09:55:42]: [Client #355] Epoch: [1/5][0/10]	Loss: 0.011363
[INFO][09:55:42]: [Client #229] Epoch: [2/5][0/10]	Loss: 0.221270
[INFO][09:55:42]: [Client #388] Epoch: [2/5][0/10]	Loss: 0.319340
[INFO][09:55:42]: [Client #355] Epoch: [2/5][0/10]	Loss: 0.001260
[INFO][09:55:42]: [Client #229] Epoch: [3/5][0/10]	Loss: 0.002958
[INFO][09:55:42]: [Client #388] Epoch: [3/5][0/10]	Loss: 0.002832
[INFO][09:55:42]: [Client #355] Epoch: [3/5][0/10]	Loss: 0.004765
[INFO][09:55:42]: [Client #229] Epoch: [4/5][0/10]	Loss: 0.001356
[INFO][09:55:43]: [Client #388] Epoch: [4/5][0/10]	Loss: 0.000024
[INFO][09:55:43]: [Client #229] Epoch: [5/5][0/10]	Loss: 0.024176
[INFO][09:55:43]: [Client #355] Epoch: [4/5][0/10]	Loss: 0.267971
[INFO][09:55:43]: [Client #388] Epoch: [5/5][0/10]	Loss: 0.142494
[INFO][09:55:43]: [Client #229] Model saved to /data/ykang/plato/results/test/model/lenet5_229_1127977.pth.
[INFO][09:55:43]: [Client #388] Model saved to /data/ykang/plato/results/test/model/lenet5_388_1127979.pth.
[INFO][09:55:43]: [Client #355] Epoch: [5/5][0/10]	Loss: 0.000323
[INFO][09:55:43]: [Client #355] Model saved to /data/ykang/plato/results/test/model/lenet5_355_1127978.pth.
[INFO][09:55:44]: [Client #229] Loading a model from /data/ykang/plato/results/test/model/lenet5_229_1127977.pth.
[INFO][09:55:44]: [Client #229] Model trained.
[INFO][09:55:44]: [Client #229] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:55:44]: [Server #1127936] Received 0.24 MB of payload data from client #229 (simulated).
[INFO][09:55:44]: [Client #388] Loading a model from /data/ykang/plato/results/test/model/lenet5_388_1127979.pth.
[INFO][09:55:44]: [Client #388] Model trained.
[INFO][09:55:44]: [Client #388] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:55:44]: [Server #1127936] Received 0.24 MB of payload data from client #388 (simulated).
[INFO][09:55:44]: [Client #355] Loading a model from /data/ykang/plato/results/test/model/lenet5_355_1127978.pth.
[INFO][09:55:44]: [Client #355] Model trained.
[INFO][09:55:44]: [Client #355] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:55:44]: [Server #1127936] Received 0.24 MB of payload data from client #355 (simulated).
[INFO][09:55:44]: [Server #1127936] Selecting client #250 for training.
[INFO][09:55:44]: [Server #1127936] Sending the current model to client #250 (simulated).
[INFO][09:55:44]: [Server #1127936] Sending 0.24 MB of payload data to client #250 (simulated).
[INFO][09:55:44]: [Server #1127936] Selecting client #265 for training.
[INFO][09:55:44]: [Server #1127936] Sending the current model to client #265 (simulated).
[INFO][09:55:44]: [Server #1127936] Sending 0.24 MB of payload data to client #265 (simulated).
[INFO][09:55:44]: [Server #1127936] Selecting client #78 for training.
[INFO][09:55:44]: [Server #1127936] Sending the current model to client #78 (simulated).
[INFO][09:55:44]: [Client #250] Selected by the server.
[INFO][09:55:44]: [Client #250] Loading its data source...
[INFO][09:55:44]: [Client #250] Dataset size: 60000
[INFO][09:55:44]: [Client #250] Sampler: noniid
[INFO][09:55:44]: [Server #1127936] Sending 0.24 MB of payload data to client #78 (simulated).
[INFO][09:55:44]: [Client #265] Selected by the server.
[INFO][09:55:44]: [Client #265] Loading its data source...
[INFO][09:55:44]: [Client #265] Dataset size: 60000
[INFO][09:55:44]: [Client #265] Sampler: noniid
[INFO][09:55:44]: [Client #78] Selected by the server.
[INFO][09:55:44]: [Client #78] Loading its data source...
[INFO][09:55:44]: [Client #78] Dataset size: 60000
[INFO][09:55:44]: [Client #78] Sampler: noniid
[INFO][09:55:44]: [Client #250] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:55:44]: [Client #78] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:55:44]: [Client #265] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:55:44]: [93m[1m[Client #78] Started training in communication round #23.[0m
[INFO][09:55:44]: [93m[1m[Client #250] Started training in communication round #23.[0m
[INFO][09:55:44]: [93m[1m[Client #265] Started training in communication round #23.[0m
[INFO][09:55:46]: [Client #265] Loading the dataset.
[INFO][09:55:46]: [Client #250] Loading the dataset.
[INFO][09:55:46]: [Client #78] Loading the dataset.
[INFO][09:55:54]: [Client #265] Epoch: [1/5][0/10]	Loss: 0.122477
[INFO][09:55:54]: [Client #78] Epoch: [1/5][0/10]	Loss: 0.032540
[INFO][09:55:54]: [Client #78] Epoch: [2/5][0/10]	Loss: 0.033933
[INFO][09:55:54]: [Client #265] Epoch: [2/5][0/10]	Loss: 0.005406
[INFO][09:55:54]: [Client #250] Epoch: [1/5][0/10]	Loss: 0.109280
[INFO][09:55:54]: [Client #78] Epoch: [3/5][0/10]	Loss: 0.000145
[INFO][09:55:54]: [Client #265] Epoch: [3/5][0/10]	Loss: 0.000036
[INFO][09:55:54]: [Client #250] Epoch: [2/5][0/10]	Loss: 0.001914
[INFO][09:55:54]: [Client #78] Epoch: [4/5][0/10]	Loss: 0.002974
[INFO][09:55:54]: [Client #78] Epoch: [5/5][0/10]	Loss: 0.000176
[INFO][09:55:54]: [Client #265] Epoch: [4/5][0/10]	Loss: 0.000073
[INFO][09:55:54]: [Client #250] Epoch: [3/5][0/10]	Loss: 0.021189
[INFO][09:55:54]: [Client #78] Model saved to /data/ykang/plato/results/test/model/lenet5_78_1127979.pth.
[INFO][09:55:54]: [Client #265] Epoch: [5/5][0/10]	Loss: 0.000347
[INFO][09:55:54]: [Client #265] Model saved to /data/ykang/plato/results/test/model/lenet5_265_1127978.pth.
[INFO][09:55:54]: [Client #250] Epoch: [4/5][0/10]	Loss: 0.001563
[INFO][09:55:54]: [Client #250] Epoch: [5/5][0/10]	Loss: 0.000004
[INFO][09:55:54]: [Client #250] Model saved to /data/ykang/plato/results/test/model/lenet5_250_1127977.pth.
[INFO][09:55:55]: [Client #78] Loading a model from /data/ykang/plato/results/test/model/lenet5_78_1127979.pth.
[INFO][09:55:55]: [Client #78] Model trained.
[INFO][09:55:55]: [Client #78] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:55:55]: [Server #1127936] Received 0.24 MB of payload data from client #78 (simulated).
[INFO][09:55:55]: [Client #265] Loading a model from /data/ykang/plato/results/test/model/lenet5_265_1127978.pth.
[INFO][09:55:55]: [Client #265] Model trained.
[INFO][09:55:55]: [Client #265] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:55:55]: [Server #1127936] Received 0.24 MB of payload data from client #265 (simulated).
[INFO][09:55:55]: [Client #250] Loading a model from /data/ykang/plato/results/test/model/lenet5_250_1127977.pth.
[INFO][09:55:55]: [Client #250] Model trained.
[INFO][09:55:55]: [Client #250] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:55:55]: [Server #1127936] Received 0.24 MB of payload data from client #250 (simulated).
[INFO][09:55:55]: [Server #1127936] Selecting client #187 for training.
[INFO][09:55:55]: [Server #1127936] Sending the current model to client #187 (simulated).
[INFO][09:55:55]: [Server #1127936] Sending 0.24 MB of payload data to client #187 (simulated).
[INFO][09:55:55]: [Client #187] Selected by the server.
[INFO][09:55:55]: [Client #187] Loading its data source...
[INFO][09:55:55]: [Client #187] Dataset size: 60000
[INFO][09:55:55]: [Client #187] Sampler: noniid
[INFO][09:55:55]: [Client #187] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:55:55]: [93m[1m[Client #187] Started training in communication round #23.[0m
[INFO][09:55:57]: [Client #187] Loading the dataset.
[INFO][09:56:03]: [Client #187] Epoch: [1/5][0/10]	Loss: 0.064535
[INFO][09:56:03]: [Client #187] Epoch: [2/5][0/10]	Loss: 0.007502
[INFO][09:56:03]: [Client #187] Epoch: [3/5][0/10]	Loss: 0.000033
[INFO][09:56:03]: [Client #187] Epoch: [4/5][0/10]	Loss: 0.000014
[INFO][09:56:03]: [Client #187] Epoch: [5/5][0/10]	Loss: 0.000027
[INFO][09:56:03]: [Client #187] Model saved to /data/ykang/plato/results/test/model/lenet5_187_1127977.pth.
[INFO][09:56:04]: [Client #187] Loading a model from /data/ykang/plato/results/test/model/lenet5_187_1127977.pth.
[INFO][09:56:04]: [Client #187] Model trained.
[INFO][09:56:04]: [Client #187] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:56:04]: [Server #1127936] Received 0.24 MB of payload data from client #187 (simulated).
[INFO][09:56:04]: [Server #1127936] Adding client #125 to the list of clients for aggregation.
[INFO][09:56:04]: [Server #1127936] Adding client #281 to the list of clients for aggregation.
[INFO][09:56:04]: [Server #1127936] Adding client #367 to the list of clients for aggregation.
[INFO][09:56:04]: [Server #1127936] Adding client #110 to the list of clients for aggregation.
[INFO][09:56:04]: [Server #1127936] Adding client #446 to the list of clients for aggregation.
[INFO][09:56:04]: [Server #1127936] Adding client #457 to the list of clients for aggregation.
[INFO][09:56:04]: [Server #1127936] Adding client #187 to the list of clients for aggregation.
[INFO][09:56:04]: [Server #1127936] Adding client #78 to the list of clients for aggregation.
[INFO][09:56:04]: [Server #1127936] Adding client #388 to the list of clients for aggregation.
[INFO][09:56:04]: [Server #1127936] Adding client #202 to the list of clients for aggregation.
[INFO][09:56:04]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01589333
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.03132203 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01309036 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.02215045 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00830308 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.04440221 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.03124571 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01873808 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.02331186 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.03377203 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 1. 3. 0. 0. 0. 0. 0. 0. 1. 6. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 2. 0. 6. 0. 1. 2. 1. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 1. 1. 0. 2. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 0. 1. 0. 0. 0. 6. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01589333
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.03132203 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01309036 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.02215045 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00830308 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.04440221 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.03124571 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01873808 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.02331186 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.03377203 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][09:56:06]: [Server #1127936] Global model accuracy: 88.27%

[INFO][09:56:06]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_23.pth.
[INFO][09:56:06]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_23.pth.
[INFO][09:56:06]: [93m[1m
[Server #1127936] Starting round 24/100.[0m
[0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002
 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.002
 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002
 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8870e+00  5e-04  9e-09  9e-09
 5:  6.8876e+00  6.8874e+00  2e-04  3e-09  3e-09
 6:  6.8875e+00  6.8873e+00  2e-04  1e-08  2e-09
 7:  6.8875e+00  6.8874e+00  1e-04  1e-08  2e-09
 8:  6.8874e+00  6.8874e+00  5e-05  6e-08  9e-09
 9:  6.8874e+00  6.8874e+00  2e-05  3e-08  5e-09
10:  6.8874e+00  6.8874e+00  2e-06  5e-09  8e-10
Optimal solution found.
The calculated probability is:  [1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86148763e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 5.01342157e-01 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 2.04299768e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86145751e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151088e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 2.65793234e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 4.89554849e-01 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86147517e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 2.21066966e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 2.41250202e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05 1.86151960e-05 1.86151960e-05
 1.86151960e-05 1.86151960e-05]
current clients pool:  [INFO][09:56:06]: [Server #1127936] Selected clients: [110 367  13 148 255 484  56 338 253 391]
[INFO][09:56:06]: [Server #1127936] Selecting client #110 for training.
[INFO][09:56:06]: [Server #1127936] Sending the current model to client #110 (simulated).
[INFO][09:56:06]: [Server #1127936] Sending 0.24 MB of payload data to client #110 (simulated).
[INFO][09:56:06]: [Server #1127936] Selecting client #367 for training.
[INFO][09:56:06]: [Server #1127936] Sending the current model to client #367 (simulated).
[INFO][09:56:06]: [Server #1127936] Sending 0.24 MB of payload data to client #367 (simulated).
[INFO][09:56:06]: [Server #1127936] Selecting client #13 for training.
[INFO][09:56:06]: [Server #1127936] Sending the current model to client #13 (simulated).
[INFO][09:56:06]: [Client #110] Selected by the server.
[INFO][09:56:06]: [Client #110] Loading its data source...
[INFO][09:56:06]: [Client #110] Dataset size: 60000
[INFO][09:56:06]: [Client #110] Sampler: noniid
[INFO][09:56:06]: [Server #1127936] Sending 0.24 MB of payload data to client #13 (simulated).
[INFO][09:56:06]: [Client #367] Selected by the server.
[INFO][09:56:06]: [Client #367] Loading its data source...
[INFO][09:56:06]: [Client #367] Dataset size: 60000
[INFO][09:56:06]: [Client #367] Sampler: noniid
[INFO][09:56:06]: [Client #110] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:56:06]: [Client #13] Selected by the server.
[INFO][09:56:06]: [Client #13] Loading its data source...
[INFO][09:56:06]: [Client #13] Dataset size: 60000
[INFO][09:56:06]: [Client #13] Sampler: noniid
[INFO][09:56:06]: [Client #367] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:56:06]: [Client #13] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:56:07]: [93m[1m[Client #13] Started training in communication round #24.[0m
[INFO][09:56:07]: [93m[1m[Client #367] Started training in communication round #24.[0m
[INFO][09:56:07]: [93m[1m[Client #110] Started training in communication round #24.[0m
[INFO][09:56:09]: [Client #367] Loading the dataset.
[INFO][09:56:09]: [Client #13] Loading the dataset.
[INFO][09:56:09]: [Client #110] Loading the dataset.
[INFO][09:56:15]: [Client #367] Epoch: [1/5][0/10]	Loss: 0.140646
[INFO][09:56:15]: [Client #110] Epoch: [1/5][0/10]	Loss: 0.140646
[INFO][09:56:15]: [Client #367] Epoch: [2/5][0/10]	Loss: 0.004862
[INFO][09:56:15]: [Client #13] Epoch: [1/5][0/10]	Loss: 0.031131
[INFO][09:56:15]: [Client #110] Epoch: [2/5][0/10]	Loss: 0.005367
[INFO][09:56:15]: [Client #367] Epoch: [3/5][0/10]	Loss: 0.000009
[INFO][09:56:15]: [Client #13] Epoch: [2/5][0/10]	Loss: 0.043371
[INFO][09:56:15]: [Client #110] Epoch: [3/5][0/10]	Loss: 0.001662
[INFO][09:56:15]: [Client #367] Epoch: [4/5][0/10]	Loss: 0.000013
[INFO][09:56:15]: [Client #110] Epoch: [4/5][0/10]	Loss: 0.001200
[INFO][09:56:15]: [Client #13] Epoch: [3/5][0/10]	Loss: 0.000001
[INFO][09:56:15]: [Client #367] Epoch: [5/5][0/10]	Loss: 0.000013
[INFO][09:56:15]: [Client #367] Model saved to /data/ykang/plato/results/test/model/lenet5_367_1127978.pth.
[INFO][09:56:15]: [Client #110] Epoch: [5/5][0/10]	Loss: 0.000072
[INFO][09:56:15]: [Client #13] Epoch: [4/5][0/10]	Loss: 0.046736
[INFO][09:56:15]: [Client #110] Model saved to /data/ykang/plato/results/test/model/lenet5_110_1127977.pth.
[INFO][09:56:15]: [Client #13] Epoch: [5/5][0/10]	Loss: 0.000566
[INFO][09:56:15]: [Client #13] Model saved to /data/ykang/plato/results/test/model/lenet5_13_1127979.pth.
[INFO][09:56:16]: [Client #367] Loading a model from /data/ykang/plato/results/test/model/lenet5_367_1127978.pth.
[INFO][09:56:16]: [Client #367] Model trained.
[INFO][09:56:16]: [Client #367] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:56:16]: [Server #1127936] Received 0.24 MB of payload data from client #367 (simulated).
[INFO][09:56:16]: [Client #110] Loading a model from /data/ykang/plato/results/test/model/lenet5_110_1127977.pth.
[INFO][09:56:16]: [Client #110] Model trained.
[INFO][09:56:16]: [Client #110] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:56:16]: [Server #1127936] Received 0.24 MB of payload data from client #110 (simulated).
[INFO][09:56:16]: [Client #13] Loading a model from /data/ykang/plato/results/test/model/lenet5_13_1127979.pth.
[INFO][09:56:16]: [Client #13] Model trained.
[INFO][09:56:16]: [Client #13] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:56:16]: [Server #1127936] Received 0.24 MB of payload data from client #13 (simulated).
[INFO][09:56:16]: [Server #1127936] Selecting client #148 for training.
[INFO][09:56:16]: [Server #1127936] Sending the current model to client #148 (simulated).
[INFO][09:56:16]: [Server #1127936] Sending 0.24 MB of payload data to client #148 (simulated).
[INFO][09:56:16]: [Server #1127936] Selecting client #255 for training.
[INFO][09:56:16]: [Server #1127936] Sending the current model to client #255 (simulated).
[INFO][09:56:16]: [Server #1127936] Sending 0.24 MB of payload data to client #255 (simulated).
[INFO][09:56:16]: [Server #1127936] Selecting client #484 for training.
[INFO][09:56:16]: [Server #1127936] Sending the current model to client #484 (simulated).
[INFO][09:56:16]: [Client #148] Selected by the server.
[INFO][09:56:16]: [Client #148] Loading its data source...
[INFO][09:56:16]: [Client #148] Dataset size: 60000
[INFO][09:56:16]: [Client #148] Sampler: noniid
[INFO][09:56:16]: [Server #1127936] Sending 0.24 MB of payload data to client #484 (simulated).
[INFO][09:56:16]: [Client #484] Selected by the server.
[INFO][09:56:16]: [Client #484] Loading its data source...
[INFO][09:56:16]: [Client #484] Dataset size: 60000
[INFO][09:56:16]: [Client #484] Sampler: noniid
[INFO][09:56:16]: [Client #255] Selected by the server.
[INFO][09:56:16]: [Client #255] Loading its data source...
[INFO][09:56:16]: [Client #255] Dataset size: 60000
[INFO][09:56:16]: [Client #255] Sampler: noniid
[INFO][09:56:16]: [Client #148] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:56:16]: [Client #255] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:56:16]: [93m[1m[Client #255] Started training in communication round #24.[0m
[INFO][09:56:16]: [93m[1m[Client #148] Started training in communication round #24.[0m
[INFO][09:56:16]: [Client #484] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:56:16]: [93m[1m[Client #484] Started training in communication round #24.[0m
[INFO][09:56:18]: [Client #148] Loading the dataset.
[INFO][09:56:18]: [Client #255] Loading the dataset.
[INFO][09:56:18]: [Client #484] Loading the dataset.
[INFO][09:56:24]: [Client #255] Epoch: [1/5][0/10]	Loss: 0.034048
[INFO][09:56:24]: [Client #148] Epoch: [1/5][0/10]	Loss: 0.083438
[INFO][09:56:24]: [Client #484] Epoch: [1/5][0/10]	Loss: 0.771370
[INFO][09:56:24]: [Client #255] Epoch: [2/5][0/10]	Loss: 0.001029
[INFO][09:56:25]: [Client #148] Epoch: [2/5][0/10]	Loss: 0.117609
[INFO][09:56:25]: [Client #484] Epoch: [2/5][0/10]	Loss: 0.147629
[INFO][09:56:25]: [Client #255] Epoch: [3/5][0/10]	Loss: 0.000100
[INFO][09:56:25]: [Client #148] Epoch: [3/5][0/10]	Loss: 0.000236
[INFO][09:56:25]: [Client #484] Epoch: [3/5][0/10]	Loss: 0.004051
[INFO][09:56:25]: [Client #255] Epoch: [4/5][0/10]	Loss: 0.000419
[INFO][09:56:25]: [Client #148] Epoch: [4/5][0/10]	Loss: 0.003413
[INFO][09:56:25]: [Client #484] Epoch: [4/5][0/10]	Loss: 0.857897
[INFO][09:56:25]: [Client #148] Epoch: [5/5][0/10]	Loss: 0.000417
[INFO][09:56:25]: [Client #255] Epoch: [5/5][0/10]	Loss: 0.095837
[INFO][09:56:25]: [Client #484] Epoch: [5/5][0/10]	Loss: 0.113863
[INFO][09:56:25]: [Client #148] Model saved to /data/ykang/plato/results/test/model/lenet5_148_1127977.pth.
[INFO][09:56:25]: [Client #255] Model saved to /data/ykang/plato/results/test/model/lenet5_255_1127978.pth.
[INFO][09:56:25]: [Client #484] Model saved to /data/ykang/plato/results/test/model/lenet5_484_1127979.pth.
[INFO][09:56:26]: [Client #148] Loading a model from /data/ykang/plato/results/test/model/lenet5_148_1127977.pth.
[INFO][09:56:26]: [Client #148] Model trained.
[INFO][09:56:26]: [Client #148] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:56:26]: [Server #1127936] Received 0.24 MB of payload data from client #148 (simulated).
[INFO][09:56:26]: [Client #255] Loading a model from /data/ykang/plato/results/test/model/lenet5_255_1127978.pth.
[INFO][09:56:26]: [Client #255] Model trained.
[INFO][09:56:26]: [Client #255] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:56:26]: [Server #1127936] Received 0.24 MB of payload data from client #255 (simulated).
[INFO][09:56:26]: [Client #484] Loading a model from /data/ykang/plato/results/test/model/lenet5_484_1127979.pth.
[INFO][09:56:26]: [Client #484] Model trained.
[INFO][09:56:26]: [Client #484] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:56:26]: [Server #1127936] Received 0.24 MB of payload data from client #484 (simulated).
[INFO][09:56:26]: [Server #1127936] Selecting client #56 for training.
[INFO][09:56:26]: [Server #1127936] Sending the current model to client #56 (simulated).
[INFO][09:56:26]: [Server #1127936] Sending 0.24 MB of payload data to client #56 (simulated).
[INFO][09:56:26]: [Server #1127936] Selecting client #338 for training.
[INFO][09:56:26]: [Server #1127936] Sending the current model to client #338 (simulated).
[INFO][09:56:26]: [Server #1127936] Sending 0.24 MB of payload data to client #338 (simulated).
[INFO][09:56:26]: [Server #1127936] Selecting client #253 for training.
[INFO][09:56:26]: [Server #1127936] Sending the current model to client #253 (simulated).
[INFO][09:56:26]: [Client #56] Selected by the server.
[INFO][09:56:26]: [Client #56] Loading its data source...
[INFO][09:56:26]: [Client #56] Dataset size: 60000
[INFO][09:56:26]: [Client #56] Sampler: noniid
[INFO][09:56:26]: [Server #1127936] Sending 0.24 MB of payload data to client #253 (simulated).
[INFO][09:56:26]: [Client #253] Selected by the server.
[INFO][09:56:26]: [Client #253] Loading its data source...
[INFO][09:56:26]: [Client #338] Selected by the server.
[INFO][09:56:26]: [Client #253] Dataset size: 60000
[INFO][09:56:26]: [Client #338] Loading its data source...
[INFO][09:56:26]: [Client #253] Sampler: noniid
[INFO][09:56:26]: [Client #338] Dataset size: 60000
[INFO][09:56:26]: [Client #338] Sampler: noniid
[INFO][09:56:26]: [Client #56] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:56:26]: [Client #253] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:56:26]: [93m[1m[Client #56] Started training in communication round #24.[0m
[INFO][09:56:26]: [93m[1m[Client #253] Started training in communication round #24.[0m
[INFO][09:56:26]: [Client #338] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:56:26]: [93m[1m[Client #338] Started training in communication round #24.[0m
[INFO][09:56:28]: [Client #253] Loading the dataset.
[INFO][09:56:28]: [Client #56] Loading the dataset.
[INFO][09:56:28]: [Client #338] Loading the dataset.
[INFO][09:56:34]: [Client #253] Epoch: [1/5][0/10]	Loss: 0.114296
[INFO][09:56:34]: [Client #253] Epoch: [2/5][0/10]	Loss: 0.019041
[INFO][09:56:34]: [Client #56] Epoch: [1/5][0/10]	Loss: 0.104170
[INFO][09:56:34]: [Client #338] Epoch: [1/5][0/10]	Loss: 0.663487
[INFO][09:56:34]: [Client #253] Epoch: [3/5][0/10]	Loss: 0.010082
[INFO][09:56:35]: [Client #56] Epoch: [2/5][0/10]	Loss: 0.010554
[INFO][09:56:35]: [Client #338] Epoch: [2/5][0/10]	Loss: 0.124022
[INFO][09:56:35]: [Client #253] Epoch: [4/5][0/10]	Loss: 0.138723
[INFO][09:56:35]: [Client #338] Epoch: [3/5][0/10]	Loss: 0.367090
[INFO][09:56:35]: [Client #253] Epoch: [5/5][0/10]	Loss: 0.376450
[INFO][09:56:35]: [Client #56] Epoch: [3/5][0/10]	Loss: 0.059210
[INFO][09:56:35]: [Client #338] Epoch: [4/5][0/10]	Loss: 0.020958
[INFO][09:56:35]: [Client #253] Model saved to /data/ykang/plato/results/test/model/lenet5_253_1127979.pth.
[INFO][09:56:35]: [Client #56] Epoch: [4/5][0/10]	Loss: 0.001121
[INFO][09:56:35]: [Client #338] Epoch: [5/5][0/10]	Loss: 0.034950
[INFO][09:56:35]: [Client #338] Model saved to /data/ykang/plato/results/test/model/lenet5_338_1127978.pth.
[INFO][09:56:35]: [Client #56] Epoch: [5/5][0/10]	Loss: 0.002812
[INFO][09:56:35]: [Client #56] Model saved to /data/ykang/plato/results/test/model/lenet5_56_1127977.pth.
[INFO][09:56:36]: [Client #253] Loading a model from /data/ykang/plato/results/test/model/lenet5_253_1127979.pth.
[INFO][09:56:36]: [Client #253] Model trained.
[INFO][09:56:36]: [Client #253] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:56:36]: [Server #1127936] Received 0.24 MB of payload data from client #253 (simulated).
[INFO][09:56:36]: [Client #56] Loading a model from /data/ykang/plato/results/test/model/lenet5_56_1127977.pth.
[INFO][09:56:36]: [Client #56] Model trained.
[INFO][09:56:36]: [Client #56] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:56:36]: [Server #1127936] Received 0.24 MB of payload data from client #56 (simulated).
[INFO][09:56:36]: [Client #338] Loading a model from /data/ykang/plato/results/test/model/lenet5_338_1127978.pth.
[INFO][09:56:36]: [Client #338] Model trained.
[INFO][09:56:36]: [Client #338] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:56:36]: [Server #1127936] Received 0.24 MB of payload data from client #338 (simulated).
[INFO][09:56:36]: [Server #1127936] Selecting client #391 for training.
[INFO][09:56:36]: [Server #1127936] Sending the current model to client #391 (simulated).
[INFO][09:56:36]: [Server #1127936] Sending 0.24 MB of payload data to client #391 (simulated).
[INFO][09:56:36]: [Client #391] Selected by the server.
[INFO][09:56:36]: [Client #391] Loading its data source...
[INFO][09:56:36]: [Client #391] Dataset size: 60000
[INFO][09:56:36]: [Client #391] Sampler: noniid
[INFO][09:56:36]: [Client #391] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:56:36]: [93m[1m[Client #391] Started training in communication round #24.[0m
[INFO][09:56:38]: [Client #391] Loading the dataset.
[INFO][09:56:43]: [Client #391] Epoch: [1/5][0/10]	Loss: 0.504586
[INFO][09:56:44]: [Client #391] Epoch: [2/5][0/10]	Loss: 0.579317
[INFO][09:56:44]: [Client #391] Epoch: [3/5][0/10]	Loss: 0.306293
[INFO][09:56:44]: [Client #391] Epoch: [4/5][0/10]	Loss: 0.705796
[INFO][09:56:44]: [Client #391] Epoch: [5/5][0/10]	Loss: 0.507897
[INFO][09:56:44]: [Client #391] Model saved to /data/ykang/plato/results/test/model/lenet5_391_1127977.pth.
[INFO][09:56:44]: [Client #391] Loading a model from /data/ykang/plato/results/test/model/lenet5_391_1127977.pth.
[INFO][09:56:44]: [Client #391] Model trained.
[INFO][09:56:44]: [Client #391] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:56:44]: [Server #1127936] Received 0.24 MB of payload data from client #391 (simulated).
[INFO][09:56:44]: [Server #1127936] Adding client #98 to the list of clients for aggregation.
[INFO][09:56:44]: [Server #1127936] Adding client #420 to the list of clients for aggregation.
[INFO][09:56:44]: [Server #1127936] Adding client #355 to the list of clients for aggregation.
[INFO][09:56:44]: [Server #1127936] Adding client #229 to the list of clients for aggregation.
[INFO][09:56:44]: [Server #1127936] Adding client #265 to the list of clients for aggregation.
[INFO][09:56:44]: [Server #1127936] Adding client #250 to the list of clients for aggregation.
[INFO][09:56:44]: [Server #1127936] Adding client #56 to the list of clients for aggregation.
[INFO][09:56:44]: [Server #1127936] Adding client #13 to the list of clients for aggregation.
[INFO][09:56:44]: [Server #1127936] Adding client #391 to the list of clients for aggregation.
[INFO][09:56:44]: [Server #1127936] Adding client #253 to the list of clients for aggregation.
[INFO][09:56:44]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 251, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01000608 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01711349 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.03564256 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01635377 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00887314 0.         0.
 0.02545671 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01670351 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01288613 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.04494877 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.02329712
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 1. 3. 0. 0. 0. 0. 0. 0. 1. 6. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 2. 0. 6. 0. 1. 2. 1. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 1. 1. 0. 2. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 0. 1. 0. 0. 0. 6. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01000608 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01711349 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.03564256 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01635377 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00887314 0.         0.
 0.02545671 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01670351 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01288613 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.04494877 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.02329712
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][09:56:47]: [Server #1127936] Global model accuracy: 91.05%

[INFO][09:56:47]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_24.pth.
[INFO][09:56:47]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_24.pth.
[INFO][09:56:47]: [93m[1m
[Server #1127936] Starting round 25/100.[0m
[0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002
 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.002
 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  4e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  4e-05  7e-10  7e-10
 6:  6.8876e+00  6.8875e+00  4e-05  4e-10  4e-10
 7:  6.8875e+00  6.8875e+00  3e-05  9e-10  4e-11
 8:  6.8875e+00  6.8875e+00  2e-05  9e-10  3e-11
 9:  6.8875e+00  6.8875e+00  1e-05  3e-09  1e-10
10:  6.8875e+00  6.8875e+00  1e-06  4e-09  1e-10
Optimal solution found.
The calculated probability is:  [6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32964532e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32923619e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 9.68627748e-01 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 1.37335584e-04
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 8.98511330e-05 6.32985785e-05 6.32848244e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 1.40746781e-04 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 1.10493966e-04 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32557169e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 2.57377693e-04 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05 6.32985785e-05 6.32985785e-05
 6.32985785e-05 6.32985785e-05]
current clients pool:  [INFO][09:56:47]: [Server #1127936] Selected clients: [ 98 195 332  37  78   9  59 329 206 166]
[INFO][09:56:47]: [Server #1127936] Selecting client #98 for training.
[INFO][09:56:47]: [Server #1127936] Sending the current model to client #98 (simulated).
[INFO][09:56:47]: [Server #1127936] Sending 0.24 MB of payload data to client #98 (simulated).
[INFO][09:56:47]: [Server #1127936] Selecting client #195 for training.
[INFO][09:56:47]: [Server #1127936] Sending the current model to client #195 (simulated).
[INFO][09:56:47]: [Server #1127936] Sending 0.24 MB of payload data to client #195 (simulated).
[INFO][09:56:47]: [Server #1127936] Selecting client #332 for training.
[INFO][09:56:47]: [Server #1127936] Sending the current model to client #332 (simulated).
[INFO][09:56:47]: [Client #98] Selected by the server.
[INFO][09:56:47]: [Client #98] Loading its data source...
[INFO][09:56:47]: [Client #98] Dataset size: 60000
[INFO][09:56:47]: [Client #98] Sampler: noniid
[INFO][09:56:47]: [Server #1127936] Sending 0.24 MB of payload data to client #332 (simulated).
[INFO][09:56:47]: [Client #195] Selected by the server.
[INFO][09:56:47]: [Client #195] Loading its data source...
[INFO][09:56:47]: [Client #195] Dataset size: 60000
[INFO][09:56:47]: [Client #195] Sampler: noniid
[INFO][09:56:47]: [Client #332] Selected by the server.
[INFO][09:56:47]: [Client #98] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:56:47]: [Client #332] Loading its data source...
[INFO][09:56:47]: [Client #332] Dataset size: 60000
[INFO][09:56:47]: [Client #332] Sampler: noniid
[INFO][09:56:47]: [Client #195] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:56:47]: [Client #332] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:56:47]: [93m[1m[Client #332] Started training in communication round #25.[0m
[INFO][09:56:47]: [93m[1m[Client #195] Started training in communication round #25.[0m
[INFO][09:56:47]: [93m[1m[Client #98] Started training in communication round #25.[0m
[INFO][09:56:49]: [Client #332] Loading the dataset.
[INFO][09:56:50]: [Client #195] Loading the dataset.
[INFO][09:56:50]: [Client #98] Loading the dataset.
[INFO][09:56:55]: [Client #332] Epoch: [1/5][0/10]	Loss: 0.237986
[INFO][09:56:56]: [Client #332] Epoch: [2/5][0/10]	Loss: 0.000013
[INFO][09:56:56]: [Client #98] Epoch: [1/5][0/10]	Loss: 0.040263
[INFO][09:56:56]: [Client #195] Epoch: [1/5][0/10]	Loss: 0.163628
[INFO][09:56:56]: [Client #332] Epoch: [3/5][0/10]	Loss: 0.000048
[INFO][09:56:56]: [Client #98] Epoch: [2/5][0/10]	Loss: 0.058766
[INFO][09:56:56]: [Client #195] Epoch: [2/5][0/10]	Loss: 0.005958
[INFO][09:56:56]: [Client #332] Epoch: [4/5][0/10]	Loss: 0.006039
[INFO][09:56:56]: [Client #98] Epoch: [3/5][0/10]	Loss: 0.000342
[INFO][09:56:56]: [Client #195] Epoch: [3/5][0/10]	Loss: 0.005626
[INFO][09:56:56]: [Client #332] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][09:56:56]: [Client #98] Epoch: [4/5][0/10]	Loss: 0.000598
[INFO][09:56:56]: [Client #332] Model saved to /data/ykang/plato/results/test/model/lenet5_332_1127979.pth.
[INFO][09:56:56]: [Client #195] Epoch: [4/5][0/10]	Loss: 0.031781
[INFO][09:56:56]: [Client #98] Epoch: [5/5][0/10]	Loss: 0.010031
[INFO][09:56:56]: [Client #195] Epoch: [5/5][0/10]	Loss: 0.006538
[INFO][09:56:56]: [Client #98] Model saved to /data/ykang/plato/results/test/model/lenet5_98_1127977.pth.
[INFO][09:56:56]: [Client #195] Model saved to /data/ykang/plato/results/test/model/lenet5_195_1127978.pth.
[INFO][09:56:57]: [Client #332] Loading a model from /data/ykang/plato/results/test/model/lenet5_332_1127979.pth.
[INFO][09:56:57]: [Client #332] Model trained.
[INFO][09:56:57]: [Client #332] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:56:57]: [Server #1127936] Received 0.24 MB of payload data from client #332 (simulated).
[INFO][09:56:57]: [Client #98] Loading a model from /data/ykang/plato/results/test/model/lenet5_98_1127977.pth.
[INFO][09:56:57]: [Client #98] Model trained.
[INFO][09:56:57]: [Client #98] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:56:57]: [Server #1127936] Received 0.24 MB of payload data from client #98 (simulated).
[INFO][09:56:57]: [Client #195] Loading a model from /data/ykang/plato/results/test/model/lenet5_195_1127978.pth.
[INFO][09:56:57]: [Client #195] Model trained.
[INFO][09:56:57]: [Client #195] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:56:57]: [Server #1127936] Received 0.24 MB of payload data from client #195 (simulated).
[INFO][09:56:57]: [Server #1127936] Selecting client #37 for training.
[INFO][09:56:57]: [Server #1127936] Sending the current model to client #37 (simulated).
[INFO][09:56:57]: [Server #1127936] Sending 0.24 MB of payload data to client #37 (simulated).
[INFO][09:56:57]: [Server #1127936] Selecting client #78 for training.
[INFO][09:56:57]: [Server #1127936] Sending the current model to client #78 (simulated).
[INFO][09:56:57]: [Server #1127936] Sending 0.24 MB of payload data to client #78 (simulated).
[INFO][09:56:57]: [Server #1127936] Selecting client #9 for training.
[INFO][09:56:57]: [Server #1127936] Sending the current model to client #9 (simulated).
[INFO][09:56:57]: [Client #37] Selected by the server.
[INFO][09:56:57]: [Client #37] Loading its data source...
[INFO][09:56:57]: [Client #37] Dataset size: 60000
[INFO][09:56:57]: [Client #37] Sampler: noniid
[INFO][09:56:57]: [Server #1127936] Sending 0.24 MB of payload data to client #9 (simulated).
[INFO][09:56:57]: [Client #78] Selected by the server.
[INFO][09:56:57]: [Client #78] Loading its data source...
[INFO][09:56:57]: [Client #78] Dataset size: 60000
[INFO][09:56:57]: [Client #78] Sampler: noniid
[INFO][09:56:57]: [Client #9] Selected by the server.
[INFO][09:56:57]: [Client #9] Loading its data source...
[INFO][09:56:57]: [Client #9] Dataset size: 60000
[INFO][09:56:57]: [Client #9] Sampler: noniid
[INFO][09:56:57]: [Client #9] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:56:57]: [Client #37] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:56:57]: [93m[1m[Client #9] Started training in communication round #25.[0m
[INFO][09:56:57]: [Client #78] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:56:57]: [93m[1m[Client #37] Started training in communication round #25.[0m
[INFO][09:56:57]: [93m[1m[Client #78] Started training in communication round #25.[0m
[INFO][09:56:59]: [Client #9] Loading the dataset.
[INFO][09:56:59]: [Client #78] Loading the dataset.
[INFO][09:56:59]: [Client #37] Loading the dataset.
[INFO][09:57:05]: [Client #9] Epoch: [1/5][0/10]	Loss: 0.164639
[INFO][09:57:05]: [Client #78] Epoch: [1/5][0/10]	Loss: 0.016511
[INFO][09:57:05]: [Client #37] Epoch: [1/5][0/10]	Loss: 0.078980
[INFO][09:57:05]: [Client #78] Epoch: [2/5][0/10]	Loss: 0.034910
[INFO][09:57:05]: [Client #9] Epoch: [2/5][0/10]	Loss: 0.002205
[INFO][09:57:05]: [Client #37] Epoch: [2/5][0/10]	Loss: 0.010632
[INFO][09:57:05]: [Client #78] Epoch: [3/5][0/10]	Loss: 0.000482
[INFO][09:57:05]: [Client #9] Epoch: [3/5][0/10]	Loss: 0.000010
[INFO][09:57:05]: [Client #37] Epoch: [3/5][0/10]	Loss: 0.005382
[INFO][09:57:06]: [Client #9] Epoch: [4/5][0/10]	Loss: 0.000153
[INFO][09:57:06]: [Client #78] Epoch: [4/5][0/10]	Loss: 0.014112
[INFO][09:57:06]: [Client #37] Epoch: [4/5][0/10]	Loss: 0.000042
[INFO][09:57:06]: [Client #9] Epoch: [5/5][0/10]	Loss: 0.000067
[INFO][09:57:06]: [Client #78] Epoch: [5/5][0/10]	Loss: 0.000473
[INFO][09:57:06]: [Client #78] Model saved to /data/ykang/plato/results/test/model/lenet5_78_1127978.pth.
[INFO][09:57:06]: [Client #37] Epoch: [5/5][0/10]	Loss: 0.293589
[INFO][09:57:06]: [Client #9] Model saved to /data/ykang/plato/results/test/model/lenet5_9_1127979.pth.
[INFO][09:57:06]: [Client #37] Model saved to /data/ykang/plato/results/test/model/lenet5_37_1127977.pth.
[INFO][09:57:06]: [Client #9] Loading a model from /data/ykang/plato/results/test/model/lenet5_9_1127979.pth.
[INFO][09:57:06]: [Client #9] Model trained.
[INFO][09:57:06]: [Client #9] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:57:06]: [Server #1127936] Received 0.24 MB of payload data from client #9 (simulated).
[INFO][09:57:07]: [Client #78] Loading a model from /data/ykang/plato/results/test/model/lenet5_78_1127978.pth.
[INFO][09:57:07]: [Client #78] Model trained.
[INFO][09:57:07]: [Client #78] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:57:07]: [Server #1127936] Received 0.24 MB of payload data from client #78 (simulated).
[INFO][09:57:07]: [Client #37] Loading a model from /data/ykang/plato/results/test/model/lenet5_37_1127977.pth.
[INFO][09:57:07]: [Client #37] Model trained.
[INFO][09:57:07]: [Client #37] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:57:07]: [Server #1127936] Received 0.24 MB of payload data from client #37 (simulated).
[INFO][09:57:07]: [Server #1127936] Selecting client #59 for training.
[INFO][09:57:07]: [Server #1127936] Sending the current model to client #59 (simulated).
[INFO][09:57:07]: [Server #1127936] Sending 0.24 MB of payload data to client #59 (simulated).
[INFO][09:57:07]: [Server #1127936] Selecting client #329 for training.
[INFO][09:57:07]: [Server #1127936] Sending the current model to client #329 (simulated).
[INFO][09:57:07]: [Server #1127936] Sending 0.24 MB of payload data to client #329 (simulated).
[INFO][09:57:07]: [Server #1127936] Selecting client #206 for training.
[INFO][09:57:07]: [Server #1127936] Sending the current model to client #206 (simulated).
[INFO][09:57:07]: [Client #59] Selected by the server.
[INFO][09:57:07]: [Client #59] Loading its data source...
[INFO][09:57:07]: [Client #59] Dataset size: 60000
[INFO][09:57:07]: [Client #59] Sampler: noniid
[INFO][09:57:07]: [Server #1127936] Sending 0.24 MB of payload data to client #206 (simulated).
[INFO][09:57:07]: [Client #329] Selected by the server.
[INFO][09:57:07]: [Client #329] Loading its data source...
[INFO][09:57:07]: [Client #206] Selected by the server.
[INFO][09:57:07]: [Client #329] Dataset size: 60000
[INFO][09:57:07]: [Client #329] Sampler: noniid
[INFO][09:57:07]: [Client #206] Loading its data source...
[INFO][09:57:07]: [Client #206] Dataset size: 60000
[INFO][09:57:07]: [Client #206] Sampler: noniid
[INFO][09:57:07]: [Client #59] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:57:07]: [Client #329] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:57:07]: [Client #206] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:57:07]: [93m[1m[Client #206] Started training in communication round #25.[0m
[INFO][09:57:07]: [93m[1m[Client #59] Started training in communication round #25.[0m
[INFO][09:57:07]: [93m[1m[Client #329] Started training in communication round #25.[0m
[INFO][09:57:09]: [Client #206] Loading the dataset.
[INFO][09:57:09]: [Client #59] Loading the dataset.
[INFO][09:57:09]: [Client #329] Loading the dataset.
[INFO][09:57:15]: [Client #59] Epoch: [1/5][0/10]	Loss: 0.330540
[INFO][09:57:15]: [Client #206] Epoch: [1/5][0/10]	Loss: 0.051742
[INFO][09:57:15]: [Client #59] Epoch: [2/5][0/10]	Loss: 0.000025
[INFO][09:57:15]: [Client #329] Epoch: [1/5][0/10]	Loss: 0.221226
[INFO][09:57:15]: [Client #206] Epoch: [2/5][0/10]	Loss: 0.011098
[INFO][09:57:15]: [Client #59] Epoch: [3/5][0/10]	Loss: 0.012274
[INFO][09:57:15]: [Client #329] Epoch: [2/5][0/10]	Loss: 0.005922
[INFO][09:57:15]: [Client #206] Epoch: [3/5][0/10]	Loss: 0.003996
[INFO][09:57:15]: [Client #329] Epoch: [3/5][0/10]	Loss: 0.000008
[INFO][09:57:15]: [Client #59] Epoch: [4/5][0/10]	Loss: 0.129628
[INFO][09:57:15]: [Client #206] Epoch: [4/5][0/10]	Loss: 0.000002
[INFO][09:57:15]: [Client #329] Epoch: [4/5][0/10]	Loss: 0.000066
[INFO][09:57:15]: [Client #59] Epoch: [5/5][0/10]	Loss: 0.004540
[INFO][09:57:16]: [Client #206] Epoch: [5/5][0/10]	Loss: 0.690254
[INFO][09:57:16]: [Client #59] Model saved to /data/ykang/plato/results/test/model/lenet5_59_1127977.pth.
[INFO][09:57:16]: [Client #329] Epoch: [5/5][0/10]	Loss: 0.000012
[INFO][09:57:16]: [Client #206] Model saved to /data/ykang/plato/results/test/model/lenet5_206_1127979.pth.
[INFO][09:57:16]: [Client #329] Model saved to /data/ykang/plato/results/test/model/lenet5_329_1127978.pth.
[INFO][09:57:16]: [Client #59] Loading a model from /data/ykang/plato/results/test/model/lenet5_59_1127977.pth.
[INFO][09:57:16]: [Client #59] Model trained.
[INFO][09:57:16]: [Client #59] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:57:16]: [Server #1127936] Received 0.24 MB of payload data from client #59 (simulated).
[INFO][09:57:16]: [Client #206] Loading a model from /data/ykang/plato/results/test/model/lenet5_206_1127979.pth.
[INFO][09:57:16]: [Client #206] Model trained.
[INFO][09:57:16]: [Client #206] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:57:16]: [Server #1127936] Received 0.24 MB of payload data from client #206 (simulated).
[INFO][09:57:16]: [Client #329] Loading a model from /data/ykang/plato/results/test/model/lenet5_329_1127978.pth.
[INFO][09:57:16]: [Client #329] Model trained.
[INFO][09:57:16]: [Client #329] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:57:17]: [Server #1127936] Received 0.24 MB of payload data from client #329 (simulated).
[INFO][09:57:17]: [Server #1127936] Selecting client #166 for training.
[INFO][09:57:17]: [Server #1127936] Sending the current model to client #166 (simulated).
[INFO][09:57:17]: [Server #1127936] Sending 0.24 MB of payload data to client #166 (simulated).
[INFO][09:57:17]: [Client #166] Selected by the server.
[INFO][09:57:17]: [Client #166] Loading its data source...
[INFO][09:57:17]: [Client #166] Dataset size: 60000
[INFO][09:57:17]: [Client #166] Sampler: noniid
[INFO][09:57:17]: [Client #166] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:57:17]: [93m[1m[Client #166] Started training in communication round #25.[0m
[INFO][09:57:18]: [Client #166] Loading the dataset.
[INFO][09:57:24]: [Client #166] Epoch: [1/5][0/10]	Loss: 0.212414
[INFO][09:57:24]: [Client #166] Epoch: [2/5][0/10]	Loss: 0.006500
[INFO][09:57:24]: [Client #166] Epoch: [3/5][0/10]	Loss: 0.538104
[INFO][09:57:24]: [Client #166] Epoch: [4/5][0/10]	Loss: 0.058813
[INFO][09:57:24]: [Client #166] Epoch: [5/5][0/10]	Loss: 0.001246
[INFO][09:57:24]: [Client #166] Model saved to /data/ykang/plato/results/test/model/lenet5_166_1127977.pth.
[INFO][09:57:25]: [Client #166] Loading a model from /data/ykang/plato/results/test/model/lenet5_166_1127977.pth.
[INFO][09:57:25]: [Client #166] Model trained.
[INFO][09:57:25]: [Client #166] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:57:25]: [Server #1127936] Received 0.24 MB of payload data from client #166 (simulated).
[INFO][09:57:25]: [Server #1127936] Adding client #255 to the list of clients for aggregation.
[INFO][09:57:25]: [Server #1127936] Adding client #484 to the list of clients for aggregation.
[INFO][09:57:25]: [Server #1127936] Adding client #148 to the list of clients for aggregation.
[INFO][09:57:25]: [Server #1127936] Adding client #88 to the list of clients for aggregation.
[INFO][09:57:25]: [Server #1127936] Adding client #338 to the list of clients for aggregation.
[INFO][09:57:25]: [Server #1127936] Adding client #22 to the list of clients for aggregation.
[INFO][09:57:25]: [Server #1127936] Adding client #252 to the list of clients for aggregation.
[INFO][09:57:25]: [Server #1127936] Adding client #166 to the list of clients for aggregation.
[INFO][09:57:25]: [Server #1127936] Adding client #78 to the list of clients for aggregation.
[INFO][09:57:25]: [Server #1127936] Adding client #9 to the list of clients for aggregation.
[INFO][09:57:25]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 253, 254, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.02068507 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.024248   0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01198904
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01967431 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.0066318  0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.03123133 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01718856
 0.         0.         0.01682326 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01855433 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.02945743 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 3. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 1. 3. 0. 0. 0. 0. 0. 0. 1. 6. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 2. 0. 6. 0. 1. 2. 1. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 1. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 4. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0.
 0. 1. 1. 1. 0. 2. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 0. 1. 0. 0. 0. 6. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.02068507 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.024248   0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01198904
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01967431 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.0066318  0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.03123133 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01718856
 0.         0.         0.01682326 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01855433 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.02945743 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][09:57:27]: [Server #1127936] Global model accuracy: 91.31%

[INFO][09:57:27]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_25.pth.
[INFO][09:57:27]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_25.pth.
[INFO][09:57:27]: [93m[1m
[Server #1127936] Starting round 26/100.[0m
[0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002
 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002
 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.002
 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  4e-04  7e-09  7e-09
 5:  6.8876e+00  6.8875e+00  8e-05  1e-09  1e-09
 6:  6.8875e+00  6.8875e+00  7e-05  5e-10  4e-10
 7:  6.8875e+00  6.8875e+00  6e-05  2e-09  2e-10
 8:  6.8875e+00  6.8875e+00  5e-05  3e-09  2e-10
 9:  6.8875e+00  6.8875e+00  2e-05  2e-08  1e-09
10:  6.8875e+00  6.8875e+00  9e-06  9e-09  7e-10
11:  6.8875e+00  6.8875e+00  1e-06  6e-10  3e-11
Optimal solution found.
The calculated probability is:  [4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30856762e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 8.69092804e-01 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30874996e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 3.59297733e-04 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.78109496e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30821631e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 1.09494573e-01 4.30884220e-05 4.30884220e-05 5.74950835e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 5.95437608e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 7.67773695e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05 4.30884220e-05 4.30884220e-05
 4.30884220e-05 4.30884220e-05]
current clients pool:  [INFO][09:57:28]: [Server #1127936] Selected clients: [ 22 349 252 244 269 449 223 440 125 135]
[INFO][09:57:28]: [Server #1127936] Selecting client #22 for training.
[INFO][09:57:28]: [Server #1127936] Sending the current model to client #22 (simulated).
[INFO][09:57:28]: [Server #1127936] Sending 0.24 MB of payload data to client #22 (simulated).
[INFO][09:57:28]: [Server #1127936] Selecting client #349 for training.
[INFO][09:57:28]: [Server #1127936] Sending the current model to client #349 (simulated).
[INFO][09:57:28]: [Server #1127936] Sending 0.24 MB of payload data to client #349 (simulated).
[INFO][09:57:28]: [Server #1127936] Selecting client #252 for training.
[INFO][09:57:28]: [Server #1127936] Sending the current model to client #252 (simulated).
[INFO][09:57:28]: [Client #22] Selected by the server.
[INFO][09:57:28]: [Client #22] Loading its data source...
[INFO][09:57:28]: [Client #22] Dataset size: 60000
[INFO][09:57:28]: [Client #22] Sampler: noniid
[INFO][09:57:28]: [Server #1127936] Sending 0.24 MB of payload data to client #252 (simulated).
[INFO][09:57:28]: [Client #349] Selected by the server.
[INFO][09:57:28]: [Client #349] Loading its data source...
[INFO][09:57:28]: [Client #349] Dataset size: 60000
[INFO][09:57:28]: [Client #349] Sampler: noniid
[INFO][09:57:28]: [Client #22] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:57:28]: [Client #349] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:57:28]: [93m[1m[Client #22] Started training in communication round #26.[0m
[INFO][09:57:28]: [93m[1m[Client #349] Started training in communication round #26.[0m
[INFO][09:57:28]: [Client #252] Selected by the server.
[INFO][09:57:28]: [Client #252] Loading its data source...
[INFO][09:57:28]: [Client #252] Dataset size: 60000
[INFO][09:57:28]: [Client #252] Sampler: noniid
[INFO][09:57:28]: [Client #252] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:57:28]: [93m[1m[Client #252] Started training in communication round #26.[0m
[INFO][09:57:30]: [Client #22] Loading the dataset.
[INFO][09:57:30]: [Client #349] Loading the dataset.
[INFO][09:57:30]: [Client #252] Loading the dataset.
[INFO][09:57:37]: [Client #349] Epoch: [1/5][0/10]	Loss: 0.045388
[INFO][09:57:37]: [Client #22] Epoch: [1/5][0/10]	Loss: 0.099066
[INFO][09:57:37]: [Client #252] Epoch: [1/5][0/10]	Loss: 0.043800
[INFO][09:57:37]: [Client #349] Epoch: [2/5][0/10]	Loss: 0.030124
[INFO][09:57:37]: [Client #22] Epoch: [2/5][0/10]	Loss: 0.005456
[INFO][09:57:37]: [Client #252] Epoch: [2/5][0/10]	Loss: 0.014613
[INFO][09:57:37]: [Client #349] Epoch: [3/5][0/10]	Loss: 0.002630
[INFO][09:57:37]: [Client #22] Epoch: [3/5][0/10]	Loss: 0.000056
[INFO][09:57:37]: [Client #349] Epoch: [4/5][0/10]	Loss: 0.062866
[INFO][09:57:37]: [Client #252] Epoch: [3/5][0/10]	Loss: 0.000098
[INFO][09:57:37]: [Client #349] Epoch: [5/5][0/10]	Loss: 0.092767
[INFO][09:57:37]: [Client #22] Epoch: [4/5][0/10]	Loss: 0.000511
[INFO][09:57:37]: [Client #252] Epoch: [4/5][0/10]	Loss: 0.001766
[INFO][09:57:37]: [Client #349] Model saved to /data/ykang/plato/results/test/model/lenet5_349_1127978.pth.
[INFO][09:57:37]: [Client #22] Epoch: [5/5][0/10]	Loss: 0.489034
[INFO][09:57:37]: [Client #22] Model saved to /data/ykang/plato/results/test/model/lenet5_22_1127977.pth.
[INFO][09:57:37]: [Client #252] Epoch: [5/5][0/10]	Loss: 0.000011
[INFO][09:57:37]: [Client #252] Model saved to /data/ykang/plato/results/test/model/lenet5_252_1127979.pth.
[INFO][09:57:38]: [Client #349] Loading a model from /data/ykang/plato/results/test/model/lenet5_349_1127978.pth.
[INFO][09:57:38]: [Client #349] Model trained.
[INFO][09:57:38]: [Client #349] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:57:38]: [Server #1127936] Received 0.24 MB of payload data from client #349 (simulated).
[INFO][09:57:38]: [Client #22] Loading a model from /data/ykang/plato/results/test/model/lenet5_22_1127977.pth.
[INFO][09:57:38]: [Client #22] Model trained.
[INFO][09:57:38]: [Client #22] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:57:38]: [Server #1127936] Received 0.24 MB of payload data from client #22 (simulated).
[INFO][09:57:38]: [Client #252] Loading a model from /data/ykang/plato/results/test/model/lenet5_252_1127979.pth.
[INFO][09:57:38]: [Client #252] Model trained.
[INFO][09:57:38]: [Client #252] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:57:38]: [Server #1127936] Received 0.24 MB of payload data from client #252 (simulated).
[INFO][09:57:38]: [Server #1127936] Selecting client #244 for training.
[INFO][09:57:38]: [Server #1127936] Sending the current model to client #244 (simulated).
[INFO][09:57:38]: [Server #1127936] Sending 0.24 MB of payload data to client #244 (simulated).
[INFO][09:57:38]: [Server #1127936] Selecting client #269 for training.
[INFO][09:57:38]: [Server #1127936] Sending the current model to client #269 (simulated).
[INFO][09:57:38]: [Server #1127936] Sending 0.24 MB of payload data to client #269 (simulated).
[INFO][09:57:38]: [Server #1127936] Selecting client #449 for training.
[INFO][09:57:38]: [Server #1127936] Sending the current model to client #449 (simulated).
[INFO][09:57:38]: [Client #244] Selected by the server.
[INFO][09:57:38]: [Client #244] Loading its data source...
[INFO][09:57:38]: [Client #244] Dataset size: 60000
[INFO][09:57:38]: [Client #244] Sampler: noniid
[INFO][09:57:38]: [Server #1127936] Sending 0.24 MB of payload data to client #449 (simulated).
[INFO][09:57:38]: [Client #269] Selected by the server.
[INFO][09:57:38]: [Client #449] Selected by the server.
[INFO][09:57:38]: [Client #269] Loading its data source...
[INFO][09:57:38]: [Client #449] Loading its data source...
[INFO][09:57:38]: [Client #449] Dataset size: 60000
[INFO][09:57:38]: [Client #269] Dataset size: 60000
[INFO][09:57:38]: [Client #449] Sampler: noniid
[INFO][09:57:38]: [Client #269] Sampler: noniid
[INFO][09:57:38]: [Client #244] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:57:38]: [Client #449] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:57:38]: [Client #269] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:57:38]: [93m[1m[Client #269] Started training in communication round #26.[0m
[INFO][09:57:38]: [93m[1m[Client #244] Started training in communication round #26.[0m
[INFO][09:57:38]: [93m[1m[Client #449] Started training in communication round #26.[0m
[INFO][09:57:40]: [Client #269] Loading the dataset.
[INFO][09:57:40]: [Client #244] Loading the dataset.
[INFO][09:57:40]: [Client #449] Loading the dataset.
[INFO][09:57:46]: [Client #269] Epoch: [1/5][0/10]	Loss: 0.324342
[INFO][09:57:46]: [Client #449] Epoch: [1/5][0/10]	Loss: 0.009425
[INFO][09:57:46]: [Client #244] Epoch: [1/5][0/10]	Loss: 0.068466
[INFO][09:57:46]: [Client #449] Epoch: [2/5][0/10]	Loss: 0.157951
[INFO][09:57:46]: [Client #269] Epoch: [2/5][0/10]	Loss: 0.000000
[INFO][09:57:46]: [Client #449] Epoch: [3/5][0/10]	Loss: 0.000209
[INFO][09:57:46]: [Client #244] Epoch: [2/5][0/10]	Loss: 0.021296
[INFO][09:57:46]: [Client #269] Epoch: [3/5][0/10]	Loss: 0.000018
[INFO][09:57:46]: [Client #244] Epoch: [3/5][0/10]	Loss: 0.001609
[INFO][09:57:46]: [Client #449] Epoch: [4/5][0/10]	Loss: 0.026747
[INFO][09:57:46]: [Client #269] Epoch: [4/5][0/10]	Loss: 0.000008
[INFO][09:57:47]: [Client #449] Epoch: [5/5][0/10]	Loss: 0.002268
[INFO][09:57:47]: [Client #244] Epoch: [4/5][0/10]	Loss: 0.222612
[INFO][09:57:47]: [Client #269] Epoch: [5/5][0/10]	Loss: 0.028399
[INFO][09:57:47]: [Client #449] Model saved to /data/ykang/plato/results/test/model/lenet5_449_1127979.pth.
[INFO][09:57:47]: [Client #269] Model saved to /data/ykang/plato/results/test/model/lenet5_269_1127978.pth.
[INFO][09:57:47]: [Client #244] Epoch: [5/5][0/10]	Loss: 0.762516
[INFO][09:57:47]: [Client #244] Model saved to /data/ykang/plato/results/test/model/lenet5_244_1127977.pth.
[INFO][09:57:47]: [Client #449] Loading a model from /data/ykang/plato/results/test/model/lenet5_449_1127979.pth.
[INFO][09:57:48]: [Client #449] Model trained.
[INFO][09:57:48]: [Client #449] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:57:48]: [Server #1127936] Received 0.24 MB of payload data from client #449 (simulated).
[INFO][09:57:48]: [Client #269] Loading a model from /data/ykang/plato/results/test/model/lenet5_269_1127978.pth.
[INFO][09:57:48]: [Client #269] Model trained.
[INFO][09:57:48]: [Client #269] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:57:48]: [Server #1127936] Received 0.24 MB of payload data from client #269 (simulated).
[INFO][09:57:48]: [Client #244] Loading a model from /data/ykang/plato/results/test/model/lenet5_244_1127977.pth.
[INFO][09:57:48]: [Client #244] Model trained.
[INFO][09:57:48]: [Client #244] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:57:48]: [Server #1127936] Received 0.24 MB of payload data from client #244 (simulated).
[INFO][09:57:48]: [Server #1127936] Selecting client #223 for training.
[INFO][09:57:48]: [Server #1127936] Sending the current model to client #223 (simulated).
[INFO][09:57:48]: [Server #1127936] Sending 0.24 MB of payload data to client #223 (simulated).
[INFO][09:57:48]: [Server #1127936] Selecting client #440 for training.
[INFO][09:57:48]: [Server #1127936] Sending the current model to client #440 (simulated).
[INFO][09:57:48]: [Server #1127936] Sending 0.24 MB of payload data to client #440 (simulated).
[INFO][09:57:48]: [Server #1127936] Selecting client #125 for training.
[INFO][09:57:48]: [Server #1127936] Sending the current model to client #125 (simulated).
[INFO][09:57:48]: [Client #223] Selected by the server.
[INFO][09:57:48]: [Client #223] Loading its data source...
[INFO][09:57:48]: [Client #223] Dataset size: 60000
[INFO][09:57:48]: [Client #223] Sampler: noniid
[INFO][09:57:48]: [Server #1127936] Sending 0.24 MB of payload data to client #125 (simulated).
[INFO][09:57:48]: [Client #125] Selected by the server.
[INFO][09:57:48]: [Client #125] Loading its data source...
[INFO][09:57:48]: [Client #125] Dataset size: 60000
[INFO][09:57:48]: [Client #125] Sampler: noniid
[INFO][09:57:48]: [Client #440] Selected by the server.
[INFO][09:57:48]: [Client #440] Loading its data source...
[INFO][09:57:48]: [Client #440] Dataset size: 60000
[INFO][09:57:48]: [Client #440] Sampler: noniid
[INFO][09:57:48]: [Client #125] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:57:48]: [Client #223] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:57:48]: [Client #440] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:57:48]: [93m[1m[Client #125] Started training in communication round #26.[0m
[INFO][09:57:48]: [93m[1m[Client #440] Started training in communication round #26.[0m
[INFO][09:57:48]: [93m[1m[Client #223] Started training in communication round #26.[0m
[INFO][09:57:50]: [Client #125] Loading the dataset.
[INFO][09:57:50]: [Client #440] Loading the dataset.
[INFO][09:57:50]: [Client #223] Loading the dataset.
[INFO][09:57:56]: [Client #125] Epoch: [1/5][0/10]	Loss: 0.045084
[INFO][09:57:56]: [Client #440] Epoch: [1/5][0/10]	Loss: 0.049899
[INFO][09:57:56]: [Client #125] Epoch: [2/5][0/10]	Loss: 0.022117
[INFO][09:57:56]: [Client #223] Epoch: [1/5][0/10]	Loss: 0.096035
[INFO][09:57:56]: [Client #440] Epoch: [2/5][0/10]	Loss: 0.032184
[INFO][09:57:56]: [Client #125] Epoch: [3/5][0/10]	Loss: 0.000312
[INFO][09:57:56]: [Client #223] Epoch: [2/5][0/10]	Loss: 0.028562
[INFO][09:57:56]: [Client #125] Epoch: [4/5][0/10]	Loss: 0.004624
[INFO][09:57:56]: [Client #440] Epoch: [3/5][0/10]	Loss: 0.001551
[INFO][09:57:56]: [Client #223] Epoch: [3/5][0/10]	Loss: 0.000101
[INFO][09:57:56]: [Client #440] Epoch: [4/5][0/10]	Loss: 0.000208
[INFO][09:57:56]: [Client #125] Epoch: [5/5][0/10]	Loss: 0.190223
[INFO][09:57:56]: [Client #125] Model saved to /data/ykang/plato/results/test/model/lenet5_125_1127979.pth.
[INFO][09:57:56]: [Client #223] Epoch: [4/5][0/10]	Loss: 0.000046
[INFO][09:57:57]: [Client #440] Epoch: [5/5][0/10]	Loss: 0.886529
[INFO][09:57:57]: [Client #440] Model saved to /data/ykang/plato/results/test/model/lenet5_440_1127978.pth.
[INFO][09:57:57]: [Client #223] Epoch: [5/5][0/10]	Loss: 0.000020
[INFO][09:57:57]: [Client #223] Model saved to /data/ykang/plato/results/test/model/lenet5_223_1127977.pth.
[INFO][09:57:57]: [Client #125] Loading a model from /data/ykang/plato/results/test/model/lenet5_125_1127979.pth.
[INFO][09:57:57]: [Client #125] Model trained.
[INFO][09:57:57]: [Client #125] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:57:57]: [Server #1127936] Received 0.24 MB of payload data from client #125 (simulated).
[INFO][09:57:57]: [Client #440] Loading a model from /data/ykang/plato/results/test/model/lenet5_440_1127978.pth.
[INFO][09:57:57]: [Client #440] Model trained.
[INFO][09:57:57]: [Client #440] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:57:57]: [Server #1127936] Received 0.24 MB of payload data from client #440 (simulated).
[INFO][09:57:57]: [Client #223] Loading a model from /data/ykang/plato/results/test/model/lenet5_223_1127977.pth.
[INFO][09:57:57]: [Client #223] Model trained.
[INFO][09:57:57]: [Client #223] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:57:57]: [Server #1127936] Received 0.24 MB of payload data from client #223 (simulated).
[INFO][09:57:57]: [Server #1127936] Selecting client #135 for training.
[INFO][09:57:57]: [Server #1127936] Sending the current model to client #135 (simulated).
[INFO][09:57:57]: [Server #1127936] Sending 0.24 MB of payload data to client #135 (simulated).
[INFO][09:57:57]: [Client #135] Selected by the server.
[INFO][09:57:57]: [Client #135] Loading its data source...
[INFO][09:57:57]: [Client #135] Dataset size: 60000
[INFO][09:57:57]: [Client #135] Sampler: noniid
[INFO][09:57:57]: [Client #135] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:57:57]: [93m[1m[Client #135] Started training in communication round #26.[0m
[INFO][09:58:00]: [Client #135] Loading the dataset.
[INFO][09:58:05]: [Client #135] Epoch: [1/5][0/10]	Loss: 0.082718
[INFO][09:58:05]: [Client #135] Epoch: [2/5][0/10]	Loss: 0.002281
[INFO][09:58:05]: [Client #135] Epoch: [3/5][0/10]	Loss: 0.000155
[INFO][09:58:05]: [Client #135] Epoch: [4/5][0/10]	Loss: 0.030885
[INFO][09:58:05]: [Client #135] Epoch: [5/5][0/10]	Loss: 0.000020
[INFO][09:58:05]: [Client #135] Model saved to /data/ykang/plato/results/test/model/lenet5_135_1127977.pth.
[INFO][09:58:06]: [Client #135] Loading a model from /data/ykang/plato/results/test/model/lenet5_135_1127977.pth.
[INFO][09:58:06]: [Client #135] Model trained.
[INFO][09:58:06]: [Client #135] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:58:06]: [Server #1127936] Received 0.24 MB of payload data from client #135 (simulated).
[INFO][09:58:06]: [Server #1127936] Adding client #206 to the list of clients for aggregation.
[INFO][09:58:06]: [Server #1127936] Adding client #329 to the list of clients for aggregation.
[INFO][09:58:06]: [Server #1127936] Adding client #98 to the list of clients for aggregation.
[INFO][09:58:06]: [Server #1127936] Adding client #59 to the list of clients for aggregation.
[INFO][09:58:06]: [Server #1127936] Adding client #37 to the list of clients for aggregation.
[INFO][09:58:06]: [Server #1127936] Adding client #332 to the list of clients for aggregation.
[INFO][09:58:06]: [Server #1127936] Adding client #195 to the list of clients for aggregation.
[INFO][09:58:06]: [Server #1127936] Adding client #440 to the list of clients for aggregation.
[INFO][09:58:06]: [Server #1127936] Adding client #223 to the list of clients for aggregation.
[INFO][09:58:06]: [Server #1127936] Adding client #449 to the list of clients for aggregation.
[INFO][09:58:06]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 330, 331, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01588493 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01321128 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01253345 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01593296 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.03249143 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01507792 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01239752 0.
 0.         0.02686353 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01167719 0.         0.         0.         0.
 0.         0.         0.         0.         0.00768532 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 3. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 1. 3. 0. 0. 0. 1. 0. 0. 1. 6. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 2. 0. 6. 0. 1. 2. 1. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 1. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.
 1. 0. 1. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 4. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0.
 0. 1. 1. 1. 0. 2. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 0. 1. 0. 0. 0. 6. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01588493 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01321128 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01253345 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01593296 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.03249143 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01507792 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01239752 0.
 0.         0.02686353 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01167719 0.         0.         0.         0.
 0.         0.         0.         0.         0.00768532 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][09:58:08]: [Server #1127936] Global model accuracy: 93.62%

[INFO][09:58:08]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_26.pth.
[INFO][09:58:08]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_26.pth.
[INFO][09:58:08]: [93m[1m
[Server #1127936] Starting round 27/100.[0m
[0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002
 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002
 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.002
 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  4e-05  6e-10  6e-10
 6:  6.8876e+00  6.8875e+00  3e-05  4e-10  4e-10
 7:  6.8875e+00  6.8875e+00  3e-05  8e-10  3e-11
 8:  6.8875e+00  6.8875e+00  2e-05  8e-10  3e-11
 9:  6.8875e+00  6.8875e+00  1e-05  2e-09  7e-11
10:  6.8875e+00  6.8875e+00  1e-06  3e-09  1e-10
Optimal solution found.
The calculated probability is:  [6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 1.52368933e-04
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 1.25487655e-04 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 1.20070492e-04
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 1.52952663e-04 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 9.66574965e-01 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62229072e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 1.19038165e-04 6.62283507e-05 6.62283507e-05 7.66833823e-04
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62250857e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62269364e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05 6.62283507e-05 6.62283507e-05
 6.62283507e-05 6.62283507e-05]
current clients pool:  [INFO][09:58:09]: [Server #1127936] Selected clients: [206 271  39 437 476 406 146 272 174 285]
[INFO][09:58:09]: [Server #1127936] Selecting client #206 for training.
[INFO][09:58:09]: [Server #1127936] Sending the current model to client #206 (simulated).
[INFO][09:58:09]: [Server #1127936] Sending 0.24 MB of payload data to client #206 (simulated).
[INFO][09:58:09]: [Server #1127936] Selecting client #271 for training.
[INFO][09:58:09]: [Server #1127936] Sending the current model to client #271 (simulated).
[INFO][09:58:09]: [Server #1127936] Sending 0.24 MB of payload data to client #271 (simulated).
[INFO][09:58:09]: [Server #1127936] Selecting client #39 for training.
[INFO][09:58:09]: [Server #1127936] Sending the current model to client #39 (simulated).
[INFO][09:58:09]: [Client #206] Selected by the server.
[INFO][09:58:09]: [Client #206] Loading its data source...
[INFO][09:58:09]: [Client #206] Dataset size: 60000
[INFO][09:58:09]: [Client #206] Sampler: noniid
[INFO][09:58:09]: [Server #1127936] Sending 0.24 MB of payload data to client #39 (simulated).
[INFO][09:58:09]: [Client #271] Selected by the server.
[INFO][09:58:09]: [Client #271] Loading its data source...
[INFO][09:58:09]: [Client #271] Dataset size: 60000
[INFO][09:58:09]: [Client #39] Selected by the server.
[INFO][09:58:09]: [Client #206] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:58:09]: [Client #271] Sampler: noniid
[INFO][09:58:09]: [Client #39] Loading its data source...
[INFO][09:58:09]: [Client #39] Dataset size: 60000
[INFO][09:58:09]: [Client #39] Sampler: noniid
[INFO][09:58:09]: [Client #271] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:58:09]: [Client #39] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:58:09]: [93m[1m[Client #39] Started training in communication round #27.[0m
[INFO][09:58:09]: [93m[1m[Client #271] Started training in communication round #27.[0m
[INFO][09:58:09]: [93m[1m[Client #206] Started training in communication round #27.[0m
[INFO][09:58:11]: [Client #206] Loading the dataset.
[INFO][09:58:11]: [Client #271] Loading the dataset.
[INFO][09:58:11]: [Client #39] Loading the dataset.
[INFO][09:58:17]: [Client #206] Epoch: [1/5][0/10]	Loss: 0.037239
[INFO][09:58:17]: [Client #39] Epoch: [1/5][0/10]	Loss: 0.072910
[INFO][09:58:17]: [Client #206] Epoch: [2/5][0/10]	Loss: 0.005724
[INFO][09:58:17]: [Client #271] Epoch: [1/5][0/10]	Loss: 0.094228
[INFO][09:58:17]: [Client #206] Epoch: [3/5][0/10]	Loss: 0.011054
[INFO][09:58:17]: [Client #39] Epoch: [2/5][0/10]	Loss: 0.008355
[INFO][09:58:17]: [Client #206] Epoch: [4/5][0/10]	Loss: 0.000077
[INFO][09:58:18]: [Client #271] Epoch: [2/5][0/10]	Loss: 0.000478
[INFO][09:58:18]: [Client #39] Epoch: [3/5][0/10]	Loss: 0.008849
[INFO][09:58:18]: [Client #206] Epoch: [5/5][0/10]	Loss: 0.261861
[INFO][09:58:18]: [Client #206] Model saved to /data/ykang/plato/results/test/model/lenet5_206_1127977.pth.
[INFO][09:58:18]: [Client #271] Epoch: [3/5][0/10]	Loss: 0.000181
[INFO][09:58:18]: [Client #39] Epoch: [4/5][0/10]	Loss: 0.002178
[INFO][09:58:18]: [Client #271] Epoch: [4/5][0/10]	Loss: 0.000003
[INFO][09:58:18]: [Client #39] Epoch: [5/5][0/10]	Loss: 0.530030
[INFO][09:58:18]: [Client #271] Epoch: [5/5][0/10]	Loss: 0.000005
[INFO][09:58:18]: [Client #39] Model saved to /data/ykang/plato/results/test/model/lenet5_39_1127979.pth.
[INFO][09:58:18]: [Client #271] Model saved to /data/ykang/plato/results/test/model/lenet5_271_1127978.pth.
[INFO][09:58:18]: [Client #206] Loading a model from /data/ykang/plato/results/test/model/lenet5_206_1127977.pth.
[INFO][09:58:18]: [Client #206] Model trained.
[INFO][09:58:18]: [Client #206] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:58:18]: [Server #1127936] Received 0.24 MB of payload data from client #206 (simulated).
[INFO][09:58:19]: [Client #39] Loading a model from /data/ykang/plato/results/test/model/lenet5_39_1127979.pth.
[INFO][09:58:19]: [Client #39] Model trained.
[INFO][09:58:19]: [Client #39] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:58:19]: [Server #1127936] Received 0.24 MB of payload data from client #39 (simulated).
[INFO][09:58:19]: [Client #271] Loading a model from /data/ykang/plato/results/test/model/lenet5_271_1127978.pth.
[INFO][09:58:19]: [Client #271] Model trained.
[INFO][09:58:19]: [Client #271] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:58:19]: [Server #1127936] Received 0.24 MB of payload data from client #271 (simulated).
[INFO][09:58:19]: [Server #1127936] Selecting client #437 for training.
[INFO][09:58:19]: [Server #1127936] Sending the current model to client #437 (simulated).
[INFO][09:58:19]: [Server #1127936] Sending 0.24 MB of payload data to client #437 (simulated).
[INFO][09:58:19]: [Server #1127936] Selecting client #476 for training.
[INFO][09:58:19]: [Server #1127936] Sending the current model to client #476 (simulated).
[INFO][09:58:19]: [Server #1127936] Sending 0.24 MB of payload data to client #476 (simulated).
[INFO][09:58:19]: [Server #1127936] Selecting client #406 for training.
[INFO][09:58:19]: [Server #1127936] Sending the current model to client #406 (simulated).
[INFO][09:58:19]: [Client #437] Selected by the server.
[INFO][09:58:19]: [Client #437] Loading its data source...
[INFO][09:58:19]: [Client #437] Dataset size: 60000
[INFO][09:58:19]: [Client #437] Sampler: noniid
[INFO][09:58:19]: [Server #1127936] Sending 0.24 MB of payload data to client #406 (simulated).
[INFO][09:58:19]: [Client #406] Selected by the server.
[INFO][09:58:19]: [Client #476] Selected by the server.
[INFO][09:58:19]: [Client #406] Loading its data source...
[INFO][09:58:19]: [Client #476] Loading its data source...
[INFO][09:58:19]: [Client #406] Dataset size: 60000
[INFO][09:58:19]: [Client #406] Sampler: noniid
[INFO][09:58:19]: [Client #476] Dataset size: 60000
[INFO][09:58:19]: [Client #476] Sampler: noniid
[INFO][09:58:19]: [Client #437] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:58:19]: [Client #476] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:58:19]: [93m[1m[Client #437] Started training in communication round #27.[0m
[INFO][09:58:19]: [Client #406] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:58:19]: [93m[1m[Client #406] Started training in communication round #27.[0m
[INFO][09:58:19]: [93m[1m[Client #476] Started training in communication round #27.[0m
[INFO][09:58:21]: [Client #437] Loading the dataset.
[INFO][09:58:21]: [Client #406] Loading the dataset.
[INFO][09:58:21]: [Client #476] Loading the dataset.
[INFO][09:58:27]: [Client #437] Epoch: [1/5][0/10]	Loss: 0.003167
[INFO][09:58:27]: [Client #406] Epoch: [1/5][0/10]	Loss: 0.050344
[INFO][09:58:27]: [Client #437] Epoch: [2/5][0/10]	Loss: 0.001078
[INFO][09:58:27]: [Client #476] Epoch: [1/5][0/10]	Loss: 0.325078
[INFO][09:58:27]: [Client #406] Epoch: [2/5][0/10]	Loss: 0.023703
[INFO][09:58:27]: [Client #437] Epoch: [3/5][0/10]	Loss: 0.000947
[INFO][09:58:27]: [Client #476] Epoch: [2/5][0/10]	Loss: 0.000107
[INFO][09:58:27]: [Client #406] Epoch: [3/5][0/10]	Loss: 0.002537
[INFO][09:58:27]: [Client #437] Epoch: [4/5][0/10]	Loss: 0.000208
[INFO][09:58:27]: [Client #476] Epoch: [3/5][0/10]	Loss: 0.000014
[INFO][09:58:27]: [Client #406] Epoch: [4/5][0/10]	Loss: 0.002061
[INFO][09:58:27]: [Client #437] Epoch: [5/5][0/10]	Loss: 0.001522
[INFO][09:58:27]: [Client #406] Epoch: [5/5][0/10]	Loss: 0.001478
[INFO][09:58:27]: [Client #476] Epoch: [4/5][0/10]	Loss: 0.000473
[INFO][09:58:27]: [Client #437] Model saved to /data/ykang/plato/results/test/model/lenet5_437_1127977.pth.
[INFO][09:58:27]: [Client #406] Model saved to /data/ykang/plato/results/test/model/lenet5_406_1127979.pth.
[INFO][09:58:28]: [Client #476] Epoch: [5/5][0/10]	Loss: 0.001049
[INFO][09:58:28]: [Client #476] Model saved to /data/ykang/plato/results/test/model/lenet5_476_1127978.pth.
[INFO][09:58:28]: [Client #437] Loading a model from /data/ykang/plato/results/test/model/lenet5_437_1127977.pth.
[INFO][09:58:28]: [Client #437] Model trained.
[INFO][09:58:28]: [Client #437] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:58:28]: [Server #1127936] Received 0.24 MB of payload data from client #437 (simulated).
[INFO][09:58:28]: [Client #406] Loading a model from /data/ykang/plato/results/test/model/lenet5_406_1127979.pth.
[INFO][09:58:28]: [Client #406] Model trained.
[INFO][09:58:28]: [Client #406] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:58:28]: [Server #1127936] Received 0.24 MB of payload data from client #406 (simulated).
[INFO][09:58:28]: [Client #476] Loading a model from /data/ykang/plato/results/test/model/lenet5_476_1127978.pth.
[INFO][09:58:28]: [Client #476] Model trained.
[INFO][09:58:28]: [Client #476] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:58:28]: [Server #1127936] Received 0.24 MB of payload data from client #476 (simulated).
[INFO][09:58:28]: [Server #1127936] Selecting client #146 for training.
[INFO][09:58:28]: [Server #1127936] Sending the current model to client #146 (simulated).
[INFO][09:58:28]: [Server #1127936] Sending 0.24 MB of payload data to client #146 (simulated).
[INFO][09:58:28]: [Server #1127936] Selecting client #272 for training.
[INFO][09:58:28]: [Server #1127936] Sending the current model to client #272 (simulated).
[INFO][09:58:28]: [Server #1127936] Sending 0.24 MB of payload data to client #272 (simulated).
[INFO][09:58:28]: [Server #1127936] Selecting client #174 for training.
[INFO][09:58:28]: [Server #1127936] Sending the current model to client #174 (simulated).
[INFO][09:58:28]: [Client #146] Selected by the server.
[INFO][09:58:28]: [Client #146] Loading its data source...
[INFO][09:58:28]: [Client #146] Dataset size: 60000
[INFO][09:58:28]: [Client #146] Sampler: noniid
[INFO][09:58:28]: [Server #1127936] Sending 0.24 MB of payload data to client #174 (simulated).
[INFO][09:58:29]: [Client #272] Selected by the server.
[INFO][09:58:29]: [Client #272] Loading its data source...
[INFO][09:58:29]: [Client #272] Dataset size: 60000
[INFO][09:58:29]: [Client #272] Sampler: noniid
[INFO][09:58:29]: [Client #174] Selected by the server.
[INFO][09:58:29]: [Client #174] Loading its data source...
[INFO][09:58:29]: [Client #174] Dataset size: 60000
[INFO][09:58:29]: [Client #174] Sampler: noniid
[INFO][09:58:29]: [Client #146] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:58:29]: [Client #272] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:58:29]: [Client #174] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:58:29]: [93m[1m[Client #146] Started training in communication round #27.[0m
[INFO][09:58:29]: [93m[1m[Client #174] Started training in communication round #27.[0m
[INFO][09:58:29]: [93m[1m[Client #272] Started training in communication round #27.[0m
[INFO][09:58:31]: [Client #146] Loading the dataset.
[INFO][09:58:31]: [Client #174] Loading the dataset.
[INFO][09:58:31]: [Client #272] Loading the dataset.
[INFO][09:58:37]: [Client #272] Epoch: [1/5][0/10]	Loss: 0.038380
[INFO][09:58:37]: [Client #146] Epoch: [1/5][0/10]	Loss: 0.333232
[INFO][09:58:37]: [Client #174] Epoch: [1/5][0/10]	Loss: 0.094228
[INFO][09:58:37]: [Client #272] Epoch: [2/5][0/10]	Loss: 0.384117
[INFO][09:58:37]: [Client #174] Epoch: [2/5][0/10]	Loss: 0.000325
[INFO][09:58:37]: [Client #146] Epoch: [2/5][0/10]	Loss: 0.000818
[INFO][09:58:37]: [Client #272] Epoch: [3/5][0/10]	Loss: 0.004601
[INFO][09:58:37]: [Client #174] Epoch: [3/5][0/10]	Loss: 0.139203
[INFO][09:58:37]: [Client #146] Epoch: [3/5][0/10]	Loss: 0.005821
[INFO][09:58:37]: [Client #272] Epoch: [4/5][0/10]	Loss: 0.069731
[INFO][09:58:37]: [Client #174] Epoch: [4/5][0/10]	Loss: 0.076027
[INFO][09:58:37]: [Client #146] Epoch: [4/5][0/10]	Loss: 0.436340
[INFO][09:58:37]: [Client #272] Epoch: [5/5][0/10]	Loss: 0.372249
[INFO][09:58:37]: [Client #174] Epoch: [5/5][0/10]	Loss: 0.082943
[INFO][09:58:37]: [Client #146] Epoch: [5/5][0/10]	Loss: 0.159244
[INFO][09:58:37]: [Client #272] Model saved to /data/ykang/plato/results/test/model/lenet5_272_1127978.pth.
[INFO][09:58:37]: [Client #174] Model saved to /data/ykang/plato/results/test/model/lenet5_174_1127979.pth.
[INFO][09:58:37]: [Client #146] Model saved to /data/ykang/plato/results/test/model/lenet5_146_1127977.pth.
[INFO][09:58:38]: [Client #272] Loading a model from /data/ykang/plato/results/test/model/lenet5_272_1127978.pth.
[INFO][09:58:38]: [Client #272] Model trained.
[INFO][09:58:38]: [Client #272] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:58:38]: [Server #1127936] Received 0.24 MB of payload data from client #272 (simulated).
[INFO][09:58:38]: [Client #174] Loading a model from /data/ykang/plato/results/test/model/lenet5_174_1127979.pth.
[INFO][09:58:38]: [Client #174] Model trained.
[INFO][09:58:38]: [Client #174] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:58:38]: [Server #1127936] Received 0.24 MB of payload data from client #174 (simulated).
[INFO][09:58:38]: [Client #146] Loading a model from /data/ykang/plato/results/test/model/lenet5_146_1127977.pth.
[INFO][09:58:38]: [Client #146] Model trained.
[INFO][09:58:38]: [Client #146] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:58:38]: [Server #1127936] Received 0.24 MB of payload data from client #146 (simulated).
[INFO][09:58:38]: [Server #1127936] Selecting client #285 for training.
[INFO][09:58:38]: [Server #1127936] Sending the current model to client #285 (simulated).
[INFO][09:58:38]: [Server #1127936] Sending 0.24 MB of payload data to client #285 (simulated).
[INFO][09:58:38]: [Client #285] Selected by the server.
[INFO][09:58:38]: [Client #285] Loading its data source...
[INFO][09:58:38]: [Client #285] Dataset size: 60000
[INFO][09:58:38]: [Client #285] Sampler: noniid
[INFO][09:58:38]: [Client #285] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:58:38]: [93m[1m[Client #285] Started training in communication round #27.[0m
[INFO][09:58:40]: [Client #285] Loading the dataset.
[INFO][09:58:46]: [Client #285] Epoch: [1/5][0/10]	Loss: 0.094228
[INFO][09:58:46]: [Client #285] Epoch: [2/5][0/10]	Loss: 0.000633
[INFO][09:58:46]: [Client #285] Epoch: [3/5][0/10]	Loss: 0.159366
[INFO][09:58:46]: [Client #285] Epoch: [4/5][0/10]	Loss: 0.030010
[INFO][09:58:46]: [Client #285] Epoch: [5/5][0/10]	Loss: 0.052092
[INFO][09:58:46]: [Client #285] Model saved to /data/ykang/plato/results/test/model/lenet5_285_1127977.pth.
[INFO][09:58:47]: [Client #285] Loading a model from /data/ykang/plato/results/test/model/lenet5_285_1127977.pth.
[INFO][09:58:47]: [Client #285] Model trained.
[INFO][09:58:47]: [Client #285] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:58:47]: [Server #1127936] Received 0.24 MB of payload data from client #285 (simulated).
[INFO][09:58:47]: [Server #1127936] Adding client #244 to the list of clients for aggregation.
[INFO][09:58:47]: [Server #1127936] Adding client #269 to the list of clients for aggregation.
[INFO][09:58:47]: [Server #1127936] Adding client #349 to the list of clients for aggregation.
[INFO][09:58:47]: [Server #1127936] Adding client #135 to the list of clients for aggregation.
[INFO][09:58:47]: [Server #1127936] Adding client #125 to the list of clients for aggregation.
[INFO][09:58:47]: [Server #1127936] Adding client #39 to the list of clients for aggregation.
[INFO][09:58:47]: [Server #1127936] Adding client #476 to the list of clients for aggregation.
[INFO][09:58:47]: [Server #1127936] Adding client #146 to the list of clients for aggregation.
[INFO][09:58:47]: [Server #1127936] Adding client #285 to the list of clients for aggregation.
[INFO][09:58:47]: [Server #1127936] Adding client #272 to the list of clients for aggregation.
[INFO][09:58:47]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 126, 127, 128, 129, 130, 131, 132, 133, 134, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 245, 246, 247, 248, 249, 250, 251, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01142717 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01319549 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00745585 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.009799   0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.02309375 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.02494885 0.
 0.         0.02128872 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01377918 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01089635 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00691998 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 3. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 1. 3. 0. 0. 0. 1. 0. 0. 1. 6. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 2. 0. 6. 0. 1. 2. 1. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 1. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.
 1. 0. 1. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 4. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0.
 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0.
 0. 1. 1. 1. 0. 2. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 0. 1. 0. 0. 0. 6. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01142717 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01319549 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00745585 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.009799   0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.02309375 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.02494885 0.
 0.         0.02128872 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01377918 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01089635 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00691998 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][09:58:49]: [Server #1127936] Global model accuracy: 94.22%

[INFO][09:58:49]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_27.pth.
[INFO][09:58:49]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_27.pth.
[INFO][09:58:49]: [93m[1m
[Server #1127936] Starting round 28/100.[0m
[0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002
 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002
 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.002
 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  3e-05  5e-10  5e-10
 6:  6.8876e+00  6.8875e+00  3e-05  3e-10  3e-10
 7:  6.8875e+00  6.8875e+00  2e-05  6e-10  2e-11
 8:  6.8875e+00  6.8875e+00  2e-05  5e-10  1e-11
 9:  6.8875e+00  6.8875e+00  1e-05  1e-09  3e-11
10:  6.8875e+00  6.8875e+00  8e-07  2e-09  5e-11
Optimal solution found.
The calculated probability is:  [6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15012571e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 1.52153781e-04 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 9.30892042e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15022289e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 2.67598888e-03 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 9.67127378e-01 6.15049292e-05
 6.14921862e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.14995901e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 1.21525014e-04 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15035825e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05 6.15049292e-05 6.15049292e-05
 6.15049292e-05 6.15049292e-05]
current clients pool:  [INFO][09:58:49]: [Server #1127936] Selected clients: [269 116  24 175  20 335 322 234 374 247]
[INFO][09:58:49]: [Server #1127936] Selecting client #269 for training.
[INFO][09:58:49]: [Server #1127936] Sending the current model to client #269 (simulated).
[INFO][09:58:49]: [Server #1127936] Sending 0.24 MB of payload data to client #269 (simulated).
[INFO][09:58:49]: [Server #1127936] Selecting client #116 for training.
[INFO][09:58:49]: [Server #1127936] Sending the current model to client #116 (simulated).
[INFO][09:58:49]: [Server #1127936] Sending 0.24 MB of payload data to client #116 (simulated).
[INFO][09:58:49]: [Server #1127936] Selecting client #24 for training.
[INFO][09:58:49]: [Server #1127936] Sending the current model to client #24 (simulated).
[INFO][09:58:49]: [Client #269] Selected by the server.
[INFO][09:58:49]: [Client #269] Loading its data source...
[INFO][09:58:49]: [Client #269] Dataset size: 60000
[INFO][09:58:49]: [Client #269] Sampler: noniid
[INFO][09:58:49]: [Server #1127936] Sending 0.24 MB of payload data to client #24 (simulated).
[INFO][09:58:49]: [Client #116] Selected by the server.
[INFO][09:58:49]: [Client #116] Loading its data source...
[INFO][09:58:49]: [Client #24] Selected by the server.
[INFO][09:58:49]: [Client #116] Dataset size: 60000
[INFO][09:58:49]: [Client #24] Loading its data source...
[INFO][09:58:49]: [Client #116] Sampler: noniid
[INFO][09:58:49]: [Client #24] Dataset size: 60000
[INFO][09:58:49]: [Client #269] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:58:49]: [Client #24] Sampler: noniid
[INFO][09:58:49]: [Client #116] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:58:49]: [Client #24] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:58:49]: [93m[1m[Client #116] Started training in communication round #28.[0m
[INFO][09:58:49]: [93m[1m[Client #269] Started training in communication round #28.[0m
[INFO][09:58:50]: [93m[1m[Client #24] Started training in communication round #28.[0m
[INFO][09:58:52]: [Client #269] Loading the dataset.
[INFO][09:58:52]: [Client #24] Loading the dataset.
[INFO][09:58:52]: [Client #116] Loading the dataset.
[INFO][09:58:58]: [Client #116] Epoch: [1/5][0/10]	Loss: 0.350863
[INFO][09:58:58]: [Client #24] Epoch: [1/5][0/10]	Loss: 0.094357
[INFO][09:58:58]: [Client #269] Epoch: [1/5][0/10]	Loss: 0.060629
[INFO][09:58:58]: [Client #116] Epoch: [2/5][0/10]	Loss: 0.024894
[INFO][09:58:58]: [Client #269] Epoch: [2/5][0/10]	Loss: 0.000003
[INFO][09:58:58]: [Client #24] Epoch: [2/5][0/10]	Loss: 0.021783
[INFO][09:58:58]: [Client #116] Epoch: [3/5][0/10]	Loss: 0.000465
[INFO][09:58:58]: [Client #269] Epoch: [3/5][0/10]	Loss: 0.000002
[INFO][09:58:58]: [Client #24] Epoch: [3/5][0/10]	Loss: 0.026956
[INFO][09:58:58]: [Client #116] Epoch: [4/5][0/10]	Loss: 0.000162
[INFO][09:58:58]: [Client #269] Epoch: [4/5][0/10]	Loss: 0.000018
[INFO][09:58:58]: [Client #116] Epoch: [5/5][0/10]	Loss: 0.000046
[INFO][09:58:58]: [Client #24] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][09:58:58]: [Client #116] Model saved to /data/ykang/plato/results/test/model/lenet5_116_1127978.pth.
[INFO][09:58:58]: [Client #24] Epoch: [5/5][0/10]	Loss: 0.964865
[INFO][09:58:58]: [Client #269] Epoch: [5/5][0/10]	Loss: 0.001265
[INFO][09:58:58]: [Client #269] Model saved to /data/ykang/plato/results/test/model/lenet5_269_1127977.pth.
[INFO][09:58:58]: [Client #24] Model saved to /data/ykang/plato/results/test/model/lenet5_24_1127979.pth.
[INFO][09:58:59]: [Client #116] Loading a model from /data/ykang/plato/results/test/model/lenet5_116_1127978.pth.
[INFO][09:58:59]: [Client #116] Model trained.
[INFO][09:58:59]: [Client #116] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:58:59]: [Server #1127936] Received 0.24 MB of payload data from client #116 (simulated).
[INFO][09:58:59]: [Client #269] Loading a model from /data/ykang/plato/results/test/model/lenet5_269_1127977.pth.
[INFO][09:58:59]: [Client #269] Model trained.
[INFO][09:58:59]: [Client #269] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:58:59]: [Server #1127936] Received 0.24 MB of payload data from client #269 (simulated).
[INFO][09:58:59]: [Client #24] Loading a model from /data/ykang/plato/results/test/model/lenet5_24_1127979.pth.
[INFO][09:58:59]: [Client #24] Model trained.
[INFO][09:58:59]: [Client #24] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:58:59]: [Server #1127936] Received 0.24 MB of payload data from client #24 (simulated).
[INFO][09:58:59]: [Server #1127936] Selecting client #175 for training.
[INFO][09:58:59]: [Server #1127936] Sending the current model to client #175 (simulated).
[INFO][09:58:59]: [Server #1127936] Sending 0.24 MB of payload data to client #175 (simulated).
[INFO][09:58:59]: [Server #1127936] Selecting client #20 for training.
[INFO][09:58:59]: [Server #1127936] Sending the current model to client #20 (simulated).
[INFO][09:58:59]: [Server #1127936] Sending 0.24 MB of payload data to client #20 (simulated).
[INFO][09:58:59]: [Server #1127936] Selecting client #335 for training.
[INFO][09:58:59]: [Server #1127936] Sending the current model to client #335 (simulated).
[INFO][09:58:59]: [Client #175] Selected by the server.
[INFO][09:58:59]: [Client #175] Loading its data source...
[INFO][09:58:59]: [Client #175] Dataset size: 60000
[INFO][09:58:59]: [Client #175] Sampler: noniid
[INFO][09:58:59]: [Server #1127936] Sending 0.24 MB of payload data to client #335 (simulated).
[INFO][09:58:59]: [Client #20] Selected by the server.
[INFO][09:58:59]: [Client #335] Selected by the server.
[INFO][09:58:59]: [Client #20] Loading its data source...
[INFO][09:58:59]: [Client #20] Dataset size: 60000
[INFO][09:58:59]: [Client #335] Loading its data source...
[INFO][09:58:59]: [Client #20] Sampler: noniid
[INFO][09:58:59]: [Client #335] Dataset size: 60000
[INFO][09:58:59]: [Client #335] Sampler: noniid
[INFO][09:58:59]: [Client #175] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:58:59]: [Client #335] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:58:59]: [Client #20] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:58:59]: [93m[1m[Client #175] Started training in communication round #28.[0m
[INFO][09:58:59]: [93m[1m[Client #20] Started training in communication round #28.[0m
[INFO][09:58:59]: [93m[1m[Client #335] Started training in communication round #28.[0m
[INFO][09:59:01]: [Client #20] Loading the dataset.
[INFO][09:59:01]: [Client #175] Loading the dataset.
[INFO][09:59:01]: [Client #335] Loading the dataset.
[INFO][09:59:08]: [Client #175] Epoch: [1/5][0/10]	Loss: 0.050033
[INFO][09:59:08]: [Client #20] Epoch: [1/5][0/10]	Loss: 0.030940
[INFO][09:59:08]: [Client #175] Epoch: [2/5][0/10]	Loss: 0.006137
[INFO][09:59:08]: [Client #335] Epoch: [1/5][0/10]	Loss: 0.021614
[INFO][09:59:08]: [Client #335] Epoch: [2/5][0/10]	Loss: 0.007181
[INFO][09:59:08]: [Client #20] Epoch: [2/5][0/10]	Loss: 0.003032
[INFO][09:59:08]: [Client #175] Epoch: [3/5][0/10]	Loss: 0.000036
[INFO][09:59:08]: [Client #335] Epoch: [3/5][0/10]	Loss: 0.001000
[INFO][09:59:08]: [Client #335] Epoch: [4/5][0/10]	Loss: 0.085587
[INFO][09:59:08]: [Client #175] Epoch: [4/5][0/10]	Loss: 0.000056
[INFO][09:59:08]: [Client #20] Epoch: [3/5][0/10]	Loss: 0.003531
[INFO][09:59:08]: [Client #175] Epoch: [5/5][0/10]	Loss: 0.001176
[INFO][09:59:08]: [Client #20] Epoch: [4/5][0/10]	Loss: 0.004439
[INFO][09:59:08]: [Client #335] Epoch: [5/5][0/10]	Loss: 0.000428
[INFO][09:59:08]: [Client #175] Model saved to /data/ykang/plato/results/test/model/lenet5_175_1127977.pth.
[INFO][09:59:08]: [Client #335] Model saved to /data/ykang/plato/results/test/model/lenet5_335_1127979.pth.
[INFO][09:59:08]: [Client #20] Epoch: [5/5][0/10]	Loss: 0.004070
[INFO][09:59:08]: [Client #20] Model saved to /data/ykang/plato/results/test/model/lenet5_20_1127978.pth.
[INFO][09:59:10]: [Client #175] Loading a model from /data/ykang/plato/results/test/model/lenet5_175_1127977.pth.
[INFO][09:59:10]: [Client #175] Model trained.
[INFO][09:59:10]: [Client #175] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:59:10]: [Server #1127936] Received 0.24 MB of payload data from client #175 (simulated).
[INFO][09:59:10]: [Client #335] Loading a model from /data/ykang/plato/results/test/model/lenet5_335_1127979.pth.
[INFO][09:59:10]: [Client #335] Model trained.
[INFO][09:59:10]: [Client #335] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:59:10]: [Server #1127936] Received 0.24 MB of payload data from client #335 (simulated).
[INFO][09:59:10]: [Client #20] Loading a model from /data/ykang/plato/results/test/model/lenet5_20_1127978.pth.
[INFO][09:59:10]: [Client #20] Model trained.
[INFO][09:59:10]: [Client #20] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:59:10]: [Server #1127936] Received 0.24 MB of payload data from client #20 (simulated).
[INFO][09:59:10]: [Server #1127936] Selecting client #322 for training.
[INFO][09:59:10]: [Server #1127936] Sending the current model to client #322 (simulated).
[INFO][09:59:10]: [Server #1127936] Sending 0.24 MB of payload data to client #322 (simulated).
[INFO][09:59:10]: [Server #1127936] Selecting client #234 for training.
[INFO][09:59:10]: [Server #1127936] Sending the current model to client #234 (simulated).
[INFO][09:59:10]: [Server #1127936] Sending 0.24 MB of payload data to client #234 (simulated).
[INFO][09:59:10]: [Server #1127936] Selecting client #374 for training.
[INFO][09:59:10]: [Server #1127936] Sending the current model to client #374 (simulated).
[INFO][09:59:10]: [Client #322] Selected by the server.
[INFO][09:59:10]: [Client #322] Loading its data source...
[INFO][09:59:10]: [Client #322] Dataset size: 60000
[INFO][09:59:10]: [Client #322] Sampler: noniid
[INFO][09:59:10]: [Server #1127936] Sending 0.24 MB of payload data to client #374 (simulated).
[INFO][09:59:10]: [Client #234] Selected by the server.
[INFO][09:59:10]: [Client #234] Loading its data source...
[INFO][09:59:10]: [Client #234] Dataset size: 60000
[INFO][09:59:10]: [Client #374] Selected by the server.
[INFO][09:59:10]: [Client #234] Sampler: noniid
[INFO][09:59:10]: [Client #374] Loading its data source...
[INFO][09:59:10]: [Client #374] Dataset size: 60000
[INFO][09:59:10]: [Client #374] Sampler: noniid
[INFO][09:59:10]: [Client #322] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:59:10]: [Client #374] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:59:10]: [Client #234] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:59:10]: [93m[1m[Client #322] Started training in communication round #28.[0m
[INFO][09:59:10]: [93m[1m[Client #234] Started training in communication round #28.[0m
[INFO][09:59:10]: [93m[1m[Client #374] Started training in communication round #28.[0m
[INFO][09:59:12]: [Client #322] Loading the dataset.
[INFO][09:59:12]: [Client #374] Loading the dataset.
[INFO][09:59:12]: [Client #234] Loading the dataset.
[INFO][09:59:18]: [Client #374] Epoch: [1/5][0/10]	Loss: 0.068975
[INFO][09:59:18]: [Client #322] Epoch: [1/5][0/10]	Loss: 0.120746
[INFO][09:59:18]: [Client #234] Epoch: [1/5][0/10]	Loss: 0.010578
[INFO][09:59:18]: [Client #374] Epoch: [2/5][0/10]	Loss: 0.010471
[INFO][09:59:19]: [Client #234] Epoch: [2/5][0/10]	Loss: 0.002510
[INFO][09:59:19]: [Client #322] Epoch: [2/5][0/10]	Loss: 0.000089
[INFO][09:59:19]: [Client #374] Epoch: [3/5][0/10]	Loss: 0.102325
[INFO][09:59:19]: [Client #322] Epoch: [3/5][0/10]	Loss: 0.017429
[INFO][09:59:19]: [Client #234] Epoch: [3/5][0/10]	Loss: 0.000071
[INFO][09:59:19]: [Client #374] Epoch: [4/5][0/10]	Loss: 0.000040
[INFO][09:59:19]: [Client #234] Epoch: [4/5][0/10]	Loss: 0.000353
[INFO][09:59:19]: [Client #374] Epoch: [5/5][0/10]	Loss: 0.573240
[INFO][09:59:19]: [Client #322] Epoch: [4/5][0/10]	Loss: 0.026145
[INFO][09:59:19]: [Client #374] Model saved to /data/ykang/plato/results/test/model/lenet5_374_1127979.pth.
[INFO][09:59:19]: [Client #234] Epoch: [5/5][0/10]	Loss: 0.065256
[INFO][09:59:19]: [Client #322] Epoch: [5/5][0/10]	Loss: 0.000135
[INFO][09:59:19]: [Client #234] Model saved to /data/ykang/plato/results/test/model/lenet5_234_1127978.pth.
[INFO][09:59:19]: [Client #322] Model saved to /data/ykang/plato/results/test/model/lenet5_322_1127977.pth.
[INFO][09:59:20]: [Client #374] Loading a model from /data/ykang/plato/results/test/model/lenet5_374_1127979.pth.
[INFO][09:59:20]: [Client #374] Model trained.
[INFO][09:59:20]: [Client #374] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:59:20]: [Server #1127936] Received 0.24 MB of payload data from client #374 (simulated).
[INFO][09:59:20]: [Client #234] Loading a model from /data/ykang/plato/results/test/model/lenet5_234_1127978.pth.
[INFO][09:59:20]: [Client #234] Model trained.
[INFO][09:59:20]: [Client #234] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:59:20]: [Server #1127936] Received 0.24 MB of payload data from client #234 (simulated).
[INFO][09:59:20]: [Client #322] Loading a model from /data/ykang/plato/results/test/model/lenet5_322_1127977.pth.
[INFO][09:59:20]: [Client #322] Model trained.
[INFO][09:59:20]: [Client #322] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:59:20]: [Server #1127936] Received 0.24 MB of payload data from client #322 (simulated).
[INFO][09:59:20]: [Server #1127936] Selecting client #247 for training.
[INFO][09:59:20]: [Server #1127936] Sending the current model to client #247 (simulated).
[INFO][09:59:20]: [Server #1127936] Sending 0.24 MB of payload data to client #247 (simulated).
[INFO][09:59:20]: [Client #247] Selected by the server.
[INFO][09:59:20]: [Client #247] Loading its data source...
[INFO][09:59:20]: [Client #247] Dataset size: 60000
[INFO][09:59:20]: [Client #247] Sampler: noniid
[INFO][09:59:20]: [Client #247] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:59:20]: [93m[1m[Client #247] Started training in communication round #28.[0m
[INFO][09:59:22]: [Client #247] Loading the dataset.
[INFO][09:59:27]: [Client #247] Epoch: [1/5][0/10]	Loss: 0.011017
[INFO][09:59:28]: [Client #247] Epoch: [2/5][0/10]	Loss: 0.010601
[INFO][09:59:28]: [Client #247] Epoch: [3/5][0/10]	Loss: 0.000610
[INFO][09:59:28]: [Client #247] Epoch: [4/5][0/10]	Loss: 0.000321
[INFO][09:59:28]: [Client #247] Epoch: [5/5][0/10]	Loss: 0.010753
[INFO][09:59:28]: [Client #247] Model saved to /data/ykang/plato/results/test/model/lenet5_247_1127977.pth.
[INFO][09:59:28]: [Client #247] Loading a model from /data/ykang/plato/results/test/model/lenet5_247_1127977.pth.
[INFO][09:59:28]: [Client #247] Model trained.
[INFO][09:59:28]: [Client #247] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:59:28]: [Server #1127936] Received 0.24 MB of payload data from client #247 (simulated).
[INFO][09:59:28]: [Server #1127936] Adding client #174 to the list of clients for aggregation.
[INFO][09:59:28]: [Server #1127936] Adding client #271 to the list of clients for aggregation.
[INFO][09:59:28]: [Server #1127936] Adding client #206 to the list of clients for aggregation.
[INFO][09:59:28]: [Server #1127936] Adding client #41 to the list of clients for aggregation.
[INFO][09:59:28]: [Server #1127936] Adding client #437 to the list of clients for aggregation.
[INFO][09:59:28]: [Server #1127936] Adding client #247 to the list of clients for aggregation.
[INFO][09:59:28]: [Server #1127936] Adding client #20 to the list of clients for aggregation.
[INFO][09:59:28]: [Server #1127936] Adding client #234 to the list of clients for aggregation.
[INFO][09:59:28]: [Server #1127936] Adding client #269 to the list of clients for aggregation.
[INFO][09:59:28]: [Server #1127936] Adding client #175 to the list of clients for aggregation.
[INFO][09:59:28]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.03054786 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.02660067 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.02082005
 0.01118966 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01800348 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01178376
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01201562 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01614405 0.
 0.02058288 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01549849 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 3. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 1. 3. 0. 0. 0. 1. 0. 0. 1. 6. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 2. 0. 6. 0. 1. 2. 1. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 1. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.
 1. 0. 1. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 4. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0.
 0. 1. 1. 1. 0. 2. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 0. 1. 0. 0. 0. 6. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1.
 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.03054786 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.02660067 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.02082005
 0.01118966 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01800348 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01178376
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01201562 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01614405 0.
 0.02058288 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01549849 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][09:59:31]: [Server #1127936] Global model accuracy: 93.14%

[INFO][09:59:31]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_28.pth.
[INFO][09:59:31]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_28.pth.
[INFO][09:59:31]: [93m[1m
[Server #1127936] Starting round 29/100.[0m
[0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002
 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.002
 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8871e+00  5e-04  8e-09  8e-09
 5:  6.8876e+00  6.8874e+00  2e-04  3e-09  3e-09
 6:  6.8875e+00  6.8874e+00  2e-04  5e-09  8e-10
 7:  6.8875e+00  6.8874e+00  1e-04  5e-09  9e-10
 8:  6.8875e+00  6.8874e+00  8e-05  6e-08  1e-08
 9:  6.8874e+00  6.8874e+00  4e-05  5e-08  9e-09
10:  6.8874e+00  6.8874e+00  3e-06  3e-08  6e-09
Optimal solution found.
The calculated probability is:  [4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00767079e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 9.80358301e-01 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 5.20348671e-05 4.00804111e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 5.00816541e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00803486e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00803233e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00797904e-05
 4.00809851e-05 5.18654421e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.84469845e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05 4.00809851e-05 4.00809851e-05
 4.00809851e-05 4.00809851e-05]
current clients pool:  [INFO][09:59:31]: [Server #1127936] Selected clients: [ 41 227   8 438 328 414 477  27 121  34]
[INFO][09:59:31]: [Server #1127936] Selecting client #41 for training.
[INFO][09:59:31]: [Server #1127936] Sending the current model to client #41 (simulated).
[INFO][09:59:31]: [Server #1127936] Sending 0.24 MB of payload data to client #41 (simulated).
[INFO][09:59:31]: [Server #1127936] Selecting client #227 for training.
[INFO][09:59:31]: [Server #1127936] Sending the current model to client #227 (simulated).
[INFO][09:59:31]: [Server #1127936] Sending 0.24 MB of payload data to client #227 (simulated).
[INFO][09:59:31]: [Server #1127936] Selecting client #8 for training.
[INFO][09:59:31]: [Server #1127936] Sending the current model to client #8 (simulated).
[INFO][09:59:31]: [Client #41] Selected by the server.
[INFO][09:59:31]: [Client #41] Loading its data source...
[INFO][09:59:31]: [Client #41] Dataset size: 60000
[INFO][09:59:31]: [Client #41] Sampler: noniid
[INFO][09:59:31]: [Server #1127936] Sending 0.24 MB of payload data to client #8 (simulated).
[INFO][09:59:31]: [Client #227] Selected by the server.
[INFO][09:59:31]: [Client #227] Loading its data source...
[INFO][09:59:31]: [Client #227] Dataset size: 60000
[INFO][09:59:31]: [Client #227] Sampler: noniid
[INFO][09:59:31]: [Client #8] Selected by the server.
[INFO][09:59:31]: [Client #41] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:59:31]: [Client #8] Loading its data source...
[INFO][09:59:31]: [Client #8] Dataset size: 60000
[INFO][09:59:31]: [Client #8] Sampler: noniid
[INFO][09:59:31]: [Client #227] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:59:31]: [Client #8] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:59:31]: [93m[1m[Client #41] Started training in communication round #29.[0m
[INFO][09:59:31]: [93m[1m[Client #227] Started training in communication round #29.[0m
[INFO][09:59:31]: [93m[1m[Client #8] Started training in communication round #29.[0m
[INFO][09:59:33]: [Client #227] Loading the dataset.
[INFO][09:59:33]: [Client #41] Loading the dataset.
[INFO][09:59:33]: [Client #8] Loading the dataset.
[INFO][09:59:40]: [Client #227] Epoch: [1/5][0/10]	Loss: 0.195523
[INFO][09:59:40]: [Client #41] Epoch: [1/5][0/10]	Loss: 0.180757
[INFO][09:59:40]: [Client #8] Epoch: [1/5][0/10]	Loss: 0.003460
[INFO][09:59:40]: [Client #227] Epoch: [2/5][0/10]	Loss: 0.002071
[INFO][09:59:40]: [Client #41] Epoch: [2/5][0/10]	Loss: 0.003300
[INFO][09:59:40]: [Client #227] Epoch: [3/5][0/10]	Loss: 0.000208
[INFO][09:59:40]: [Client #8] Epoch: [2/5][0/10]	Loss: 0.001990
[INFO][09:59:40]: [Client #41] Epoch: [3/5][0/10]	Loss: 0.001722
[INFO][09:59:40]: [Client #8] Epoch: [3/5][0/10]	Loss: 0.000165
[INFO][09:59:40]: [Client #227] Epoch: [4/5][0/10]	Loss: 0.011629
[INFO][09:59:40]: [Client #41] Epoch: [4/5][0/10]	Loss: 0.463836
[INFO][09:59:40]: [Client #8] Epoch: [4/5][0/10]	Loss: 0.000967
[INFO][09:59:40]: [Client #227] Epoch: [5/5][0/10]	Loss: 0.003691
[INFO][09:59:40]: [Client #227] Model saved to /data/ykang/plato/results/test/model/lenet5_227_1127978.pth.
[INFO][09:59:40]: [Client #41] Epoch: [5/5][0/10]	Loss: 0.054313
[INFO][09:59:40]: [Client #41] Model saved to /data/ykang/plato/results/test/model/lenet5_41_1127977.pth.
[INFO][09:59:40]: [Client #8] Epoch: [5/5][0/10]	Loss: 0.001045
[INFO][09:59:40]: [Client #8] Model saved to /data/ykang/plato/results/test/model/lenet5_8_1127979.pth.
[INFO][09:59:41]: [Client #227] Loading a model from /data/ykang/plato/results/test/model/lenet5_227_1127978.pth.
[INFO][09:59:41]: [Client #227] Model trained.
[INFO][09:59:41]: [Client #227] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:59:41]: [Server #1127936] Received 0.24 MB of payload data from client #227 (simulated).
[INFO][09:59:41]: [Client #41] Loading a model from /data/ykang/plato/results/test/model/lenet5_41_1127977.pth.
[INFO][09:59:41]: [Client #41] Model trained.
[INFO][09:59:41]: [Client #41] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:59:41]: [Server #1127936] Received 0.24 MB of payload data from client #41 (simulated).
[INFO][09:59:41]: [Client #8] Loading a model from /data/ykang/plato/results/test/model/lenet5_8_1127979.pth.
[INFO][09:59:41]: [Client #8] Model trained.
[INFO][09:59:41]: [Client #8] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:59:41]: [Server #1127936] Received 0.24 MB of payload data from client #8 (simulated).
[INFO][09:59:41]: [Server #1127936] Selecting client #438 for training.
[INFO][09:59:41]: [Server #1127936] Sending the current model to client #438 (simulated).
[INFO][09:59:41]: [Server #1127936] Sending 0.24 MB of payload data to client #438 (simulated).
[INFO][09:59:41]: [Server #1127936] Selecting client #328 for training.
[INFO][09:59:41]: [Server #1127936] Sending the current model to client #328 (simulated).
[INFO][09:59:41]: [Server #1127936] Sending 0.24 MB of payload data to client #328 (simulated).
[INFO][09:59:41]: [Server #1127936] Selecting client #414 for training.
[INFO][09:59:41]: [Server #1127936] Sending the current model to client #414 (simulated).
[INFO][09:59:41]: [Client #438] Selected by the server.
[INFO][09:59:41]: [Client #438] Loading its data source...
[INFO][09:59:41]: [Client #438] Dataset size: 60000
[INFO][09:59:41]: [Client #438] Sampler: noniid
[INFO][09:59:41]: [Server #1127936] Sending 0.24 MB of payload data to client #414 (simulated).
[INFO][09:59:41]: [Client #328] Selected by the server.
[INFO][09:59:41]: [Client #414] Selected by the server.
[INFO][09:59:41]: [Client #328] Loading its data source...
[INFO][09:59:41]: [Client #414] Loading its data source...
[INFO][09:59:41]: [Client #328] Dataset size: 60000
[INFO][09:59:41]: [Client #414] Dataset size: 60000
[INFO][09:59:41]: [Client #328] Sampler: noniid
[INFO][09:59:41]: [Client #414] Sampler: noniid
[INFO][09:59:41]: [Client #438] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:59:41]: [Client #414] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:59:41]: [Client #328] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:59:41]: [93m[1m[Client #438] Started training in communication round #29.[0m
[INFO][09:59:41]: [93m[1m[Client #414] Started training in communication round #29.[0m
[INFO][09:59:41]: [93m[1m[Client #328] Started training in communication round #29.[0m
[INFO][09:59:43]: [Client #438] Loading the dataset.
[INFO][09:59:43]: [Client #328] Loading the dataset.
[INFO][09:59:43]: [Client #414] Loading the dataset.
[INFO][09:59:49]: [Client #438] Epoch: [1/5][0/10]	Loss: 0.017361
[INFO][09:59:49]: [Client #414] Epoch: [1/5][0/10]	Loss: 0.018899
[INFO][09:59:49]: [Client #438] Epoch: [2/5][0/10]	Loss: 0.029173
[INFO][09:59:49]: [Client #328] Epoch: [1/5][0/10]	Loss: 0.284908
[INFO][09:59:49]: [Client #414] Epoch: [2/5][0/10]	Loss: 0.016403
[INFO][09:59:49]: [Client #438] Epoch: [3/5][0/10]	Loss: 0.100185
[INFO][09:59:49]: [Client #328] Epoch: [2/5][0/10]	Loss: 0.017898
[INFO][09:59:50]: [Client #414] Epoch: [3/5][0/10]	Loss: 0.000062
[INFO][09:59:50]: [Client #438] Epoch: [4/5][0/10]	Loss: 0.029862
[INFO][09:59:50]: [Client #328] Epoch: [3/5][0/10]	Loss: 0.342705
[INFO][09:59:50]: [Client #414] Epoch: [4/5][0/10]	Loss: 0.107203
[INFO][09:59:50]: [Client #328] Epoch: [4/5][0/10]	Loss: 0.008429
[INFO][09:59:50]: [Client #438] Epoch: [5/5][0/10]	Loss: 0.006795
[INFO][09:59:50]: [Client #328] Epoch: [5/5][0/10]	Loss: 0.041438
[INFO][09:59:50]: [Client #414] Epoch: [5/5][0/10]	Loss: 0.000008
[INFO][09:59:50]: [Client #438] Model saved to /data/ykang/plato/results/test/model/lenet5_438_1127977.pth.
[INFO][09:59:50]: [Client #328] Model saved to /data/ykang/plato/results/test/model/lenet5_328_1127978.pth.
[INFO][09:59:50]: [Client #414] Model saved to /data/ykang/plato/results/test/model/lenet5_414_1127979.pth.
[INFO][09:59:51]: [Client #438] Loading a model from /data/ykang/plato/results/test/model/lenet5_438_1127977.pth.
[INFO][09:59:51]: [Client #438] Model trained.
[INFO][09:59:51]: [Client #438] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:59:51]: [Server #1127936] Received 0.24 MB of payload data from client #438 (simulated).
[INFO][09:59:51]: [Client #414] Loading a model from /data/ykang/plato/results/test/model/lenet5_414_1127979.pth.
[INFO][09:59:51]: [Client #414] Model trained.
[INFO][09:59:51]: [Client #414] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:59:51]: [Server #1127936] Received 0.24 MB of payload data from client #414 (simulated).
[INFO][09:59:51]: [Client #328] Loading a model from /data/ykang/plato/results/test/model/lenet5_328_1127978.pth.
[INFO][09:59:51]: [Client #328] Model trained.
[INFO][09:59:51]: [Client #328] Sent 0.24 MB of payload data to the server (simulated).
[INFO][09:59:51]: [Server #1127936] Received 0.24 MB of payload data from client #328 (simulated).
[INFO][09:59:51]: [Server #1127936] Selecting client #477 for training.
[INFO][09:59:51]: [Server #1127936] Sending the current model to client #477 (simulated).
[INFO][09:59:51]: [Server #1127936] Sending 0.24 MB of payload data to client #477 (simulated).
[INFO][09:59:51]: [Server #1127936] Selecting client #27 for training.
[INFO][09:59:51]: [Server #1127936] Sending the current model to client #27 (simulated).
[INFO][09:59:51]: [Server #1127936] Sending 0.24 MB of payload data to client #27 (simulated).
[INFO][09:59:51]: [Server #1127936] Selecting client #121 for training.
[INFO][09:59:51]: [Server #1127936] Sending the current model to client #121 (simulated).
[INFO][09:59:51]: [Client #477] Selected by the server.
[INFO][09:59:51]: [Client #477] Loading its data source...
[INFO][09:59:51]: [Client #477] Dataset size: 60000
[INFO][09:59:51]: [Client #477] Sampler: noniid
[INFO][09:59:51]: [Server #1127936] Sending 0.24 MB of payload data to client #121 (simulated).
[INFO][09:59:51]: [Client #27] Selected by the server.
[INFO][09:59:51]: [Client #27] Loading its data source...
[INFO][09:59:51]: [Client #27] Dataset size: 60000
[INFO][09:59:51]: [Client #121] Selected by the server.
[INFO][09:59:51]: [Client #27] Sampler: noniid
[INFO][09:59:51]: [Client #121] Loading its data source...
[INFO][09:59:51]: [Client #121] Dataset size: 60000
[INFO][09:59:51]: [Client #121] Sampler: noniid
[INFO][09:59:51]: [Client #477] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:59:51]: [Client #27] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:59:51]: [Client #121] Received 0.24 MB of payload data from the server (simulated).
[INFO][09:59:51]: [93m[1m[Client #477] Started training in communication round #29.[0m
[INFO][09:59:51]: [93m[1m[Client #121] Started training in communication round #29.[0m
[INFO][09:59:51]: [93m[1m[Client #27] Started training in communication round #29.[0m
[INFO][09:59:53]: [Client #27] Loading the dataset.
[INFO][09:59:53]: [Client #477] Loading the dataset.
[INFO][09:59:53]: [Client #121] Loading the dataset.
[INFO][09:59:59]: [Client #27] Epoch: [1/5][0/10]	Loss: 0.176842
[INFO][09:59:59]: [Client #121] Epoch: [1/5][0/10]	Loss: 0.164128
[INFO][09:59:59]: [Client #27] Epoch: [2/5][0/10]	Loss: 0.030856
[INFO][09:59:59]: [Client #477] Epoch: [1/5][0/10]	Loss: 0.040215
[INFO][09:59:59]: [Client #121] Epoch: [2/5][0/10]	Loss: 0.000004
[INFO][09:59:59]: [Client #27] Epoch: [3/5][0/10]	Loss: 0.000085
[INFO][09:59:59]: [Client #477] Epoch: [2/5][0/10]	Loss: 0.007415
[INFO][09:59:59]: [Client #121] Epoch: [3/5][0/10]	Loss: 0.000001
[INFO][09:59:59]: [Client #27] Epoch: [4/5][0/10]	Loss: 0.000406
[INFO][09:59:59]: [Client #477] Epoch: [3/5][0/10]	Loss: 0.001026
[INFO][10:00:00]: [Client #121] Epoch: [4/5][0/10]	Loss: 0.000004
[INFO][10:00:00]: [Client #477] Epoch: [4/5][0/10]	Loss: 0.023929
[INFO][10:00:00]: [Client #27] Epoch: [5/5][0/10]	Loss: 0.141531
[INFO][10:00:00]: [Client #121] Epoch: [5/5][0/10]	Loss: 0.000032
[INFO][10:00:00]: [Client #477] Epoch: [5/5][0/10]	Loss: 0.006010
[INFO][10:00:00]: [Client #27] Model saved to /data/ykang/plato/results/test/model/lenet5_27_1127978.pth.
[INFO][10:00:00]: [Client #121] Model saved to /data/ykang/plato/results/test/model/lenet5_121_1127979.pth.
[INFO][10:00:00]: [Client #477] Model saved to /data/ykang/plato/results/test/model/lenet5_477_1127977.pth.
[INFO][10:00:00]: [Client #27] Loading a model from /data/ykang/plato/results/test/model/lenet5_27_1127978.pth.
[INFO][10:00:01]: [Client #27] Model trained.
[INFO][10:00:01]: [Client #27] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:00:01]: [Server #1127936] Received 0.24 MB of payload data from client #27 (simulated).
[INFO][10:00:01]: [Client #121] Loading a model from /data/ykang/plato/results/test/model/lenet5_121_1127979.pth.
[INFO][10:00:01]: [Client #121] Model trained.
[INFO][10:00:01]: [Client #121] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:00:01]: [Server #1127936] Received 0.24 MB of payload data from client #121 (simulated).
[INFO][10:00:01]: [Client #477] Loading a model from /data/ykang/plato/results/test/model/lenet5_477_1127977.pth.
[INFO][10:00:01]: [Client #477] Model trained.
[INFO][10:00:01]: [Client #477] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:00:01]: [Server #1127936] Received 0.24 MB of payload data from client #477 (simulated).
[INFO][10:00:01]: [Server #1127936] Selecting client #34 for training.
[INFO][10:00:01]: [Server #1127936] Sending the current model to client #34 (simulated).
[INFO][10:00:01]: [Server #1127936] Sending 0.24 MB of payload data to client #34 (simulated).
[INFO][10:00:01]: [Client #34] Selected by the server.
[INFO][10:00:01]: [Client #34] Loading its data source...
[INFO][10:00:01]: [Client #34] Dataset size: 60000
[INFO][10:00:01]: [Client #34] Sampler: noniid
[INFO][10:00:01]: [Client #34] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:00:01]: [93m[1m[Client #34] Started training in communication round #29.[0m
[INFO][10:00:03]: [Client #34] Loading the dataset.
[INFO][10:00:08]: [Client #34] Epoch: [1/5][0/10]	Loss: 0.045055
[INFO][10:00:08]: [Client #34] Epoch: [2/5][0/10]	Loss: 0.000216
[INFO][10:00:08]: [Client #34] Epoch: [3/5][0/10]	Loss: 0.000000
[INFO][10:00:08]: [Client #34] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:00:08]: [Client #34] Epoch: [5/5][0/10]	Loss: 0.000001
[INFO][10:00:09]: [Client #34] Model saved to /data/ykang/plato/results/test/model/lenet5_34_1127977.pth.
[INFO][10:00:09]: [Client #34] Loading a model from /data/ykang/plato/results/test/model/lenet5_34_1127977.pth.
[INFO][10:00:09]: [Client #34] Model trained.
[INFO][10:00:09]: [Client #34] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:00:09]: [Server #1127936] Received 0.24 MB of payload data from client #34 (simulated).
[INFO][10:00:09]: [Server #1127936] Adding client #322 to the list of clients for aggregation.
[INFO][10:00:09]: [Server #1127936] Adding client #24 to the list of clients for aggregation.
[INFO][10:00:09]: [Server #1127936] Adding client #374 to the list of clients for aggregation.
[INFO][10:00:09]: [Server #1127936] Adding client #116 to the list of clients for aggregation.
[INFO][10:00:09]: [Server #1127936] Adding client #335 to the list of clients for aggregation.
[INFO][10:00:09]: [Server #1127936] Adding client #406 to the list of clients for aggregation.
[INFO][10:00:09]: [Server #1127936] Adding client #22 to the list of clients for aggregation.
[INFO][10:00:09]: [Server #1127936] Adding client #121 to the list of clients for aggregation.
[INFO][10:00:09]: [Server #1127936] Adding client #438 to the list of clients for aggregation.
[INFO][10:00:09]: [Server #1127936] Adding client #27 to the list of clients for aggregation.
[INFO][10:00:09]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01165822 0.         0.02910431
 0.         0.         0.00986367 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.02826171 0.         0.         0.         0.
 0.04600035 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.02198441 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00676904 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01218521 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01328995 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01610813
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 3. 0. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 3. 0. 0. 0. 1. 0. 0. 1. 6. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 2. 0. 6. 0. 1. 2. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 1. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.
 1. 0. 1. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 4. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0.
 0. 1. 1. 1. 0. 2. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 0. 1. 0. 0. 0. 6. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1.
 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01165822 0.         0.02910431
 0.         0.         0.00986367 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.02826171 0.         0.         0.         0.
 0.04600035 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.02198441 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00676904 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01218521 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01328995 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01610813
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:00:12]: [Server #1127936] Global model accuracy: 94.36%

[INFO][10:00:12]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_29.pth.
[INFO][10:00:12]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_29.pth.
[INFO][10:00:12]: [93m[1m
[Server #1127936] Starting round 30/100.[0m
[0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002
 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  4e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  4e-05  7e-10  7e-10
 6:  6.8876e+00  6.8875e+00  3e-05  4e-10  4e-10
 7:  6.8875e+00  6.8875e+00  3e-05  9e-10  3e-11
 8:  6.8875e+00  6.8875e+00  2e-05  8e-10  3e-11
 9:  6.8875e+00  6.8875e+00  1e-05  3e-09  1e-10
10:  6.8875e+00  6.8875e+00  1e-06  4e-09  1e-10
Optimal solution found.
The calculated probability is:  [6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 9.66613387e-01 6.42416079e-05 8.25270011e-04 6.42416079e-05
 6.42416079e-05 6.42394856e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.54566198e-04
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.41954799e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 2.33006663e-04
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 8.32982363e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 1.08795789e-04 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 4.53033257e-04 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42359481e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05 6.42416079e-05 6.42416079e-05
 6.42416079e-05 6.42416079e-05]
current clients pool:  [INFO][10:00:12]: [Server #1127936] Selected clients: [ 22 475 470 306 487 164 446 210 116 469]
[INFO][10:00:12]: [Server #1127936] Selecting client #22 for training.
[INFO][10:00:12]: [Server #1127936] Sending the current model to client #22 (simulated).
[INFO][10:00:12]: [Server #1127936] Sending 0.24 MB of payload data to client #22 (simulated).
[INFO][10:00:12]: [Server #1127936] Selecting client #475 for training.
[INFO][10:00:12]: [Server #1127936] Sending the current model to client #475 (simulated).
[INFO][10:00:12]: [Server #1127936] Sending 0.24 MB of payload data to client #475 (simulated).
[INFO][10:00:12]: [Server #1127936] Selecting client #470 for training.
[INFO][10:00:12]: [Server #1127936] Sending the current model to client #470 (simulated).
[INFO][10:00:12]: [Client #22] Selected by the server.
[INFO][10:00:12]: [Client #22] Loading its data source...
[INFO][10:00:12]: [Client #22] Dataset size: 60000
[INFO][10:00:12]: [Client #22] Sampler: noniid
[INFO][10:00:12]: [Server #1127936] Sending 0.24 MB of payload data to client #470 (simulated).
[INFO][10:00:12]: [Client #475] Selected by the server.
[INFO][10:00:12]: [Client #475] Loading its data source...
[INFO][10:00:12]: [Client #475] Dataset size: 60000
[INFO][10:00:12]: [Client #22] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:00:12]: [Client #475] Sampler: noniid
[INFO][10:00:12]: [Client #475] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:00:12]: [93m[1m[Client #22] Started training in communication round #30.[0m
[INFO][10:00:12]: [93m[1m[Client #475] Started training in communication round #30.[0m
[INFO][10:00:12]: [Client #470] Selected by the server.
[INFO][10:00:12]: [Client #470] Loading its data source...
[INFO][10:00:12]: [Client #470] Dataset size: 60000
[INFO][10:00:12]: [Client #470] Sampler: noniid
[INFO][10:00:12]: [Client #470] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:00:12]: [93m[1m[Client #470] Started training in communication round #30.[0m
[INFO][10:00:14]: [Client #475] Loading the dataset.
[INFO][10:00:14]: [Client #470] Loading the dataset.
[INFO][10:00:14]: [Client #22] Loading the dataset.
[INFO][10:00:21]: [Client #475] Epoch: [1/5][0/10]	Loss: 0.179889
[INFO][10:00:21]: [Client #22] Epoch: [1/5][0/10]	Loss: 0.096363
[INFO][10:00:21]: [Client #470] Epoch: [1/5][0/10]	Loss: 0.010422
[INFO][10:00:21]: [Client #475] Epoch: [2/5][0/10]	Loss: 0.000329
[INFO][10:00:21]: [Client #22] Epoch: [2/5][0/10]	Loss: 0.001805
[INFO][10:00:21]: [Client #470] Epoch: [2/5][0/10]	Loss: 0.002406
[INFO][10:00:21]: [Client #22] Epoch: [3/5][0/10]	Loss: 0.000037
[INFO][10:00:21]: [Client #475] Epoch: [3/5][0/10]	Loss: 0.000000
[INFO][10:00:21]: [Client #470] Epoch: [3/5][0/10]	Loss: 0.003083
[INFO][10:00:21]: [Client #22] Epoch: [4/5][0/10]	Loss: 0.000172
[INFO][10:00:21]: [Client #470] Epoch: [4/5][0/10]	Loss: 0.010385
[INFO][10:00:21]: [Client #475] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:00:21]: [Client #22] Epoch: [5/5][0/10]	Loss: 0.500370
[INFO][10:00:21]: [Client #475] Epoch: [5/5][0/10]	Loss: 0.000002
[INFO][10:00:21]: [Client #470] Epoch: [5/5][0/10]	Loss: 0.066274
[INFO][10:00:21]: [Client #22] Model saved to /data/ykang/plato/results/test/model/lenet5_22_1127977.pth.
[INFO][10:00:21]: [Client #475] Model saved to /data/ykang/plato/results/test/model/lenet5_475_1127978.pth.
[INFO][10:00:21]: [Client #470] Model saved to /data/ykang/plato/results/test/model/lenet5_470_1127979.pth.
[INFO][10:00:22]: [Client #22] Loading a model from /data/ykang/plato/results/test/model/lenet5_22_1127977.pth.
[INFO][10:00:22]: [Client #22] Model trained.
[INFO][10:00:22]: [Client #22] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:00:22]: [Server #1127936] Received 0.24 MB of payload data from client #22 (simulated).
[INFO][10:00:22]: [Client #475] Loading a model from /data/ykang/plato/results/test/model/lenet5_475_1127978.pth.
[INFO][10:00:22]: [Client #475] Model trained.
[INFO][10:00:22]: [Client #475] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:00:22]: [Server #1127936] Received 0.24 MB of payload data from client #475 (simulated).
[INFO][10:00:22]: [Client #470] Loading a model from /data/ykang/plato/results/test/model/lenet5_470_1127979.pth.
[INFO][10:00:22]: [Client #470] Model trained.
[INFO][10:00:22]: [Client #470] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:00:22]: [Server #1127936] Received 0.24 MB of payload data from client #470 (simulated).
[INFO][10:00:22]: [Server #1127936] Selecting client #306 for training.
[INFO][10:00:22]: [Server #1127936] Sending the current model to client #306 (simulated).
[INFO][10:00:22]: [Server #1127936] Sending 0.24 MB of payload data to client #306 (simulated).
[INFO][10:00:22]: [Server #1127936] Selecting client #487 for training.
[INFO][10:00:22]: [Server #1127936] Sending the current model to client #487 (simulated).
[INFO][10:00:22]: [Server #1127936] Sending 0.24 MB of payload data to client #487 (simulated).
[INFO][10:00:22]: [Server #1127936] Selecting client #164 for training.
[INFO][10:00:22]: [Server #1127936] Sending the current model to client #164 (simulated).
[INFO][10:00:22]: [Client #306] Selected by the server.
[INFO][10:00:22]: [Client #306] Loading its data source...
[INFO][10:00:22]: [Client #306] Dataset size: 60000
[INFO][10:00:22]: [Client #306] Sampler: noniid
[INFO][10:00:22]: [Server #1127936] Sending 0.24 MB of payload data to client #164 (simulated).
[INFO][10:00:22]: [Client #164] Selected by the server.
[INFO][10:00:22]: [Client #487] Selected by the server.
[INFO][10:00:22]: [Client #164] Loading its data source...
[INFO][10:00:22]: [Client #164] Dataset size: 60000
[INFO][10:00:22]: [Client #487] Loading its data source...
[INFO][10:00:22]: [Client #164] Sampler: noniid
[INFO][10:00:22]: [Client #487] Dataset size: 60000
[INFO][10:00:22]: [Client #487] Sampler: noniid
[INFO][10:00:22]: [Client #306] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:00:22]: [Client #487] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:00:22]: [93m[1m[Client #306] Started training in communication round #30.[0m
[INFO][10:00:22]: [Client #164] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:00:22]: [93m[1m[Client #487] Started training in communication round #30.[0m
[INFO][10:00:22]: [93m[1m[Client #164] Started training in communication round #30.[0m
[INFO][10:00:24]: [Client #487] Loading the dataset.
[INFO][10:00:24]: [Client #164] Loading the dataset.
[INFO][10:00:24]: [Client #306] Loading the dataset.
[INFO][10:00:30]: [Client #306] Epoch: [1/5][0/10]	Loss: 0.230288
[INFO][10:00:30]: [Client #487] Epoch: [1/5][0/10]	Loss: 0.275879
[INFO][10:00:30]: [Client #164] Epoch: [1/5][0/10]	Loss: 0.145054
[INFO][10:00:31]: [Client #306] Epoch: [2/5][0/10]	Loss: 0.063871
[INFO][10:00:31]: [Client #164] Epoch: [2/5][0/10]	Loss: 0.005184
[INFO][10:00:31]: [Client #487] Epoch: [2/5][0/10]	Loss: 0.048775
[INFO][10:00:31]: [Client #306] Epoch: [3/5][0/10]	Loss: 0.275317
[INFO][10:00:31]: [Client #164] Epoch: [3/5][0/10]	Loss: 0.000024
[INFO][10:00:31]: [Client #487] Epoch: [3/5][0/10]	Loss: 0.016811
[INFO][10:00:31]: [Client #306] Epoch: [4/5][0/10]	Loss: 0.001013
[INFO][10:00:31]: [Client #487] Epoch: [4/5][0/10]	Loss: 0.001983
[INFO][10:00:31]: [Client #164] Epoch: [4/5][0/10]	Loss: 0.000487
[INFO][10:00:31]: [Client #487] Epoch: [5/5][0/10]	Loss: 0.033078
[INFO][10:00:31]: [Client #306] Epoch: [5/5][0/10]	Loss: 0.000182
[INFO][10:00:31]: [Client #487] Model saved to /data/ykang/plato/results/test/model/lenet5_487_1127978.pth.
[INFO][10:00:31]: [Client #164] Epoch: [5/5][0/10]	Loss: 0.004213
[INFO][10:00:31]: [Client #306] Model saved to /data/ykang/plato/results/test/model/lenet5_306_1127977.pth.
[INFO][10:00:31]: [Client #164] Model saved to /data/ykang/plato/results/test/model/lenet5_164_1127979.pth.
[INFO][10:00:32]: [Client #487] Loading a model from /data/ykang/plato/results/test/model/lenet5_487_1127978.pth.
[INFO][10:00:32]: [Client #487] Model trained.
[INFO][10:00:32]: [Client #487] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:00:32]: [Server #1127936] Received 0.24 MB of payload data from client #487 (simulated).
[INFO][10:00:32]: [Client #164] Loading a model from /data/ykang/plato/results/test/model/lenet5_164_1127979.pth.
[INFO][10:00:32]: [Client #164] Model trained.
[INFO][10:00:32]: [Client #164] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:00:32]: [Server #1127936] Received 0.24 MB of payload data from client #164 (simulated).
[INFO][10:00:32]: [Client #306] Loading a model from /data/ykang/plato/results/test/model/lenet5_306_1127977.pth.
[INFO][10:00:32]: [Client #306] Model trained.
[INFO][10:00:32]: [Client #306] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:00:32]: [Server #1127936] Received 0.24 MB of payload data from client #306 (simulated).
[INFO][10:00:32]: [Server #1127936] Selecting client #446 for training.
[INFO][10:00:32]: [Server #1127936] Sending the current model to client #446 (simulated).
[INFO][10:00:32]: [Server #1127936] Sending 0.24 MB of payload data to client #446 (simulated).
[INFO][10:00:32]: [Server #1127936] Selecting client #210 for training.
[INFO][10:00:32]: [Server #1127936] Sending the current model to client #210 (simulated).
[INFO][10:00:32]: [Server #1127936] Sending 0.24 MB of payload data to client #210 (simulated).
[INFO][10:00:32]: [Server #1127936] Selecting client #116 for training.
[INFO][10:00:32]: [Server #1127936] Sending the current model to client #116 (simulated).
[INFO][10:00:32]: [Client #446] Selected by the server.
[INFO][10:00:32]: [Client #446] Loading its data source...
[INFO][10:00:32]: [Client #446] Dataset size: 60000
[INFO][10:00:32]: [Client #446] Sampler: noniid
[INFO][10:00:32]: [Server #1127936] Sending 0.24 MB of payload data to client #116 (simulated).
[INFO][10:00:32]: [Client #210] Selected by the server.
[INFO][10:00:32]: [Client #210] Loading its data source...
[INFO][10:00:32]: [Client #210] Dataset size: 60000
[INFO][10:00:32]: [Client #210] Sampler: noniid
[INFO][10:00:32]: [Client #116] Selected by the server.
[INFO][10:00:32]: [Client #116] Loading its data source...
[INFO][10:00:32]: [Client #116] Dataset size: 60000
[INFO][10:00:32]: [Client #116] Sampler: noniid
[INFO][10:00:32]: [Client #446] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:00:32]: [Client #210] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:00:32]: [Client #116] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:00:32]: [93m[1m[Client #116] Started training in communication round #30.[0m
[INFO][10:00:32]: [93m[1m[Client #210] Started training in communication round #30.[0m
[INFO][10:00:32]: [93m[1m[Client #446] Started training in communication round #30.[0m
[INFO][10:00:34]: [Client #210] Loading the dataset.
[INFO][10:00:34]: [Client #446] Loading the dataset.
[INFO][10:00:34]: [Client #116] Loading the dataset.
[INFO][10:00:40]: [Client #116] Epoch: [1/5][0/10]	Loss: 0.207751
[INFO][10:00:40]: [Client #446] Epoch: [1/5][0/10]	Loss: 0.089443
[INFO][10:00:40]: [Client #210] Epoch: [1/5][0/10]	Loss: 0.241510
[INFO][10:00:41]: [Client #210] Epoch: [2/5][0/10]	Loss: 0.018862
[INFO][10:00:41]: [Client #446] Epoch: [2/5][0/10]	Loss: 0.025730
[INFO][10:00:41]: [Client #116] Epoch: [2/5][0/10]	Loss: 0.014467
[INFO][10:00:41]: [Client #210] Epoch: [3/5][0/10]	Loss: 0.058931
[INFO][10:00:41]: [Client #446] Epoch: [3/5][0/10]	Loss: 0.000359
[INFO][10:00:41]: [Client #116] Epoch: [3/5][0/10]	Loss: 0.000003
[INFO][10:00:41]: [Client #210] Epoch: [4/5][0/10]	Loss: 0.002919
[INFO][10:00:41]: [Client #446] Epoch: [4/5][0/10]	Loss: 0.006272
[INFO][10:00:41]: [Client #210] Epoch: [5/5][0/10]	Loss: 0.006951
[INFO][10:00:41]: [Client #116] Epoch: [4/5][0/10]	Loss: 0.000015
[INFO][10:00:41]: [Client #446] Epoch: [5/5][0/10]	Loss: 0.141841
[INFO][10:00:41]: [Client #210] Model saved to /data/ykang/plato/results/test/model/lenet5_210_1127978.pth.
[INFO][10:00:41]: [Client #446] Model saved to /data/ykang/plato/results/test/model/lenet5_446_1127977.pth.
[INFO][10:00:41]: [Client #116] Epoch: [5/5][0/10]	Loss: 0.000038
[INFO][10:00:41]: [Client #116] Model saved to /data/ykang/plato/results/test/model/lenet5_116_1127979.pth.
[INFO][10:00:42]: [Client #446] Loading a model from /data/ykang/plato/results/test/model/lenet5_446_1127977.pth.
[INFO][10:00:42]: [Client #446] Model trained.
[INFO][10:00:42]: [Client #446] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:00:42]: [Server #1127936] Received 0.24 MB of payload data from client #446 (simulated).
[INFO][10:00:42]: [Client #210] Loading a model from /data/ykang/plato/results/test/model/lenet5_210_1127978.pth.
[INFO][10:00:42]: [Client #210] Model trained.
[INFO][10:00:42]: [Client #210] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:00:42]: [Server #1127936] Received 0.24 MB of payload data from client #210 (simulated).
[INFO][10:00:42]: [Client #116] Loading a model from /data/ykang/plato/results/test/model/lenet5_116_1127979.pth.
[INFO][10:00:42]: [Client #116] Model trained.
[INFO][10:00:42]: [Client #116] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:00:42]: [Server #1127936] Received 0.24 MB of payload data from client #116 (simulated).
[INFO][10:00:42]: [Server #1127936] Selecting client #469 for training.
[INFO][10:00:42]: [Server #1127936] Sending the current model to client #469 (simulated).
[INFO][10:00:42]: [Server #1127936] Sending 0.24 MB of payload data to client #469 (simulated).
[INFO][10:00:42]: [Client #469] Selected by the server.
[INFO][10:00:42]: [Client #469] Loading its data source...
[INFO][10:00:42]: [Client #469] Dataset size: 60000
[INFO][10:00:42]: [Client #469] Sampler: noniid
[INFO][10:00:42]: [Client #469] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:00:42]: [93m[1m[Client #469] Started training in communication round #30.[0m
[INFO][10:00:44]: [Client #469] Loading the dataset.
[INFO][10:00:50]: [Client #469] Epoch: [1/5][0/10]	Loss: 0.069239
[INFO][10:00:50]: [Client #469] Epoch: [2/5][0/10]	Loss: 0.030922
[INFO][10:00:50]: [Client #469] Epoch: [3/5][0/10]	Loss: 0.005323
[INFO][10:00:50]: [Client #469] Epoch: [4/5][0/10]	Loss: 0.071987
[INFO][10:00:50]: [Client #469] Epoch: [5/5][0/10]	Loss: 0.067401
[INFO][10:00:50]: [Client #469] Model saved to /data/ykang/plato/results/test/model/lenet5_469_1127977.pth.
[INFO][10:00:50]: [Client #469] Loading a model from /data/ykang/plato/results/test/model/lenet5_469_1127977.pth.
[INFO][10:00:50]: [Client #469] Model trained.
[INFO][10:00:50]: [Client #469] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:00:50]: [Server #1127936] Received 0.24 MB of payload data from client #469 (simulated).
[INFO][10:00:50]: [Server #1127936] Adding client #8 to the list of clients for aggregation.
[INFO][10:00:50]: [Server #1127936] Adding client #34 to the list of clients for aggregation.
[INFO][10:00:50]: [Server #1127936] Adding client #227 to the list of clients for aggregation.
[INFO][10:00:50]: [Server #1127936] Adding client #328 to the list of clients for aggregation.
[INFO][10:00:50]: [Server #1127936] Adding client #414 to the list of clients for aggregation.
[INFO][10:00:50]: [Server #1127936] Adding client #477 to the list of clients for aggregation.
[INFO][10:00:50]: [Server #1127936] Adding client #367 to the list of clients for aggregation.
[INFO][10:00:50]: [Server #1127936] Adding client #110 to the list of clients for aggregation.
[INFO][10:00:50]: [Server #1127936] Adding client #252 to the list of clients for aggregation.
[INFO][10:00:50]: [Server #1127936] Adding client #487 to the list of clients for aggregation.
[INFO][10:00:50]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.00597072 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01641088 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01538106 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00973527 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01277469
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.02754381 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01514475 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01467957
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.0164116  0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01531784 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 3. 0. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 3. 1. 0. 0. 1. 0. 0. 1. 6. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 2. 0. 6. 0. 1. 2. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 1. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.
 1. 0. 1. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 4. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0.
 0. 1. 1. 1. 0. 2. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 0. 1. 0. 0. 0. 6. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1.
 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.00597072 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01641088 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01538106 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00973527 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01277469
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.02754381 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01514475 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01467957
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.0164116  0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01531784 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:00:53]: [Server #1127936] Global model accuracy: 92.77%

[INFO][10:00:53]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_30.pth.
[INFO][10:00:53]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_30.pth.
[INFO][10:00:53]: [93m[1m
[Server #1127936] Starting round 31/100.[0m
[0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002
 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8871e+00  4e-04  7e-09  7e-09
 5:  6.8876e+00  6.8875e+00  1e-04  2e-09  2e-09
 6:  6.8875e+00  6.8875e+00  9e-05  8e-10  6e-11
 7:  6.8875e+00  6.8875e+00  8e-05  1e-09  1e-10
 8:  6.8875e+00  6.8875e+00  4e-05  1e-08  1e-09
 9:  6.8875e+00  6.8875e+00  2e-05  9e-09  6e-10
10:  6.8875e+00  6.8875e+00  6e-06  5e-09  4e-10
Optimal solution found.
The calculated probability is:  [1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.37624910e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.58930481e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 5.49270420e-01
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.44618341e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 3.25784752e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.90294297e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 3.87978572e-01 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.54954640e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.58932169e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27813202e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04 1.27816782e-04 1.27816782e-04
 1.27816782e-04 1.27816782e-04]
current clients pool:  [INFO][10:00:53]: [Server #1127936] Selected clients: [367 110  66 419 309 265 126 149 434 213]
[INFO][10:00:53]: [Server #1127936] Selecting client #367 for training.
[INFO][10:00:53]: [Server #1127936] Sending the current model to client #367 (simulated).
[INFO][10:00:53]: [Server #1127936] Sending 0.24 MB of payload data to client #367 (simulated).
[INFO][10:00:53]: [Server #1127936] Selecting client #110 for training.
[INFO][10:00:53]: [Server #1127936] Sending the current model to client #110 (simulated).
[INFO][10:00:53]: [Server #1127936] Sending 0.24 MB of payload data to client #110 (simulated).
[INFO][10:00:53]: [Server #1127936] Selecting client #66 for training.
[INFO][10:00:53]: [Server #1127936] Sending the current model to client #66 (simulated).
[INFO][10:00:53]: [Client #367] Selected by the server.
[INFO][10:00:53]: [Client #367] Loading its data source...
[INFO][10:00:53]: [Client #367] Dataset size: 60000
[INFO][10:00:53]: [Client #367] Sampler: noniid
[INFO][10:00:53]: [Server #1127936] Sending 0.24 MB of payload data to client #66 (simulated).
[INFO][10:00:53]: [Client #110] Selected by the server.
[INFO][10:00:53]: [Client #110] Loading its data source...
[INFO][10:00:53]: [Client #110] Dataset size: 60000
[INFO][10:00:53]: [Client #110] Sampler: noniid
[INFO][10:00:53]: [Client #66] Selected by the server.
[INFO][10:00:53]: [Client #66] Loading its data source...
[INFO][10:00:53]: [Client #66] Dataset size: 60000
[INFO][10:00:53]: [Client #66] Sampler: noniid
[INFO][10:00:53]: [Client #367] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:00:53]: [Client #110] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:00:53]: [Client #66] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:00:53]: [93m[1m[Client #66] Started training in communication round #31.[0m
[INFO][10:00:53]: [93m[1m[Client #110] Started training in communication round #31.[0m
[INFO][10:00:53]: [93m[1m[Client #367] Started training in communication round #31.[0m
[INFO][10:00:55]: [Client #66] Loading the dataset.
[INFO][10:00:55]: [Client #367] Loading the dataset.
[INFO][10:00:56]: [Client #110] Loading the dataset.
[INFO][10:01:02]: [Client #66] Epoch: [1/5][0/10]	Loss: 0.065480
[INFO][10:01:02]: [Client #110] Epoch: [1/5][0/10]	Loss: 0.061213
[INFO][10:01:02]: [Client #367] Epoch: [1/5][0/10]	Loss: 0.061213
[INFO][10:01:02]: [Client #66] Epoch: [2/5][0/10]	Loss: 0.016038
[INFO][10:01:02]: [Client #367] Epoch: [2/5][0/10]	Loss: 0.001967
[INFO][10:01:02]: [Client #110] Epoch: [2/5][0/10]	Loss: 0.001667
[INFO][10:01:02]: [Client #66] Epoch: [3/5][0/10]	Loss: 0.000847
[INFO][10:01:02]: [Client #367] Epoch: [3/5][0/10]	Loss: 0.000022
[INFO][10:01:02]: [Client #110] Epoch: [3/5][0/10]	Loss: 0.000196
[INFO][10:01:02]: [Client #66] Epoch: [4/5][0/10]	Loss: 0.223534
[INFO][10:01:02]: [Client #367] Epoch: [4/5][0/10]	Loss: 0.000023
[INFO][10:01:02]: [Client #110] Epoch: [4/5][0/10]	Loss: 0.001047
[INFO][10:01:02]: [Client #66] Epoch: [5/5][0/10]	Loss: 0.037458
[INFO][10:01:02]: [Client #367] Epoch: [5/5][0/10]	Loss: 0.000028
[INFO][10:01:02]: [Client #66] Model saved to /data/ykang/plato/results/test/model/lenet5_66_1127979.pth.
[INFO][10:01:02]: [Client #110] Epoch: [5/5][0/10]	Loss: 0.000013
[INFO][10:01:02]: [Client #367] Model saved to /data/ykang/plato/results/test/model/lenet5_367_1127977.pth.
[INFO][10:01:02]: [Client #110] Model saved to /data/ykang/plato/results/test/model/lenet5_110_1127978.pth.
[INFO][10:01:03]: [Client #66] Loading a model from /data/ykang/plato/results/test/model/lenet5_66_1127979.pth.
[INFO][10:01:03]: [Client #66] Model trained.
[INFO][10:01:03]: [Client #66] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:01:03]: [Server #1127936] Received 0.24 MB of payload data from client #66 (simulated).
[INFO][10:01:03]: [Client #367] Loading a model from /data/ykang/plato/results/test/model/lenet5_367_1127977.pth.
[INFO][10:01:03]: [Client #367] Model trained.
[INFO][10:01:03]: [Client #367] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:01:03]: [Server #1127936] Received 0.24 MB of payload data from client #367 (simulated).
[INFO][10:01:03]: [Client #110] Loading a model from /data/ykang/plato/results/test/model/lenet5_110_1127978.pth.
[INFO][10:01:03]: [Client #110] Model trained.
[INFO][10:01:03]: [Client #110] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:01:03]: [Server #1127936] Received 0.24 MB of payload data from client #110 (simulated).
[INFO][10:01:03]: [Server #1127936] Selecting client #419 for training.
[INFO][10:01:03]: [Server #1127936] Sending the current model to client #419 (simulated).
[INFO][10:01:03]: [Server #1127936] Sending 0.24 MB of payload data to client #419 (simulated).
[INFO][10:01:03]: [Server #1127936] Selecting client #309 for training.
[INFO][10:01:03]: [Server #1127936] Sending the current model to client #309 (simulated).
[INFO][10:01:03]: [Server #1127936] Sending 0.24 MB of payload data to client #309 (simulated).
[INFO][10:01:03]: [Server #1127936] Selecting client #265 for training.
[INFO][10:01:03]: [Server #1127936] Sending the current model to client #265 (simulated).
[INFO][10:01:03]: [Client #419] Selected by the server.
[INFO][10:01:03]: [Client #419] Loading its data source...
[INFO][10:01:03]: [Client #419] Dataset size: 60000
[INFO][10:01:03]: [Client #419] Sampler: noniid
[INFO][10:01:03]: [Server #1127936] Sending 0.24 MB of payload data to client #265 (simulated).
[INFO][10:01:03]: [Client #309] Selected by the server.
[INFO][10:01:03]: [Client #309] Loading its data source...
[INFO][10:01:03]: [Client #309] Dataset size: 60000
[INFO][10:01:03]: [Client #309] Sampler: noniid
[INFO][10:01:03]: [Client #265] Selected by the server.
[INFO][10:01:03]: [Client #265] Loading its data source...
[INFO][10:01:03]: [Client #265] Dataset size: 60000
[INFO][10:01:03]: [Client #265] Sampler: noniid
[INFO][10:01:03]: [Client #419] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:01:03]: [Client #309] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:01:03]: [Client #265] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:01:03]: [93m[1m[Client #265] Started training in communication round #31.[0m
[INFO][10:01:03]: [93m[1m[Client #419] Started training in communication round #31.[0m
[INFO][10:01:03]: [93m[1m[Client #309] Started training in communication round #31.[0m
[INFO][10:01:05]: [Client #265] Loading the dataset.
[INFO][10:01:05]: [Client #419] Loading the dataset.
[INFO][10:01:05]: [Client #309] Loading the dataset.
[INFO][10:01:11]: [Client #309] Epoch: [1/5][0/10]	Loss: 0.198741
[INFO][10:01:11]: [Client #419] Epoch: [1/5][0/10]	Loss: 0.168466
[INFO][10:01:11]: [Client #265] Epoch: [1/5][0/10]	Loss: 0.067678
[INFO][10:01:11]: [Client #309] Epoch: [2/5][0/10]	Loss: 0.000450
[INFO][10:01:12]: [Client #419] Epoch: [2/5][0/10]	Loss: 0.000000
[INFO][10:01:12]: [Client #309] Epoch: [3/5][0/10]	Loss: 0.018096
[INFO][10:01:12]: [Client #265] Epoch: [2/5][0/10]	Loss: 0.000993
[INFO][10:01:12]: [Client #419] Epoch: [3/5][0/10]	Loss: 0.000000
[INFO][10:01:12]: [Client #309] Epoch: [4/5][0/10]	Loss: 0.062300
[INFO][10:01:12]: [Client #419] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:01:12]: [Client #265] Epoch: [3/5][0/10]	Loss: 0.000093
[INFO][10:01:12]: [Client #309] Epoch: [5/5][0/10]	Loss: 0.009664
[INFO][10:01:12]: [Client #309] Model saved to /data/ykang/plato/results/test/model/lenet5_309_1127978.pth.
[INFO][10:01:12]: [Client #265] Epoch: [4/5][0/10]	Loss: 0.000023
[INFO][10:01:12]: [Client #419] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:01:12]: [Client #419] Model saved to /data/ykang/plato/results/test/model/lenet5_419_1127977.pth.
[INFO][10:01:12]: [Client #265] Epoch: [5/5][0/10]	Loss: 0.000062
[INFO][10:01:12]: [Client #265] Model saved to /data/ykang/plato/results/test/model/lenet5_265_1127979.pth.
[INFO][10:01:13]: [Client #309] Loading a model from /data/ykang/plato/results/test/model/lenet5_309_1127978.pth.
[INFO][10:01:13]: [Client #309] Model trained.
[INFO][10:01:13]: [Client #309] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:01:13]: [Server #1127936] Received 0.24 MB of payload data from client #309 (simulated).
[INFO][10:01:13]: [Client #419] Loading a model from /data/ykang/plato/results/test/model/lenet5_419_1127977.pth.
[INFO][10:01:13]: [Client #419] Model trained.
[INFO][10:01:13]: [Client #419] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:01:13]: [Server #1127936] Received 0.24 MB of payload data from client #419 (simulated).
[INFO][10:01:13]: [Client #265] Loading a model from /data/ykang/plato/results/test/model/lenet5_265_1127979.pth.
[INFO][10:01:13]: [Client #265] Model trained.
[INFO][10:01:13]: [Client #265] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:01:13]: [Server #1127936] Received 0.24 MB of payload data from client #265 (simulated).
[INFO][10:01:13]: [Server #1127936] Selecting client #126 for training.
[INFO][10:01:13]: [Server #1127936] Sending the current model to client #126 (simulated).
[INFO][10:01:13]: [Server #1127936] Sending 0.24 MB of payload data to client #126 (simulated).
[INFO][10:01:13]: [Server #1127936] Selecting client #149 for training.
[INFO][10:01:13]: [Server #1127936] Sending the current model to client #149 (simulated).
[INFO][10:01:13]: [Server #1127936] Sending 0.24 MB of payload data to client #149 (simulated).
[INFO][10:01:13]: [Server #1127936] Selecting client #434 for training.
[INFO][10:01:13]: [Server #1127936] Sending the current model to client #434 (simulated).
[INFO][10:01:13]: [Client #126] Selected by the server.
[INFO][10:01:13]: [Client #126] Loading its data source...
[INFO][10:01:13]: [Client #126] Dataset size: 60000
[INFO][10:01:13]: [Client #126] Sampler: noniid
[INFO][10:01:13]: [Server #1127936] Sending 0.24 MB of payload data to client #434 (simulated).
[INFO][10:01:13]: [Client #149] Selected by the server.
[INFO][10:01:13]: [Client #149] Loading its data source...
[INFO][10:01:13]: [Client #149] Dataset size: 60000
[INFO][10:01:13]: [Client #149] Sampler: noniid
[INFO][10:01:13]: [Client #434] Selected by the server.
[INFO][10:01:13]: [Client #434] Loading its data source...
[INFO][10:01:13]: [Client #434] Dataset size: 60000
[INFO][10:01:13]: [Client #434] Sampler: noniid
[INFO][10:01:13]: [Client #126] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:01:13]: [Client #149] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:01:13]: [93m[1m[Client #126] Started training in communication round #31.[0m
[INFO][10:01:13]: [Client #434] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:01:13]: [93m[1m[Client #434] Started training in communication round #31.[0m
[INFO][10:01:13]: [93m[1m[Client #149] Started training in communication round #31.[0m
[INFO][10:01:15]: [Client #126] Loading the dataset.
[INFO][10:01:15]: [Client #149] Loading the dataset.
[INFO][10:01:15]: [Client #434] Loading the dataset.
[INFO][10:01:21]: [Client #126] Epoch: [1/5][0/10]	Loss: 0.128723
[INFO][10:01:21]: [Client #434] Epoch: [1/5][0/10]	Loss: 0.059846
[INFO][10:01:21]: [Client #149] Epoch: [1/5][0/10]	Loss: 0.160654
[INFO][10:01:21]: [Client #126] Epoch: [2/5][0/10]	Loss: 0.004208
[INFO][10:01:21]: [Client #434] Epoch: [2/5][0/10]	Loss: 0.012514
[INFO][10:01:21]: [Client #149] Epoch: [2/5][0/10]	Loss: 0.020190
[INFO][10:01:21]: [Client #126] Epoch: [3/5][0/10]	Loss: 0.001723
[INFO][10:01:21]: [Client #434] Epoch: [3/5][0/10]	Loss: 0.000806
[INFO][10:01:22]: [Client #149] Epoch: [3/5][0/10]	Loss: 0.005587
[INFO][10:01:22]: [Client #434] Epoch: [4/5][0/10]	Loss: 0.000132
[INFO][10:01:22]: [Client #126] Epoch: [4/5][0/10]	Loss: 0.000032
[INFO][10:01:22]: [Client #149] Epoch: [4/5][0/10]	Loss: 0.014631
[INFO][10:01:22]: [Client #126] Epoch: [5/5][0/10]	Loss: 0.000091
[INFO][10:01:22]: [Client #434] Epoch: [5/5][0/10]	Loss: 0.000084
[INFO][10:01:22]: [Client #149] Epoch: [5/5][0/10]	Loss: 0.143421
[INFO][10:01:22]: [Client #126] Model saved to /data/ykang/plato/results/test/model/lenet5_126_1127977.pth.
[INFO][10:01:22]: [Client #434] Model saved to /data/ykang/plato/results/test/model/lenet5_434_1127979.pth.
[INFO][10:01:22]: [Client #149] Model saved to /data/ykang/plato/results/test/model/lenet5_149_1127978.pth.
[INFO][10:01:23]: [Client #126] Loading a model from /data/ykang/plato/results/test/model/lenet5_126_1127977.pth.
[INFO][10:01:23]: [Client #126] Model trained.
[INFO][10:01:23]: [Client #126] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:01:23]: [Server #1127936] Received 0.24 MB of payload data from client #126 (simulated).
[INFO][10:01:23]: [Client #149] Loading a model from /data/ykang/plato/results/test/model/lenet5_149_1127978.pth.
[INFO][10:01:23]: [Client #149] Model trained.
[INFO][10:01:23]: [Client #149] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:01:23]: [Server #1127936] Received 0.24 MB of payload data from client #149 (simulated).
[INFO][10:01:23]: [Client #434] Loading a model from /data/ykang/plato/results/test/model/lenet5_434_1127979.pth.
[INFO][10:01:23]: [Client #434] Model trained.
[INFO][10:01:23]: [Client #434] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:01:23]: [Server #1127936] Received 0.24 MB of payload data from client #434 (simulated).
[INFO][10:01:23]: [Server #1127936] Selecting client #213 for training.
[INFO][10:01:23]: [Server #1127936] Sending the current model to client #213 (simulated).
[INFO][10:01:23]: [Server #1127936] Sending 0.24 MB of payload data to client #213 (simulated).
[INFO][10:01:23]: [Client #213] Selected by the server.
[INFO][10:01:23]: [Client #213] Loading its data source...
[INFO][10:01:23]: [Client #213] Dataset size: 60000
[INFO][10:01:23]: [Client #213] Sampler: noniid
[INFO][10:01:23]: [Client #213] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:01:23]: [93m[1m[Client #213] Started training in communication round #31.[0m
[INFO][10:01:25]: [Client #213] Loading the dataset.
[INFO][10:01:30]: [Client #213] Epoch: [1/5][0/10]	Loss: 0.001496
[INFO][10:01:30]: [Client #213] Epoch: [2/5][0/10]	Loss: 0.001174
[INFO][10:01:30]: [Client #213] Epoch: [3/5][0/10]	Loss: 0.005613
[INFO][10:01:31]: [Client #213] Epoch: [4/5][0/10]	Loss: 0.006429
[INFO][10:01:31]: [Client #213] Epoch: [5/5][0/10]	Loss: 0.001099
[INFO][10:01:31]: [Client #213] Model saved to /data/ykang/plato/results/test/model/lenet5_213_1127977.pth.
[INFO][10:01:31]: [Client #213] Loading a model from /data/ykang/plato/results/test/model/lenet5_213_1127977.pth.
[INFO][10:01:31]: [Client #213] Model trained.
[INFO][10:01:31]: [Client #213] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:01:31]: [Server #1127936] Received 0.24 MB of payload data from client #213 (simulated).
[INFO][10:01:31]: [Server #1127936] Adding client #470 to the list of clients for aggregation.
[INFO][10:01:31]: [Server #1127936] Adding client #306 to the list of clients for aggregation.
[INFO][10:01:31]: [Server #1127936] Adding client #469 to the list of clients for aggregation.
[INFO][10:01:31]: [Server #1127936] Adding client #116 to the list of clients for aggregation.
[INFO][10:01:31]: [Server #1127936] Adding client #164 to the list of clients for aggregation.
[INFO][10:01:31]: [Server #1127936] Adding client #446 to the list of clients for aggregation.
[INFO][10:01:31]: [Server #1127936] Adding client #213 to the list of clients for aggregation.
[INFO][10:01:31]: [Server #1127936] Adding client #210 to the list of clients for aggregation.
[INFO][10:01:31]: [Server #1127936] Adding client #434 to the list of clients for aggregation.
[INFO][10:01:31]: [Server #1127936] Adding client #66 to the list of clients for aggregation.
[INFO][10:01:31]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 471, 472, 473, 474, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00690249
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.03377631 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.013525   0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01836486
 0.         0.         0.00774017 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00851389
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00555035 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01278417 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01653069 0.00981718 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 3. 0. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 3. 1. 0. 0. 1. 0. 0. 1. 6. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 2. 0. 6. 0. 1. 2. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 1. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.
 1. 0. 1. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 4. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0.
 0. 1. 1. 1. 0. 2. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 0. 1. 0. 0. 0. 6. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1.
 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00690249
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.03377631 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.013525   0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01836486
 0.         0.         0.00774017 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00851389
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00555035 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01278417 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01653069 0.00981718 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:01:34]: [Server #1127936] Global model accuracy: 93.83%

[INFO][10:01:34]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_31.pth.
[INFO][10:01:34]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_31.pth.
[INFO][10:01:34]: [93m[1m
[Server #1127936] Starting round 32/100.[0m
[0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002
 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002 0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  4e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  4e-05  6e-10  6e-10
 6:  6.8876e+00  6.8875e+00  3e-05  4e-10  4e-10
 7:  6.8875e+00  6.8875e+00  3e-05  9e-10  3e-11
 8:  6.8875e+00  6.8875e+00  2e-05  8e-10  3e-11
 9:  6.8875e+00  6.8875e+00  1e-05  2e-09  8e-11
10:  6.8875e+00  6.8875e+00  1e-06  3e-09  1e-10
Optimal solution found.
The calculated probability is:  [6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49102449e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 9.67892057e-01 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 1.22079814e-04 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 1.75517760e-04 6.49113289e-05 6.49113289e-05 6.49099658e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 9.22761739e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49106280e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 1.16558659e-04 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 1.50768275e-04
 9.85735913e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05 6.49113289e-05 6.49113289e-05
 6.49113289e-05 6.49113289e-05]
current clients pool:  [INFO][10:01:34]: [Server #1127936] Selected clients: [116 145 445 286  59 117 300 123  68  83]
[INFO][10:01:34]: [Server #1127936] Selecting client #116 for training.
[INFO][10:01:34]: [Server #1127936] Sending the current model to client #116 (simulated).
[INFO][10:01:34]: [Server #1127936] Sending 0.24 MB of payload data to client #116 (simulated).
[INFO][10:01:34]: [Server #1127936] Selecting client #145 for training.
[INFO][10:01:34]: [Server #1127936] Sending the current model to client #145 (simulated).
[INFO][10:01:34]: [Server #1127936] Sending 0.24 MB of payload data to client #145 (simulated).
[INFO][10:01:34]: [Server #1127936] Selecting client #445 for training.
[INFO][10:01:34]: [Server #1127936] Sending the current model to client #445 (simulated).
[INFO][10:01:34]: [Client #116] Selected by the server.
[INFO][10:01:34]: [Client #116] Loading its data source...
[INFO][10:01:34]: [Client #116] Dataset size: 60000
[INFO][10:01:34]: [Client #116] Sampler: noniid
[INFO][10:01:34]: [Server #1127936] Sending 0.24 MB of payload data to client #445 (simulated).
[INFO][10:01:34]: [Client #145] Selected by the server.
[INFO][10:01:34]: [Client #145] Loading its data source...
[INFO][10:01:34]: [Client #145] Dataset size: 60000
[INFO][10:01:34]: [Client #145] Sampler: noniid
[INFO][10:01:34]: [Client #116] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:01:34]: [Client #445] Selected by the server.
[INFO][10:01:34]: [Client #445] Loading its data source...
[INFO][10:01:34]: [Client #445] Dataset size: 60000
[INFO][10:01:34]: [Client #445] Sampler: noniid
[INFO][10:01:34]: [Client #145] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:01:34]: [Client #445] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:01:34]: [93m[1m[Client #145] Started training in communication round #32.[0m
[INFO][10:01:34]: [93m[1m[Client #445] Started training in communication round #32.[0m
[INFO][10:01:34]: [93m[1m[Client #116] Started training in communication round #32.[0m
[INFO][10:01:36]: [Client #145] Loading the dataset.
[INFO][10:01:36]: [Client #116] Loading the dataset.
[INFO][10:01:36]: [Client #445] Loading the dataset.
[INFO][10:01:42]: [Client #145] Epoch: [1/5][0/10]	Loss: 0.200286
[INFO][10:01:42]: [Client #445] Epoch: [1/5][0/10]	Loss: 0.015832
[INFO][10:01:42]: [Client #116] Epoch: [1/5][0/10]	Loss: 0.207399
[INFO][10:01:43]: [Client #145] Epoch: [2/5][0/10]	Loss: 0.000027
[INFO][10:01:43]: [Client #116] Epoch: [2/5][0/10]	Loss: 0.033333
[INFO][10:01:43]: [Client #445] Epoch: [2/5][0/10]	Loss: 0.242853
[INFO][10:01:43]: [Client #145] Epoch: [3/5][0/10]	Loss: 0.023171
[INFO][10:01:43]: [Client #116] Epoch: [3/5][0/10]	Loss: 0.000016
[INFO][10:01:43]: [Client #445] Epoch: [3/5][0/10]	Loss: 0.007974
[INFO][10:01:43]: [Client #145] Epoch: [4/5][0/10]	Loss: 0.000122
[INFO][10:01:43]: [Client #116] Epoch: [4/5][0/10]	Loss: 0.000058
[INFO][10:01:43]: [Client #445] Epoch: [4/5][0/10]	Loss: 0.003664
[INFO][10:01:43]: [Client #116] Epoch: [5/5][0/10]	Loss: 0.000028
[INFO][10:01:43]: [Client #145] Epoch: [5/5][0/10]	Loss: 0.764603
[INFO][10:01:43]: [Client #145] Model saved to /data/ykang/plato/results/test/model/lenet5_145_1127978.pth.
[INFO][10:01:43]: [Client #445] Epoch: [5/5][0/10]	Loss: 0.001705
[INFO][10:01:43]: [Client #116] Model saved to /data/ykang/plato/results/test/model/lenet5_116_1127977.pth.
[INFO][10:01:43]: [Client #445] Model saved to /data/ykang/plato/results/test/model/lenet5_445_1127979.pth.
[INFO][10:01:44]: [Client #145] Loading a model from /data/ykang/plato/results/test/model/lenet5_145_1127978.pth.
[INFO][10:01:44]: [Client #145] Model trained.
[INFO][10:01:44]: [Client #145] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:01:44]: [Server #1127936] Received 0.24 MB of payload data from client #145 (simulated).
[INFO][10:01:44]: [Client #116] Loading a model from /data/ykang/plato/results/test/model/lenet5_116_1127977.pth.
[INFO][10:01:44]: [Client #116] Model trained.
[INFO][10:01:44]: [Client #116] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:01:44]: [Server #1127936] Received 0.24 MB of payload data from client #116 (simulated).
[INFO][10:01:44]: [Client #445] Loading a model from /data/ykang/plato/results/test/model/lenet5_445_1127979.pth.
[INFO][10:01:44]: [Client #445] Model trained.
[INFO][10:01:44]: [Client #445] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:01:44]: [Server #1127936] Received 0.24 MB of payload data from client #445 (simulated).
[INFO][10:01:44]: [Server #1127936] Selecting client #286 for training.
[INFO][10:01:44]: [Server #1127936] Sending the current model to client #286 (simulated).
[INFO][10:01:44]: [Server #1127936] Sending 0.24 MB of payload data to client #286 (simulated).
[INFO][10:01:44]: [Server #1127936] Selecting client #59 for training.
[INFO][10:01:44]: [Server #1127936] Sending the current model to client #59 (simulated).
[INFO][10:01:44]: [Server #1127936] Sending 0.24 MB of payload data to client #59 (simulated).
[INFO][10:01:44]: [Server #1127936] Selecting client #117 for training.
[INFO][10:01:44]: [Server #1127936] Sending the current model to client #117 (simulated).
[INFO][10:01:44]: [Client #286] Selected by the server.
[INFO][10:01:44]: [Client #286] Loading its data source...
[INFO][10:01:44]: [Client #286] Dataset size: 60000
[INFO][10:01:44]: [Client #286] Sampler: noniid
[INFO][10:01:44]: [Server #1127936] Sending 0.24 MB of payload data to client #117 (simulated).
[INFO][10:01:44]: [Client #59] Selected by the server.
[INFO][10:01:44]: [Client #117] Selected by the server.
[INFO][10:01:44]: [Client #59] Loading its data source...
[INFO][10:01:44]: [Client #59] Dataset size: 60000
[INFO][10:01:44]: [Client #117] Loading its data source...
[INFO][10:01:44]: [Client #59] Sampler: noniid
[INFO][10:01:44]: [Client #117] Dataset size: 60000
[INFO][10:01:44]: [Client #117] Sampler: noniid
[INFO][10:01:44]: [Client #286] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:01:44]: [Client #59] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:01:44]: [Client #117] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:01:44]: [93m[1m[Client #59] Started training in communication round #32.[0m
[INFO][10:01:44]: [93m[1m[Client #286] Started training in communication round #32.[0m
[INFO][10:01:44]: [93m[1m[Client #117] Started training in communication round #32.[0m
[INFO][10:01:46]: [Client #286] Loading the dataset.
[INFO][10:01:46]: [Client #117] Loading the dataset.
[INFO][10:01:46]: [Client #59] Loading the dataset.
[INFO][10:01:52]: [Client #286] Epoch: [1/5][0/10]	Loss: 0.100686
[INFO][10:01:52]: [Client #117] Epoch: [1/5][0/10]	Loss: 0.049549
[INFO][10:01:52]: [Client #59] Epoch: [1/5][0/10]	Loss: 0.058980
[INFO][10:01:52]: [Client #117] Epoch: [2/5][0/10]	Loss: 0.200429
[INFO][10:01:52]: [Client #286] Epoch: [2/5][0/10]	Loss: 0.004387
[INFO][10:01:52]: [Client #59] Epoch: [2/5][0/10]	Loss: 0.000520
[INFO][10:01:53]: [Client #286] Epoch: [3/5][0/10]	Loss: 0.000037
[INFO][10:01:53]: [Client #117] Epoch: [3/5][0/10]	Loss: 0.000295
[INFO][10:01:53]: [Client #59] Epoch: [3/5][0/10]	Loss: 0.014324
[INFO][10:01:53]: [Client #286] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:01:53]: [Client #117] Epoch: [4/5][0/10]	Loss: 0.000021
[INFO][10:01:53]: [Client #59] Epoch: [4/5][0/10]	Loss: 0.011280
[INFO][10:01:53]: [Client #286] Epoch: [5/5][0/10]	Loss: 0.000071
[INFO][10:01:53]: [Client #286] Model saved to /data/ykang/plato/results/test/model/lenet5_286_1127977.pth.
[INFO][10:01:53]: [Client #117] Epoch: [5/5][0/10]	Loss: 0.000008
[INFO][10:01:53]: [Client #59] Epoch: [5/5][0/10]	Loss: 0.000650
[INFO][10:01:53]: [Client #59] Model saved to /data/ykang/plato/results/test/model/lenet5_59_1127978.pth.
[INFO][10:01:53]: [Client #117] Model saved to /data/ykang/plato/results/test/model/lenet5_117_1127979.pth.
[INFO][10:01:54]: [Client #286] Loading a model from /data/ykang/plato/results/test/model/lenet5_286_1127977.pth.
[INFO][10:01:54]: [Client #286] Model trained.
[INFO][10:01:54]: [Client #286] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:01:54]: [Server #1127936] Received 0.24 MB of payload data from client #286 (simulated).
[INFO][10:01:54]: [Client #117] Loading a model from /data/ykang/plato/results/test/model/lenet5_117_1127979.pth.
[INFO][10:01:54]: [Client #117] Model trained.
[INFO][10:01:54]: [Client #117] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:01:54]: [Server #1127936] Received 0.24 MB of payload data from client #117 (simulated).
[INFO][10:01:54]: [Client #59] Loading a model from /data/ykang/plato/results/test/model/lenet5_59_1127978.pth.
[INFO][10:01:54]: [Client #59] Model trained.
[INFO][10:01:54]: [Client #59] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:01:54]: [Server #1127936] Received 0.24 MB of payload data from client #59 (simulated).
[INFO][10:01:54]: [Server #1127936] Selecting client #300 for training.
[INFO][10:01:54]: [Server #1127936] Sending the current model to client #300 (simulated).
[INFO][10:01:54]: [Server #1127936] Sending 0.24 MB of payload data to client #300 (simulated).
[INFO][10:01:54]: [Server #1127936] Selecting client #123 for training.
[INFO][10:01:54]: [Server #1127936] Sending the current model to client #123 (simulated).
[INFO][10:01:54]: [Server #1127936] Sending 0.24 MB of payload data to client #123 (simulated).
[INFO][10:01:54]: [Server #1127936] Selecting client #68 for training.
[INFO][10:01:54]: [Server #1127936] Sending the current model to client #68 (simulated).
[INFO][10:01:54]: [Client #300] Selected by the server.
[INFO][10:01:54]: [Client #300] Loading its data source...
[INFO][10:01:54]: [Client #300] Dataset size: 60000
[INFO][10:01:54]: [Client #300] Sampler: noniid
[INFO][10:01:54]: [Server #1127936] Sending 0.24 MB of payload data to client #68 (simulated).
[INFO][10:01:54]: [Client #123] Selected by the server.
[INFO][10:01:54]: [Client #123] Loading its data source...
[INFO][10:01:54]: [Client #123] Dataset size: 60000
[INFO][10:01:54]: [Client #123] Sampler: noniid
[INFO][10:01:54]: [Client #300] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:01:54]: [Client #68] Selected by the server.
[INFO][10:01:54]: [Client #68] Loading its data source...
[INFO][10:01:54]: [Client #68] Dataset size: 60000
[INFO][10:01:54]: [Client #68] Sampler: noniid
[INFO][10:01:54]: [Client #123] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:01:54]: [Client #68] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:01:54]: [93m[1m[Client #300] Started training in communication round #32.[0m
[INFO][10:01:54]: [93m[1m[Client #68] Started training in communication round #32.[0m
[INFO][10:01:54]: [93m[1m[Client #123] Started training in communication round #32.[0m
[INFO][10:01:56]: [Client #300] Loading the dataset.
[INFO][10:01:56]: [Client #68] Loading the dataset.
[INFO][10:01:56]: [Client #123] Loading the dataset.
[INFO][10:02:02]: [Client #123] Epoch: [1/5][0/10]	Loss: 0.079947
[INFO][10:02:02]: [Client #300] Epoch: [1/5][0/10]	Loss: 0.093150
[INFO][10:02:02]: [Client #68] Epoch: [1/5][0/10]	Loss: 0.109391
[INFO][10:02:02]: [Client #123] Epoch: [2/5][0/10]	Loss: 0.000768
[INFO][10:02:02]: [Client #300] Epoch: [2/5][0/10]	Loss: 0.000092
[INFO][10:02:02]: [Client #68] Epoch: [2/5][0/10]	Loss: 0.000806
[INFO][10:02:02]: [Client #300] Epoch: [3/5][0/10]	Loss: 0.000562
[INFO][10:02:02]: [Client #123] Epoch: [3/5][0/10]	Loss: 0.000005
[INFO][10:02:02]: [Client #68] Epoch: [3/5][0/10]	Loss: 0.009921
[INFO][10:02:02]: [Client #300] Epoch: [4/5][0/10]	Loss: 0.000414
[INFO][10:02:03]: [Client #123] Epoch: [4/5][0/10]	Loss: 0.003042
[INFO][10:02:03]: [Client #300] Epoch: [5/5][0/10]	Loss: 0.079759
[INFO][10:02:03]: [Client #68] Epoch: [4/5][0/10]	Loss: 0.001049
[INFO][10:02:03]: [Client #123] Epoch: [5/5][0/10]	Loss: 0.000113
[INFO][10:02:03]: [Client #300] Model saved to /data/ykang/plato/results/test/model/lenet5_300_1127977.pth.
[INFO][10:02:03]: [Client #68] Epoch: [5/5][0/10]	Loss: 0.000298
[INFO][10:02:03]: [Client #123] Model saved to /data/ykang/plato/results/test/model/lenet5_123_1127978.pth.
[INFO][10:02:03]: [Client #68] Model saved to /data/ykang/plato/results/test/model/lenet5_68_1127979.pth.
[INFO][10:02:03]: [Client #300] Loading a model from /data/ykang/plato/results/test/model/lenet5_300_1127977.pth.
[INFO][10:02:03]: [Client #300] Model trained.
[INFO][10:02:03]: [Client #300] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:02:03]: [Server #1127936] Received 0.24 MB of payload data from client #300 (simulated).
[INFO][10:02:03]: [Client #123] Loading a model from /data/ykang/plato/results/test/model/lenet5_123_1127978.pth.
[INFO][10:02:04]: [Client #123] Model trained.
[INFO][10:02:04]: [Client #123] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:02:04]: [Server #1127936] Received 0.24 MB of payload data from client #123 (simulated).
[INFO][10:02:04]: [Client #68] Loading a model from /data/ykang/plato/results/test/model/lenet5_68_1127979.pth.
[INFO][10:02:04]: [Client #68] Model trained.
[INFO][10:02:04]: [Client #68] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:02:04]: [Server #1127936] Received 0.24 MB of payload data from client #68 (simulated).
[INFO][10:02:04]: [Server #1127936] Selecting client #83 for training.
[INFO][10:02:04]: [Server #1127936] Sending the current model to client #83 (simulated).
[INFO][10:02:04]: [Server #1127936] Sending 0.24 MB of payload data to client #83 (simulated).
[INFO][10:02:04]: [Client #83] Selected by the server.
[INFO][10:02:04]: [Client #83] Loading its data source...
[INFO][10:02:04]: [Client #83] Dataset size: 60000
[INFO][10:02:04]: [Client #83] Sampler: noniid
[INFO][10:02:04]: [Client #83] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:02:04]: [93m[1m[Client #83] Started training in communication round #32.[0m
[INFO][10:02:06]: [Client #83] Loading the dataset.
[INFO][10:02:11]: [Client #83] Epoch: [1/5][0/10]	Loss: 0.191962
[INFO][10:02:11]: [Client #83] Epoch: [2/5][0/10]	Loss: 0.039867
[INFO][10:02:11]: [Client #83] Epoch: [3/5][0/10]	Loss: 0.000000
[INFO][10:02:11]: [Client #83] Epoch: [4/5][0/10]	Loss: 0.000010
[INFO][10:02:11]: [Client #83] Epoch: [5/5][0/10]	Loss: 0.000050
[INFO][10:02:11]: [Client #83] Model saved to /data/ykang/plato/results/test/model/lenet5_83_1127977.pth.
[INFO][10:02:12]: [Client #83] Loading a model from /data/ykang/plato/results/test/model/lenet5_83_1127977.pth.
[INFO][10:02:12]: [Client #83] Model trained.
[INFO][10:02:12]: [Client #83] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:02:12]: [Server #1127936] Received 0.24 MB of payload data from client #83 (simulated).
[INFO][10:02:12]: [Server #1127936] Adding client #126 to the list of clients for aggregation.
[INFO][10:02:12]: [Server #1127936] Adding client #149 to the list of clients for aggregation.
[INFO][10:02:12]: [Server #1127936] Adding client #309 to the list of clients for aggregation.
[INFO][10:02:12]: [Server #1127936] Adding client #419 to the list of clients for aggregation.
[INFO][10:02:12]: [Server #1127936] Adding client #475 to the list of clients for aggregation.
[INFO][10:02:12]: [Server #1127936] Adding client #265 to the list of clients for aggregation.
[INFO][10:02:12]: [Server #1127936] Adding client #123 to the list of clients for aggregation.
[INFO][10:02:12]: [Server #1127936] Adding client #145 to the list of clients for aggregation.
[INFO][10:02:12]: [Server #1127936] Adding client #300 to the list of clients for aggregation.
[INFO][10:02:12]: [Server #1127936] Adding client #68 to the list of clients for aggregation.
[INFO][10:02:12]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01457783 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01201187 0.         0.         0.02592483
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.02022247 0.         0.         0.         0.01348247 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01704368 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01039288
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01517861 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01628113 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.02440005 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 3. 0. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 3. 1. 0. 0. 1. 0. 0. 1. 6. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 2. 0. 6. 0. 1. 2. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 1. 1. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.
 1. 0. 1. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 4. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0.
 0. 1. 1. 1. 0. 2. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 0. 1. 0. 0. 0. 6. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1.
 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 2. 0. 1. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01457783 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01201187 0.         0.         0.02592483
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.02022247 0.         0.         0.         0.01348247 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01704368 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01039288
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01517861 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01628113 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.02440005 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:02:14]: [Server #1127936] Global model accuracy: 94.24%

[INFO][10:02:14]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_32.pth.
[INFO][10:02:14]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_32.pth.
[INFO][10:02:14]: [93m[1m
[Server #1127936] Starting round 33/100.[0m
[0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002
 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  4e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  5e-05  9e-10  9e-10
 6:  6.8876e+00  6.8875e+00  5e-05  5e-10  5e-10
 7:  6.8875e+00  6.8875e+00  5e-05  1e-09  7e-11
 8:  6.8875e+00  6.8875e+00  4e-05  1e-09  7e-11
 9:  6.8875e+00  6.8875e+00  1e-05  1e-08  5e-10
10:  6.8875e+00  6.8875e+00  2e-06  4e-09  2e-10
Optimal solution found.
The calculated probability is:  [1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04643152e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04644809e-04
 1.04648311e-04 1.04648311e-04 2.59459599e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04638384e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.52093050e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.72661949e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04645689e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.61249734e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.67808737e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 9.48436965e-01 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04 1.04648311e-04 1.04648311e-04
 1.04648311e-04 1.04648311e-04]
current clients pool:  [INFO][10:02:15]: [Server #1127936] Selected clients: [475 268  99 377   4 167 459 296 427 304]
[INFO][10:02:15]: [Server #1127936] Selecting client #475 for training.
[INFO][10:02:15]: [Server #1127936] Sending the current model to client #475 (simulated).
[INFO][10:02:15]: [Server #1127936] Sending 0.24 MB of payload data to client #475 (simulated).
[INFO][10:02:15]: [Server #1127936] Selecting client #268 for training.
[INFO][10:02:15]: [Server #1127936] Sending the current model to client #268 (simulated).
[INFO][10:02:15]: [Server #1127936] Sending 0.24 MB of payload data to client #268 (simulated).
[INFO][10:02:15]: [Server #1127936] Selecting client #99 for training.
[INFO][10:02:15]: [Server #1127936] Sending the current model to client #99 (simulated).
[INFO][10:02:15]: [Client #475] Selected by the server.
[INFO][10:02:15]: [Client #475] Loading its data source...
[INFO][10:02:15]: [Client #475] Dataset size: 60000
[INFO][10:02:15]: [Client #475] Sampler: noniid
[INFO][10:02:15]: [Server #1127936] Sending 0.24 MB of payload data to client #99 (simulated).
[INFO][10:02:15]: [Client #268] Selected by the server.
[INFO][10:02:15]: [Client #268] Loading its data source...
[INFO][10:02:15]: [Client #268] Dataset size: 60000
[INFO][10:02:15]: [Client #268] Sampler: noniid
[INFO][10:02:15]: [Client #99] Selected by the server.
[INFO][10:02:15]: [Client #475] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:02:15]: [Client #99] Loading its data source...
[INFO][10:02:15]: [Client #99] Dataset size: 60000
[INFO][10:02:15]: [Client #99] Sampler: noniid
[INFO][10:02:15]: [Client #268] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:02:15]: [Client #99] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:02:15]: [93m[1m[Client #475] Started training in communication round #33.[0m
[INFO][10:02:15]: [93m[1m[Client #268] Started training in communication round #33.[0m
[INFO][10:02:15]: [93m[1m[Client #99] Started training in communication round #33.[0m
[INFO][10:02:17]: [Client #268] Loading the dataset.
[INFO][10:02:17]: [Client #99] Loading the dataset.
[INFO][10:02:17]: [Client #475] Loading the dataset.
[INFO][10:02:23]: [Client #268] Epoch: [1/5][0/10]	Loss: 0.003853
[INFO][10:02:23]: [Client #268] Epoch: [2/5][0/10]	Loss: 0.000299
[INFO][10:02:23]: [Client #475] Epoch: [1/5][0/10]	Loss: 0.028370
[INFO][10:02:23]: [Client #99] Epoch: [1/5][0/10]	Loss: 0.003352
[INFO][10:02:23]: [Client #268] Epoch: [3/5][0/10]	Loss: 0.005087
[INFO][10:02:23]: [Client #99] Epoch: [2/5][0/10]	Loss: 0.022658
[INFO][10:02:23]: [Client #475] Epoch: [2/5][0/10]	Loss: 0.013199
[INFO][10:02:23]: [Client #99] Epoch: [3/5][0/10]	Loss: 0.000093
[INFO][10:02:23]: [Client #475] Epoch: [3/5][0/10]	Loss: 0.000001
[INFO][10:02:23]: [Client #268] Epoch: [4/5][0/10]	Loss: 0.000829
[INFO][10:02:23]: [Client #99] Epoch: [4/5][0/10]	Loss: 0.000003
[INFO][10:02:24]: [Client #268] Epoch: [5/5][0/10]	Loss: 0.000151
[INFO][10:02:24]: [Client #475] Epoch: [4/5][0/10]	Loss: 0.000001
[INFO][10:02:24]: [Client #99] Epoch: [5/5][0/10]	Loss: 0.000588
[INFO][10:02:24]: [Client #268] Model saved to /data/ykang/plato/results/test/model/lenet5_268_1127978.pth.
[INFO][10:02:24]: [Client #99] Model saved to /data/ykang/plato/results/test/model/lenet5_99_1127979.pth.
[INFO][10:02:24]: [Client #475] Epoch: [5/5][0/10]	Loss: 0.000024
[INFO][10:02:24]: [Client #475] Model saved to /data/ykang/plato/results/test/model/lenet5_475_1127977.pth.
[INFO][10:02:24]: [Client #268] Loading a model from /data/ykang/plato/results/test/model/lenet5_268_1127978.pth.
[INFO][10:02:24]: [Client #268] Model trained.
[INFO][10:02:24]: [Client #268] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:02:24]: [Server #1127936] Received 0.24 MB of payload data from client #268 (simulated).
[INFO][10:02:24]: [Client #99] Loading a model from /data/ykang/plato/results/test/model/lenet5_99_1127979.pth.
[INFO][10:02:24]: [Client #99] Model trained.
[INFO][10:02:24]: [Client #99] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:02:24]: [Server #1127936] Received 0.24 MB of payload data from client #99 (simulated).
[INFO][10:02:25]: [Client #475] Loading a model from /data/ykang/plato/results/test/model/lenet5_475_1127977.pth.
[INFO][10:02:25]: [Client #475] Model trained.
[INFO][10:02:25]: [Client #475] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:02:25]: [Server #1127936] Received 0.24 MB of payload data from client #475 (simulated).
[INFO][10:02:25]: [Server #1127936] Selecting client #377 for training.
[INFO][10:02:25]: [Server #1127936] Sending the current model to client #377 (simulated).
[INFO][10:02:25]: [Server #1127936] Sending 0.24 MB of payload data to client #377 (simulated).
[INFO][10:02:25]: [Server #1127936] Selecting client #4 for training.
[INFO][10:02:25]: [Server #1127936] Sending the current model to client #4 (simulated).
[INFO][10:02:25]: [Server #1127936] Sending 0.24 MB of payload data to client #4 (simulated).
[INFO][10:02:25]: [Server #1127936] Selecting client #167 for training.
[INFO][10:02:25]: [Server #1127936] Sending the current model to client #167 (simulated).
[INFO][10:02:25]: [Client #377] Selected by the server.
[INFO][10:02:25]: [Client #377] Loading its data source...
[INFO][10:02:25]: [Client #377] Dataset size: 60000
[INFO][10:02:25]: [Client #377] Sampler: noniid
[INFO][10:02:25]: [Server #1127936] Sending 0.24 MB of payload data to client #167 (simulated).
[INFO][10:02:25]: [Client #4] Selected by the server.
[INFO][10:02:25]: [Client #4] Loading its data source...
[INFO][10:02:25]: [Client #4] Dataset size: 60000
[INFO][10:02:25]: [Client #4] Sampler: noniid
[INFO][10:02:25]: [Client #167] Selected by the server.
[INFO][10:02:25]: [Client #167] Loading its data source...
[INFO][10:02:25]: [Client #167] Dataset size: 60000
[INFO][10:02:25]: [Client #167] Sampler: noniid
[INFO][10:02:25]: [Client #377] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:02:25]: [Client #4] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:02:25]: [Client #167] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:02:25]: [93m[1m[Client #4] Started training in communication round #33.[0m
[INFO][10:02:25]: [93m[1m[Client #377] Started training in communication round #33.[0m
[INFO][10:02:25]: [93m[1m[Client #167] Started training in communication round #33.[0m
[INFO][10:02:27]: [Client #167] Loading the dataset.
[INFO][10:02:27]: [Client #377] Loading the dataset.
[INFO][10:02:27]: [Client #4] Loading the dataset.
[INFO][10:02:32]: [Client #167] Epoch: [1/5][0/10]	Loss: 0.003853
[INFO][10:02:32]: [Client #377] Epoch: [1/5][0/10]	Loss: 0.189596
[INFO][10:02:32]: [Client #167] Epoch: [2/5][0/10]	Loss: 0.001261
[INFO][10:02:32]: [Client #4] Epoch: [1/5][0/10]	Loss: 0.015610
[INFO][10:02:33]: [Client #377] Epoch: [2/5][0/10]	Loss: 0.003022
[INFO][10:02:33]: [Client #167] Epoch: [3/5][0/10]	Loss: 0.000370
[INFO][10:02:33]: [Client #4] Epoch: [2/5][0/10]	Loss: 0.002962
[INFO][10:02:33]: [Client #377] Epoch: [3/5][0/10]	Loss: 0.000017
[INFO][10:02:33]: [Client #4] Epoch: [3/5][0/10]	Loss: 0.000758
[INFO][10:02:33]: [Client #167] Epoch: [4/5][0/10]	Loss: 0.001535
[INFO][10:02:33]: [Client #377] Epoch: [4/5][0/10]	Loss: 0.024902
[INFO][10:02:33]: [Client #4] Epoch: [4/5][0/10]	Loss: 0.000001
[INFO][10:02:33]: [Client #167] Epoch: [5/5][0/10]	Loss: 0.001047
[INFO][10:02:33]: [Client #377] Epoch: [5/5][0/10]	Loss: 0.083527
[INFO][10:02:33]: [Client #167] Model saved to /data/ykang/plato/results/test/model/lenet5_167_1127979.pth.
[INFO][10:02:33]: [Client #4] Epoch: [5/5][0/10]	Loss: 0.000008
[INFO][10:02:33]: [Client #377] Model saved to /data/ykang/plato/results/test/model/lenet5_377_1127977.pth.
[INFO][10:02:33]: [Client #4] Model saved to /data/ykang/plato/results/test/model/lenet5_4_1127978.pth.
[INFO][10:02:34]: [Client #167] Loading a model from /data/ykang/plato/results/test/model/lenet5_167_1127979.pth.
[INFO][10:02:34]: [Client #167] Model trained.
[INFO][10:02:34]: [Client #167] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:02:34]: [Server #1127936] Received 0.24 MB of payload data from client #167 (simulated).
[INFO][10:02:34]: [Client #377] Loading a model from /data/ykang/plato/results/test/model/lenet5_377_1127977.pth.
[INFO][10:02:34]: [Client #377] Model trained.
[INFO][10:02:34]: [Client #377] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:02:34]: [Server #1127936] Received 0.24 MB of payload data from client #377 (simulated).
[INFO][10:02:34]: [Client #4] Loading a model from /data/ykang/plato/results/test/model/lenet5_4_1127978.pth.
[INFO][10:02:34]: [Client #4] Model trained.
[INFO][10:02:34]: [Client #4] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:02:34]: [Server #1127936] Received 0.24 MB of payload data from client #4 (simulated).
[INFO][10:02:34]: [Server #1127936] Selecting client #459 for training.
[INFO][10:02:34]: [Server #1127936] Sending the current model to client #459 (simulated).
[INFO][10:02:34]: [Server #1127936] Sending 0.24 MB of payload data to client #459 (simulated).
[INFO][10:02:34]: [Server #1127936] Selecting client #296 for training.
[INFO][10:02:34]: [Server #1127936] Sending the current model to client #296 (simulated).
[INFO][10:02:34]: [Server #1127936] Sending 0.24 MB of payload data to client #296 (simulated).
[INFO][10:02:34]: [Server #1127936] Selecting client #427 for training.
[INFO][10:02:34]: [Server #1127936] Sending the current model to client #427 (simulated).
[INFO][10:02:34]: [Client #459] Selected by the server.
[INFO][10:02:34]: [Client #459] Loading its data source...
[INFO][10:02:34]: [Client #459] Dataset size: 60000
[INFO][10:02:34]: [Client #459] Sampler: noniid
[INFO][10:02:34]: [Server #1127936] Sending 0.24 MB of payload data to client #427 (simulated).
[INFO][10:02:34]: [Client #296] Selected by the server.
[INFO][10:02:34]: [Client #296] Loading its data source...
[INFO][10:02:34]: [Client #296] Dataset size: 60000
[INFO][10:02:34]: [Client #296] Sampler: noniid
[INFO][10:02:34]: [Client #427] Selected by the server.
[INFO][10:02:34]: [Client #427] Loading its data source...
[INFO][10:02:34]: [Client #427] Dataset size: 60000
[INFO][10:02:34]: [Client #427] Sampler: noniid
[INFO][10:02:34]: [Client #459] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:02:34]: [Client #296] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:02:34]: [Client #427] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:02:34]: [93m[1m[Client #459] Started training in communication round #33.[0m
[INFO][10:02:34]: [93m[1m[Client #427] Started training in communication round #33.[0m
[INFO][10:02:34]: [93m[1m[Client #296] Started training in communication round #33.[0m
[INFO][10:02:36]: [Client #296] Loading the dataset.
[INFO][10:02:36]: [Client #459] Loading the dataset.
[INFO][10:02:36]: [Client #427] Loading the dataset.
[INFO][10:02:42]: [Client #296] Epoch: [1/5][0/10]	Loss: 0.054189
[INFO][10:02:42]: [Client #427] Epoch: [1/5][0/10]	Loss: 0.072707
[INFO][10:02:42]: [Client #296] Epoch: [2/5][0/10]	Loss: 0.042099
[INFO][10:02:42]: [Client #459] Epoch: [1/5][0/10]	Loss: 0.265027
[INFO][10:02:42]: [Client #427] Epoch: [2/5][0/10]	Loss: 0.006710
[INFO][10:02:42]: [Client #296] Epoch: [3/5][0/10]	Loss: 0.014904
[INFO][10:02:42]: [Client #459] Epoch: [2/5][0/10]	Loss: 0.003856
[INFO][10:02:42]: [Client #427] Epoch: [3/5][0/10]	Loss: 0.000315
[INFO][10:02:42]: [Client #296] Epoch: [4/5][0/10]	Loss: 0.282628
[INFO][10:02:42]: [Client #459] Epoch: [3/5][0/10]	Loss: 0.000232
[INFO][10:02:42]: [Client #427] Epoch: [4/5][0/10]	Loss: 0.000249
[INFO][10:02:42]: [Client #459] Epoch: [4/5][0/10]	Loss: 0.000099
[INFO][10:02:42]: [Client #296] Epoch: [5/5][0/10]	Loss: 0.465408
[INFO][10:02:42]: [Client #296] Model saved to /data/ykang/plato/results/test/model/lenet5_296_1127978.pth.
[INFO][10:02:42]: [Client #427] Epoch: [5/5][0/10]	Loss: 0.001193
[INFO][10:02:42]: [Client #459] Epoch: [5/5][0/10]	Loss: 0.001106
[INFO][10:02:42]: [Client #427] Model saved to /data/ykang/plato/results/test/model/lenet5_427_1127979.pth.
[INFO][10:02:42]: [Client #459] Model saved to /data/ykang/plato/results/test/model/lenet5_459_1127977.pth.
[INFO][10:02:43]: [Client #296] Loading a model from /data/ykang/plato/results/test/model/lenet5_296_1127978.pth.
[INFO][10:02:43]: [Client #296] Model trained.
[INFO][10:02:43]: [Client #296] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:02:43]: [Server #1127936] Received 0.24 MB of payload data from client #296 (simulated).
[INFO][10:02:43]: [Client #427] Loading a model from /data/ykang/plato/results/test/model/lenet5_427_1127979.pth.
[INFO][10:02:43]: [Client #427] Model trained.
[INFO][10:02:43]: [Client #427] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:02:43]: [Server #1127936] Received 0.24 MB of payload data from client #427 (simulated).
[INFO][10:02:43]: [Client #459] Loading a model from /data/ykang/plato/results/test/model/lenet5_459_1127977.pth.
[INFO][10:02:43]: [Client #459] Model trained.
[INFO][10:02:43]: [Client #459] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:02:43]: [Server #1127936] Received 0.24 MB of payload data from client #459 (simulated).
[INFO][10:02:43]: [Server #1127936] Selecting client #304 for training.
[INFO][10:02:43]: [Server #1127936] Sending the current model to client #304 (simulated).
[INFO][10:02:43]: [Server #1127936] Sending 0.24 MB of payload data to client #304 (simulated).
[INFO][10:02:43]: [Client #304] Selected by the server.
[INFO][10:02:43]: [Client #304] Loading its data source...
[INFO][10:02:43]: [Client #304] Dataset size: 60000
[INFO][10:02:43]: [Client #304] Sampler: noniid
[INFO][10:02:43]: [Client #304] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:02:43]: [93m[1m[Client #304] Started training in communication round #33.[0m
[INFO][10:02:45]: [Client #304] Loading the dataset.
[INFO][10:02:50]: [Client #304] Epoch: [1/5][0/10]	Loss: 0.009396
[INFO][10:02:51]: [Client #304] Epoch: [2/5][0/10]	Loss: 0.000887
[INFO][10:02:51]: [Client #304] Epoch: [3/5][0/10]	Loss: 0.000060
[INFO][10:02:51]: [Client #304] Epoch: [4/5][0/10]	Loss: 0.000956
[INFO][10:02:51]: [Client #304] Epoch: [5/5][0/10]	Loss: 0.026388
[INFO][10:02:51]: [Client #304] Model saved to /data/ykang/plato/results/test/model/lenet5_304_1127977.pth.
[INFO][10:02:51]: [Client #304] Loading a model from /data/ykang/plato/results/test/model/lenet5_304_1127977.pth.
[INFO][10:02:51]: [Client #304] Model trained.
[INFO][10:02:51]: [Client #304] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:02:51]: [Server #1127936] Received 0.24 MB of payload data from client #304 (simulated).
[INFO][10:02:51]: [Server #1127936] Adding client #59 to the list of clients for aggregation.
[INFO][10:02:51]: [Server #1127936] Adding client #83 to the list of clients for aggregation.
[INFO][10:02:51]: [Server #1127936] Adding client #117 to the list of clients for aggregation.
[INFO][10:02:51]: [Server #1127936] Adding client #116 to the list of clients for aggregation.
[INFO][10:02:51]: [Server #1127936] Adding client #286 to the list of clients for aggregation.
[INFO][10:02:51]: [Server #1127936] Adding client #22 to the list of clients for aggregation.
[INFO][10:02:51]: [Server #1127936] Adding client #99 to the list of clients for aggregation.
[INFO][10:02:51]: [Server #1127936] Adding client #427 to the list of clients for aggregation.
[INFO][10:02:51]: [Server #1127936] Adding client #296 to the list of clients for aggregation.
[INFO][10:02:51]: [Server #1127936] Adding client #167 to the list of clients for aggregation.
[INFO][10:02:51]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01651398 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00899494 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01219359 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00711113 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.02284442 0.00941114 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01005408 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.02400404 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01445464 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01041279 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 3. 0. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 3. 1. 0. 0. 1. 0. 0. 1. 6. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 2. 0. 6. 0. 1. 2. 1. 0. 1. 1. 0. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 1. 1. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.
 1. 0. 1. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 4. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0.
 0. 1. 1. 1. 0. 2. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 0. 1. 0. 0. 0. 6. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1.
 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 2. 0. 1. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01651398 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00899494 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01219359 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00711113 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.02284442 0.00941114 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01005408 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.02400404 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01445464 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01041279 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:02:53]: [Server #1127936] Global model accuracy: 94.67%

[INFO][10:02:53]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_33.pth.
[INFO][10:02:53]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_33.pth.
[INFO][10:02:53]: [93m[1m
[Server #1127936] Starting round 34/100.[0m
[0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  4e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  5e-05  9e-10  9e-10
 6:  6.8876e+00  6.8875e+00  5e-05  5e-10  5e-10
 7:  6.8875e+00  6.8875e+00  5e-05  1e-09  7e-11
 8:  6.8875e+00  6.8875e+00  4e-05  1e-09  7e-11
 9:  6.8875e+00  6.8875e+00  1e-05  1e-08  5e-10
10:  6.8875e+00  6.8875e+00  3e-06  4e-09  2e-10
Optimal solution found.
The calculated probability is:  [1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 9.34614306e-01 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.66669993e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.83211033e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32861867e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 2.73006023e-04 1.68652483e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32860352e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 2.88291023e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32857117e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32860132e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04 1.32863384e-04 1.32863384e-04
 1.32863384e-04 1.32863384e-04]
current clients pool:  [INFO][10:02:54]: [Server #1127936] Selected clients: [ 22 393   7 152  12 499 289 147 465 252]
[INFO][10:02:54]: [Server #1127936] Selecting client #22 for training.
[INFO][10:02:54]: [Server #1127936] Sending the current model to client #22 (simulated).
[INFO][10:02:54]: [Server #1127936] Sending 0.24 MB of payload data to client #22 (simulated).
[INFO][10:02:54]: [Server #1127936] Selecting client #393 for training.
[INFO][10:02:54]: [Server #1127936] Sending the current model to client #393 (simulated).
[INFO][10:02:54]: [Server #1127936] Sending 0.24 MB of payload data to client #393 (simulated).
[INFO][10:02:54]: [Server #1127936] Selecting client #7 for training.
[INFO][10:02:54]: [Server #1127936] Sending the current model to client #7 (simulated).
[INFO][10:02:54]: [Client #22] Selected by the server.
[INFO][10:02:54]: [Client #22] Loading its data source...
[INFO][10:02:54]: [Client #22] Dataset size: 60000
[INFO][10:02:54]: [Client #22] Sampler: noniid
[INFO][10:02:54]: [Server #1127936] Sending 0.24 MB of payload data to client #7 (simulated).
[INFO][10:02:54]: [Client #393] Selected by the server.
[INFO][10:02:54]: [Client #393] Loading its data source...
[INFO][10:02:54]: [Client #393] Dataset size: 60000
[INFO][10:02:54]: [Client #393] Sampler: noniid
[INFO][10:02:54]: [Client #22] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:02:54]: [Client #7] Selected by the server.
[INFO][10:02:54]: [Client #7] Loading its data source...
[INFO][10:02:54]: [Client #7] Dataset size: 60000
[INFO][10:02:54]: [Client #7] Sampler: noniid
[INFO][10:02:54]: [Client #393] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:02:54]: [Client #7] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:02:54]: [93m[1m[Client #7] Started training in communication round #34.[0m
[INFO][10:02:54]: [93m[1m[Client #393] Started training in communication round #34.[0m
[INFO][10:02:54]: [93m[1m[Client #22] Started training in communication round #34.[0m
[INFO][10:02:56]: [Client #393] Loading the dataset.
[INFO][10:02:56]: [Client #22] Loading the dataset.
[INFO][10:02:56]: [Client #7] Loading the dataset.
[INFO][10:03:02]: [Client #22] Epoch: [1/5][0/10]	Loss: 0.028354
[INFO][10:03:02]: [Client #393] Epoch: [1/5][0/10]	Loss: 0.004033
[INFO][10:03:02]: [Client #22] Epoch: [2/5][0/10]	Loss: 0.011530
[INFO][10:03:02]: [Client #7] Epoch: [1/5][0/10]	Loss: 0.003899
[INFO][10:03:02]: [Client #393] Epoch: [2/5][0/10]	Loss: 0.006888
[INFO][10:03:02]: [Client #22] Epoch: [3/5][0/10]	Loss: 0.000061
[INFO][10:03:02]: [Client #7] Epoch: [2/5][0/10]	Loss: 0.000321
[INFO][10:03:02]: [Client #7] Epoch: [3/5][0/10]	Loss: 0.000000
[INFO][10:03:02]: [Client #393] Epoch: [3/5][0/10]	Loss: 0.000000
[INFO][10:03:02]: [Client #22] Epoch: [4/5][0/10]	Loss: 0.000660
[INFO][10:03:02]: [Client #393] Epoch: [4/5][0/10]	Loss: 0.007347
[INFO][10:03:02]: [Client #7] Epoch: [4/5][0/10]	Loss: 0.000001
[INFO][10:03:02]: [Client #22] Epoch: [5/5][0/10]	Loss: 0.061714
[INFO][10:03:02]: [Client #393] Epoch: [5/5][0/10]	Loss: 0.000556
[INFO][10:03:02]: [Client #22] Model saved to /data/ykang/plato/results/test/model/lenet5_22_1127977.pth.
[INFO][10:03:02]: [Client #7] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:03:02]: [Client #393] Model saved to /data/ykang/plato/results/test/model/lenet5_393_1127978.pth.
[INFO][10:03:02]: [Client #7] Model saved to /data/ykang/plato/results/test/model/lenet5_7_1127979.pth.
[INFO][10:03:03]: [Client #22] Loading a model from /data/ykang/plato/results/test/model/lenet5_22_1127977.pth.
[INFO][10:03:03]: [Client #22] Model trained.
[INFO][10:03:03]: [Client #22] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:03:03]: [Server #1127936] Received 0.24 MB of payload data from client #22 (simulated).
[INFO][10:03:03]: [Client #7] Loading a model from /data/ykang/plato/results/test/model/lenet5_7_1127979.pth.
[INFO][10:03:03]: [Client #7] Model trained.
[INFO][10:03:03]: [Client #7] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:03:03]: [Server #1127936] Received 0.24 MB of payload data from client #7 (simulated).
[INFO][10:03:03]: [Client #393] Loading a model from /data/ykang/plato/results/test/model/lenet5_393_1127978.pth.
[INFO][10:03:03]: [Client #393] Model trained.
[INFO][10:03:03]: [Client #393] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:03:03]: [Server #1127936] Received 0.24 MB of payload data from client #393 (simulated).
[INFO][10:03:03]: [Server #1127936] Selecting client #152 for training.
[INFO][10:03:03]: [Server #1127936] Sending the current model to client #152 (simulated).
[INFO][10:03:03]: [Server #1127936] Sending 0.24 MB of payload data to client #152 (simulated).
[INFO][10:03:03]: [Server #1127936] Selecting client #12 for training.
[INFO][10:03:03]: [Server #1127936] Sending the current model to client #12 (simulated).
[INFO][10:03:03]: [Server #1127936] Sending 0.24 MB of payload data to client #12 (simulated).
[INFO][10:03:03]: [Server #1127936] Selecting client #499 for training.
[INFO][10:03:03]: [Server #1127936] Sending the current model to client #499 (simulated).
[INFO][10:03:03]: [Client #152] Selected by the server.
[INFO][10:03:03]: [Client #152] Loading its data source...
[INFO][10:03:03]: [Client #152] Dataset size: 60000
[INFO][10:03:03]: [Client #152] Sampler: noniid
[INFO][10:03:03]: [Server #1127936] Sending 0.24 MB of payload data to client #499 (simulated).
[INFO][10:03:03]: [Client #12] Selected by the server.
[INFO][10:03:03]: [Client #12] Loading its data source...
[INFO][10:03:03]: [Client #12] Dataset size: 60000
[INFO][10:03:03]: [Client #12] Sampler: noniid
[INFO][10:03:03]: [Client #499] Selected by the server.
[INFO][10:03:03]: [Client #499] Loading its data source...
[INFO][10:03:03]: [Client #499] Dataset size: 60000
[INFO][10:03:03]: [Client #499] Sampler: noniid
[INFO][10:03:03]: [Client #152] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:03:03]: [Client #12] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:03:03]: [93m[1m[Client #152] Started training in communication round #34.[0m
[INFO][10:03:03]: [93m[1m[Client #12] Started training in communication round #34.[0m
[INFO][10:03:03]: [Client #499] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:03:03]: [93m[1m[Client #499] Started training in communication round #34.[0m
[INFO][10:03:05]: [Client #152] Loading the dataset.
[INFO][10:03:05]: [Client #499] Loading the dataset.
[INFO][10:03:05]: [Client #12] Loading the dataset.
[INFO][10:03:11]: [Client #12] Epoch: [1/5][0/10]	Loss: 0.148552
[INFO][10:03:11]: [Client #499] Epoch: [1/5][0/10]	Loss: 0.074139
[INFO][10:03:11]: [Client #12] Epoch: [2/5][0/10]	Loss: 0.003124
[INFO][10:03:11]: [Client #152] Epoch: [1/5][0/10]	Loss: 0.096894
[INFO][10:03:11]: [Client #499] Epoch: [2/5][0/10]	Loss: 0.000004
[INFO][10:03:11]: [Client #12] Epoch: [3/5][0/10]	Loss: 0.185547
[INFO][10:03:11]: [Client #152] Epoch: [2/5][0/10]	Loss: 0.003092
[INFO][10:03:11]: [Client #499] Epoch: [3/5][0/10]	Loss: 0.000188
[INFO][10:03:12]: [Client #12] Epoch: [4/5][0/10]	Loss: 0.050601
[INFO][10:03:12]: [Client #499] Epoch: [4/5][0/10]	Loss: 0.000002
[INFO][10:03:12]: [Client #12] Epoch: [5/5][0/10]	Loss: 1.377949
[INFO][10:03:12]: [Client #152] Epoch: [3/5][0/10]	Loss: 0.033744
[INFO][10:03:12]: [Client #12] Model saved to /data/ykang/plato/results/test/model/lenet5_12_1127978.pth.
[INFO][10:03:12]: [Client #499] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:03:12]: [Client #499] Model saved to /data/ykang/plato/results/test/model/lenet5_499_1127979.pth.
[INFO][10:03:12]: [Client #152] Epoch: [4/5][0/10]	Loss: 0.000299
[INFO][10:03:12]: [Client #152] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:03:12]: [Client #152] Model saved to /data/ykang/plato/results/test/model/lenet5_152_1127977.pth.
[INFO][10:03:12]: [Client #12] Loading a model from /data/ykang/plato/results/test/model/lenet5_12_1127978.pth.
[INFO][10:03:12]: [Client #12] Model trained.
[INFO][10:03:12]: [Client #12] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:03:12]: [Server #1127936] Received 0.24 MB of payload data from client #12 (simulated).
[INFO][10:03:13]: [Client #499] Loading a model from /data/ykang/plato/results/test/model/lenet5_499_1127979.pth.
[INFO][10:03:13]: [Client #499] Model trained.
[INFO][10:03:13]: [Client #499] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:03:13]: [Server #1127936] Received 0.24 MB of payload data from client #499 (simulated).
[INFO][10:03:13]: [Client #152] Loading a model from /data/ykang/plato/results/test/model/lenet5_152_1127977.pth.
[INFO][10:03:13]: [Client #152] Model trained.
[INFO][10:03:13]: [Client #152] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:03:13]: [Server #1127936] Received 0.24 MB of payload data from client #152 (simulated).
[INFO][10:03:13]: [Server #1127936] Selecting client #289 for training.
[INFO][10:03:13]: [Server #1127936] Sending the current model to client #289 (simulated).
[INFO][10:03:13]: [Server #1127936] Sending 0.24 MB of payload data to client #289 (simulated).
[INFO][10:03:13]: [Server #1127936] Selecting client #147 for training.
[INFO][10:03:13]: [Server #1127936] Sending the current model to client #147 (simulated).
[INFO][10:03:13]: [Server #1127936] Sending 0.24 MB of payload data to client #147 (simulated).
[INFO][10:03:13]: [Server #1127936] Selecting client #465 for training.
[INFO][10:03:13]: [Server #1127936] Sending the current model to client #465 (simulated).
[INFO][10:03:13]: [Client #289] Selected by the server.
[INFO][10:03:13]: [Client #289] Loading its data source...
[INFO][10:03:13]: [Client #289] Dataset size: 60000
[INFO][10:03:13]: [Client #289] Sampler: noniid
[INFO][10:03:13]: [Server #1127936] Sending 0.24 MB of payload data to client #465 (simulated).
[INFO][10:03:13]: [Client #147] Selected by the server.
[INFO][10:03:13]: [Client #147] Loading its data source...
[INFO][10:03:13]: [Client #147] Dataset size: 60000
[INFO][10:03:13]: [Client #147] Sampler: noniid
[INFO][10:03:13]: [Client #465] Selected by the server.
[INFO][10:03:13]: [Client #465] Loading its data source...
[INFO][10:03:13]: [Client #465] Dataset size: 60000
[INFO][10:03:13]: [Client #465] Sampler: noniid
[INFO][10:03:13]: [Client #289] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:03:13]: [Client #147] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:03:13]: [93m[1m[Client #289] Started training in communication round #34.[0m
[INFO][10:03:13]: [Client #465] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:03:13]: [93m[1m[Client #147] Started training in communication round #34.[0m
[INFO][10:03:13]: [93m[1m[Client #465] Started training in communication round #34.[0m
[INFO][10:03:15]: [Client #465] Loading the dataset.
[INFO][10:03:15]: [Client #289] Loading the dataset.
[INFO][10:03:15]: [Client #147] Loading the dataset.
[INFO][10:03:21]: [Client #465] Epoch: [1/5][0/10]	Loss: 0.043661
[INFO][10:03:21]: [Client #147] Epoch: [1/5][0/10]	Loss: 0.003899
[INFO][10:03:21]: [Client #289] Epoch: [1/5][0/10]	Loss: 0.077061
[INFO][10:03:21]: [Client #465] Epoch: [2/5][0/10]	Loss: 0.004095
[INFO][10:03:21]: [Client #289] Epoch: [2/5][0/10]	Loss: 0.044811
[INFO][10:03:21]: [Client #147] Epoch: [2/5][0/10]	Loss: 0.000302
[INFO][10:03:21]: [Client #465] Epoch: [3/5][0/10]	Loss: 0.009473
[INFO][10:03:21]: [Client #147] Epoch: [3/5][0/10]	Loss: 0.000000
[INFO][10:03:21]: [Client #289] Epoch: [3/5][0/10]	Loss: 0.000162
[INFO][10:03:21]: [Client #465] Epoch: [4/5][0/10]	Loss: 0.153093
[INFO][10:03:21]: [Client #289] Epoch: [4/5][0/10]	Loss: 0.051630
[INFO][10:03:21]: [Client #147] Epoch: [4/5][0/10]	Loss: 0.000001
[INFO][10:03:21]: [Client #465] Epoch: [5/5][0/10]	Loss: 0.003937
[INFO][10:03:21]: [Client #289] Epoch: [5/5][0/10]	Loss: 0.000208
[INFO][10:03:21]: [Client #147] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:03:21]: [Client #465] Model saved to /data/ykang/plato/results/test/model/lenet5_465_1127979.pth.
[INFO][10:03:21]: [Client #289] Model saved to /data/ykang/plato/results/test/model/lenet5_289_1127977.pth.
[INFO][10:03:21]: [Client #147] Model saved to /data/ykang/plato/results/test/model/lenet5_147_1127978.pth.
[INFO][10:03:22]: [Client #465] Loading a model from /data/ykang/plato/results/test/model/lenet5_465_1127979.pth.
[INFO][10:03:22]: [Client #465] Model trained.
[INFO][10:03:22]: [Client #465] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:03:22]: [Server #1127936] Received 0.24 MB of payload data from client #465 (simulated).
[INFO][10:03:22]: [Client #289] Loading a model from /data/ykang/plato/results/test/model/lenet5_289_1127977.pth.
[INFO][10:03:22]: [Client #289] Model trained.
[INFO][10:03:22]: [Client #289] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:03:22]: [Server #1127936] Received 0.24 MB of payload data from client #289 (simulated).
[INFO][10:03:22]: [Client #147] Loading a model from /data/ykang/plato/results/test/model/lenet5_147_1127978.pth.
[INFO][10:03:22]: [Client #147] Model trained.
[INFO][10:03:22]: [Client #147] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:03:22]: [Server #1127936] Received 0.24 MB of payload data from client #147 (simulated).
[INFO][10:03:22]: [Server #1127936] Selecting client #252 for training.
[INFO][10:03:22]: [Server #1127936] Sending the current model to client #252 (simulated).
[INFO][10:03:22]: [Server #1127936] Sending 0.24 MB of payload data to client #252 (simulated).
[INFO][10:03:22]: [Client #252] Selected by the server.
[INFO][10:03:22]: [Client #252] Loading its data source...
[INFO][10:03:22]: [Client #252] Dataset size: 60000
[INFO][10:03:22]: [Client #252] Sampler: noniid
[INFO][10:03:22]: [Client #252] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:03:22]: [93m[1m[Client #252] Started training in communication round #34.[0m
[INFO][10:03:24]: [Client #252] Loading the dataset.
[INFO][10:03:29]: [Client #252] Epoch: [1/5][0/10]	Loss: 0.020006
[INFO][10:03:29]: [Client #252] Epoch: [2/5][0/10]	Loss: 0.005299
[INFO][10:03:30]: [Client #252] Epoch: [3/5][0/10]	Loss: 0.000028
[INFO][10:03:30]: [Client #252] Epoch: [4/5][0/10]	Loss: 0.000338
[INFO][10:03:30]: [Client #252] Epoch: [5/5][0/10]	Loss: 0.000001
[INFO][10:03:30]: [Client #252] Model saved to /data/ykang/plato/results/test/model/lenet5_252_1127977.pth.
[INFO][10:03:30]: [Client #252] Loading a model from /data/ykang/plato/results/test/model/lenet5_252_1127977.pth.
[INFO][10:03:30]: [Client #252] Model trained.
[INFO][10:03:30]: [Client #252] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:03:30]: [Server #1127936] Received 0.24 MB of payload data from client #252 (simulated).
[INFO][10:03:30]: [Server #1127936] Adding client #459 to the list of clients for aggregation.
[INFO][10:03:30]: [Server #1127936] Adding client #4 to the list of clients for aggregation.
[INFO][10:03:30]: [Server #1127936] Adding client #268 to the list of clients for aggregation.
[INFO][10:03:30]: [Server #1127936] Adding client #304 to the list of clients for aggregation.
[INFO][10:03:30]: [Server #1127936] Adding client #377 to the list of clients for aggregation.
[INFO][10:03:30]: [Server #1127936] Adding client #445 to the list of clients for aggregation.
[INFO][10:03:30]: [Server #1127936] Adding client #7 to the list of clients for aggregation.
[INFO][10:03:30]: [Server #1127936] Adding client #147 to the list of clients for aggregation.
[INFO][10:03:30]: [Server #1127936] Adding client #393 to the list of clients for aggregation.
[INFO][10:03:30]: [Server #1127936] Adding client #289 to the list of clients for aggregation.
[INFO][10:03:30]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.00888411 0.         0.
 0.01181086 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01204892 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00759909 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.0078291  0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01244737 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01764682 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01124208 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00461136 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01000177 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 3. 0. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 3. 1. 0. 0. 1. 0. 0. 1. 6. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 2. 0. 6. 0. 1. 2. 1. 0. 1. 1. 0. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.
 1. 0. 1. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 4. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0.
 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0.
 0. 1. 1. 1. 0. 2. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 0. 1. 0. 0. 0. 6. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1.
 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 2. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 2. 0. 1. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.00888411 0.         0.
 0.01181086 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01204892 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00759909 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.0078291  0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01244737 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01764682 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01124208 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00461136 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01000177 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:03:32]: [Server #1127936] Global model accuracy: 94.64%

[INFO][10:03:32]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_34.pth.
[INFO][10:03:32]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_34.pth.
[INFO][10:03:32]: [93m[1m
[Server #1127936] Starting round 35/100.[0m
[INFO][10:03:33]: [Server #1127936] Selected clients: [377 355 329  99 459 376 403 375  42 242]
[INFO][10:03:33]: [Server #1127936] Selecting client #377 for training.
[INFO][10:03:33]: [Server #1127936] Sending the current model to client #377 (simulated).
[INFO][10:03:33]: [Server #1127936] Sending 0.24 MB of payload data to client #377 (simulated).
[INFO][10:03:33]: [Server #1127936] Selecting client #355 for training.
[INFO][10:03:33]: [Server #1127936] Sending the current model to client #355 (simulated).
[INFO][10:03:33]: [Server #1127936] Sending 0.24 MB of payload data to client #355 (simulated).
[INFO][10:03:33]: [Server #1127936] Selecting client #329 for training.
[INFO][10:03:33]: [Server #1127936] Sending the current model to client #329 (simulated).
[INFO][10:03:33]: [Client #377] Selected by the server.
[INFO][10:03:33]: [Client #377] Loading its data source...
[INFO][10:03:33]: [Client #377] Dataset size: 60000
[INFO][10:03:33]: [Client #377] Sampler: noniid
[INFO][10:03:33]: [Server #1127936] Sending 0.24 MB of payload data to client #329 (simulated).
[INFO][10:03:33]: [Client #355] Selected by the server.
[INFO][10:03:33]: [Client #355] Loading its data source...
[INFO][10:03:33]: [Client #355] Dataset size: 60000
[INFO][10:03:33]: [Client #329] Selected by the server.
[INFO][10:03:33]: [Client #355] Sampler: noniid
[INFO][10:03:33]: [Client #329] Loading its data source...
[INFO][10:03:33]: [Client #329] Dataset size: 60000
[INFO][10:03:33]: [Client #377] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:03:33]: [Client #329] Sampler: noniid
[INFO][10:03:33]: [Client #355] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:03:33]: [Client #329] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:03:33]: [93m[1m[Client #329] Started training in communication round #35.[0m
[INFO][10:03:33]: [93m[1m[Client #355] Started training in communication round #35.[0m
[INFO][10:03:33]: [93m[1m[Client #377] Started training in communication round #35.[0m
[INFO][10:03:35]: [Client #355] Loading the dataset.
[INFO][10:03:35]: [Client #377] Loading the dataset.
[INFO][10:03:35]: [Client #329] Loading the dataset.
[INFO][10:03:41]: [Client #329] Epoch: [1/5][0/10]	Loss: 0.046720
[INFO][10:03:41]: [Client #377] Epoch: [1/5][0/10]	Loss: 0.175602
[INFO][10:03:41]: [Client #355] Epoch: [1/5][0/10]	Loss: 0.000687
[INFO][10:03:41]: [Client #329] Epoch: [2/5][0/10]	Loss: 0.005450
[INFO][10:03:41]: [Client #377] Epoch: [2/5][0/10]	Loss: 0.005691
[INFO][10:03:41]: [Client #355] Epoch: [2/5][0/10]	Loss: 0.000556
[INFO][10:03:41]: [Client #329] Epoch: [3/5][0/10]	Loss: 0.000012
[INFO][10:03:41]: [Client #377] Epoch: [3/5][0/10]	Loss: 0.000016
[INFO][10:03:41]: [Client #355] Epoch: [3/5][0/10]	Loss: 0.001643
[INFO][10:03:41]: [Client #329] Epoch: [4/5][0/10]	Loss: 0.000020
[INFO][10:03:41]: [Client #355] Epoch: [4/5][0/10]	Loss: 0.088641
[INFO][10:03:41]: [Client #377] Epoch: [4/5][0/10]	Loss: 0.004684
[INFO][10:03:41]: [Client #329] Epoch: [5/5][0/10]	Loss: 0.000020
[INFO][10:03:41]: [Client #355] Epoch: [5/5][0/10]	Loss: 0.000104
[INFO][10:03:41]: [Client #329] Model saved to /data/ykang/plato/results/test/model/lenet5_329_1127979.pth.
[INFO][10:03:41]: [Client #377] Epoch: [5/5][0/10]	Loss: 0.064605
[INFO][10:03:41]: [Client #355] Model saved to /data/ykang/plato/results/test/model/lenet5_355_1127978.pth.
[INFO][10:03:41]: [Client #377] Model saved to /data/ykang/plato/results/test/model/lenet5_377_1127977.pth.
[INFO][10:03:42]: [Client #329] Loading a model from /data/ykang/plato/results/test/model/lenet5_329_1127979.pth.
[INFO][10:03:42]: [Client #329] Model trained.
[INFO][10:03:42]: [Client #329] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:03:42]: [Server #1127936] Received 0.24 MB of payload data from client #329 (simulated).
[INFO][10:03:42]: [Client #377] Loading a model from /data/ykang/plato/results/test/model/lenet5_377_1127977.pth.
[INFO][10:03:42]: [Client #377] Model trained.
[INFO][10:03:42]: [Client #377] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:03:42]: [Server #1127936] Received 0.24 MB of payload data from client #377 (simulated).
[INFO][10:03:42]: [Client #355] Loading a model from /data/ykang/plato/results/test/model/lenet5_355_1127978.pth.
[INFO][10:03:42]: [Client #355] Model trained.
[INFO][10:03:42]: [Client #355] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:03:42]: [Server #1127936] Received 0.24 MB of payload data from client #355 (simulated).
[INFO][10:03:42]: [Server #1127936] Selecting client #99 for training.
[INFO][10:03:42]: [Server #1127936] Sending the current model to client #99 (simulated).
[INFO][10:03:42]: [Server #1127936] Sending 0.24 MB of payload data to client #99 (simulated).
[INFO][10:03:42]: [Server #1127936] Selecting client #459 for training.
[INFO][10:03:42]: [Server #1127936] Sending the current model to client #459 (simulated).
[INFO][10:03:42]: [Server #1127936] Sending 0.24 MB of payload data to client #459 (simulated).
[INFO][10:03:42]: [Server #1127936] Selecting client #376 for training.
[INFO][10:03:42]: [Server #1127936] Sending the current model to client #376 (simulated).
[INFO][10:03:42]: [Client #99] Selected by the server.
[INFO][10:03:42]: [Client #99] Loading its data source...
[INFO][10:03:42]: [Client #99] Dataset size: 60000
[INFO][10:03:42]: [Client #99] Sampler: noniid
[INFO][10:03:42]: [Server #1127936] Sending 0.24 MB of payload data to client #376 (simulated).
[INFO][10:03:42]: [Client #99] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:03:42]: [Client #376] Selected by the server.
[INFO][10:03:42]: [Client #376] Loading its data source...
[INFO][10:03:42]: [Client #376] Dataset size: 60000
[INFO][10:03:42]: [Client #376] Sampler: noniid
[INFO][10:03:42]: [Client #459] Selected by the server.
[INFO][10:03:42]: [Client #459] Loading its data source...
[INFO][10:03:42]: [Client #459] Dataset size: 60000
[INFO][10:03:42]: [Client #459] Sampler: noniid
[INFO][10:03:42]: [Client #376] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:03:42]: [93m[1m[Client #99] Started training in communication round #35.[0m
[INFO][10:03:42]: [93m[1m[Client #376] Started training in communication round #35.[0m
[INFO][10:03:42]: [Client #459] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:03:42]: [93m[1m[Client #459] Started training in communication round #35.[0m
[INFO][10:03:45]: [Client #376] Loading the dataset.
[INFO][10:03:45]: [Client #99] Loading the dataset.
[INFO][10:03:45]: [Client #459] Loading the dataset.
[INFO][10:03:50]: [Client #99] Epoch: [1/5][0/10]	Loss: 0.005949
[INFO][10:03:50]: [Client #376] Epoch: [1/5][0/10]	Loss: 0.004289
[INFO][10:03:50]: [Client #459] Epoch: [1/5][0/10]	Loss: 0.267057
[INFO][10:03:51]: [Client #376] Epoch: [2/5][0/10]	Loss: 0.003087
[INFO][10:03:51]: [Client #99] Epoch: [2/5][0/10]	Loss: 0.012818
[INFO][10:03:51]: [Client #459] Epoch: [2/5][0/10]	Loss: 0.002730
[INFO][10:03:51]: [Client #376] Epoch: [3/5][0/10]	Loss: 0.000377
[INFO][10:03:51]: [Client #99] Epoch: [3/5][0/10]	Loss: 0.000059
[INFO][10:03:51]: [Client #459] Epoch: [3/5][0/10]	Loss: 0.000296
[INFO][10:03:51]: [Client #376] Epoch: [4/5][0/10]	Loss: 0.002597
[INFO][10:03:51]: [Client #99] Epoch: [4/5][0/10]	Loss: 0.000005
[INFO][10:03:51]: [Client #459] Epoch: [4/5][0/10]	Loss: 0.000367
[INFO][10:03:51]: [Client #376] Epoch: [5/5][0/10]	Loss: 0.016589
[INFO][10:03:51]: [Client #376] Model saved to /data/ykang/plato/results/test/model/lenet5_376_1127979.pth.
[INFO][10:03:51]: [Client #459] Epoch: [5/5][0/10]	Loss: 0.000818
[INFO][10:03:51]: [Client #99] Epoch: [5/5][0/10]	Loss: 0.001344
[INFO][10:03:51]: [Client #459] Model saved to /data/ykang/plato/results/test/model/lenet5_459_1127978.pth.
[INFO][10:03:51]: [Client #99] Model saved to /data/ykang/plato/results/test/model/lenet5_99_1127977.pth.
[INFO][10:03:52]: [Client #376] Loading a model from /data/ykang/plato/results/test/model/lenet5_376_1127979.pth.
[INFO][10:03:52]: [Client #376] Model trained.
[INFO][10:03:52]: [Client #376] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:03:52]: [Server #1127936] Received 0.24 MB of payload data from client #376 (simulated).
[INFO][10:03:52]: [Client #459] Loading a model from /data/ykang/plato/results/test/model/lenet5_459_1127978.pth.
[INFO][10:03:52]: [Client #459] Model trained.
[INFO][10:03:52]: [Client #459] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:03:52]: [Server #1127936] Received 0.24 MB of payload data from client #459 (simulated).
[INFO][10:03:52]: [Client #99] Loading a model from /data/ykang/plato/results/test/model/lenet5_99_1127977.pth.
[INFO][10:03:52]: [Client #99] Model trained.
[INFO][10:03:52]: [Client #99] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:03:52]: [Server #1127936] Received 0.24 MB of payload data from client #99 (simulated).
[INFO][10:03:52]: [Server #1127936] Selecting client #403 for training.
[INFO][10:03:52]: [Server #1127936] Sending the current model to client #403 (simulated).
[INFO][10:03:52]: [Server #1127936] Sending 0.24 MB of payload data to client #403 (simulated).
[INFO][10:03:52]: [Server #1127936] Selecting client #375 for training.
[INFO][10:03:52]: [Server #1127936] Sending the current model to client #375 (simulated).
[INFO][10:03:52]: [Server #1127936] Sending 0.24 MB of payload data to client #375 (simulated).
[INFO][10:03:52]: [Server #1127936] Selecting client #42 for training.
[INFO][10:03:52]: [Server #1127936] Sending the current model to client #42 (simulated).
[INFO][10:03:52]: [Client #403] Selected by the server.
[INFO][10:03:52]: [Client #403] Loading its data source...
[INFO][10:03:52]: [Client #403] Dataset size: 60000
[INFO][10:03:52]: [Client #403] Sampler: noniid
[INFO][10:03:52]: [Server #1127936] Sending 0.24 MB of payload data to client #42 (simulated).
[INFO][10:03:52]: [Client #375] Selected by the server.
[INFO][10:03:52]: [Client #375] Loading its data source...
[INFO][10:03:52]: [Client #375] Dataset size: 60000
[INFO][10:03:52]: [Client #375] Sampler: noniid
[INFO][10:03:52]: [Client #42] Selected by the server.
[INFO][10:03:52]: [Client #42] Loading its data source...
[INFO][10:03:52]: [Client #42] Dataset size: 60000
[INFO][10:03:52]: [Client #42] Sampler: noniid
[INFO][10:03:52]: [Client #403] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:03:52]: [Client #375] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:03:52]: [Client #42] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:03:52]: [93m[1m[Client #403] Started training in communication round #35.[0m
[INFO][10:03:52]: [93m[1m[Client #375] Started training in communication round #35.[0m
[INFO][10:03:52]: [93m[1m[Client #42] Started training in communication round #35.[0m
[INFO][10:03:54]: [Client #375] Loading the dataset.
[INFO][10:03:54]: [Client #403] Loading the dataset.
[INFO][10:03:54]: [Client #42] Loading the dataset.
[INFO][10:04:00]: [Client #375] Epoch: [1/5][0/10]	Loss: 0.157819
[INFO][10:04:00]: [Client #403] Epoch: [1/5][0/10]	Loss: 0.267068
[INFO][10:04:00]: [Client #375] Epoch: [2/5][0/10]	Loss: 0.003041
[INFO][10:04:00]: [Client #42] Epoch: [1/5][0/10]	Loss: 0.001219
[INFO][10:04:00]: [Client #375] Epoch: [3/5][0/10]	Loss: 0.001807
[INFO][10:04:00]: [Client #403] Epoch: [2/5][0/10]	Loss: 0.004053
[INFO][10:04:00]: [Client #375] Epoch: [4/5][0/10]	Loss: 0.001264
[INFO][10:04:00]: [Client #42] Epoch: [2/5][0/10]	Loss: 0.002062
[INFO][10:04:00]: [Client #375] Epoch: [5/5][0/10]	Loss: 1.283568
[INFO][10:04:00]: [Client #403] Epoch: [3/5][0/10]	Loss: 0.000027
[INFO][10:04:00]: [Client #42] Epoch: [3/5][0/10]	Loss: 0.000080
[INFO][10:04:00]: [Client #375] Model saved to /data/ykang/plato/results/test/model/lenet5_375_1127978.pth.
[INFO][10:04:00]: [Client #403] Epoch: [4/5][0/10]	Loss: 0.000029
[INFO][10:04:00]: [Client #42] Epoch: [4/5][0/10]	Loss: 0.000055
[INFO][10:04:00]: [Client #403] Epoch: [5/5][0/10]	Loss: 0.002198
[INFO][10:04:00]: [Client #42] Epoch: [5/5][0/10]	Loss: 0.246851
[INFO][10:04:01]: [Client #403] Model saved to /data/ykang/plato/results/test/model/lenet5_403_1127977.pth.
[INFO][10:04:01]: [Client #42] Model saved to /data/ykang/plato/results/test/model/lenet5_42_1127979.pth.
[INFO][10:04:01]: [Client #375] Loading a model from /data/ykang/plato/results/test/model/lenet5_375_1127978.pth.
[INFO][10:04:01]: [Client #375] Model trained.
[INFO][10:04:01]: [Client #375] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:04:01]: [Server #1127936] Received 0.24 MB of payload data from client #375 (simulated).
[INFO][10:04:01]: [Client #403] Loading a model from /data/ykang/plato/results/test/model/lenet5_403_1127977.pth.
[INFO][10:04:01]: [Client #403] Model trained.
[INFO][10:04:01]: [Client #403] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:04:01]: [Server #1127936] Received 0.24 MB of payload data from client #403 (simulated).
[INFO][10:04:01]: [Client #42] Loading a model from /data/ykang/plato/results/test/model/lenet5_42_1127979.pth.
[INFO][10:04:01]: [Client #42] Model trained.
[INFO][10:04:01]: [Client #42] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:04:01]: [Server #1127936] Received 0.24 MB of payload data from client #42 (simulated).
[INFO][10:04:01]: [Server #1127936] Selecting client #242 for training.
[INFO][10:04:01]: [Server #1127936] Sending the current model to client #242 (simulated).
[INFO][10:04:01]: [Server #1127936] Sending 0.24 MB of payload data to client #242 (simulated).
[INFO][10:04:01]: [Client #242] Selected by the server.
[INFO][10:04:01]: [Client #242] Loading its data source...
[INFO][10:04:01]: [Client #242] Dataset size: 60000
[INFO][10:04:01]: [Client #242] Sampler: noniid
[INFO][10:04:01]: [Client #242] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:04:01]: [93m[1m[Client #242] Started training in communication round #35.[0m
[INFO][10:04:03]: [Client #242] Loading the dataset.
[INFO][10:04:09]: [Client #242] Epoch: [1/5][0/10]	Loss: 0.025457
[INFO][10:04:09]: [Client #242] Epoch: [2/5][0/10]	Loss: 0.001467
[INFO][10:04:09]: [Client #242] Epoch: [3/5][0/10]	Loss: 0.000064
[INFO][10:04:09]: [Client #242] Epoch: [4/5][0/10]	Loss: 0.000222
[INFO][10:04:09]: [Client #242] Epoch: [5/5][0/10]	Loss: 0.001443
[INFO][10:04:09]: [Client #242] Model saved to /data/ykang/plato/results/test/model/lenet5_242_1127977.pth.
[INFO][10:04:09]: [Client #242] Loading a model from /data/ykang/plato/results/test/model/lenet5_242_1127977.pth.
[INFO][10:04:09]: [Client #242] Model trained.
[INFO][10:04:09]: [Client #242] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:04:09]: [Server #1127936] Received 0.24 MB of payload data from client #242 (simulated).
[INFO][10:04:09]: [Server #1127936] Adding client #12 to the list of clients for aggregation.
[INFO][10:04:09]: [Server #1127936] Adding client #465 to the list of clients for aggregation.
[INFO][10:04:09]: [Server #1127936] Adding client #475 to the list of clients for aggregation.
[INFO][10:04:09]: [Server #1127936] Adding client #41 to the list of clients for aggregation.
[INFO][10:04:09]: [Server #1127936] Adding client #99 to the list of clients for aggregation.
[INFO][10:04:09]: [Server #1127936] Adding client #375 to the list of clients for aggregation.
[INFO][10:04:09]: [Server #1127936] Adding client #403 to the list of clients for aggregation.
[INFO][10:04:09]: [Server #1127936] Adding client #499 to the list of clients for aggregation.
[INFO][10:04:09]: [Server #1127936] Adding client #459 to the list of clients for aggregation.
[INFO][10:04:09]: [Server #1127936] Adding client #376 to the list of clients for aggregation.
[INFO][10:04:09]: [Server #1127936] Aggregating 10 clients in total.
[0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1
 0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  2e-05  4e-10  4e-10
 6:  6.8876e+00  6.8875e+00  2e-05  2e-10  2e-10
 7:  6.8875e+00  6.8875e+00  2e-05  4e-10  8e-12
 8:  6.8875e+00  6.8875e+00  1e-05  3e-10  6e-12
 9:  6.8875e+00  6.8875e+00  7e-06  6e-10  1e-11
Optimal solution found.
The calculated probability is:  [0.00074753 0.00074753 0.00074753 0.00157164 0.00074753 0.00074753
 0.00074746 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074746 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00136    0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.0007475  0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00270611 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.62910754 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074747 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00163841
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00181414 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753 0.00074753
 0.00074753 0.00074753 0.00074753 0.00074753]
current clients pool:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 466, 467, 468, 469, 470, 471, 472, 473, 474, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.02323334
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01214658 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.0091436  0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01938021 0.00566496 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00998093 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01136985 0.         0.         0.
 0.         0.         0.01182135 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00761465 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01098602 0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 3. 0. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 3. 1. 0. 0. 1. 0. 0. 1. 6. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 2. 0. 6. 0. 1. 2. 1. 0. 1. 1. 0. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.
 1. 0. 1. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 4. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0.
 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0.
 0. 1. 1. 1. 0. 2. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 0. 1. 0. 0. 0. 6. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1.
 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 2. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 2. 0. 1. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]
local_gradient_bounds:  [INFO][10:04:11]: [Server #1127936] Global model accuracy: 94.09%

[INFO][10:04:11]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_35.pth.
[INFO][10:04:11]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_35.pth.
[INFO][10:04:11]: [93m[1m
[Server #1127936] Starting round 36/100.[0m
[0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.02323334
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01214658 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.0091436  0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01938021 0.00566496 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00998093 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01136985 0.         0.         0.
 0.         0.         0.01182135 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00761465 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01098602 0.        ]
!!!!!!The aggregation weights of this round are:  [0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1
 0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  4e-04  7e-09  7e-09
 5:  6.8876e+00  6.8875e+00  8e-05  1e-09  1e-09
 6:  6.8876e+00  6.8875e+00  7e-05  6e-10  6e-10
 7:  6.8875e+00  6.8875e+00  7e-05  2e-09  2e-10
 8:  6.8875e+00  6.8875e+00  5e-05  3e-09  2e-10
 9:  6.8875e+00  6.8875e+00  2e-05  2e-08  2e-09
10:  6.8875e+00  6.8875e+00  7e-06  1e-08  8e-10
Optimal solution found.
The calculated probability is:  [1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 3.10752085e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 9.03009286e-01
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97808478e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97799409e-04 1.97810079e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97807981e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97807060e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 2.42814159e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 2.59819260e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 1.97811076e-04 1.97811076e-04 1.97811076e-04 1.97811076e-04
 2.38979058e-04 1.97811076e-04]
current clients pool:  [INFO][10:04:12]: [Server #1127936] Selected clients: [ 41 454 119  90 197 162 411 308 384 215]
[INFO][10:04:12]: [Server #1127936] Selecting client #41 for training.
[INFO][10:04:12]: [Server #1127936] Sending the current model to client #41 (simulated).
[INFO][10:04:12]: [Server #1127936] Sending 0.24 MB of payload data to client #41 (simulated).
[INFO][10:04:12]: [Server #1127936] Selecting client #454 for training.
[INFO][10:04:12]: [Server #1127936] Sending the current model to client #454 (simulated).
[INFO][10:04:12]: [Server #1127936] Sending 0.24 MB of payload data to client #454 (simulated).
[INFO][10:04:12]: [Server #1127936] Selecting client #119 for training.
[INFO][10:04:12]: [Server #1127936] Sending the current model to client #119 (simulated).
[INFO][10:04:12]: [Client #41] Selected by the server.
[INFO][10:04:12]: [Client #41] Loading its data source...
[INFO][10:04:12]: [Client #41] Dataset size: 60000
[INFO][10:04:12]: [Client #41] Sampler: noniid
[INFO][10:04:12]: [Server #1127936] Sending 0.24 MB of payload data to client #119 (simulated).
[INFO][10:04:12]: [Client #454] Selected by the server.
[INFO][10:04:12]: [Client #454] Loading its data source...
[INFO][10:04:12]: [Client #119] Selected by the server.
[INFO][10:04:12]: [Client #454] Dataset size: 60000
[INFO][10:04:12]: [Client #119] Loading its data source...
[INFO][10:04:12]: [Client #454] Sampler: noniid
[INFO][10:04:12]: [Client #119] Dataset size: 60000
[INFO][10:04:12]: [Client #119] Sampler: noniid
[INFO][10:04:12]: [Client #41] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:04:12]: [Client #454] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:04:12]: [Client #119] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:04:12]: [93m[1m[Client #454] Started training in communication round #36.[0m
[INFO][10:04:12]: [93m[1m[Client #119] Started training in communication round #36.[0m
[INFO][10:04:12]: [93m[1m[Client #41] Started training in communication round #36.[0m
[INFO][10:04:14]: [Client #119] Loading the dataset.
[INFO][10:04:14]: [Client #454] Loading the dataset.
[INFO][10:04:14]: [Client #41] Loading the dataset.
[INFO][10:04:20]: [Client #119] Epoch: [1/5][0/10]	Loss: 0.010836
[INFO][10:04:20]: [Client #454] Epoch: [1/5][0/10]	Loss: 0.005579
[INFO][10:04:20]: [Client #41] Epoch: [1/5][0/10]	Loss: 0.061467
[INFO][10:04:20]: [Client #41] Epoch: [2/5][0/10]	Loss: 0.003620
[INFO][10:04:20]: [Client #119] Epoch: [2/5][0/10]	Loss: 0.002420
[INFO][10:04:20]: [Client #454] Epoch: [2/5][0/10]	Loss: 0.006308
[INFO][10:04:20]: [Client #119] Epoch: [3/5][0/10]	Loss: 0.000441
[INFO][10:04:20]: [Client #454] Epoch: [3/5][0/10]	Loss: 0.001905
[INFO][10:04:20]: [Client #41] Epoch: [3/5][0/10]	Loss: 0.003029
[INFO][10:04:20]: [Client #41] Epoch: [4/5][0/10]	Loss: 0.359920
[INFO][10:04:20]: [Client #119] Epoch: [4/5][0/10]	Loss: 0.000037
[INFO][10:04:20]: [Client #454] Epoch: [4/5][0/10]	Loss: 0.023317
[INFO][10:04:21]: [Client #119] Epoch: [5/5][0/10]	Loss: 0.000106
[INFO][10:04:21]: [Client #41] Epoch: [5/5][0/10]	Loss: 0.047681
[INFO][10:04:21]: [Client #454] Epoch: [5/5][0/10]	Loss: 0.005300
[INFO][10:04:21]: [Client #119] Model saved to /data/ykang/plato/results/test/model/lenet5_119_1127979.pth.
[INFO][10:04:21]: [Client #41] Model saved to /data/ykang/plato/results/test/model/lenet5_41_1127977.pth.
[INFO][10:04:21]: [Client #454] Model saved to /data/ykang/plato/results/test/model/lenet5_454_1127978.pth.
[INFO][10:04:21]: [Client #119] Loading a model from /data/ykang/plato/results/test/model/lenet5_119_1127979.pth.
[INFO][10:04:21]: [Client #119] Model trained.
[INFO][10:04:21]: [Client #119] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:04:21]: [Server #1127936] Received 0.24 MB of payload data from client #119 (simulated).
[INFO][10:04:21]: [Client #454] Loading a model from /data/ykang/plato/results/test/model/lenet5_454_1127978.pth.
[INFO][10:04:21]: [Client #454] Model trained.
[INFO][10:04:21]: [Client #454] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:04:21]: [Server #1127936] Received 0.24 MB of payload data from client #454 (simulated).
[INFO][10:04:21]: [Client #41] Loading a model from /data/ykang/plato/results/test/model/lenet5_41_1127977.pth.
[INFO][10:04:21]: [Client #41] Model trained.
[INFO][10:04:21]: [Client #41] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:04:21]: [Server #1127936] Received 0.24 MB of payload data from client #41 (simulated).
[INFO][10:04:21]: [Server #1127936] Selecting client #90 for training.
[INFO][10:04:21]: [Server #1127936] Sending the current model to client #90 (simulated).
[INFO][10:04:21]: [Server #1127936] Sending 0.24 MB of payload data to client #90 (simulated).
[INFO][10:04:21]: [Server #1127936] Selecting client #197 for training.
[INFO][10:04:21]: [Server #1127936] Sending the current model to client #197 (simulated).
[INFO][10:04:21]: [Server #1127936] Sending 0.24 MB of payload data to client #197 (simulated).
[INFO][10:04:21]: [Server #1127936] Selecting client #162 for training.
[INFO][10:04:21]: [Server #1127936] Sending the current model to client #162 (simulated).
[INFO][10:04:21]: [Client #90] Selected by the server.
[INFO][10:04:21]: [Client #90] Loading its data source...
[INFO][10:04:21]: [Client #90] Dataset size: 60000
[INFO][10:04:21]: [Client #90] Sampler: noniid
[INFO][10:04:21]: [Server #1127936] Sending 0.24 MB of payload data to client #162 (simulated).
[INFO][10:04:21]: [Client #197] Selected by the server.
[INFO][10:04:21]: [Client #197] Loading its data source...
[INFO][10:04:21]: [Client #162] Selected by the server.
[INFO][10:04:21]: [Client #197] Dataset size: 60000
[INFO][10:04:21]: [Client #162] Loading its data source...
[INFO][10:04:21]: [Client #162] Dataset size: 60000
[INFO][10:04:21]: [Client #197] Sampler: noniid
[INFO][10:04:21]: [Client #162] Sampler: noniid
[INFO][10:04:21]: [Client #90] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:04:21]: [Client #197] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:04:21]: [Client #162] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:04:22]: [93m[1m[Client #90] Started training in communication round #36.[0m
[INFO][10:04:22]: [93m[1m[Client #197] Started training in communication round #36.[0m
[INFO][10:04:22]: [93m[1m[Client #162] Started training in communication round #36.[0m
[INFO][10:04:24]: [Client #90] Loading the dataset.
[INFO][10:04:24]: [Client #197] Loading the dataset.
[INFO][10:04:24]: [Client #162] Loading the dataset.
[INFO][10:04:29]: [Client #90] Epoch: [1/5][0/10]	Loss: 0.001477
[INFO][10:04:29]: [Client #162] Epoch: [1/5][0/10]	Loss: 0.010014
[INFO][10:04:30]: [Client #197] Epoch: [1/5][0/10]	Loss: 0.001477
[INFO][10:04:30]: [Client #90] Epoch: [2/5][0/10]	Loss: 0.000730
[INFO][10:04:30]: [Client #162] Epoch: [2/5][0/10]	Loss: 0.003083
[INFO][10:04:30]: [Client #197] Epoch: [2/5][0/10]	Loss: 0.001185
[INFO][10:04:30]: [Client #90] Epoch: [3/5][0/10]	Loss: 0.000456
[INFO][10:04:30]: [Client #162] Epoch: [3/5][0/10]	Loss: 0.000021
[INFO][10:04:30]: [Client #197] Epoch: [3/5][0/10]	Loss: 0.001816
[INFO][10:04:30]: [Client #90] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:04:30]: [Client #162] Epoch: [4/5][0/10]	Loss: 0.000004
[INFO][10:04:30]: [Client #197] Epoch: [4/5][0/10]	Loss: 0.000024
[INFO][10:04:30]: [Client #90] Epoch: [5/5][0/10]	Loss: 0.002479
[INFO][10:04:30]: [Client #162] Epoch: [5/5][0/10]	Loss: 0.295959
[INFO][10:04:30]: [Client #197] Epoch: [5/5][0/10]	Loss: 0.003748
[INFO][10:04:30]: [Client #90] Model saved to /data/ykang/plato/results/test/model/lenet5_90_1127977.pth.
[INFO][10:04:30]: [Client #162] Model saved to /data/ykang/plato/results/test/model/lenet5_162_1127979.pth.
[INFO][10:04:30]: [Client #197] Model saved to /data/ykang/plato/results/test/model/lenet5_197_1127978.pth.
[INFO][10:04:31]: [Client #162] Loading a model from /data/ykang/plato/results/test/model/lenet5_162_1127979.pth.
[INFO][10:04:31]: [Client #162] Model trained.
[INFO][10:04:31]: [Client #162] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:04:31]: [Server #1127936] Received 0.24 MB of payload data from client #162 (simulated).
[INFO][10:04:31]: [Client #90] Loading a model from /data/ykang/plato/results/test/model/lenet5_90_1127977.pth.
[INFO][10:04:31]: [Client #90] Model trained.
[INFO][10:04:31]: [Client #90] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:04:31]: [Server #1127936] Received 0.24 MB of payload data from client #90 (simulated).
[INFO][10:04:31]: [Client #197] Loading a model from /data/ykang/plato/results/test/model/lenet5_197_1127978.pth.
[INFO][10:04:31]: [Client #197] Model trained.
[INFO][10:04:31]: [Client #197] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:04:31]: [Server #1127936] Received 0.24 MB of payload data from client #197 (simulated).
[INFO][10:04:31]: [Server #1127936] Selecting client #411 for training.
[INFO][10:04:31]: [Server #1127936] Sending the current model to client #411 (simulated).
[INFO][10:04:31]: [Server #1127936] Sending 0.24 MB of payload data to client #411 (simulated).
[INFO][10:04:31]: [Server #1127936] Selecting client #308 for training.
[INFO][10:04:31]: [Server #1127936] Sending the current model to client #308 (simulated).
[INFO][10:04:31]: [Server #1127936] Sending 0.24 MB of payload data to client #308 (simulated).
[INFO][10:04:31]: [Server #1127936] Selecting client #384 for training.
[INFO][10:04:31]: [Server #1127936] Sending the current model to client #384 (simulated).
[INFO][10:04:31]: [Client #411] Selected by the server.
[INFO][10:04:31]: [Client #411] Loading its data source...
[INFO][10:04:31]: [Client #411] Dataset size: 60000
[INFO][10:04:31]: [Client #411] Sampler: noniid
[INFO][10:04:31]: [Server #1127936] Sending 0.24 MB of payload data to client #384 (simulated).
[INFO][10:04:31]: [Client #384] Selected by the server.
[INFO][10:04:31]: [Client #384] Loading its data source...
[INFO][10:04:31]: [Client #384] Dataset size: 60000
[INFO][10:04:31]: [Client #384] Sampler: noniid
[INFO][10:04:31]: [Client #308] Selected by the server.
[INFO][10:04:31]: [Client #308] Loading its data source...
[INFO][10:04:31]: [Client #308] Dataset size: 60000
[INFO][10:04:31]: [Client #308] Sampler: noniid
[INFO][10:04:31]: [Client #384] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:04:31]: [Client #308] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:04:31]: [93m[1m[Client #384] Started training in communication round #36.[0m
[INFO][10:04:31]: [93m[1m[Client #308] Started training in communication round #36.[0m
[INFO][10:04:31]: [Client #411] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:04:31]: [93m[1m[Client #411] Started training in communication round #36.[0m
[INFO][10:04:33]: [Client #308] Loading the dataset.
[INFO][10:04:33]: [Client #384] Loading the dataset.
[INFO][10:04:33]: [Client #411] Loading the dataset.
[INFO][10:04:39]: [Client #384] Epoch: [1/5][0/10]	Loss: 0.066009
[INFO][10:04:39]: [Client #411] Epoch: [1/5][0/10]	Loss: 0.013820
[INFO][10:04:39]: [Client #308] Epoch: [1/5][0/10]	Loss: 0.009224
[INFO][10:04:39]: [Client #411] Epoch: [2/5][0/10]	Loss: 0.007393
[INFO][10:04:39]: [Client #308] Epoch: [2/5][0/10]	Loss: 0.058516
[INFO][10:04:39]: [Client #384] Epoch: [2/5][0/10]	Loss: 0.012611
[INFO][10:04:39]: [Client #411] Epoch: [3/5][0/10]	Loss: 0.000606
[INFO][10:04:39]: [Client #308] Epoch: [3/5][0/10]	Loss: 0.000732
[INFO][10:04:39]: [Client #411] Epoch: [4/5][0/10]	Loss: 0.000189
[INFO][10:04:39]: [Client #308] Epoch: [4/5][0/10]	Loss: 0.000181
[INFO][10:04:39]: [Client #384] Epoch: [3/5][0/10]	Loss: 0.003431
[INFO][10:04:39]: [Client #411] Epoch: [5/5][0/10]	Loss: 0.000713
[INFO][10:04:39]: [Client #384] Epoch: [4/5][0/10]	Loss: 0.003014
[INFO][10:04:39]: [Client #308] Epoch: [5/5][0/10]	Loss: 0.000248
[INFO][10:04:39]: [Client #411] Model saved to /data/ykang/plato/results/test/model/lenet5_411_1127977.pth.
[INFO][10:04:39]: [Client #308] Model saved to /data/ykang/plato/results/test/model/lenet5_308_1127978.pth.
[INFO][10:04:39]: [Client #384] Epoch: [5/5][0/10]	Loss: 0.031350
[INFO][10:04:39]: [Client #384] Model saved to /data/ykang/plato/results/test/model/lenet5_384_1127979.pth.
[INFO][10:04:40]: [Client #411] Loading a model from /data/ykang/plato/results/test/model/lenet5_411_1127977.pth.
[INFO][10:04:40]: [Client #411] Model trained.
[INFO][10:04:40]: [Client #411] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:04:40]: [Server #1127936] Received 0.24 MB of payload data from client #411 (simulated).
[INFO][10:04:40]: [Client #308] Loading a model from /data/ykang/plato/results/test/model/lenet5_308_1127978.pth.
[INFO][10:04:40]: [Client #308] Model trained.
[INFO][10:04:40]: [Client #308] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:04:40]: [Server #1127936] Received 0.24 MB of payload data from client #308 (simulated).
[INFO][10:04:40]: [Client #384] Loading a model from /data/ykang/plato/results/test/model/lenet5_384_1127979.pth.
[INFO][10:04:40]: [Client #384] Model trained.
[INFO][10:04:40]: [Client #384] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:04:40]: [Server #1127936] Received 0.24 MB of payload data from client #384 (simulated).
[INFO][10:04:40]: [Server #1127936] Selecting client #215 for training.
[INFO][10:04:40]: [Server #1127936] Sending the current model to client #215 (simulated).
[INFO][10:04:40]: [Server #1127936] Sending 0.24 MB of payload data to client #215 (simulated).
[INFO][10:04:40]: [Client #215] Selected by the server.
[INFO][10:04:40]: [Client #215] Loading its data source...
[INFO][10:04:40]: [Client #215] Dataset size: 60000
[INFO][10:04:40]: [Client #215] Sampler: noniid
[INFO][10:04:40]: [Client #215] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:04:40]: [93m[1m[Client #215] Started training in communication round #36.[0m
[INFO][10:04:42]: [Client #215] Loading the dataset.
[INFO][10:04:47]: [Client #215] Epoch: [1/5][0/10]	Loss: 0.040704
[INFO][10:04:47]: [Client #215] Epoch: [2/5][0/10]	Loss: 0.006825
[INFO][10:04:47]: [Client #215] Epoch: [3/5][0/10]	Loss: 0.000034
[INFO][10:04:48]: [Client #215] Epoch: [4/5][0/10]	Loss: 0.000760
[INFO][10:04:48]: [Client #215] Epoch: [5/5][0/10]	Loss: 0.000728
[INFO][10:04:48]: [Client #215] Model saved to /data/ykang/plato/results/test/model/lenet5_215_1127977.pth.
[INFO][10:04:48]: [Client #215] Loading a model from /data/ykang/plato/results/test/model/lenet5_215_1127977.pth.
[INFO][10:04:48]: [Client #215] Model trained.
[INFO][10:04:48]: [Client #215] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:04:48]: [Server #1127936] Received 0.24 MB of payload data from client #215 (simulated).
[INFO][10:04:48]: [Server #1127936] Adding client #329 to the list of clients for aggregation.
[INFO][10:04:48]: [Server #1127936] Adding client #355 to the list of clients for aggregation.
[INFO][10:04:48]: [Server #1127936] Adding client #42 to the list of clients for aggregation.
[INFO][10:04:48]: [Server #1127936] Adding client #377 to the list of clients for aggregation.
[INFO][10:04:48]: [Server #1127936] Adding client #242 to the list of clients for aggregation.
[INFO][10:04:48]: [Server #1127936] Adding client #197 to the list of clients for aggregation.
[INFO][10:04:48]: [Server #1127936] Adding client #411 to the list of clients for aggregation.
[INFO][10:04:48]: [Server #1127936] Adding client #215 to the list of clients for aggregation.
[INFO][10:04:48]: [Server #1127936] Adding client #384 to the list of clients for aggregation.
[INFO][10:04:48]: [Server #1127936] Adding client #308 to the list of clients for aggregation.
[INFO][10:04:48]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 243, 244, 245, 246, 247, 248, 249, 250, 251, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00853794
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.0048146  0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01126032 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01140371 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00729589 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01041325 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00827205 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01461417 0.
 0.         0.         0.         0.         0.         0.01131753
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00554849 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 3. 0. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 3. 1. 0. 0. 1. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 2. 0. 6. 0. 1. 2. 1. 0. 1. 1. 0. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.
 1. 0. 1. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 4. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0.
 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0.
 0. 1. 1. 1. 0. 2. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 0. 1. 0. 0. 0. 6. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1.
 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 2. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 2. 0. 1. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00853794
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.0048146  0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01126032 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01140371 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00729589 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01041325 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00827205 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01461417 0.
 0.         0.         0.         0.         0.         0.01131753
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00554849 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:04:50]: [Server #1127936] Global model accuracy: 94.95%

[INFO][10:04:50]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_36.pth.
[INFO][10:04:50]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_36.pth.
[INFO][10:04:50]: [93m[1m
[Server #1127936] Starting round 37/100.[0m
[INFO][10:04:51]: [Server #1127936] Selected clients: [140 164 377 269 125 325 237 100  13 364]
[INFO][10:04:51]: [Server #1127936] Selecting client #140 for training.
[INFO][10:04:51]: [Server #1127936] Sending the current model to client #140 (simulated).
[INFO][10:04:51]: [Server #1127936] Sending 0.24 MB of payload data to client #140 (simulated).
[INFO][10:04:51]: [Server #1127936] Selecting client #164 for training.
[INFO][10:04:51]: [Server #1127936] Sending the current model to client #164 (simulated).
[INFO][10:04:51]: [Server #1127936] Sending 0.24 MB of payload data to client #164 (simulated).
[INFO][10:04:51]: [Server #1127936] Selecting client #377 for training.
[INFO][10:04:51]: [Server #1127936] Sending the current model to client #377 (simulated).
[INFO][10:04:51]: [Client #140] Selected by the server.
[INFO][10:04:51]: [Client #140] Loading its data source...
[INFO][10:04:51]: [Client #140] Dataset size: 60000
[INFO][10:04:51]: [Client #140] Sampler: noniid
[INFO][10:04:51]: [Server #1127936] Sending 0.24 MB of payload data to client #377 (simulated).
[INFO][10:04:51]: [Client #164] Selected by the server.
[INFO][10:04:51]: [Client #164] Loading its data source...
[INFO][10:04:51]: [Client #164] Dataset size: 60000
[INFO][10:04:51]: [Client #164] Sampler: noniid
[INFO][10:04:51]: [Client #377] Selected by the server.
[INFO][10:04:51]: [Client #377] Loading its data source...
[INFO][10:04:51]: [Client #377] Dataset size: 60000
[INFO][10:04:51]: [Client #140] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:04:51]: [Client #377] Sampler: noniid
[INFO][10:04:51]: [Client #164] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:04:51]: [Client #377] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:04:51]: [93m[1m[Client #140] Started training in communication round #37.[0m
[INFO][10:04:51]: [93m[1m[Client #377] Started training in communication round #37.[0m
[INFO][10:04:51]: [93m[1m[Client #164] Started training in communication round #37.[0m
[INFO][10:04:53]: [Client #377] Loading the dataset.
[INFO][10:04:53]: [Client #140] Loading the dataset.
[INFO][10:04:53]: [Client #164] Loading the dataset.
[INFO][10:04:59]: [Client #140] Epoch: [1/5][0/10]	Loss: 0.019150
[INFO][10:04:59]: [Client #164] Epoch: [1/5][0/10]	Loss: 0.020123
[INFO][10:04:59]: [Client #377] Epoch: [1/5][0/10]	Loss: 0.193899
[INFO][10:04:59]: [Client #140] Epoch: [2/5][0/10]	Loss: 0.001409
[INFO][10:04:59]: [Client #164] Epoch: [2/5][0/10]	Loss: 0.020927
[INFO][10:04:59]: [Client #377] Epoch: [2/5][0/10]	Loss: 0.006401
[INFO][10:04:59]: [Client #140] Epoch: [3/5][0/10]	Loss: 0.004665
[INFO][10:04:59]: [Client #164] Epoch: [3/5][0/10]	Loss: 0.000007
[INFO][10:04:59]: [Client #377] Epoch: [3/5][0/10]	Loss: 0.000022
[INFO][10:04:59]: [Client #140] Epoch: [4/5][0/10]	Loss: 0.005065
[INFO][10:04:59]: [Client #164] Epoch: [4/5][0/10]	Loss: 0.001298
[INFO][10:04:59]: [Client #140] Epoch: [5/5][0/10]	Loss: 0.034805
[INFO][10:04:59]: [Client #377] Epoch: [4/5][0/10]	Loss: 0.001537
[INFO][10:04:59]: [Client #164] Epoch: [5/5][0/10]	Loss: 0.009952
[INFO][10:04:59]: [Client #140] Model saved to /data/ykang/plato/results/test/model/lenet5_140_1127977.pth.
[INFO][10:04:59]: [Client #164] Model saved to /data/ykang/plato/results/test/model/lenet5_164_1127978.pth.
[INFO][10:04:59]: [Client #377] Epoch: [5/5][0/10]	Loss: 0.019479
[INFO][10:04:59]: [Client #377] Model saved to /data/ykang/plato/results/test/model/lenet5_377_1127979.pth.
[INFO][10:05:00]: [Client #140] Loading a model from /data/ykang/plato/results/test/model/lenet5_140_1127977.pth.
[INFO][10:05:00]: [Client #140] Model trained.
[INFO][10:05:00]: [Client #140] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:05:00]: [Server #1127936] Received 0.24 MB of payload data from client #140 (simulated).
[INFO][10:05:00]: [Client #164] Loading a model from /data/ykang/plato/results/test/model/lenet5_164_1127978.pth.
[INFO][10:05:00]: [Client #164] Model trained.
[INFO][10:05:00]: [Client #164] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:05:00]: [Server #1127936] Received 0.24 MB of payload data from client #164 (simulated).
[INFO][10:05:00]: [Client #377] Loading a model from /data/ykang/plato/results/test/model/lenet5_377_1127979.pth.
[INFO][10:05:00]: [Client #377] Model trained.
[INFO][10:05:00]: [Client #377] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:05:00]: [Server #1127936] Received 0.24 MB of payload data from client #377 (simulated).
[INFO][10:05:00]: [Server #1127936] Selecting client #269 for training.
[INFO][10:05:00]: [Server #1127936] Sending the current model to client #269 (simulated).
[INFO][10:05:00]: [Server #1127936] Sending 0.24 MB of payload data to client #269 (simulated).
[INFO][10:05:00]: [Server #1127936] Selecting client #125 for training.
[INFO][10:05:00]: [Server #1127936] Sending the current model to client #125 (simulated).
[INFO][10:05:00]: [Server #1127936] Sending 0.24 MB of payload data to client #125 (simulated).
[INFO][10:05:00]: [Server #1127936] Selecting client #325 for training.
[INFO][10:05:00]: [Server #1127936] Sending the current model to client #325 (simulated).
[INFO][10:05:00]: [Client #269] Selected by the server.
[INFO][10:05:00]: [Client #269] Loading its data source...
[INFO][10:05:00]: [Client #269] Dataset size: 60000
[INFO][10:05:00]: [Client #269] Sampler: noniid
[INFO][10:05:00]: [Server #1127936] Sending 0.24 MB of payload data to client #325 (simulated).
[INFO][10:05:00]: [Client #125] Selected by the server.
[INFO][10:05:00]: [Client #325] Selected by the server.
[INFO][10:05:00]: [Client #325] Loading its data source...
[INFO][10:05:00]: [Client #125] Loading its data source...
[INFO][10:05:00]: [Client #325] Dataset size: 60000
[INFO][10:05:00]: [Client #125] Dataset size: 60000
[INFO][10:05:00]: [Client #325] Sampler: noniid
[INFO][10:05:00]: [Client #125] Sampler: noniid
[INFO][10:05:00]: [Client #269] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:05:00]: [Client #125] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:05:00]: [Client #325] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:05:00]: [93m[1m[Client #269] Started training in communication round #37.[0m
[INFO][10:05:00]: [93m[1m[Client #125] Started training in communication round #37.[0m
[INFO][10:05:00]: [93m[1m[Client #325] Started training in communication round #37.[0m
[INFO][10:05:02]: [Client #269] Loading the dataset.
[INFO][10:05:02]: [Client #325] Loading the dataset.
[INFO][10:05:02]: [Client #125] Loading the dataset.
[INFO][10:05:08]: [Client #269] Epoch: [1/5][0/10]	Loss: 0.007718
[INFO][10:05:08]: [Client #269] Epoch: [2/5][0/10]	Loss: 0.000001
[INFO][10:05:08]: [Client #125] Epoch: [1/5][0/10]	Loss: 0.041392
[INFO][10:05:08]: [Client #325] Epoch: [1/5][0/10]	Loss: 0.007365
[INFO][10:05:08]: [Client #269] Epoch: [3/5][0/10]	Loss: 0.000057
[INFO][10:05:08]: [Client #125] Epoch: [2/5][0/10]	Loss: 0.000948
[INFO][10:05:08]: [Client #325] Epoch: [2/5][0/10]	Loss: 0.001499
[INFO][10:05:09]: [Client #269] Epoch: [4/5][0/10]	Loss: 0.000721
[INFO][10:05:09]: [Client #125] Epoch: [3/5][0/10]	Loss: 0.000022
[INFO][10:05:09]: [Client #325] Epoch: [3/5][0/10]	Loss: 0.002763
[INFO][10:05:09]: [Client #269] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:05:09]: [Client #125] Epoch: [4/5][0/10]	Loss: 0.000668
[INFO][10:05:09]: [Client #269] Model saved to /data/ykang/plato/results/test/model/lenet5_269_1127977.pth.
[INFO][10:05:09]: [Client #325] Epoch: [4/5][0/10]	Loss: 0.001181
[INFO][10:05:09]: [Client #125] Epoch: [5/5][0/10]	Loss: 0.232208
[INFO][10:05:09]: [Client #125] Model saved to /data/ykang/plato/results/test/model/lenet5_125_1127978.pth.
[INFO][10:05:09]: [Client #325] Epoch: [5/5][0/10]	Loss: 0.206044
[INFO][10:05:09]: [Client #325] Model saved to /data/ykang/plato/results/test/model/lenet5_325_1127979.pth.
[INFO][10:05:09]: [Client #269] Loading a model from /data/ykang/plato/results/test/model/lenet5_269_1127977.pth.
[INFO][10:05:09]: [Client #269] Model trained.
[INFO][10:05:09]: [Client #269] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:05:09]: [Server #1127936] Received 0.24 MB of payload data from client #269 (simulated).
[INFO][10:05:10]: [Client #125] Loading a model from /data/ykang/plato/results/test/model/lenet5_125_1127978.pth.
[INFO][10:05:10]: [Client #125] Model trained.
[INFO][10:05:10]: [Client #125] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:05:10]: [Server #1127936] Received 0.24 MB of payload data from client #125 (simulated).
[INFO][10:05:10]: [Client #325] Loading a model from /data/ykang/plato/results/test/model/lenet5_325_1127979.pth.
[INFO][10:05:10]: [Client #325] Model trained.
[INFO][10:05:10]: [Client #325] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:05:10]: [Server #1127936] Received 0.24 MB of payload data from client #325 (simulated).
[INFO][10:05:10]: [Server #1127936] Selecting client #237 for training.
[INFO][10:05:10]: [Server #1127936] Sending the current model to client #237 (simulated).
[INFO][10:05:10]: [Server #1127936] Sending 0.24 MB of payload data to client #237 (simulated).
[INFO][10:05:10]: [Server #1127936] Selecting client #100 for training.
[INFO][10:05:10]: [Server #1127936] Sending the current model to client #100 (simulated).
[INFO][10:05:10]: [Server #1127936] Sending 0.24 MB of payload data to client #100 (simulated).
[INFO][10:05:10]: [Server #1127936] Selecting client #13 for training.
[INFO][10:05:10]: [Server #1127936] Sending the current model to client #13 (simulated).
[INFO][10:05:10]: [Client #237] Selected by the server.
[INFO][10:05:10]: [Client #237] Loading its data source...
[INFO][10:05:10]: [Client #237] Dataset size: 60000
[INFO][10:05:10]: [Client #237] Sampler: noniid
[INFO][10:05:10]: [Server #1127936] Sending 0.24 MB of payload data to client #13 (simulated).
[INFO][10:05:10]: [Client #237] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:05:10]: [Client #100] Selected by the server.
[INFO][10:05:10]: [Client #100] Loading its data source...
[INFO][10:05:10]: [Client #100] Dataset size: 60000
[INFO][10:05:10]: [Client #100] Sampler: noniid
[INFO][10:05:10]: [Client #13] Selected by the server.
[INFO][10:05:10]: [Client #13] Loading its data source...
[INFO][10:05:10]: [Client #13] Dataset size: 60000
[INFO][10:05:10]: [Client #13] Sampler: noniid
[INFO][10:05:10]: [Client #100] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:05:10]: [Client #13] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:05:10]: [93m[1m[Client #237] Started training in communication round #37.[0m
[INFO][10:05:10]: [93m[1m[Client #100] Started training in communication round #37.[0m
[INFO][10:05:10]: [93m[1m[Client #13] Started training in communication round #37.[0m
[INFO][10:05:12]: [Client #13] Loading the dataset.
[INFO][10:05:12]: [Client #100] Loading the dataset.
[INFO][10:05:12]: [Client #237] Loading the dataset.
[INFO][10:05:18]: [Client #100] Epoch: [1/5][0/10]	Loss: 0.013914
[INFO][10:05:18]: [Client #13] Epoch: [1/5][0/10]	Loss: 0.050557
[INFO][10:05:18]: [Client #237] Epoch: [1/5][0/10]	Loss: 0.014830
[INFO][10:05:18]: [Client #100] Epoch: [2/5][0/10]	Loss: 0.021688
[INFO][10:05:18]: [Client #13] Epoch: [2/5][0/10]	Loss: 0.003149
[INFO][10:05:18]: [Client #237] Epoch: [2/5][0/10]	Loss: 0.133760
[INFO][10:05:18]: [Client #100] Epoch: [3/5][0/10]	Loss: 0.000224
[INFO][10:05:18]: [Client #237] Epoch: [3/5][0/10]	Loss: 0.000076
[INFO][10:05:18]: [Client #100] Epoch: [4/5][0/10]	Loss: 0.004346
[INFO][10:05:18]: [Client #13] Epoch: [3/5][0/10]	Loss: 0.000000
[INFO][10:05:18]: [Client #237] Epoch: [4/5][0/10]	Loss: 0.002561
[INFO][10:05:18]: [Client #100] Epoch: [5/5][0/10]	Loss: 0.026047
[INFO][10:05:18]: [Client #100] Model saved to /data/ykang/plato/results/test/model/lenet5_100_1127978.pth.
[INFO][10:05:18]: [Client #13] Epoch: [4/5][0/10]	Loss: 0.000007
[INFO][10:05:18]: [Client #237] Epoch: [5/5][0/10]	Loss: 0.053214
[INFO][10:05:18]: [Client #237] Model saved to /data/ykang/plato/results/test/model/lenet5_237_1127977.pth.
[INFO][10:05:18]: [Client #13] Epoch: [5/5][0/10]	Loss: 0.000019
[INFO][10:05:18]: [Client #13] Model saved to /data/ykang/plato/results/test/model/lenet5_13_1127979.pth.
[INFO][10:05:19]: [Client #100] Loading a model from /data/ykang/plato/results/test/model/lenet5_100_1127978.pth.
[INFO][10:05:19]: [Client #100] Model trained.
[INFO][10:05:19]: [Client #100] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:05:19]: [Server #1127936] Received 0.24 MB of payload data from client #100 (simulated).
[INFO][10:05:19]: [Client #237] Loading a model from /data/ykang/plato/results/test/model/lenet5_237_1127977.pth.
[INFO][10:05:19]: [Client #237] Model trained.
[INFO][10:05:19]: [Client #237] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:05:19]: [Server #1127936] Received 0.24 MB of payload data from client #237 (simulated).
[INFO][10:05:19]: [Client #13] Loading a model from /data/ykang/plato/results/test/model/lenet5_13_1127979.pth.
[INFO][10:05:19]: [Client #13] Model trained.
[INFO][10:05:19]: [Client #13] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:05:19]: [Server #1127936] Received 0.24 MB of payload data from client #13 (simulated).
[INFO][10:05:19]: [Server #1127936] Selecting client #364 for training.
[INFO][10:05:19]: [Server #1127936] Sending the current model to client #364 (simulated).
[INFO][10:05:19]: [Server #1127936] Sending 0.24 MB of payload data to client #364 (simulated).
[INFO][10:05:19]: [Client #364] Selected by the server.
[INFO][10:05:19]: [Client #364] Loading its data source...
[INFO][10:05:19]: [Client #364] Dataset size: 60000
[INFO][10:05:19]: [Client #364] Sampler: noniid
[INFO][10:05:19]: [Client #364] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:05:19]: [93m[1m[Client #364] Started training in communication round #37.[0m
[INFO][10:05:21]: [Client #364] Loading the dataset.
[INFO][10:05:26]: [Client #364] Epoch: [1/5][0/10]	Loss: 0.016561
[INFO][10:05:26]: [Client #364] Epoch: [2/5][0/10]	Loss: 0.000656
[INFO][10:05:26]: [Client #364] Epoch: [3/5][0/10]	Loss: 0.000123
[INFO][10:05:27]: [Client #364] Epoch: [4/5][0/10]	Loss: 0.508550
[INFO][10:05:27]: [Client #364] Epoch: [5/5][0/10]	Loss: 0.263201
[INFO][10:05:27]: [Client #364] Model saved to /data/ykang/plato/results/test/model/lenet5_364_1127977.pth.
[INFO][10:05:27]: [Client #364] Loading a model from /data/ykang/plato/results/test/model/lenet5_364_1127977.pth.
[INFO][10:05:27]: [Client #364] Model trained.
[INFO][10:05:27]: [Client #364] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:05:27]: [Server #1127936] Received 0.24 MB of payload data from client #364 (simulated).
[INFO][10:05:27]: [Server #1127936] Adding client #162 to the list of clients for aggregation.
[INFO][10:05:27]: [Server #1127936] Adding client #90 to the list of clients for aggregation.
[INFO][10:05:27]: [Server #1127936] Adding client #119 to the list of clients for aggregation.
[INFO][10:05:27]: [Server #1127936] Adding client #110 to the list of clients for aggregation.
[INFO][10:05:27]: [Server #1127936] Adding client #367 to the list of clients for aggregation.
[INFO][10:05:27]: [Server #1127936] Adding client #152 to the list of clients for aggregation.
[INFO][10:05:27]: [Server #1127936] Adding client #22 to the list of clients for aggregation.
[INFO][10:05:27]: [Server #1127936] Adding client #237 to the list of clients for aggregation.
[INFO][10:05:27]: [Server #1127936] Adding client #13 to the list of clients for aggregation.
[INFO][10:05:27]: [Server #1127936] Adding client #269 to the list of clients for aggregation.
[INFO][10:05:27]: [Server #1127936] Aggregating 10 clients in total.
[0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1
 0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  2e-05  3e-10  3e-10
 6:  6.8876e+00  6.8875e+00  1e-05  2e-10  2e-10
 7:  6.8875e+00  6.8875e+00  1e-05  3e-10  5e-12
 8:  6.8875e+00  6.8875e+00  9e-06  2e-10  4e-12
 9:  6.8875e+00  6.8875e+00  5e-06  4e-10  6e-12
Optimal solution found.
The calculated probability is:  [0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00184775 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072965 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072959 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00355923 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072963
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00270808 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00176673 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.63623301 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072959 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072964
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966 0.00072966
 0.00072966 0.00072966 0.00072966 0.00072966]
current clients pool:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 153, 154, 155, 156, 157, 158, 159, 160, 161, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01022187 0.         0.         0.         0.         0.
 0.         0.         0.         0.00798461 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00841265
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01371175 0.         0.         0.         0.
 0.         0.         0.         0.         0.00540452 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01247616 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.0057604
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01170893 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00914567 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00617887 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 3. 0. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 3. 1. 0. 0. 1. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 3. 0. 1. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 2. 0. 6. 0. 1. 2. 1. 0. 1. 1. 0. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.
 1. 0. 1. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 4. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0.
 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0.
 0. 1. 1. 1. 0. 2. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 0. 1. 0. 0. 0. 6. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1.
 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 2. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 2. 0. 1. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]
local_gradient_bounds:  [INFO][10:05:29]: [Server #1127936] Global model accuracy: 94.86%

[INFO][10:05:29]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_37.pth.
[INFO][10:05:29]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_37.pth.
[INFO][10:05:29]: [93m[1m
[Server #1127936] Starting round 38/100.[0m
[0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01022187 0.         0.         0.         0.         0.
 0.         0.         0.         0.00798461 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00841265
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01371175 0.         0.         0.         0.
 0.         0.         0.         0.         0.00540452 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01247616 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.0057604
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01170893 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00914567 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00617887 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1
 0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  4e-04  7e-09  7e-09
 5:  6.8876e+00  6.8875e+00  9e-05  1e-09  1e-09
 6:  6.8875e+00  6.8875e+00  8e-05  6e-10  5e-10
 7:  6.8875e+00  6.8875e+00  7e-05  3e-09  3e-10
 8:  6.8875e+00  6.8875e+00  6e-05  4e-09  3e-10
 9:  6.8875e+00  6.8875e+00  2e-05  3e-08  2e-09
10:  6.8875e+00  6.8875e+00  6e-06  1e-08  1e-09
Optimal solution found.
The calculated probability is:  [1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66238433e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 2.51879614e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.88868663e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 9.18201807e-01
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.80110931e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 3.52760467e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.81104949e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66237660e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66238927e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 3.49116185e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04 1.66240910e-04 1.66240910e-04
 1.66240910e-04 1.66240910e-04]
current clients pool:  [INFO][10:05:30]: [Server #1127936] Selected clients: [110 154 436 456 167 343 450  37 440  95]
[INFO][10:05:30]: [Server #1127936] Selecting client #110 for training.
[INFO][10:05:30]: [Server #1127936] Sending the current model to client #110 (simulated).
[INFO][10:05:30]: [Server #1127936] Sending 0.24 MB of payload data to client #110 (simulated).
[INFO][10:05:30]: [Server #1127936] Selecting client #154 for training.
[INFO][10:05:30]: [Server #1127936] Sending the current model to client #154 (simulated).
[INFO][10:05:30]: [Server #1127936] Sending 0.24 MB of payload data to client #154 (simulated).
[INFO][10:05:30]: [Server #1127936] Selecting client #436 for training.
[INFO][10:05:30]: [Server #1127936] Sending the current model to client #436 (simulated).
[INFO][10:05:30]: [Client #110] Selected by the server.
[INFO][10:05:30]: [Client #110] Loading its data source...
[INFO][10:05:30]: [Client #110] Dataset size: 60000
[INFO][10:05:30]: [Client #110] Sampler: noniid
[INFO][10:05:30]: [Server #1127936] Sending 0.24 MB of payload data to client #436 (simulated).
[INFO][10:05:30]: [Client #154] Selected by the server.
[INFO][10:05:30]: [Client #154] Loading its data source...
[INFO][10:05:30]: [Client #436] Selected by the server.
[INFO][10:05:30]: [Client #154] Dataset size: 60000
[INFO][10:05:30]: [Client #436] Loading its data source...
[INFO][10:05:30]: [Client #154] Sampler: noniid
[INFO][10:05:30]: [Client #436] Dataset size: 60000
[INFO][10:05:30]: [Client #436] Sampler: noniid
[INFO][10:05:30]: [Client #110] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:05:30]: [Client #154] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:05:30]: [Client #436] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:05:30]: [93m[1m[Client #436] Started training in communication round #38.[0m
[INFO][10:05:30]: [93m[1m[Client #154] Started training in communication round #38.[0m
[INFO][10:05:30]: [93m[1m[Client #110] Started training in communication round #38.[0m
[INFO][10:05:32]: [Client #110] Loading the dataset.
[INFO][10:05:32]: [Client #436] Loading the dataset.
[INFO][10:05:32]: [Client #154] Loading the dataset.
[INFO][10:05:38]: [Client #436] Epoch: [1/5][0/10]	Loss: 0.019505
[INFO][10:05:38]: [Client #154] Epoch: [1/5][0/10]	Loss: 0.018977
[INFO][10:05:38]: [Client #110] Epoch: [1/5][0/10]	Loss: 0.049289
[INFO][10:05:38]: [Client #436] Epoch: [2/5][0/10]	Loss: 0.134709
[INFO][10:05:38]: [Client #154] Epoch: [2/5][0/10]	Loss: 0.021450
[INFO][10:05:38]: [Client #436] Epoch: [3/5][0/10]	Loss: 0.000229
[INFO][10:05:38]: [Client #154] Epoch: [3/5][0/10]	Loss: 0.000289
[INFO][10:05:38]: [Client #110] Epoch: [2/5][0/10]	Loss: 0.001145
[INFO][10:05:38]: [Client #436] Epoch: [4/5][0/10]	Loss: 0.000075
[INFO][10:05:38]: [Client #154] Epoch: [4/5][0/10]	Loss: 0.002564
[INFO][10:05:38]: [Client #110] Epoch: [3/5][0/10]	Loss: 0.001404
[INFO][10:05:38]: [Client #436] Epoch: [5/5][0/10]	Loss: 0.000335
[INFO][10:05:38]: [Client #110] Epoch: [4/5][0/10]	Loss: 0.000041
[INFO][10:05:38]: [Client #436] Model saved to /data/ykang/plato/results/test/model/lenet5_436_1127979.pth.
[INFO][10:05:38]: [Client #154] Epoch: [5/5][0/10]	Loss: 0.003687
[INFO][10:05:38]: [Client #154] Model saved to /data/ykang/plato/results/test/model/lenet5_154_1127978.pth.
[INFO][10:05:38]: [Client #110] Epoch: [5/5][0/10]	Loss: 0.000005
[INFO][10:05:38]: [Client #110] Model saved to /data/ykang/plato/results/test/model/lenet5_110_1127977.pth.
[INFO][10:05:39]: [Client #436] Loading a model from /data/ykang/plato/results/test/model/lenet5_436_1127979.pth.
[INFO][10:05:39]: [Client #436] Model trained.
[INFO][10:05:39]: [Client #436] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:05:39]: [Server #1127936] Received 0.24 MB of payload data from client #436 (simulated).
[INFO][10:05:39]: [Client #154] Loading a model from /data/ykang/plato/results/test/model/lenet5_154_1127978.pth.
[INFO][10:05:39]: [Client #154] Model trained.
[INFO][10:05:39]: [Client #154] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:05:39]: [Server #1127936] Received 0.24 MB of payload data from client #154 (simulated).
[INFO][10:05:39]: [Client #110] Loading a model from /data/ykang/plato/results/test/model/lenet5_110_1127977.pth.
[INFO][10:05:39]: [Client #110] Model trained.
[INFO][10:05:39]: [Client #110] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:05:39]: [Server #1127936] Received 0.24 MB of payload data from client #110 (simulated).
[INFO][10:05:39]: [Server #1127936] Selecting client #456 for training.
[INFO][10:05:39]: [Server #1127936] Sending the current model to client #456 (simulated).
[INFO][10:05:39]: [Server #1127936] Sending 0.24 MB of payload data to client #456 (simulated).
[INFO][10:05:39]: [Server #1127936] Selecting client #167 for training.
[INFO][10:05:39]: [Server #1127936] Sending the current model to client #167 (simulated).
[INFO][10:05:39]: [Server #1127936] Sending 0.24 MB of payload data to client #167 (simulated).
[INFO][10:05:39]: [Server #1127936] Selecting client #343 for training.
[INFO][10:05:39]: [Server #1127936] Sending the current model to client #343 (simulated).
[INFO][10:05:39]: [Client #456] Selected by the server.
[INFO][10:05:39]: [Client #456] Loading its data source...
[INFO][10:05:39]: [Client #456] Dataset size: 60000
[INFO][10:05:39]: [Client #456] Sampler: noniid
[INFO][10:05:39]: [Server #1127936] Sending 0.24 MB of payload data to client #343 (simulated).
[INFO][10:05:39]: [Client #167] Selected by the server.
[INFO][10:05:39]: [Client #167] Loading its data source...
[INFO][10:05:39]: [Client #343] Selected by the server.
[INFO][10:05:39]: [Client #167] Dataset size: 60000
[INFO][10:05:39]: [Client #343] Loading its data source...
[INFO][10:05:39]: [Client #167] Sampler: noniid
[INFO][10:05:39]: [Client #343] Dataset size: 60000
[INFO][10:05:39]: [Client #343] Sampler: noniid
[INFO][10:05:39]: [Client #456] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:05:39]: [Client #167] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:05:39]: [Client #343] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:05:39]: [93m[1m[Client #167] Started training in communication round #38.[0m
[INFO][10:05:39]: [93m[1m[Client #456] Started training in communication round #38.[0m
[INFO][10:05:39]: [93m[1m[Client #343] Started training in communication round #38.[0m
[INFO][10:05:41]: [Client #167] Loading the dataset.
[INFO][10:05:41]: [Client #456] Loading the dataset.
[INFO][10:05:41]: [Client #343] Loading the dataset.
[INFO][10:05:47]: [Client #167] Epoch: [1/5][0/10]	Loss: 0.001079
[INFO][10:05:47]: [Client #456] Epoch: [1/5][0/10]	Loss: 0.048742
[INFO][10:05:47]: [Client #343] Epoch: [1/5][0/10]	Loss: 0.036434
[INFO][10:05:47]: [Client #167] Epoch: [2/5][0/10]	Loss: 0.000982
[INFO][10:05:48]: [Client #456] Epoch: [2/5][0/10]	Loss: 0.003589
[INFO][10:05:48]: [Client #167] Epoch: [3/5][0/10]	Loss: 0.000145
[INFO][10:05:48]: [Client #343] Epoch: [2/5][0/10]	Loss: 0.055004
[INFO][10:05:48]: [Client #456] Epoch: [3/5][0/10]	Loss: 0.000225
[INFO][10:05:48]: [Client #167] Epoch: [4/5][0/10]	Loss: 0.000590
[INFO][10:05:48]: [Client #343] Epoch: [3/5][0/10]	Loss: 0.021709
[INFO][10:05:48]: [Client #167] Epoch: [5/5][0/10]	Loss: 0.001413
[INFO][10:05:48]: [Client #343] Epoch: [4/5][0/10]	Loss: 0.039014
[INFO][10:05:48]: [Client #456] Epoch: [4/5][0/10]	Loss: 0.003518
[INFO][10:05:48]: [Client #167] Model saved to /data/ykang/plato/results/test/model/lenet5_167_1127978.pth.
[INFO][10:05:48]: [Client #343] Epoch: [5/5][0/10]	Loss: 0.674978
[INFO][10:05:48]: [Client #456] Epoch: [5/5][0/10]	Loss: 0.286578
[INFO][10:05:48]: [Client #343] Model saved to /data/ykang/plato/results/test/model/lenet5_343_1127979.pth.
[INFO][10:05:48]: [Client #456] Model saved to /data/ykang/plato/results/test/model/lenet5_456_1127977.pth.
[INFO][10:05:49]: [Client #167] Loading a model from /data/ykang/plato/results/test/model/lenet5_167_1127978.pth.
[INFO][10:05:49]: [Client #167] Model trained.
[INFO][10:05:49]: [Client #167] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:05:49]: [Server #1127936] Received 0.24 MB of payload data from client #167 (simulated).
[INFO][10:05:49]: [Client #343] Loading a model from /data/ykang/plato/results/test/model/lenet5_343_1127979.pth.
[INFO][10:05:49]: [Client #343] Model trained.
[INFO][10:05:49]: [Client #343] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:05:49]: [Server #1127936] Received 0.24 MB of payload data from client #343 (simulated).
[INFO][10:05:49]: [Client #456] Loading a model from /data/ykang/plato/results/test/model/lenet5_456_1127977.pth.
[INFO][10:05:49]: [Client #456] Model trained.
[INFO][10:05:49]: [Client #456] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:05:49]: [Server #1127936] Received 0.24 MB of payload data from client #456 (simulated).
[INFO][10:05:49]: [Server #1127936] Selecting client #450 for training.
[INFO][10:05:49]: [Server #1127936] Sending the current model to client #450 (simulated).
[INFO][10:05:49]: [Server #1127936] Sending 0.24 MB of payload data to client #450 (simulated).
[INFO][10:05:49]: [Server #1127936] Selecting client #37 for training.
[INFO][10:05:49]: [Server #1127936] Sending the current model to client #37 (simulated).
[INFO][10:05:49]: [Server #1127936] Sending 0.24 MB of payload data to client #37 (simulated).
[INFO][10:05:49]: [Server #1127936] Selecting client #440 for training.
[INFO][10:05:49]: [Server #1127936] Sending the current model to client #440 (simulated).
[INFO][10:05:49]: [Client #450] Selected by the server.
[INFO][10:05:49]: [Client #450] Loading its data source...
[INFO][10:05:49]: [Client #450] Dataset size: 60000
[INFO][10:05:49]: [Client #450] Sampler: noniid
[INFO][10:05:49]: [Server #1127936] Sending 0.24 MB of payload data to client #440 (simulated).
[INFO][10:05:49]: [Client #37] Selected by the server.
[INFO][10:05:49]: [Client #37] Loading its data source...
[INFO][10:05:49]: [Client #37] Dataset size: 60000
[INFO][10:05:49]: [Client #37] Sampler: noniid
[INFO][10:05:49]: [Client #440] Selected by the server.
[INFO][10:05:49]: [Client #440] Loading its data source...
[INFO][10:05:49]: [Client #440] Dataset size: 60000
[INFO][10:05:49]: [Client #440] Sampler: noniid
[INFO][10:05:49]: [Client #450] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:05:49]: [Client #37] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:05:49]: [Client #440] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:05:49]: [93m[1m[Client #450] Started training in communication round #38.[0m
[INFO][10:05:49]: [93m[1m[Client #440] Started training in communication round #38.[0m
[INFO][10:05:49]: [93m[1m[Client #37] Started training in communication round #38.[0m
[INFO][10:05:51]: [Client #37] Loading the dataset.
[INFO][10:05:51]: [Client #440] Loading the dataset.
[INFO][10:05:51]: [Client #450] Loading the dataset.
[INFO][10:05:58]: [Client #450] Epoch: [1/5][0/10]	Loss: 0.015882
[INFO][10:05:58]: [Client #37] Epoch: [1/5][0/10]	Loss: 0.024388
[INFO][10:05:58]: [Client #440] Epoch: [1/5][0/10]	Loss: 0.022932
[INFO][10:05:58]: [Client #450] Epoch: [2/5][0/10]	Loss: 0.002451
[INFO][10:05:58]: [Client #37] Epoch: [2/5][0/10]	Loss: 0.032040
[INFO][10:05:58]: [Client #440] Epoch: [2/5][0/10]	Loss: 0.002816
[INFO][10:05:58]: [Client #450] Epoch: [3/5][0/10]	Loss: 0.154765
[INFO][10:05:58]: [Client #37] Epoch: [3/5][0/10]	Loss: 0.000154
[INFO][10:05:58]: [Client #440] Epoch: [3/5][0/10]	Loss: 0.000060
[INFO][10:05:58]: [Client #440] Epoch: [4/5][0/10]	Loss: 0.001208
[INFO][10:05:58]: [Client #450] Epoch: [4/5][0/10]	Loss: 0.000367
[INFO][10:05:58]: [Client #440] Epoch: [5/5][0/10]	Loss: 0.321857
[INFO][10:05:58]: [Client #37] Epoch: [4/5][0/10]	Loss: 0.000003
[INFO][10:05:58]: [Client #450] Epoch: [5/5][0/10]	Loss: 0.747394
[INFO][10:05:58]: [Client #440] Model saved to /data/ykang/plato/results/test/model/lenet5_440_1127979.pth.
[INFO][10:05:58]: [Client #450] Model saved to /data/ykang/plato/results/test/model/lenet5_450_1127977.pth.
[INFO][10:05:58]: [Client #37] Epoch: [5/5][0/10]	Loss: 0.048394
[INFO][10:05:59]: [Client #37] Model saved to /data/ykang/plato/results/test/model/lenet5_37_1127978.pth.
[INFO][10:06:00]: [Client #440] Loading a model from /data/ykang/plato/results/test/model/lenet5_440_1127979.pth.
[INFO][10:06:00]: [Client #440] Model trained.
[INFO][10:06:00]: [Client #440] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:06:00]: [Server #1127936] Received 0.24 MB of payload data from client #440 (simulated).
[INFO][10:06:00]: [Client #450] Loading a model from /data/ykang/plato/results/test/model/lenet5_450_1127977.pth.
[INFO][10:06:00]: [Client #450] Model trained.
[INFO][10:06:00]: [Client #450] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:06:00]: [Server #1127936] Received 0.24 MB of payload data from client #450 (simulated).
[INFO][10:06:00]: [Client #37] Loading a model from /data/ykang/plato/results/test/model/lenet5_37_1127978.pth.
[INFO][10:06:00]: [Client #37] Model trained.
[INFO][10:06:00]: [Client #37] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:06:00]: [Server #1127936] Received 0.24 MB of payload data from client #37 (simulated).
[INFO][10:06:00]: [Server #1127936] Selecting client #95 for training.
[INFO][10:06:00]: [Server #1127936] Sending the current model to client #95 (simulated).
[INFO][10:06:00]: [Server #1127936] Sending 0.24 MB of payload data to client #95 (simulated).
[INFO][10:06:00]: [Client #95] Selected by the server.
[INFO][10:06:00]: [Client #95] Loading its data source...
[INFO][10:06:00]: [Client #95] Dataset size: 60000
[INFO][10:06:00]: [Client #95] Sampler: noniid
[INFO][10:06:00]: [Client #95] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:06:00]: [93m[1m[Client #95] Started training in communication round #38.[0m
[INFO][10:06:02]: [Client #95] Loading the dataset.
[INFO][10:06:08]: [Client #95] Epoch: [1/5][0/10]	Loss: 0.009550
[INFO][10:06:08]: [Client #95] Epoch: [2/5][0/10]	Loss: 0.001198
[INFO][10:06:08]: [Client #95] Epoch: [3/5][0/10]	Loss: 0.003013
[INFO][10:06:08]: [Client #95] Epoch: [4/5][0/10]	Loss: 0.011572
[INFO][10:06:08]: [Client #95] Epoch: [5/5][0/10]	Loss: 0.201674
[INFO][10:06:08]: [Client #95] Model saved to /data/ykang/plato/results/test/model/lenet5_95_1127977.pth.
[INFO][10:06:09]: [Client #95] Loading a model from /data/ykang/plato/results/test/model/lenet5_95_1127977.pth.
[INFO][10:06:09]: [Client #95] Model trained.
[INFO][10:06:09]: [Client #95] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:06:09]: [Server #1127936] Received 0.24 MB of payload data from client #95 (simulated).
[INFO][10:06:09]: [Server #1127936] Adding client #325 to the list of clients for aggregation.
[INFO][10:06:09]: [Server #1127936] Adding client #364 to the list of clients for aggregation.
[INFO][10:06:09]: [Server #1127936] Adding client #100 to the list of clients for aggregation.
[INFO][10:06:09]: [Server #1127936] Adding client #140 to the list of clients for aggregation.
[INFO][10:06:09]: [Server #1127936] Adding client #164 to the list of clients for aggregation.
[INFO][10:06:09]: [Server #1127936] Adding client #125 to the list of clients for aggregation.
[INFO][10:06:09]: [Server #1127936] Adding client #377 to the list of clients for aggregation.
[INFO][10:06:09]: [Server #1127936] Adding client #252 to the list of clients for aggregation.
[INFO][10:06:09]: [Server #1127936] Adding client #167 to the list of clients for aggregation.
[INFO][10:06:09]: [Server #1127936] Adding client #440 to the list of clients for aggregation.
[INFO][10:06:09]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01504573 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.0091557  0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.04353035 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.0126132  0.         0.         0.00833142 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00800853
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01506329 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01576749 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01499236 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00938963 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 3. 0. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 3. 1. 0. 0. 1. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 3. 0. 1. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 2. 0. 6. 0. 1. 2. 1. 0. 1. 1. 0. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.
 1. 0. 1. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 4. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0.
 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0.
 0. 1. 1. 1. 0. 2. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 0. 1. 1. 0. 0. 6. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1.
 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 2. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 2. 0. 1. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01504573 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.0091557  0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.04353035 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.0126132  0.         0.         0.00833142 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00800853
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01506329 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01576749 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01499236 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00938963 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:06:11]: [Server #1127936] Global model accuracy: 95.35%

[INFO][10:06:11]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_38.pth.
[INFO][10:06:11]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_38.pth.
[INFO][10:06:11]: [93m[1m
[Server #1127936] Starting round 39/100.[0m
[0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1
 0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  4e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  5e-05  8e-10  8e-10
 6:  6.8876e+00  6.8875e+00  4e-05  5e-10  5e-10
 7:  6.8875e+00  6.8875e+00  4e-05  1e-09  5e-11
 8:  6.8875e+00  6.8875e+00  3e-05  1e-09  5e-11
 9:  6.8875e+00  6.8875e+00  1e-05  6e-09  3e-10
10:  6.8875e+00  6.8875e+00  8e-07  4e-09  2e-10
Optimal solution found.
The calculated probability is:  [3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 6.94553502e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 5.35633455e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 9.80402990e-01
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 6.19308729e-05 3.92220882e-05
 3.92220882e-05 3.92212825e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 2.96200127e-04 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 6.95158928e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 7.20282725e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 6.92719655e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92210649e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05 3.92220882e-05 3.92220882e-05
 3.92220882e-05 3.92220882e-05]
current clients pool:  [INFO][10:06:12]: [Server #1127936] Selected clients: [140 102 219 173 496 195 497 257  89  39]
[INFO][10:06:12]: [Server #1127936] Selecting client #140 for training.
[INFO][10:06:12]: [Server #1127936] Sending the current model to client #140 (simulated).
[INFO][10:06:12]: [Server #1127936] Sending 0.24 MB of payload data to client #140 (simulated).
[INFO][10:06:12]: [Server #1127936] Selecting client #102 for training.
[INFO][10:06:12]: [Server #1127936] Sending the current model to client #102 (simulated).
[INFO][10:06:12]: [Server #1127936] Sending 0.24 MB of payload data to client #102 (simulated).
[INFO][10:06:12]: [Server #1127936] Selecting client #219 for training.
[INFO][10:06:12]: [Server #1127936] Sending the current model to client #219 (simulated).
[INFO][10:06:12]: [Client #140] Selected by the server.
[INFO][10:06:12]: [Client #140] Loading its data source...
[INFO][10:06:12]: [Client #140] Dataset size: 60000
[INFO][10:06:12]: [Client #140] Sampler: noniid
[INFO][10:06:12]: [Server #1127936] Sending 0.24 MB of payload data to client #219 (simulated).
[INFO][10:06:12]: [Client #102] Selected by the server.
[INFO][10:06:12]: [Client #102] Loading its data source...
[INFO][10:06:12]: [Client #102] Dataset size: 60000
[INFO][10:06:12]: [Client #102] Sampler: noniid
[INFO][10:06:12]: [Client #140] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:06:12]: [Client #102] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:06:12]: [Client #219] Selected by the server.
[INFO][10:06:12]: [Client #219] Loading its data source...
[INFO][10:06:12]: [Client #219] Dataset size: 60000
[INFO][10:06:12]: [Client #219] Sampler: noniid
[INFO][10:06:12]: [Client #219] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:06:12]: [93m[1m[Client #219] Started training in communication round #39.[0m
[INFO][10:06:12]: [93m[1m[Client #102] Started training in communication round #39.[0m
[INFO][10:06:12]: [93m[1m[Client #140] Started training in communication round #39.[0m
[INFO][10:06:14]: [Client #102] Loading the dataset.
[INFO][10:06:14]: [Client #140] Loading the dataset.
[INFO][10:06:14]: [Client #219] Loading the dataset.
[INFO][10:06:20]: [Client #219] Epoch: [1/5][0/10]	Loss: 0.028110
[INFO][10:06:20]: [Client #140] Epoch: [1/5][0/10]	Loss: 0.017591
[INFO][10:06:20]: [Client #102] Epoch: [1/5][0/10]	Loss: 0.030983
[INFO][10:06:20]: [Client #219] Epoch: [2/5][0/10]	Loss: 0.009866
[INFO][10:06:20]: [Client #140] Epoch: [2/5][0/10]	Loss: 0.001415
[INFO][10:06:20]: [Client #102] Epoch: [2/5][0/10]	Loss: 0.000001
[INFO][10:06:20]: [Client #219] Epoch: [3/5][0/10]	Loss: 0.001713
[INFO][10:06:20]: [Client #140] Epoch: [3/5][0/10]	Loss: 0.002278
[INFO][10:06:20]: [Client #102] Epoch: [3/5][0/10]	Loss: 0.000075
[INFO][10:06:20]: [Client #219] Epoch: [4/5][0/10]	Loss: 0.000003
[INFO][10:06:20]: [Client #140] Epoch: [4/5][0/10]	Loss: 0.000188
[INFO][10:06:20]: [Client #102] Epoch: [4/5][0/10]	Loss: 0.000002
[INFO][10:06:20]: [Client #219] Epoch: [5/5][0/10]	Loss: 0.000007
[INFO][10:06:20]: [Client #140] Epoch: [5/5][0/10]	Loss: 0.001080
[INFO][10:06:21]: [Client #219] Model saved to /data/ykang/plato/results/test/model/lenet5_219_1127979.pth.
[INFO][10:06:21]: [Client #140] Model saved to /data/ykang/plato/results/test/model/lenet5_140_1127977.pth.
[INFO][10:06:21]: [Client #102] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:06:21]: [Client #102] Model saved to /data/ykang/plato/results/test/model/lenet5_102_1127978.pth.
[INFO][10:06:21]: [Client #219] Loading a model from /data/ykang/plato/results/test/model/lenet5_219_1127979.pth.
[INFO][10:06:21]: [Client #219] Model trained.
[INFO][10:06:21]: [Client #219] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:06:21]: [Server #1127936] Received 0.24 MB of payload data from client #219 (simulated).
[INFO][10:06:21]: [Client #140] Loading a model from /data/ykang/plato/results/test/model/lenet5_140_1127977.pth.
[INFO][10:06:21]: [Client #140] Model trained.
[INFO][10:06:21]: [Client #140] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:06:21]: [Server #1127936] Received 0.24 MB of payload data from client #140 (simulated).
[INFO][10:06:21]: [Client #102] Loading a model from /data/ykang/plato/results/test/model/lenet5_102_1127978.pth.
[INFO][10:06:21]: [Client #102] Model trained.
[INFO][10:06:21]: [Client #102] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:06:21]: [Server #1127936] Received 0.24 MB of payload data from client #102 (simulated).
[INFO][10:06:21]: [Server #1127936] Selecting client #173 for training.
[INFO][10:06:21]: [Server #1127936] Sending the current model to client #173 (simulated).
[INFO][10:06:21]: [Server #1127936] Sending 0.24 MB of payload data to client #173 (simulated).
[INFO][10:06:21]: [Server #1127936] Selecting client #496 for training.
[INFO][10:06:21]: [Server #1127936] Sending the current model to client #496 (simulated).
[INFO][10:06:21]: [Server #1127936] Sending 0.24 MB of payload data to client #496 (simulated).
[INFO][10:06:21]: [Server #1127936] Selecting client #195 for training.
[INFO][10:06:21]: [Server #1127936] Sending the current model to client #195 (simulated).
[INFO][10:06:21]: [Client #173] Selected by the server.
[INFO][10:06:21]: [Client #173] Loading its data source...
[INFO][10:06:21]: [Client #173] Dataset size: 60000
[INFO][10:06:21]: [Client #173] Sampler: noniid
[INFO][10:06:21]: [Server #1127936] Sending 0.24 MB of payload data to client #195 (simulated).
[INFO][10:06:21]: [Client #173] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:06:21]: [Client #496] Selected by the server.
[INFO][10:06:21]: [Client #496] Loading its data source...
[INFO][10:06:21]: [Client #496] Dataset size: 60000
[INFO][10:06:21]: [Client #496] Sampler: noniid
[INFO][10:06:21]: [Client #195] Selected by the server.
[INFO][10:06:21]: [Client #195] Loading its data source...
[INFO][10:06:21]: [Client #195] Dataset size: 60000
[INFO][10:06:21]: [Client #195] Sampler: noniid
[INFO][10:06:21]: [Client #496] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:06:21]: [93m[1m[Client #173] Started training in communication round #39.[0m
[INFO][10:06:21]: [Client #195] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:06:21]: [93m[1m[Client #496] Started training in communication round #39.[0m
[INFO][10:06:21]: [93m[1m[Client #195] Started training in communication round #39.[0m
[INFO][10:06:24]: [Client #173] Loading the dataset.
[INFO][10:06:24]: [Client #496] Loading the dataset.
[INFO][10:06:24]: [Client #195] Loading the dataset.
[INFO][10:06:30]: [Client #173] Epoch: [1/5][0/10]	Loss: 0.009039
[INFO][10:06:30]: [Client #195] Epoch: [1/5][0/10]	Loss: 0.009017
[INFO][10:06:30]: [Client #496] Epoch: [1/5][0/10]	Loss: 0.010689
[INFO][10:06:30]: [Client #173] Epoch: [2/5][0/10]	Loss: 0.003163
[INFO][10:06:30]: [Client #195] Epoch: [2/5][0/10]	Loss: 0.012686
[INFO][10:06:30]: [Client #496] Epoch: [2/5][0/10]	Loss: 0.003593
[INFO][10:06:30]: [Client #173] Epoch: [3/5][0/10]	Loss: 0.000373
[INFO][10:06:30]: [Client #195] Epoch: [3/5][0/10]	Loss: 0.000167
[INFO][10:06:30]: [Client #496] Epoch: [3/5][0/10]	Loss: 0.067707
[INFO][10:06:30]: [Client #173] Epoch: [4/5][0/10]	Loss: 0.007201
[INFO][10:06:30]: [Client #195] Epoch: [4/5][0/10]	Loss: 0.011466
[INFO][10:06:30]: [Client #496] Epoch: [4/5][0/10]	Loss: 0.018670
[INFO][10:06:30]: [Client #195] Epoch: [5/5][0/10]	Loss: 0.099901
[INFO][10:06:30]: [Client #173] Epoch: [5/5][0/10]	Loss: 0.165568
[INFO][10:06:30]: [Client #195] Model saved to /data/ykang/plato/results/test/model/lenet5_195_1127979.pth.
[INFO][10:06:30]: [Client #173] Model saved to /data/ykang/plato/results/test/model/lenet5_173_1127977.pth.
[INFO][10:06:30]: [Client #496] Epoch: [5/5][0/10]	Loss: 0.000012
[INFO][10:06:30]: [Client #496] Model saved to /data/ykang/plato/results/test/model/lenet5_496_1127978.pth.
[INFO][10:06:31]: [Client #195] Loading a model from /data/ykang/plato/results/test/model/lenet5_195_1127979.pth.
[INFO][10:06:31]: [Client #195] Model trained.
[INFO][10:06:31]: [Client #195] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:06:31]: [Server #1127936] Received 0.24 MB of payload data from client #195 (simulated).
[INFO][10:06:31]: [Client #173] Loading a model from /data/ykang/plato/results/test/model/lenet5_173_1127977.pth.
[INFO][10:06:31]: [Client #173] Model trained.
[INFO][10:06:31]: [Client #173] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:06:31]: [Server #1127936] Received 0.24 MB of payload data from client #173 (simulated).
[INFO][10:06:31]: [Client #496] Loading a model from /data/ykang/plato/results/test/model/lenet5_496_1127978.pth.
[INFO][10:06:31]: [Client #496] Model trained.
[INFO][10:06:31]: [Client #496] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:06:31]: [Server #1127936] Received 0.24 MB of payload data from client #496 (simulated).
[INFO][10:06:31]: [Server #1127936] Selecting client #497 for training.
[INFO][10:06:31]: [Server #1127936] Sending the current model to client #497 (simulated).
[INFO][10:06:31]: [Server #1127936] Sending 0.24 MB of payload data to client #497 (simulated).
[INFO][10:06:31]: [Server #1127936] Selecting client #257 for training.
[INFO][10:06:31]: [Server #1127936] Sending the current model to client #257 (simulated).
[INFO][10:06:31]: [Server #1127936] Sending 0.24 MB of payload data to client #257 (simulated).
[INFO][10:06:31]: [Server #1127936] Selecting client #89 for training.
[INFO][10:06:31]: [Server #1127936] Sending the current model to client #89 (simulated).
[INFO][10:06:31]: [Client #497] Selected by the server.
[INFO][10:06:31]: [Client #497] Loading its data source...
[INFO][10:06:31]: [Client #497] Dataset size: 60000
[INFO][10:06:31]: [Client #497] Sampler: noniid
[INFO][10:06:31]: [Server #1127936] Sending 0.24 MB of payload data to client #89 (simulated).
[INFO][10:06:31]: [Client #497] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:06:31]: [Client #257] Selected by the server.
[INFO][10:06:31]: [Client #257] Loading its data source...
[INFO][10:06:31]: [Client #257] Dataset size: 60000
[INFO][10:06:31]: [Client #257] Sampler: noniid
[INFO][10:06:31]: [Client #89] Selected by the server.
[INFO][10:06:31]: [Client #89] Loading its data source...
[INFO][10:06:31]: [Client #89] Dataset size: 60000
[INFO][10:06:31]: [Client #89] Sampler: noniid
[INFO][10:06:31]: [Client #257] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:06:31]: [Client #89] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:06:31]: [93m[1m[Client #89] Started training in communication round #39.[0m
[INFO][10:06:31]: [93m[1m[Client #497] Started training in communication round #39.[0m
[INFO][10:06:31]: [93m[1m[Client #257] Started training in communication round #39.[0m
[INFO][10:06:33]: [Client #497] Loading the dataset.
[INFO][10:06:33]: [Client #257] Loading the dataset.
[INFO][10:06:33]: [Client #89] Loading the dataset.
[INFO][10:06:39]: [Client #497] Epoch: [1/5][0/10]	Loss: 0.024500
[INFO][10:06:40]: [Client #497] Epoch: [2/5][0/10]	Loss: 0.000590
[INFO][10:06:40]: [Client #257] Epoch: [1/5][0/10]	Loss: 0.008444
[INFO][10:06:40]: [Client #89] Epoch: [1/5][0/10]	Loss: 0.032977
[INFO][10:06:40]: [Client #497] Epoch: [3/5][0/10]	Loss: 0.000023
[INFO][10:06:40]: [Client #257] Epoch: [2/5][0/10]	Loss: 0.004070
[INFO][10:06:40]: [Client #89] Epoch: [2/5][0/10]	Loss: 0.000079
[INFO][10:06:40]: [Client #497] Epoch: [4/5][0/10]	Loss: 0.000423
[INFO][10:06:40]: [Client #89] Epoch: [3/5][0/10]	Loss: 0.028180
[INFO][10:06:40]: [Client #257] Epoch: [3/5][0/10]	Loss: 0.000214
[INFO][10:06:40]: [Client #497] Epoch: [5/5][0/10]	Loss: 0.000017
[INFO][10:06:40]: [Client #89] Epoch: [4/5][0/10]	Loss: 0.000057
[INFO][10:06:40]: [Client #257] Epoch: [4/5][0/10]	Loss: 0.015275
[INFO][10:06:40]: [Client #497] Model saved to /data/ykang/plato/results/test/model/lenet5_497_1127977.pth.
[INFO][10:06:40]: [Client #89] Epoch: [5/5][0/10]	Loss: 0.077880
[INFO][10:06:40]: [Client #257] Epoch: [5/5][0/10]	Loss: 0.341262
[INFO][10:06:40]: [Client #89] Model saved to /data/ykang/plato/results/test/model/lenet5_89_1127979.pth.
[INFO][10:06:40]: [Client #257] Model saved to /data/ykang/plato/results/test/model/lenet5_257_1127978.pth.
[INFO][10:06:41]: [Client #497] Loading a model from /data/ykang/plato/results/test/model/lenet5_497_1127977.pth.
[INFO][10:06:41]: [Client #497] Model trained.
[INFO][10:06:41]: [Client #497] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:06:41]: [Server #1127936] Received 0.24 MB of payload data from client #497 (simulated).
[INFO][10:06:41]: [Client #89] Loading a model from /data/ykang/plato/results/test/model/lenet5_89_1127979.pth.
[INFO][10:06:41]: [Client #89] Model trained.
[INFO][10:06:41]: [Client #89] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:06:41]: [Server #1127936] Received 0.24 MB of payload data from client #89 (simulated).
[INFO][10:06:41]: [Client #257] Loading a model from /data/ykang/plato/results/test/model/lenet5_257_1127978.pth.
[INFO][10:06:41]: [Client #257] Model trained.
[INFO][10:06:41]: [Client #257] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:06:41]: [Server #1127936] Received 0.24 MB of payload data from client #257 (simulated).
[INFO][10:06:41]: [Server #1127936] Selecting client #39 for training.
[INFO][10:06:41]: [Server #1127936] Sending the current model to client #39 (simulated).
[INFO][10:06:41]: [Server #1127936] Sending 0.24 MB of payload data to client #39 (simulated).
[INFO][10:06:41]: [Client #39] Selected by the server.
[INFO][10:06:41]: [Client #39] Loading its data source...
[INFO][10:06:41]: [Client #39] Dataset size: 60000
[INFO][10:06:41]: [Client #39] Sampler: noniid
[INFO][10:06:41]: [Client #39] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:06:41]: [93m[1m[Client #39] Started training in communication round #39.[0m
[INFO][10:06:43]: [Client #39] Loading the dataset.
[INFO][10:06:49]: [Client #39] Epoch: [1/5][0/10]	Loss: 0.020233
[INFO][10:06:49]: [Client #39] Epoch: [2/5][0/10]	Loss: 0.002186
[INFO][10:06:49]: [Client #39] Epoch: [3/5][0/10]	Loss: 0.000461
[INFO][10:06:49]: [Client #39] Epoch: [4/5][0/10]	Loss: 0.003732
[INFO][10:06:49]: [Client #39] Epoch: [5/5][0/10]	Loss: 0.045543
[INFO][10:06:49]: [Client #39] Model saved to /data/ykang/plato/results/test/model/lenet5_39_1127977.pth.
[INFO][10:06:49]: [Client #39] Loading a model from /data/ykang/plato/results/test/model/lenet5_39_1127977.pth.
[INFO][10:06:49]: [Client #39] Model trained.
[INFO][10:06:49]: [Client #39] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:06:49]: [Server #1127936] Received 0.24 MB of payload data from client #39 (simulated).
[INFO][10:06:49]: [Server #1127936] Adding client #450 to the list of clients for aggregation.
[INFO][10:06:49]: [Server #1127936] Adding client #154 to the list of clients for aggregation.
[INFO][10:06:49]: [Server #1127936] Adding client #436 to the list of clients for aggregation.
[INFO][10:06:49]: [Server #1127936] Adding client #95 to the list of clients for aggregation.
[INFO][10:06:49]: [Server #1127936] Adding client #37 to the list of clients for aggregation.
[INFO][10:06:49]: [Server #1127936] Adding client #456 to the list of clients for aggregation.
[INFO][10:06:49]: [Server #1127936] Adding client #343 to the list of clients for aggregation.
[INFO][10:06:49]: [Server #1127936] Adding client #39 to the list of clients for aggregation.
[INFO][10:06:49]: [Server #1127936] Adding client #102 to the list of clients for aggregation.
[INFO][10:06:49]: [Server #1127936] Adding client #257 to the list of clients for aggregation.
[INFO][10:06:49]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 451, 452, 453, 455, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.0058086  0.         0.01411679 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00617924 0.
 0.         0.         0.         0.         0.         0.00963853
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01244527 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.0156052  0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01204896 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01385184 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.02090795
 0.         0.         0.         0.         0.         0.00791404
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 3. 0. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 3. 1. 0. 0. 1. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 3. 0. 1. 0. 0. 0. 0. 1. 0.
 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 2. 0. 6. 0. 1. 2. 1. 0. 1. 1. 0. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.
 1. 0. 1. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 4. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 0. 1. 1. 0. 0. 6. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 2. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 2. 0. 1. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.0058086  0.         0.01411679 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00617924 0.
 0.         0.         0.         0.         0.         0.00963853
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01244527 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.0156052  0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01204896 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01385184 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.02090795
 0.         0.         0.         0.         0.         0.00791404
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:06:52]: [Server #1127936] Global model accuracy: 95.42%

[INFO][10:06:52]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_39.pth.
[INFO][10:06:52]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_39.pth.
[INFO][10:06:52]: [93m[1m
[Server #1127936] Starting round 40/100.[0m
[0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.002 0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1
 0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002 0.002 0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  2e-05  4e-10  4e-10
 6:  6.8876e+00  6.8875e+00  2e-05  3e-10  3e-10
 7:  6.8875e+00  6.8875e+00  2e-05  5e-10  1e-11
 8:  6.8875e+00  6.8875e+00  1e-05  4e-10  1e-11
 9:  6.8875e+00  6.8875e+00  8e-06  8e-10  2e-11
10:  6.8875e+00  6.8875e+00  5e-07  1e-09  3e-11
Optimal solution found.
The calculated probability is:  [5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 7.44281668e-05 5.11173275e-05 5.11118142e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 7.66451667e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11147572e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 1.51975341e-04 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11105905e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 1.43274891e-04
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 1.93002230e-04 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 9.74582005e-01 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 8.90144946e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05 5.11173275e-05 5.11173275e-05
 5.11173275e-05 5.11173275e-05]
current clients pool:  [INFO][10:06:52]: [Server #1127936] Selected clients: [450 391 247 212 133 455  12 411  95 344]
[INFO][10:06:52]: [Server #1127936] Selecting client #450 for training.
[INFO][10:06:52]: [Server #1127936] Sending the current model to client #450 (simulated).
[INFO][10:06:52]: [Server #1127936] Sending 0.24 MB of payload data to client #450 (simulated).
[INFO][10:06:52]: [Server #1127936] Selecting client #391 for training.
[INFO][10:06:52]: [Server #1127936] Sending the current model to client #391 (simulated).
[INFO][10:06:52]: [Server #1127936] Sending 0.24 MB of payload data to client #391 (simulated).
[INFO][10:06:52]: [Server #1127936] Selecting client #247 for training.
[INFO][10:06:52]: [Server #1127936] Sending the current model to client #247 (simulated).
[INFO][10:06:52]: [Client #450] Selected by the server.
[INFO][10:06:52]: [Client #450] Loading its data source...
[INFO][10:06:52]: [Client #450] Dataset size: 60000
[INFO][10:06:52]: [Client #450] Sampler: noniid
[INFO][10:06:52]: [Server #1127936] Sending 0.24 MB of payload data to client #247 (simulated).
[INFO][10:06:52]: [Client #391] Selected by the server.
[INFO][10:06:52]: [Client #391] Loading its data source...
[INFO][10:06:52]: [Client #391] Dataset size: 60000
[INFO][10:06:52]: [Client #391] Sampler: noniid
[INFO][10:06:52]: [Client #247] Selected by the server.
[INFO][10:06:52]: [Client #247] Loading its data source...
[INFO][10:06:52]: [Client #450] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:06:52]: [Client #247] Dataset size: 60000
[INFO][10:06:52]: [Client #247] Sampler: noniid
[INFO][10:06:52]: [Client #391] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:06:52]: [Client #247] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:06:52]: [93m[1m[Client #247] Started training in communication round #40.[0m
[INFO][10:06:52]: [93m[1m[Client #391] Started training in communication round #40.[0m
[INFO][10:06:52]: [93m[1m[Client #450] Started training in communication round #40.[0m
[INFO][10:06:55]: [Client #450] Loading the dataset.
[INFO][10:06:55]: [Client #247] Loading the dataset.
[INFO][10:06:55]: [Client #391] Loading the dataset.
[INFO][10:07:01]: [Client #450] Epoch: [1/5][0/10]	Loss: 0.010702
[INFO][10:07:01]: [Client #391] Epoch: [1/5][0/10]	Loss: 0.032939
[INFO][10:07:01]: [Client #450] Epoch: [2/5][0/10]	Loss: 0.003927
[INFO][10:07:01]: [Client #247] Epoch: [1/5][0/10]	Loss: 0.039984
[INFO][10:07:01]: [Client #391] Epoch: [2/5][0/10]	Loss: 0.112713
[INFO][10:07:01]: [Client #247] Epoch: [2/5][0/10]	Loss: 0.001708
[INFO][10:07:01]: [Client #450] Epoch: [3/5][0/10]	Loss: 0.035441
[INFO][10:07:01]: [Client #391] Epoch: [3/5][0/10]	Loss: 0.001492
[INFO][10:07:01]: [Client #247] Epoch: [3/5][0/10]	Loss: 0.000046
[INFO][10:07:01]: [Client #450] Epoch: [4/5][0/10]	Loss: 0.000658
[INFO][10:07:01]: [Client #391] Epoch: [4/5][0/10]	Loss: 0.011760
[INFO][10:07:01]: [Client #450] Epoch: [5/5][0/10]	Loss: 1.318599
[INFO][10:07:01]: [Client #247] Epoch: [4/5][0/10]	Loss: 0.000007
[INFO][10:07:01]: [Client #391] Epoch: [5/5][0/10]	Loss: 0.032211
[INFO][10:07:01]: [Client #450] Model saved to /data/ykang/plato/results/test/model/lenet5_450_1127977.pth.
[INFO][10:07:01]: [Client #247] Epoch: [5/5][0/10]	Loss: 0.039309
[INFO][10:07:01]: [Client #391] Model saved to /data/ykang/plato/results/test/model/lenet5_391_1127978.pth.
[INFO][10:07:01]: [Client #247] Model saved to /data/ykang/plato/results/test/model/lenet5_247_1127979.pth.
[INFO][10:07:02]: [Client #450] Loading a model from /data/ykang/plato/results/test/model/lenet5_450_1127977.pth.
[INFO][10:07:02]: [Client #450] Model trained.
[INFO][10:07:02]: [Client #450] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:07:02]: [Server #1127936] Received 0.24 MB of payload data from client #450 (simulated).
[INFO][10:07:02]: [Client #391] Loading a model from /data/ykang/plato/results/test/model/lenet5_391_1127978.pth.
[INFO][10:07:02]: [Client #391] Model trained.
[INFO][10:07:02]: [Client #391] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:07:02]: [Server #1127936] Received 0.24 MB of payload data from client #391 (simulated).
[INFO][10:07:02]: [Client #247] Loading a model from /data/ykang/plato/results/test/model/lenet5_247_1127979.pth.
[INFO][10:07:02]: [Client #247] Model trained.
[INFO][10:07:02]: [Client #247] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:07:02]: [Server #1127936] Received 0.24 MB of payload data from client #247 (simulated).
[INFO][10:07:02]: [Server #1127936] Selecting client #212 for training.
[INFO][10:07:02]: [Server #1127936] Sending the current model to client #212 (simulated).
[INFO][10:07:02]: [Server #1127936] Sending 0.24 MB of payload data to client #212 (simulated).
[INFO][10:07:02]: [Server #1127936] Selecting client #133 for training.
[INFO][10:07:02]: [Server #1127936] Sending the current model to client #133 (simulated).
[INFO][10:07:02]: [Server #1127936] Sending 0.24 MB of payload data to client #133 (simulated).
[INFO][10:07:02]: [Server #1127936] Selecting client #455 for training.
[INFO][10:07:02]: [Server #1127936] Sending the current model to client #455 (simulated).
[INFO][10:07:02]: [Client #212] Selected by the server.
[INFO][10:07:02]: [Client #212] Loading its data source...
[INFO][10:07:02]: [Client #212] Dataset size: 60000
[INFO][10:07:02]: [Client #212] Sampler: noniid
[INFO][10:07:02]: [Server #1127936] Sending 0.24 MB of payload data to client #455 (simulated).
[INFO][10:07:02]: [Client #133] Selected by the server.
[INFO][10:07:02]: [Client #133] Loading its data source...
[INFO][10:07:02]: [Client #455] Selected by the server.
[INFO][10:07:02]: [Client #133] Dataset size: 60000
[INFO][10:07:02]: [Client #133] Sampler: noniid
[INFO][10:07:02]: [Client #455] Loading its data source...
[INFO][10:07:02]: [Client #455] Dataset size: 60000
[INFO][10:07:02]: [Client #455] Sampler: noniid
[INFO][10:07:02]: [Client #212] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:07:02]: [Client #455] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:07:02]: [93m[1m[Client #455] Started training in communication round #40.[0m
[INFO][10:07:02]: [Client #133] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:07:02]: [93m[1m[Client #133] Started training in communication round #40.[0m
[INFO][10:07:02]: [93m[1m[Client #212] Started training in communication round #40.[0m
[INFO][10:07:04]: [Client #133] Loading the dataset.
[INFO][10:07:04]: [Client #455] Loading the dataset.
[INFO][10:07:04]: [Client #212] Loading the dataset.
[INFO][10:07:10]: [Client #212] Epoch: [1/5][0/10]	Loss: 0.133363
[INFO][10:07:10]: [Client #455] Epoch: [1/5][0/10]	Loss: 0.078716
[INFO][10:07:10]: [Client #133] Epoch: [1/5][0/10]	Loss: 0.025971
[INFO][10:07:10]: [Client #212] Epoch: [2/5][0/10]	Loss: 0.188167
[INFO][10:07:10]: [Client #455] Epoch: [2/5][0/10]	Loss: 0.000586
[INFO][10:07:10]: [Client #133] Epoch: [2/5][0/10]	Loss: 0.000026
[INFO][10:07:10]: [Client #212] Epoch: [3/5][0/10]	Loss: 0.005050
[INFO][10:07:10]: [Client #133] Epoch: [3/5][0/10]	Loss: 0.002300
[INFO][10:07:10]: [Client #212] Epoch: [4/5][0/10]	Loss: 0.005415
[INFO][10:07:10]: [Client #455] Epoch: [3/5][0/10]	Loss: 0.000002
[INFO][10:07:11]: [Client #133] Epoch: [4/5][0/10]	Loss: 0.008301
[INFO][10:07:11]: [Client #212] Epoch: [5/5][0/10]	Loss: 0.004869
[INFO][10:07:11]: [Client #455] Epoch: [4/5][0/10]	Loss: 0.000173
[INFO][10:07:11]: [Client #133] Epoch: [5/5][0/10]	Loss: 0.000045
[INFO][10:07:11]: [Client #212] Model saved to /data/ykang/plato/results/test/model/lenet5_212_1127977.pth.
[INFO][10:07:11]: [Client #133] Model saved to /data/ykang/plato/results/test/model/lenet5_133_1127978.pth.
[INFO][10:07:11]: [Client #455] Epoch: [5/5][0/10]	Loss: 0.003209
[INFO][10:07:11]: [Client #455] Model saved to /data/ykang/plato/results/test/model/lenet5_455_1127979.pth.
[INFO][10:07:11]: [Client #212] Loading a model from /data/ykang/plato/results/test/model/lenet5_212_1127977.pth.
[INFO][10:07:11]: [Client #212] Model trained.
[INFO][10:07:11]: [Client #212] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:07:11]: [Server #1127936] Received 0.24 MB of payload data from client #212 (simulated).
[INFO][10:07:12]: [Client #133] Loading a model from /data/ykang/plato/results/test/model/lenet5_133_1127978.pth.
[INFO][10:07:12]: [Client #133] Model trained.
[INFO][10:07:12]: [Client #133] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:07:12]: [Server #1127936] Received 0.24 MB of payload data from client #133 (simulated).
[INFO][10:07:12]: [Client #455] Loading a model from /data/ykang/plato/results/test/model/lenet5_455_1127979.pth.
[INFO][10:07:12]: [Client #455] Model trained.
[INFO][10:07:12]: [Client #455] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:07:12]: [Server #1127936] Received 0.24 MB of payload data from client #455 (simulated).
[INFO][10:07:12]: [Server #1127936] Selecting client #12 for training.
[INFO][10:07:12]: [Server #1127936] Sending the current model to client #12 (simulated).
[INFO][10:07:12]: [Server #1127936] Sending 0.24 MB of payload data to client #12 (simulated).
[INFO][10:07:12]: [Server #1127936] Selecting client #411 for training.
[INFO][10:07:12]: [Server #1127936] Sending the current model to client #411 (simulated).
[INFO][10:07:12]: [Server #1127936] Sending 0.24 MB of payload data to client #411 (simulated).
[INFO][10:07:12]: [Server #1127936] Selecting client #95 for training.
[INFO][10:07:12]: [Server #1127936] Sending the current model to client #95 (simulated).
[INFO][10:07:12]: [Client #12] Selected by the server.
[INFO][10:07:12]: [Client #12] Loading its data source...
[INFO][10:07:12]: [Client #12] Dataset size: 60000
[INFO][10:07:12]: [Client #12] Sampler: noniid
[INFO][10:07:12]: [Server #1127936] Sending 0.24 MB of payload data to client #95 (simulated).
[INFO][10:07:12]: [Client #411] Selected by the server.
[INFO][10:07:12]: [Client #95] Selected by the server.
[INFO][10:07:12]: [Client #411] Loading its data source...
[INFO][10:07:12]: [Client #95] Loading its data source...
[INFO][10:07:12]: [Client #411] Dataset size: 60000
[INFO][10:07:12]: [Client #95] Dataset size: 60000
[INFO][10:07:12]: [Client #411] Sampler: noniid
[INFO][10:07:12]: [Client #95] Sampler: noniid
[INFO][10:07:12]: [Client #12] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:07:12]: [Client #95] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:07:12]: [Client #411] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:07:12]: [93m[1m[Client #411] Started training in communication round #40.[0m
[INFO][10:07:12]: [93m[1m[Client #12] Started training in communication round #40.[0m
[INFO][10:07:12]: [93m[1m[Client #95] Started training in communication round #40.[0m
[INFO][10:07:14]: [Client #95] Loading the dataset.
[INFO][10:07:14]: [Client #12] Loading the dataset.
[INFO][10:07:14]: [Client #411] Loading the dataset.
[INFO][10:07:20]: [Client #95] Epoch: [1/5][0/10]	Loss: 0.009402
[INFO][10:07:20]: [Client #12] Epoch: [1/5][0/10]	Loss: 0.050110
[INFO][10:07:20]: [Client #411] Epoch: [1/5][0/10]	Loss: 0.008236
[INFO][10:07:20]: [Client #95] Epoch: [2/5][0/10]	Loss: 0.002072
[INFO][10:07:20]: [Client #12] Epoch: [2/5][0/10]	Loss: 0.003912
[INFO][10:07:20]: [Client #411] Epoch: [2/5][0/10]	Loss: 0.006243
[INFO][10:07:20]: [Client #95] Epoch: [3/5][0/10]	Loss: 0.002203
[INFO][10:07:20]: [Client #12] Epoch: [3/5][0/10]	Loss: 0.178340
[INFO][10:07:20]: [Client #411] Epoch: [3/5][0/10]	Loss: 0.000471
[INFO][10:07:20]: [Client #12] Epoch: [4/5][0/10]	Loss: 0.000615
[INFO][10:07:20]: [Client #95] Epoch: [4/5][0/10]	Loss: 0.002019
[INFO][10:07:20]: [Client #411] Epoch: [4/5][0/10]	Loss: 0.000166
[INFO][10:07:20]: [Client #12] Epoch: [5/5][0/10]	Loss: 1.342809
[INFO][10:07:20]: [Client #95] Epoch: [5/5][0/10]	Loss: 0.213688
[INFO][10:07:21]: [Client #12] Model saved to /data/ykang/plato/results/test/model/lenet5_12_1127977.pth.
[INFO][10:07:21]: [Client #411] Epoch: [5/5][0/10]	Loss: 0.000702
[INFO][10:07:21]: [Client #95] Model saved to /data/ykang/plato/results/test/model/lenet5_95_1127979.pth.
[INFO][10:07:21]: [Client #411] Model saved to /data/ykang/plato/results/test/model/lenet5_411_1127978.pth.
[INFO][10:07:21]: [Client #12] Loading a model from /data/ykang/plato/results/test/model/lenet5_12_1127977.pth.
[INFO][10:07:21]: [Client #12] Model trained.
[INFO][10:07:21]: [Client #12] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:07:21]: [Server #1127936] Received 0.24 MB of payload data from client #12 (simulated).
[INFO][10:07:21]: [Client #95] Loading a model from /data/ykang/plato/results/test/model/lenet5_95_1127979.pth.
[INFO][10:07:21]: [Client #95] Model trained.
[INFO][10:07:21]: [Client #95] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:07:21]: [Server #1127936] Received 0.24 MB of payload data from client #95 (simulated).
[INFO][10:07:21]: [Client #411] Loading a model from /data/ykang/plato/results/test/model/lenet5_411_1127978.pth.
[INFO][10:07:21]: [Client #411] Model trained.
[INFO][10:07:21]: [Client #411] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:07:21]: [Server #1127936] Received 0.24 MB of payload data from client #411 (simulated).
[INFO][10:07:21]: [Server #1127936] Selecting client #344 for training.
[INFO][10:07:21]: [Server #1127936] Sending the current model to client #344 (simulated).
[INFO][10:07:21]: [Server #1127936] Sending 0.24 MB of payload data to client #344 (simulated).
[INFO][10:07:21]: [Client #344] Selected by the server.
[INFO][10:07:21]: [Client #344] Loading its data source...
[INFO][10:07:21]: [Client #344] Dataset size: 60000
[INFO][10:07:21]: [Client #344] Sampler: noniid
[INFO][10:07:21]: [Client #344] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:07:21]: [93m[1m[Client #344] Started training in communication round #40.[0m
[INFO][10:07:23]: [Client #344] Loading the dataset.
[INFO][10:07:29]: [Client #344] Epoch: [1/5][0/10]	Loss: 0.017828
[INFO][10:07:29]: [Client #344] Epoch: [2/5][0/10]	Loss: 0.001081
[INFO][10:07:29]: [Client #344] Epoch: [3/5][0/10]	Loss: 0.000013
[INFO][10:07:29]: [Client #344] Epoch: [4/5][0/10]	Loss: 0.000003
[INFO][10:07:29]: [Client #344] Epoch: [5/5][0/10]	Loss: 0.000002
[INFO][10:07:29]: [Client #344] Model saved to /data/ykang/plato/results/test/model/lenet5_344_1127977.pth.
[INFO][10:07:30]: [Client #344] Loading a model from /data/ykang/plato/results/test/model/lenet5_344_1127977.pth.
[INFO][10:07:30]: [Client #344] Model trained.
[INFO][10:07:30]: [Client #344] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:07:30]: [Server #1127936] Received 0.24 MB of payload data from client #344 (simulated).
[INFO][10:07:30]: [Server #1127936] Adding client #173 to the list of clients for aggregation.
[INFO][10:07:30]: [Server #1127936] Adding client #89 to the list of clients for aggregation.
[INFO][10:07:30]: [Server #1127936] Adding client #140 to the list of clients for aggregation.
[INFO][10:07:30]: [Server #1127936] Adding client #219 to the list of clients for aggregation.
[INFO][10:07:30]: [Server #1127936] Adding client #195 to the list of clients for aggregation.
[INFO][10:07:30]: [Server #1127936] Adding client #247 to the list of clients for aggregation.
[INFO][10:07:30]: [Server #1127936] Adding client #411 to the list of clients for aggregation.
[INFO][10:07:30]: [Server #1127936] Adding client #450 to the list of clients for aggregation.
[INFO][10:07:30]: [Server #1127936] Adding client #391 to the list of clients for aggregation.
[INFO][10:07:30]: [Server #1127936] Adding client #95 to the list of clients for aggregation.
[INFO][10:07:30]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01142959 0.
 0.         0.         0.         0.         0.00678571 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01493966 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01059848 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01252463 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01250416 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00944258 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01588393 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00275018 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01146599
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 3. 0. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 3. 1. 0. 0. 1. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 2. 0. 6. 0. 1. 2. 1. 0. 1. 1. 0. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.
 1. 0. 1. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 4. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 0. 1. 1. 0. 0. 6. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 2. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 2. 0. 1. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01142959 0.
 0.         0.         0.         0.         0.00678571 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01493966 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01059848 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01252463 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01250416 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00944258 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01588393 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00275018 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01146599
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:07:32]: [Server #1127936] Global model accuracy: 95.53%

[INFO][10:07:32]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_40.pth.
[INFO][10:07:32]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_40.pth.
[INFO][10:07:32]: [93m[1m
[Server #1127936] Starting round 41/100.[0m
[INFO][10:07:33]: [Server #1127936] Selected clients: [140 463 346   2 102 443 432 343 227 408]
[INFO][10:07:33]: [Server #1127936] Selecting client #140 for training.
[INFO][10:07:33]: [Server #1127936] Sending the current model to client #140 (simulated).
[INFO][10:07:33]: [Server #1127936] Sending 0.24 MB of payload data to client #140 (simulated).
[INFO][10:07:33]: [Server #1127936] Selecting client #463 for training.
[INFO][10:07:33]: [Server #1127936] Sending the current model to client #463 (simulated).
[INFO][10:07:33]: [Server #1127936] Sending 0.24 MB of payload data to client #463 (simulated).
[INFO][10:07:33]: [Server #1127936] Selecting client #346 for training.
[INFO][10:07:33]: [Server #1127936] Sending the current model to client #346 (simulated).
[INFO][10:07:33]: [Client #140] Selected by the server.
[INFO][10:07:33]: [Client #140] Loading its data source...
[INFO][10:07:33]: [Client #140] Dataset size: 60000
[INFO][10:07:33]: [Client #140] Sampler: noniid
[INFO][10:07:33]: [Server #1127936] Sending 0.24 MB of payload data to client #346 (simulated).
[INFO][10:07:33]: [Client #463] Selected by the server.
[INFO][10:07:33]: [Client #463] Loading its data source...
[INFO][10:07:33]: [Client #463] Dataset size: 60000
[INFO][10:07:33]: [Client #346] Selected by the server.
[INFO][10:07:33]: [Client #463] Sampler: noniid
[INFO][10:07:33]: [Client #346] Loading its data source...
[INFO][10:07:33]: [Client #140] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:07:33]: [Client #346] Dataset size: 60000
[INFO][10:07:33]: [Client #346] Sampler: noniid
[INFO][10:07:33]: [Client #463] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:07:33]: [Client #346] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:07:33]: [93m[1m[Client #140] Started training in communication round #41.[0m
[INFO][10:07:33]: [93m[1m[Client #463] Started training in communication round #41.[0m
[INFO][10:07:33]: [93m[1m[Client #346] Started training in communication round #41.[0m
[INFO][10:07:35]: [Client #346] Loading the dataset.
[INFO][10:07:35]: [Client #140] Loading the dataset.
[INFO][10:07:35]: [Client #463] Loading the dataset.
[INFO][10:07:41]: [Client #140] Epoch: [1/5][0/10]	Loss: 0.005464
[INFO][10:07:41]: [Client #346] Epoch: [1/5][0/10]	Loss: 0.011349
[INFO][10:07:41]: [Client #463] Epoch: [1/5][0/10]	Loss: 0.009702
[INFO][10:07:41]: [Client #140] Epoch: [2/5][0/10]	Loss: 0.001427
[INFO][10:07:41]: [Client #463] Epoch: [2/5][0/10]	Loss: 0.039193
[INFO][10:07:41]: [Client #346] Epoch: [2/5][0/10]	Loss: 0.000274
[INFO][10:07:41]: [Client #140] Epoch: [3/5][0/10]	Loss: 0.001933
[INFO][10:07:41]: [Client #346] Epoch: [3/5][0/10]	Loss: 0.000042
[INFO][10:07:41]: [Client #463] Epoch: [3/5][0/10]	Loss: 0.002858
[INFO][10:07:42]: [Client #346] Epoch: [4/5][0/10]	Loss: 0.000035
[INFO][10:07:42]: [Client #140] Epoch: [4/5][0/10]	Loss: 0.000360
[INFO][10:07:42]: [Client #463] Epoch: [4/5][0/10]	Loss: 0.005101
[INFO][10:07:42]: [Client #140] Epoch: [5/5][0/10]	Loss: 0.000784
[INFO][10:07:42]: [Client #346] Epoch: [5/5][0/10]	Loss: 0.000065
[INFO][10:07:42]: [Client #463] Epoch: [5/5][0/10]	Loss: 0.004445
[INFO][10:07:42]: [Client #346] Model saved to /data/ykang/plato/results/test/model/lenet5_346_1127979.pth.
[INFO][10:07:42]: [Client #140] Model saved to /data/ykang/plato/results/test/model/lenet5_140_1127977.pth.
[INFO][10:07:42]: [Client #463] Model saved to /data/ykang/plato/results/test/model/lenet5_463_1127978.pth.
[INFO][10:07:42]: [Client #140] Loading a model from /data/ykang/plato/results/test/model/lenet5_140_1127977.pth.
[INFO][10:07:43]: [Client #140] Model trained.
[INFO][10:07:43]: [Client #140] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:07:43]: [Server #1127936] Received 0.24 MB of payload data from client #140 (simulated).
[INFO][10:07:43]: [Client #346] Loading a model from /data/ykang/plato/results/test/model/lenet5_346_1127979.pth.
[INFO][10:07:43]: [Client #346] Model trained.
[INFO][10:07:43]: [Client #346] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:07:43]: [Server #1127936] Received 0.24 MB of payload data from client #346 (simulated).
[INFO][10:07:43]: [Client #463] Loading a model from /data/ykang/plato/results/test/model/lenet5_463_1127978.pth.
[INFO][10:07:43]: [Client #463] Model trained.
[INFO][10:07:43]: [Client #463] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:07:43]: [Server #1127936] Received 0.24 MB of payload data from client #463 (simulated).
[INFO][10:07:43]: [Server #1127936] Selecting client #2 for training.
[INFO][10:07:43]: [Server #1127936] Sending the current model to client #2 (simulated).
[INFO][10:07:43]: [Server #1127936] Sending 0.24 MB of payload data to client #2 (simulated).
[INFO][10:07:43]: [Server #1127936] Selecting client #102 for training.
[INFO][10:07:43]: [Server #1127936] Sending the current model to client #102 (simulated).
[INFO][10:07:43]: [Server #1127936] Sending 0.24 MB of payload data to client #102 (simulated).
[INFO][10:07:43]: [Server #1127936] Selecting client #443 for training.
[INFO][10:07:43]: [Server #1127936] Sending the current model to client #443 (simulated).
[INFO][10:07:43]: [Client #2] Selected by the server.
[INFO][10:07:43]: [Client #2] Loading its data source...
[INFO][10:07:43]: [Client #2] Dataset size: 60000
[INFO][10:07:43]: [Client #2] Sampler: noniid
[INFO][10:07:43]: [Server #1127936] Sending 0.24 MB of payload data to client #443 (simulated).
[INFO][10:07:43]: [Client #102] Selected by the server.
[INFO][10:07:43]: [Client #102] Loading its data source...
[INFO][10:07:43]: [Client #102] Dataset size: 60000
[INFO][10:07:43]: [Client #102] Sampler: noniid
[INFO][10:07:43]: [Client #443] Selected by the server.
[INFO][10:07:43]: [Client #443] Loading its data source...
[INFO][10:07:43]: [Client #443] Dataset size: 60000
[INFO][10:07:43]: [Client #443] Sampler: noniid
[INFO][10:07:43]: [Client #2] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:07:43]: [Client #102] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:07:43]: [Client #443] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:07:43]: [93m[1m[Client #2] Started training in communication round #41.[0m
[INFO][10:07:43]: [93m[1m[Client #102] Started training in communication round #41.[0m
[INFO][10:07:43]: [93m[1m[Client #443] Started training in communication round #41.[0m
[INFO][10:07:45]: [Client #443] Loading the dataset.
[INFO][10:07:45]: [Client #2] Loading the dataset.
[INFO][10:07:45]: [Client #102] Loading the dataset.
[INFO][10:07:51]: [Client #443] Epoch: [1/5][0/10]	Loss: 0.001075
[INFO][10:07:51]: [Client #2] Epoch: [1/5][0/10]	Loss: 0.017044
[INFO][10:07:51]: [Client #102] Epoch: [1/5][0/10]	Loss: 0.085005
[INFO][10:07:51]: [Client #443] Epoch: [2/5][0/10]	Loss: 0.003191
[INFO][10:07:51]: [Client #2] Epoch: [2/5][0/10]	Loss: 0.000419
[INFO][10:07:51]: [Client #102] Epoch: [2/5][0/10]	Loss: 0.000000
[INFO][10:07:51]: [Client #443] Epoch: [3/5][0/10]	Loss: 0.015795
[INFO][10:07:51]: [Client #2] Epoch: [3/5][0/10]	Loss: 0.037612
[INFO][10:07:51]: [Client #2] Epoch: [4/5][0/10]	Loss: 0.000064
[INFO][10:07:51]: [Client #102] Epoch: [3/5][0/10]	Loss: 0.000025
[INFO][10:07:51]: [Client #443] Epoch: [4/5][0/10]	Loss: 0.035280
[INFO][10:07:51]: [Client #2] Epoch: [5/5][0/10]	Loss: 0.034669
[INFO][10:07:51]: [Client #443] Epoch: [5/5][0/10]	Loss: 0.014790
[INFO][10:07:51]: [Client #102] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:07:51]: [Client #2] Model saved to /data/ykang/plato/results/test/model/lenet5_2_1127977.pth.
[INFO][10:07:51]: [Client #443] Model saved to /data/ykang/plato/results/test/model/lenet5_443_1127979.pth.
[INFO][10:07:51]: [Client #102] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:07:51]: [Client #102] Model saved to /data/ykang/plato/results/test/model/lenet5_102_1127978.pth.
[INFO][10:07:52]: [Client #2] Loading a model from /data/ykang/plato/results/test/model/lenet5_2_1127977.pth.
[INFO][10:07:52]: [Client #2] Model trained.
[INFO][10:07:52]: [Client #2] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:07:52]: [Server #1127936] Received 0.24 MB of payload data from client #2 (simulated).
[INFO][10:07:52]: [Client #443] Loading a model from /data/ykang/plato/results/test/model/lenet5_443_1127979.pth.
[INFO][10:07:52]: [Client #443] Model trained.
[INFO][10:07:52]: [Client #443] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:07:52]: [Server #1127936] Received 0.24 MB of payload data from client #443 (simulated).
[INFO][10:07:52]: [Client #102] Loading a model from /data/ykang/plato/results/test/model/lenet5_102_1127978.pth.
[INFO][10:07:52]: [Client #102] Model trained.
[INFO][10:07:52]: [Client #102] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:07:52]: [Server #1127936] Received 0.24 MB of payload data from client #102 (simulated).
[INFO][10:07:52]: [Server #1127936] Selecting client #432 for training.
[INFO][10:07:52]: [Server #1127936] Sending the current model to client #432 (simulated).
[INFO][10:07:52]: [Server #1127936] Sending 0.24 MB of payload data to client #432 (simulated).
[INFO][10:07:52]: [Server #1127936] Selecting client #343 for training.
[INFO][10:07:52]: [Server #1127936] Sending the current model to client #343 (simulated).
[INFO][10:07:52]: [Server #1127936] Sending 0.24 MB of payload data to client #343 (simulated).
[INFO][10:07:52]: [Server #1127936] Selecting client #227 for training.
[INFO][10:07:52]: [Server #1127936] Sending the current model to client #227 (simulated).
[INFO][10:07:52]: [Client #432] Selected by the server.
[INFO][10:07:52]: [Client #432] Loading its data source...
[INFO][10:07:52]: [Client #432] Dataset size: 60000
[INFO][10:07:52]: [Client #432] Sampler: noniid
[INFO][10:07:52]: [Server #1127936] Sending 0.24 MB of payload data to client #227 (simulated).
[INFO][10:07:52]: [Client #343] Selected by the server.
[INFO][10:07:52]: [Client #343] Loading its data source...
[INFO][10:07:52]: [Client #343] Dataset size: 60000
[INFO][10:07:52]: [Client #343] Sampler: noniid
[INFO][10:07:52]: [Client #227] Selected by the server.
[INFO][10:07:52]: [Client #227] Loading its data source...
[INFO][10:07:52]: [Client #227] Dataset size: 60000
[INFO][10:07:52]: [Client #227] Sampler: noniid
[INFO][10:07:52]: [Client #432] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:07:52]: [93m[1m[Client #432] Started training in communication round #41.[0m
[INFO][10:07:52]: [Client #227] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:07:52]: [93m[1m[Client #227] Started training in communication round #41.[0m
[INFO][10:07:52]: [Client #343] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:07:52]: [93m[1m[Client #343] Started training in communication round #41.[0m
[INFO][10:07:54]: [Client #227] Loading the dataset.
[INFO][10:07:54]: [Client #432] Loading the dataset.
[INFO][10:07:54]: [Client #343] Loading the dataset.
[INFO][10:08:01]: [Client #343] Epoch: [1/5][0/10]	Loss: 0.048390
[INFO][10:08:01]: [Client #227] Epoch: [1/5][0/10]	Loss: 0.022116
[INFO][10:08:01]: [Client #432] Epoch: [1/5][0/10]	Loss: 0.016682
[INFO][10:08:01]: [Client #343] Epoch: [2/5][0/10]	Loss: 0.028501
[INFO][10:08:01]: [Client #432] Epoch: [2/5][0/10]	Loss: 0.000059
[INFO][10:08:01]: [Client #227] Epoch: [2/5][0/10]	Loss: 0.015243
[INFO][10:08:01]: [Client #343] Epoch: [3/5][0/10]	Loss: 0.007554
[INFO][10:08:01]: [Client #432] Epoch: [3/5][0/10]	Loss: 0.000193
[INFO][10:08:01]: [Client #227] Epoch: [3/5][0/10]	Loss: 0.000243
[INFO][10:08:01]: [Client #343] Epoch: [4/5][0/10]	Loss: 0.096575
[INFO][10:08:01]: [Client #432] Epoch: [4/5][0/10]	Loss: 0.000049
[INFO][10:08:01]: [Client #227] Epoch: [4/5][0/10]	Loss: 0.088683
[INFO][10:08:01]: [Client #432] Epoch: [5/5][0/10]	Loss: 0.000163
[INFO][10:08:01]: [Client #343] Epoch: [5/5][0/10]	Loss: 0.793331
[INFO][10:08:01]: [Client #227] Epoch: [5/5][0/10]	Loss: 0.001222
[INFO][10:08:01]: [Client #343] Model saved to /data/ykang/plato/results/test/model/lenet5_343_1127978.pth.
[INFO][10:08:01]: [Client #432] Model saved to /data/ykang/plato/results/test/model/lenet5_432_1127977.pth.
[INFO][10:08:01]: [Client #227] Model saved to /data/ykang/plato/results/test/model/lenet5_227_1127979.pth.
[INFO][10:08:02]: [Client #432] Loading a model from /data/ykang/plato/results/test/model/lenet5_432_1127977.pth.
[INFO][10:08:02]: [Client #432] Model trained.
[INFO][10:08:02]: [Client #432] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:08:02]: [Server #1127936] Received 0.24 MB of payload data from client #432 (simulated).
[INFO][10:08:02]: [Client #227] Loading a model from /data/ykang/plato/results/test/model/lenet5_227_1127979.pth.
[INFO][10:08:02]: [Client #227] Model trained.
[INFO][10:08:02]: [Client #227] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:08:02]: [Server #1127936] Received 0.24 MB of payload data from client #227 (simulated).
[INFO][10:08:02]: [Client #343] Loading a model from /data/ykang/plato/results/test/model/lenet5_343_1127978.pth.
[INFO][10:08:02]: [Client #343] Model trained.
[INFO][10:08:02]: [Client #343] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:08:02]: [Server #1127936] Received 0.24 MB of payload data from client #343 (simulated).
[INFO][10:08:02]: [Server #1127936] Selecting client #408 for training.
[INFO][10:08:02]: [Server #1127936] Sending the current model to client #408 (simulated).
[INFO][10:08:02]: [Server #1127936] Sending 0.24 MB of payload data to client #408 (simulated).
[INFO][10:08:02]: [Client #408] Selected by the server.
[INFO][10:08:02]: [Client #408] Loading its data source...
[INFO][10:08:02]: [Client #408] Dataset size: 60000
[INFO][10:08:02]: [Client #408] Sampler: noniid
[INFO][10:08:02]: [Client #408] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:08:02]: [93m[1m[Client #408] Started training in communication round #41.[0m
[INFO][10:08:04]: [Client #408] Loading the dataset.
[INFO][10:08:10]: [Client #408] Epoch: [1/5][0/10]	Loss: 0.001001
[INFO][10:08:10]: [Client #408] Epoch: [2/5][0/10]	Loss: 0.000554
[INFO][10:08:10]: [Client #408] Epoch: [3/5][0/10]	Loss: 0.000002
[INFO][10:08:10]: [Client #408] Epoch: [4/5][0/10]	Loss: 0.115129
[INFO][10:08:10]: [Client #408] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:08:10]: [Client #408] Model saved to /data/ykang/plato/results/test/model/lenet5_408_1127977.pth.
[INFO][10:08:11]: [Client #408] Loading a model from /data/ykang/plato/results/test/model/lenet5_408_1127977.pth.
[INFO][10:08:11]: [Client #408] Model trained.
[INFO][10:08:11]: [Client #408] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:08:11]: [Server #1127936] Received 0.24 MB of payload data from client #408 (simulated).
[INFO][10:08:11]: [Server #1127936] Adding client #133 to the list of clients for aggregation.
[INFO][10:08:11]: [Server #1127936] Adding client #12 to the list of clients for aggregation.
[INFO][10:08:11]: [Server #1127936] Adding client #496 to the list of clients for aggregation.
[INFO][10:08:11]: [Server #1127936] Adding client #455 to the list of clients for aggregation.
[INFO][10:08:11]: [Server #1127936] Adding client #497 to the list of clients for aggregation.
[INFO][10:08:11]: [Server #1127936] Adding client #344 to the list of clients for aggregation.
[INFO][10:08:11]: [Server #1127936] Adding client #463 to the list of clients for aggregation.
[INFO][10:08:11]: [Server #1127936] Adding client #408 to the list of clients for aggregation.
[INFO][10:08:11]: [Server #1127936] Adding client #432 to the list of clients for aggregation.
[INFO][10:08:11]: [Server #1127936] Adding client #102 to the list of clients for aggregation.
[INFO][10:08:11]: [Server #1127936] Aggregating 10 clients in total.
[0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1
 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.002 0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1
 0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002 0.002 0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  2e-05  3e-10  3e-10
 6:  6.8876e+00  6.8875e+00  2e-05  2e-10  2e-10
 7:  6.8875e+00  6.8875e+00  1e-05  3e-10  5e-12
 8:  6.8875e+00  6.8875e+00  9e-06  2e-10  4e-12
 9:  6.8875e+00  6.8875e+00  5e-06  4e-10  6e-12
Optimal solution found.
The calculated probability is:  [0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00333194 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.0007322  0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.62921128 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00268453 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00484111 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00480081 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073218 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.0007321  0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073222 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073216
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223 0.00073223
 0.00073223 0.00073223 0.00073223 0.00073223]
current clients pool:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00794571
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01448801
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00712093 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.0092884  0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01515553
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00776643
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01390644 0.
 0.         0.         0.         0.         0.         0.
 0.0079436  0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01068339 0.00654813 0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 3. 0. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 3. 1. 0. 0. 1. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 2. 0. 6. 0. 1. 2. 1. 0. 1. 1. 0. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.
 1. 0. 1. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 4. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 0. 1. 1. 0. 0. 6. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 2. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 2. 0. 1. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 2. 2. 0. 1. 0.]
local_gradient_bounds:  [INFO][10:08:13]: [Server #1127936] Global model accuracy: 95.00%

[INFO][10:08:13]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_41.pth.
[INFO][10:08:13]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_41.pth.
[INFO][10:08:13]: [93m[1m
[Server #1127936] Starting round 42/100.[0m
[0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00794571
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01448801
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00712093 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.0092884  0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01515553
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00776643
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01390644 0.
 0.         0.         0.         0.         0.         0.
 0.0079436  0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01068339 0.00654813 0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1
 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.002 0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1
 0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  2e-05  4e-10  4e-10
 6:  6.8876e+00  6.8875e+00  2e-05  3e-10  3e-10
 7:  6.8875e+00  6.8875e+00  2e-05  5e-10  1e-11
 8:  6.8875e+00  6.8875e+00  1e-05  4e-10  1e-11
 9:  6.8875e+00  6.8875e+00  8e-06  8e-10  2e-11
10:  6.8875e+00  6.8875e+00  5e-07  1e-09  3e-11
Optimal solution found.
The calculated probability is:  [5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 8.92691901e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18680892e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 8.31028614e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 1.01470315e-04 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18675449e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18722047e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 1.87709454e-04 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18721281e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 5.18738644e-05 5.18738644e-05 5.18738644e-05
 5.18738644e-05 9.74267646e-01 1.63867017e-04 5.18738644e-05
 5.18738644e-05 5.18738644e-05]
current clients pool:  [INFO][10:08:14]: [Server #1127936] Selected clients: [496 362  96  60 421 375 298 123 314 322]
[INFO][10:08:14]: [Server #1127936] Selecting client #496 for training.
[INFO][10:08:14]: [Server #1127936] Sending the current model to client #496 (simulated).
[INFO][10:08:14]: [Server #1127936] Sending 0.24 MB of payload data to client #496 (simulated).
[INFO][10:08:14]: [Server #1127936] Selecting client #362 for training.
[INFO][10:08:14]: [Server #1127936] Sending the current model to client #362 (simulated).
[INFO][10:08:14]: [Server #1127936] Sending 0.24 MB of payload data to client #362 (simulated).
[INFO][10:08:14]: [Server #1127936] Selecting client #96 for training.
[INFO][10:08:14]: [Server #1127936] Sending the current model to client #96 (simulated).
[INFO][10:08:14]: [Client #496] Selected by the server.
[INFO][10:08:14]: [Client #496] Loading its data source...
[INFO][10:08:14]: [Client #496] Dataset size: 60000
[INFO][10:08:14]: [Client #496] Sampler: noniid
[INFO][10:08:14]: [Server #1127936] Sending 0.24 MB of payload data to client #96 (simulated).
[INFO][10:08:14]: [Client #362] Selected by the server.
[INFO][10:08:14]: [Client #362] Loading its data source...
[INFO][10:08:14]: [Client #362] Dataset size: 60000
[INFO][10:08:14]: [Client #362] Sampler: noniid
[INFO][10:08:14]: [Client #96] Selected by the server.
[INFO][10:08:14]: [Client #496] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:08:14]: [Client #96] Loading its data source...
[INFO][10:08:14]: [Client #96] Dataset size: 60000
[INFO][10:08:14]: [Client #96] Sampler: noniid
[INFO][10:08:14]: [Client #362] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:08:14]: [Client #96] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:08:14]: [93m[1m[Client #362] Started training in communication round #42.[0m
[INFO][10:08:14]: [93m[1m[Client #96] Started training in communication round #42.[0m
[INFO][10:08:14]: [93m[1m[Client #496] Started training in communication round #42.[0m
[INFO][10:08:16]: [Client #496] Loading the dataset.
[INFO][10:08:16]: [Client #362] Loading the dataset.
[INFO][10:08:16]: [Client #96] Loading the dataset.
[INFO][10:08:22]: [Client #362] Epoch: [1/5][0/10]	Loss: 0.063993
[INFO][10:08:22]: [Client #96] Epoch: [1/5][0/10]	Loss: 0.012084
[INFO][10:08:22]: [Client #362] Epoch: [2/5][0/10]	Loss: 0.002014
[INFO][10:08:22]: [Client #496] Epoch: [1/5][0/10]	Loss: 0.010218
[INFO][10:08:22]: [Client #96] Epoch: [2/5][0/10]	Loss: 0.001672
[INFO][10:08:22]: [Client #362] Epoch: [3/5][0/10]	Loss: 0.000021
[INFO][10:08:22]: [Client #496] Epoch: [2/5][0/10]	Loss: 0.003312
[INFO][10:08:22]: [Client #362] Epoch: [4/5][0/10]	Loss: 0.004795
[INFO][10:08:22]: [Client #96] Epoch: [3/5][0/10]	Loss: 0.000182
[INFO][10:08:22]: [Client #496] Epoch: [3/5][0/10]	Loss: 0.002686
[INFO][10:08:22]: [Client #96] Epoch: [4/5][0/10]	Loss: 0.002674
[INFO][10:08:22]: [Client #362] Epoch: [5/5][0/10]	Loss: 0.022746
[INFO][10:08:22]: [Client #96] Epoch: [5/5][0/10]	Loss: 0.000108
[INFO][10:08:22]: [Client #496] Epoch: [4/5][0/10]	Loss: 0.000235
[INFO][10:08:22]: [Client #362] Model saved to /data/ykang/plato/results/test/model/lenet5_362_1127978.pth.
[INFO][10:08:22]: [Client #96] Model saved to /data/ykang/plato/results/test/model/lenet5_96_1127979.pth.
[INFO][10:08:22]: [Client #496] Epoch: [5/5][0/10]	Loss: 0.003244
[INFO][10:08:22]: [Client #496] Model saved to /data/ykang/plato/results/test/model/lenet5_496_1127977.pth.
[INFO][10:08:23]: [Client #362] Loading a model from /data/ykang/plato/results/test/model/lenet5_362_1127978.pth.
[INFO][10:08:23]: [Client #362] Model trained.
[INFO][10:08:23]: [Client #362] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:08:23]: [Server #1127936] Received 0.24 MB of payload data from client #362 (simulated).
[INFO][10:08:23]: [Client #96] Loading a model from /data/ykang/plato/results/test/model/lenet5_96_1127979.pth.
[INFO][10:08:23]: [Client #96] Model trained.
[INFO][10:08:23]: [Client #96] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:08:23]: [Server #1127936] Received 0.24 MB of payload data from client #96 (simulated).
[INFO][10:08:23]: [Client #496] Loading a model from /data/ykang/plato/results/test/model/lenet5_496_1127977.pth.
[INFO][10:08:23]: [Client #496] Model trained.
[INFO][10:08:23]: [Client #496] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:08:23]: [Server #1127936] Received 0.24 MB of payload data from client #496 (simulated).
[INFO][10:08:23]: [Server #1127936] Selecting client #60 for training.
[INFO][10:08:23]: [Server #1127936] Sending the current model to client #60 (simulated).
[INFO][10:08:23]: [Server #1127936] Sending 0.24 MB of payload data to client #60 (simulated).
[INFO][10:08:23]: [Server #1127936] Selecting client #421 for training.
[INFO][10:08:23]: [Server #1127936] Sending the current model to client #421 (simulated).
[INFO][10:08:23]: [Server #1127936] Sending 0.24 MB of payload data to client #421 (simulated).
[INFO][10:08:23]: [Server #1127936] Selecting client #375 for training.
[INFO][10:08:23]: [Server #1127936] Sending the current model to client #375 (simulated).
[INFO][10:08:23]: [Client #60] Selected by the server.
[INFO][10:08:23]: [Client #60] Loading its data source...
[INFO][10:08:23]: [Client #60] Dataset size: 60000
[INFO][10:08:23]: [Client #60] Sampler: noniid
[INFO][10:08:23]: [Server #1127936] Sending 0.24 MB of payload data to client #375 (simulated).
[INFO][10:08:23]: [Client #421] Selected by the server.
[INFO][10:08:23]: [Client #421] Loading its data source...
[INFO][10:08:23]: [Client #421] Dataset size: 60000
[INFO][10:08:23]: [Client #375] Selected by the server.
[INFO][10:08:23]: [Client #421] Sampler: noniid
[INFO][10:08:23]: [Client #375] Loading its data source...
[INFO][10:08:23]: [Client #375] Dataset size: 60000
[INFO][10:08:23]: [Client #375] Sampler: noniid
[INFO][10:08:23]: [Client #60] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:08:23]: [Client #421] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:08:23]: [Client #375] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:08:23]: [93m[1m[Client #60] Started training in communication round #42.[0m
[INFO][10:08:23]: [93m[1m[Client #421] Started training in communication round #42.[0m
[INFO][10:08:23]: [93m[1m[Client #375] Started training in communication round #42.[0m
[INFO][10:08:26]: [Client #375] Loading the dataset.
[INFO][10:08:26]: [Client #421] Loading the dataset.
[INFO][10:08:26]: [Client #60] Loading the dataset.
[INFO][10:08:32]: [Client #375] Epoch: [1/5][0/10]	Loss: 0.053749
[INFO][10:08:32]: [Client #421] Epoch: [1/5][0/10]	Loss: 0.028748
[INFO][10:08:32]: [Client #375] Epoch: [2/5][0/10]	Loss: 0.002614
[INFO][10:08:32]: [Client #60] Epoch: [1/5][0/10]	Loss: 0.016022
[INFO][10:08:32]: [Client #421] Epoch: [2/5][0/10]	Loss: 0.002774
[INFO][10:08:32]: [Client #60] Epoch: [2/5][0/10]	Loss: 0.000046
[INFO][10:08:32]: [Client #421] Epoch: [3/5][0/10]	Loss: 0.000003
[INFO][10:08:32]: [Client #375] Epoch: [3/5][0/10]	Loss: 0.000180
[INFO][10:08:32]: [Client #60] Epoch: [3/5][0/10]	Loss: 0.000410
[INFO][10:08:32]: [Client #421] Epoch: [4/5][0/10]	Loss: 0.000030
[INFO][10:08:32]: [Client #375] Epoch: [4/5][0/10]	Loss: 0.000342
[INFO][10:08:32]: [Client #421] Epoch: [5/5][0/10]	Loss: 0.000214
[INFO][10:08:32]: [Client #60] Epoch: [4/5][0/10]	Loss: 0.000017
[INFO][10:08:32]: [Client #421] Model saved to /data/ykang/plato/results/test/model/lenet5_421_1127978.pth.
[INFO][10:08:32]: [Client #375] Epoch: [5/5][0/10]	Loss: 0.089013
[INFO][10:08:32]: [Client #60] Epoch: [5/5][0/10]	Loss: 0.014208
[INFO][10:08:32]: [Client #375] Model saved to /data/ykang/plato/results/test/model/lenet5_375_1127979.pth.
[INFO][10:08:32]: [Client #60] Model saved to /data/ykang/plato/results/test/model/lenet5_60_1127977.pth.
[INFO][10:08:33]: [Client #421] Loading a model from /data/ykang/plato/results/test/model/lenet5_421_1127978.pth.
[INFO][10:08:33]: [Client #421] Model trained.
[INFO][10:08:33]: [Client #421] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:08:33]: [Server #1127936] Received 0.24 MB of payload data from client #421 (simulated).
[INFO][10:08:33]: [Client #375] Loading a model from /data/ykang/plato/results/test/model/lenet5_375_1127979.pth.
[INFO][10:08:33]: [Client #375] Model trained.
[INFO][10:08:33]: [Client #375] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:08:33]: [Server #1127936] Received 0.24 MB of payload data from client #375 (simulated).
[INFO][10:08:33]: [Client #60] Loading a model from /data/ykang/plato/results/test/model/lenet5_60_1127977.pth.
[INFO][10:08:33]: [Client #60] Model trained.
[INFO][10:08:33]: [Client #60] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:08:33]: [Server #1127936] Received 0.24 MB of payload data from client #60 (simulated).
[INFO][10:08:33]: [Server #1127936] Selecting client #298 for training.
[INFO][10:08:33]: [Server #1127936] Sending the current model to client #298 (simulated).
[INFO][10:08:33]: [Server #1127936] Sending 0.24 MB of payload data to client #298 (simulated).
[INFO][10:08:33]: [Server #1127936] Selecting client #123 for training.
[INFO][10:08:33]: [Server #1127936] Sending the current model to client #123 (simulated).
[INFO][10:08:33]: [Server #1127936] Sending 0.24 MB of payload data to client #123 (simulated).
[INFO][10:08:33]: [Server #1127936] Selecting client #314 for training.
[INFO][10:08:33]: [Server #1127936] Sending the current model to client #314 (simulated).
[INFO][10:08:33]: [Client #298] Selected by the server.
[INFO][10:08:33]: [Client #298] Loading its data source...
[INFO][10:08:33]: [Client #298] Dataset size: 60000
[INFO][10:08:33]: [Client #298] Sampler: noniid
[INFO][10:08:33]: [Server #1127936] Sending 0.24 MB of payload data to client #314 (simulated).
[INFO][10:08:33]: [Client #123] Selected by the server.
[INFO][10:08:33]: [Client #123] Loading its data source...
[INFO][10:08:33]: [Client #123] Dataset size: 60000
[INFO][10:08:33]: [Client #123] Sampler: noniid
[INFO][10:08:33]: [Client #314] Selected by the server.
[INFO][10:08:33]: [Client #314] Loading its data source...
[INFO][10:08:33]: [Client #314] Dataset size: 60000
[INFO][10:08:33]: [Client #314] Sampler: noniid
[INFO][10:08:33]: [Client #123] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:08:33]: [Client #298] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:08:33]: [93m[1m[Client #123] Started training in communication round #42.[0m
[INFO][10:08:33]: [Client #314] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:08:33]: [93m[1m[Client #298] Started training in communication round #42.[0m
[INFO][10:08:33]: [93m[1m[Client #314] Started training in communication round #42.[0m
[INFO][10:08:35]: [Client #298] Loading the dataset.
[INFO][10:08:35]: [Client #123] Loading the dataset.
[INFO][10:08:35]: [Client #314] Loading the dataset.
[INFO][10:08:41]: [Client #314] Epoch: [1/5][0/10]	Loss: 0.002296
[INFO][10:08:41]: [Client #298] Epoch: [1/5][0/10]	Loss: 0.203457
[INFO][10:08:41]: [Client #123] Epoch: [1/5][0/10]	Loss: 0.072302
[INFO][10:08:41]: [Client #298] Epoch: [2/5][0/10]	Loss: 0.007339
[INFO][10:08:41]: [Client #314] Epoch: [2/5][0/10]	Loss: 0.007726
[INFO][10:08:41]: [Client #123] Epoch: [2/5][0/10]	Loss: 0.000264
[INFO][10:08:42]: [Client #298] Epoch: [3/5][0/10]	Loss: 0.000328
[INFO][10:08:42]: [Client #123] Epoch: [3/5][0/10]	Loss: 0.000004
[INFO][10:08:42]: [Client #314] Epoch: [3/5][0/10]	Loss: 0.000505
[INFO][10:08:42]: [Client #298] Epoch: [4/5][0/10]	Loss: 0.349583
[INFO][10:08:42]: [Client #314] Epoch: [4/5][0/10]	Loss: 0.000217
[INFO][10:08:42]: [Client #123] Epoch: [4/5][0/10]	Loss: 0.000033
[INFO][10:08:42]: [Client #298] Epoch: [5/5][0/10]	Loss: 0.400965
[INFO][10:08:42]: [Client #298] Model saved to /data/ykang/plato/results/test/model/lenet5_298_1127977.pth.
[INFO][10:08:42]: [Client #314] Epoch: [5/5][0/10]	Loss: 0.000077
[INFO][10:08:42]: [Client #123] Epoch: [5/5][0/10]	Loss: 0.000008
[INFO][10:08:42]: [Client #314] Model saved to /data/ykang/plato/results/test/model/lenet5_314_1127979.pth.
[INFO][10:08:42]: [Client #123] Model saved to /data/ykang/plato/results/test/model/lenet5_123_1127978.pth.
[INFO][10:08:43]: [Client #298] Loading a model from /data/ykang/plato/results/test/model/lenet5_298_1127977.pth.
[INFO][10:08:43]: [Client #298] Model trained.
[INFO][10:08:43]: [Client #298] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:08:43]: [Server #1127936] Received 0.24 MB of payload data from client #298 (simulated).
[INFO][10:08:43]: [Client #314] Loading a model from /data/ykang/plato/results/test/model/lenet5_314_1127979.pth.
[INFO][10:08:43]: [Client #314] Model trained.
[INFO][10:08:43]: [Client #314] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:08:43]: [Server #1127936] Received 0.24 MB of payload data from client #314 (simulated).
[INFO][10:08:43]: [Client #123] Loading a model from /data/ykang/plato/results/test/model/lenet5_123_1127978.pth.
[INFO][10:08:43]: [Client #123] Model trained.
[INFO][10:08:43]: [Client #123] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:08:43]: [Server #1127936] Received 0.24 MB of payload data from client #123 (simulated).
[INFO][10:08:43]: [Server #1127936] Selecting client #322 for training.
[INFO][10:08:43]: [Server #1127936] Sending the current model to client #322 (simulated).
[INFO][10:08:43]: [Server #1127936] Sending 0.24 MB of payload data to client #322 (simulated).
[INFO][10:08:43]: [Client #322] Selected by the server.
[INFO][10:08:43]: [Client #322] Loading its data source...
[INFO][10:08:43]: [Client #322] Dataset size: 60000
[INFO][10:08:43]: [Client #322] Sampler: noniid
[INFO][10:08:43]: [Client #322] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:08:43]: [93m[1m[Client #322] Started training in communication round #42.[0m
[INFO][10:08:45]: [Client #322] Loading the dataset.
[INFO][10:08:51]: [Client #322] Epoch: [1/5][0/10]	Loss: 0.030244
[INFO][10:08:51]: [Client #322] Epoch: [2/5][0/10]	Loss: 0.000101
[INFO][10:08:51]: [Client #322] Epoch: [3/5][0/10]	Loss: 0.014609
[INFO][10:08:51]: [Client #322] Epoch: [4/5][0/10]	Loss: 0.014053
[INFO][10:08:51]: [Client #322] Epoch: [5/5][0/10]	Loss: 0.000229
[INFO][10:08:51]: [Client #322] Model saved to /data/ykang/plato/results/test/model/lenet5_322_1127977.pth.
[INFO][10:08:51]: [Client #322] Loading a model from /data/ykang/plato/results/test/model/lenet5_322_1127977.pth.
[INFO][10:08:51]: [Client #322] Model trained.
[INFO][10:08:51]: [Client #322] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:08:51]: [Server #1127936] Received 0.24 MB of payload data from client #322 (simulated).
[INFO][10:08:51]: [Server #1127936] Adding client #443 to the list of clients for aggregation.
[INFO][10:08:51]: [Server #1127936] Adding client #227 to the list of clients for aggregation.
[INFO][10:08:51]: [Server #1127936] Adding client #346 to the list of clients for aggregation.
[INFO][10:08:51]: [Server #1127936] Adding client #2 to the list of clients for aggregation.
[INFO][10:08:51]: [Server #1127936] Adding client #140 to the list of clients for aggregation.
[INFO][10:08:51]: [Server #1127936] Adding client #343 to the list of clients for aggregation.
[INFO][10:08:51]: [Server #1127936] Adding client #41 to the list of clients for aggregation.
[INFO][10:08:51]: [Server #1127936] Adding client #454 to the list of clients for aggregation.
[INFO][10:08:51]: [Server #1127936] Adding client #123 to the list of clients for aggregation.
[INFO][10:08:51]: [Server #1127936] Adding client #375 to the list of clients for aggregation.
[INFO][10:08:51]: [Server #1127936] Aggregating 10 clients in total.
[1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 344, 345, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.01142663 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.0102487  0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01505488 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00603253 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.0168884  0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01638272 0.         0.         0.00843907 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01298935 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01045236 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01061591 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 3. 0. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 3. 1. 0. 0. 1. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 2. 0. 6. 0. 1. 2. 1. 0. 1. 1. 0. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.
 1. 0. 1. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 4. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 0. 1. 1. 0. 0. 6. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 0. 0. 0. 0. 1. 0. 0. 6. 1. 1.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 2. 0. 1. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 2. 2. 0. 1. 0.]
local_gradient_bounds:  [0.         0.01142663 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.0102487  0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01505488 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00603253 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.0168884  0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01638272 0.         0.         0.00843907 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01298935 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01045236 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01061591 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:08:54]: [Server #1127936] Global model accuracy: 95.24%

[INFO][10:08:54]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_42.pth.
[INFO][10:08:54]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_42.pth.
[INFO][10:08:54]: [93m[1m
[Server #1127936] Starting round 43/100.[0m
[0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1
 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  4e-04  7e-09  7e-09
 5:  6.8876e+00  6.8875e+00  7e-05  1e-09  1e-09
 6:  6.8875e+00  6.8875e+00  6e-05  5e-10  5e-10
 7:  6.8875e+00  6.8875e+00  6e-05  1e-09  1e-10
 8:  6.8875e+00  6.8875e+00  5e-05  2e-09  1e-10
 9:  6.8875e+00  6.8875e+00  2e-05  1e-08  8e-10
10:  6.8875e+00  6.8875e+00  7e-06  2e-09  9e-11
11:  6.8875e+00  6.8875e+00  1e-06  7e-10  4e-11
Optimal solution found.
The calculated probability is:  [3.70203931e-05 4.59727803e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 2.18598756e-01 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70189628e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 4.12631806e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 5.19772546e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 5.13563582e-05
 3.70203931e-05 3.70203931e-05 4.32396146e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70193283e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 4.50443329e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 7.63278563e-01 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05 3.70203931e-05 3.70203931e-05
 3.70203931e-05 3.70203931e-05]
current clients pool:  [INFO][10:08:54]: [Server #1127936] Selected clients: [454  41 124  39 152 371 307 337   4 340]
[INFO][10:08:54]: [Server #1127936] Selecting client #454 for training.
[INFO][10:08:54]: [Server #1127936] Sending the current model to client #454 (simulated).
[INFO][10:08:54]: [Server #1127936] Sending 0.24 MB of payload data to client #454 (simulated).
[INFO][10:08:54]: [Server #1127936] Selecting client #41 for training.
[INFO][10:08:54]: [Server #1127936] Sending the current model to client #41 (simulated).
[INFO][10:08:54]: [Server #1127936] Sending 0.24 MB of payload data to client #41 (simulated).
[INFO][10:08:54]: [Server #1127936] Selecting client #124 for training.
[INFO][10:08:54]: [Server #1127936] Sending the current model to client #124 (simulated).
[INFO][10:08:54]: [Client #454] Selected by the server.
[INFO][10:08:54]: [Client #454] Loading its data source...
[INFO][10:08:54]: [Client #454] Dataset size: 60000
[INFO][10:08:54]: [Client #454] Sampler: noniid
[INFO][10:08:54]: [Server #1127936] Sending 0.24 MB of payload data to client #124 (simulated).
[INFO][10:08:54]: [Client #41] Selected by the server.
[INFO][10:08:54]: [Client #41] Loading its data source...
[INFO][10:08:54]: [Client #41] Dataset size: 60000
[INFO][10:08:54]: [Client #41] Sampler: noniid
[INFO][10:08:54]: [Client #124] Selected by the server.
[INFO][10:08:54]: [Client #454] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:08:54]: [Client #124] Loading its data source...
[INFO][10:08:54]: [Client #124] Dataset size: 60000
[INFO][10:08:54]: [Client #124] Sampler: noniid
[INFO][10:08:54]: [Client #41] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:08:54]: [Client #124] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:08:54]: [93m[1m[Client #454] Started training in communication round #43.[0m
[INFO][10:08:54]: [93m[1m[Client #124] Started training in communication round #43.[0m
[INFO][10:08:54]: [93m[1m[Client #41] Started training in communication round #43.[0m
[INFO][10:08:57]: [Client #41] Loading the dataset.
[INFO][10:08:57]: [Client #124] Loading the dataset.
[INFO][10:08:57]: [Client #454] Loading the dataset.
[INFO][10:09:03]: [Client #41] Epoch: [1/5][0/10]	Loss: 0.082264
[INFO][10:09:03]: [Client #124] Epoch: [1/5][0/10]	Loss: 0.041768
[INFO][10:09:03]: [Client #454] Epoch: [1/5][0/10]	Loss: 0.003375
[INFO][10:09:03]: [Client #41] Epoch: [2/5][0/10]	Loss: 0.001899
[INFO][10:09:03]: [Client #124] Epoch: [2/5][0/10]	Loss: 0.004269
[INFO][10:09:03]: [Client #454] Epoch: [2/5][0/10]	Loss: 0.003980
[INFO][10:09:03]: [Client #124] Epoch: [3/5][0/10]	Loss: 0.000323
[INFO][10:09:03]: [Client #41] Epoch: [3/5][0/10]	Loss: 0.005748
[INFO][10:09:03]: [Client #454] Epoch: [3/5][0/10]	Loss: 0.000914
[INFO][10:09:03]: [Client #124] Epoch: [4/5][0/10]	Loss: 0.091077
[INFO][10:09:03]: [Client #454] Epoch: [4/5][0/10]	Loss: 0.003894
[INFO][10:09:03]: [Client #41] Epoch: [4/5][0/10]	Loss: 0.158267
[INFO][10:09:03]: [Client #124] Epoch: [5/5][0/10]	Loss: 0.007162
[INFO][10:09:03]: [Client #124] Model saved to /data/ykang/plato/results/test/model/lenet5_124_1127979.pth.
[INFO][10:09:03]: [Client #454] Epoch: [5/5][0/10]	Loss: 0.002497
[INFO][10:09:03]: [Client #41] Epoch: [5/5][0/10]	Loss: 0.016872
[INFO][10:09:03]: [Client #454] Model saved to /data/ykang/plato/results/test/model/lenet5_454_1127977.pth.
[INFO][10:09:03]: [Client #41] Model saved to /data/ykang/plato/results/test/model/lenet5_41_1127978.pth.
[INFO][10:09:04]: [Client #124] Loading a model from /data/ykang/plato/results/test/model/lenet5_124_1127979.pth.
[INFO][10:09:04]: [Client #124] Model trained.
[INFO][10:09:04]: [Client #124] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:09:04]: [Server #1127936] Received 0.24 MB of payload data from client #124 (simulated).
[INFO][10:09:04]: [Client #454] Loading a model from /data/ykang/plato/results/test/model/lenet5_454_1127977.pth.
[INFO][10:09:04]: [Client #454] Model trained.
[INFO][10:09:04]: [Client #454] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:09:04]: [Server #1127936] Received 0.24 MB of payload data from client #454 (simulated).
[INFO][10:09:04]: [Client #41] Loading a model from /data/ykang/plato/results/test/model/lenet5_41_1127978.pth.
[INFO][10:09:04]: [Client #41] Model trained.
[INFO][10:09:04]: [Client #41] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:09:04]: [Server #1127936] Received 0.24 MB of payload data from client #41 (simulated).
[INFO][10:09:04]: [Server #1127936] Selecting client #39 for training.
[INFO][10:09:04]: [Server #1127936] Sending the current model to client #39 (simulated).
[INFO][10:09:04]: [Server #1127936] Sending 0.24 MB of payload data to client #39 (simulated).
[INFO][10:09:04]: [Server #1127936] Selecting client #152 for training.
[INFO][10:09:04]: [Server #1127936] Sending the current model to client #152 (simulated).
[INFO][10:09:04]: [Server #1127936] Sending 0.24 MB of payload data to client #152 (simulated).
[INFO][10:09:04]: [Server #1127936] Selecting client #371 for training.
[INFO][10:09:04]: [Server #1127936] Sending the current model to client #371 (simulated).
[INFO][10:09:04]: [Client #39] Selected by the server.
[INFO][10:09:04]: [Client #39] Loading its data source...
[INFO][10:09:04]: [Client #39] Dataset size: 60000
[INFO][10:09:04]: [Client #39] Sampler: noniid
[INFO][10:09:04]: [Server #1127936] Sending 0.24 MB of payload data to client #371 (simulated).
[INFO][10:09:04]: [Client #371] Selected by the server.
[INFO][10:09:04]: [Client #371] Loading its data source...
[INFO][10:09:04]: [Client #371] Dataset size: 60000
[INFO][10:09:04]: [Client #371] Sampler: noniid
[INFO][10:09:04]: [Client #39] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:09:04]: [Client #152] Selected by the server.
[INFO][10:09:04]: [Client #152] Loading its data source...
[INFO][10:09:04]: [Client #152] Dataset size: 60000
[INFO][10:09:04]: [Client #152] Sampler: noniid
[INFO][10:09:04]: [Client #371] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:09:04]: [Client #152] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:09:04]: [93m[1m[Client #39] Started training in communication round #43.[0m
[INFO][10:09:04]: [93m[1m[Client #371] Started training in communication round #43.[0m
[INFO][10:09:04]: [93m[1m[Client #152] Started training in communication round #43.[0m
[INFO][10:09:06]: [Client #39] Loading the dataset.
[INFO][10:09:06]: [Client #371] Loading the dataset.
[INFO][10:09:06]: [Client #152] Loading the dataset.
[INFO][10:09:12]: [Client #39] Epoch: [1/5][0/10]	Loss: 0.022750
[INFO][10:09:12]: [Client #39] Epoch: [2/5][0/10]	Loss: 0.007146
[INFO][10:09:12]: [Client #371] Epoch: [1/5][0/10]	Loss: 0.042044
[INFO][10:09:12]: [Client #39] Epoch: [3/5][0/10]	Loss: 0.000847
[INFO][10:09:12]: [Client #371] Epoch: [2/5][0/10]	Loss: 0.002376
[INFO][10:09:13]: [Client #39] Epoch: [4/5][0/10]	Loss: 0.000238
[INFO][10:09:13]: [Client #152] Epoch: [1/5][0/10]	Loss: 0.051764
[INFO][10:09:13]: [Client #371] Epoch: [3/5][0/10]	Loss: 0.003286
[INFO][10:09:13]: [Client #39] Epoch: [5/5][0/10]	Loss: 0.385019
[INFO][10:09:13]: [Client #152] Epoch: [2/5][0/10]	Loss: 0.001266
[INFO][10:09:13]: [Client #371] Epoch: [4/5][0/10]	Loss: 0.045508
[INFO][10:09:13]: [Client #39] Model saved to /data/ykang/plato/results/test/model/lenet5_39_1127977.pth.
[INFO][10:09:13]: [Client #371] Epoch: [5/5][0/10]	Loss: 0.007692
[INFO][10:09:13]: [Client #152] Epoch: [3/5][0/10]	Loss: 0.001596
[INFO][10:09:13]: [Client #371] Model saved to /data/ykang/plato/results/test/model/lenet5_371_1127979.pth.
[INFO][10:09:13]: [Client #152] Epoch: [4/5][0/10]	Loss: 0.000144
[INFO][10:09:13]: [Client #152] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:09:13]: [Client #152] Model saved to /data/ykang/plato/results/test/model/lenet5_152_1127978.pth.
[INFO][10:09:13]: [Client #39] Loading a model from /data/ykang/plato/results/test/model/lenet5_39_1127977.pth.
[INFO][10:09:13]: [Client #39] Model trained.
[INFO][10:09:13]: [Client #39] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:09:13]: [Server #1127936] Received 0.24 MB of payload data from client #39 (simulated).
[INFO][10:09:14]: [Client #371] Loading a model from /data/ykang/plato/results/test/model/lenet5_371_1127979.pth.
[INFO][10:09:14]: [Client #371] Model trained.
[INFO][10:09:14]: [Client #371] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:09:14]: [Server #1127936] Received 0.24 MB of payload data from client #371 (simulated).
[INFO][10:09:14]: [Client #152] Loading a model from /data/ykang/plato/results/test/model/lenet5_152_1127978.pth.
[INFO][10:09:14]: [Client #152] Model trained.
[INFO][10:09:14]: [Client #152] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:09:14]: [Server #1127936] Received 0.24 MB of payload data from client #152 (simulated).
[INFO][10:09:14]: [Server #1127936] Selecting client #307 for training.
[INFO][10:09:14]: [Server #1127936] Sending the current model to client #307 (simulated).
[INFO][10:09:14]: [Server #1127936] Sending 0.24 MB of payload data to client #307 (simulated).
[INFO][10:09:14]: [Server #1127936] Selecting client #337 for training.
[INFO][10:09:14]: [Server #1127936] Sending the current model to client #337 (simulated).
[INFO][10:09:14]: [Server #1127936] Sending 0.24 MB of payload data to client #337 (simulated).
[INFO][10:09:14]: [Server #1127936] Selecting client #4 for training.
[INFO][10:09:14]: [Server #1127936] Sending the current model to client #4 (simulated).
[INFO][10:09:14]: [Client #307] Selected by the server.
[INFO][10:09:14]: [Client #307] Loading its data source...
[INFO][10:09:14]: [Client #307] Dataset size: 60000
[INFO][10:09:14]: [Client #307] Sampler: noniid
[INFO][10:09:14]: [Server #1127936] Sending 0.24 MB of payload data to client #4 (simulated).
[INFO][10:09:14]: [Client #337] Selected by the server.
[INFO][10:09:14]: [Client #4] Selected by the server.
[INFO][10:09:14]: [Client #337] Loading its data source...
[INFO][10:09:14]: [Client #4] Loading its data source...
[INFO][10:09:14]: [Client #337] Dataset size: 60000
[INFO][10:09:14]: [Client #4] Dataset size: 60000
[INFO][10:09:14]: [Client #4] Sampler: noniid
[INFO][10:09:14]: [Client #337] Sampler: noniid
[INFO][10:09:14]: [Client #307] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:09:14]: [Client #4] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:09:14]: [93m[1m[Client #307] Started training in communication round #43.[0m
[INFO][10:09:14]: [Client #337] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:09:14]: [93m[1m[Client #4] Started training in communication round #43.[0m
[INFO][10:09:14]: [93m[1m[Client #337] Started training in communication round #43.[0m
[INFO][10:09:16]: [Client #4] Loading the dataset.
[INFO][10:09:16]: [Client #337] Loading the dataset.
[INFO][10:09:16]: [Client #307] Loading the dataset.
[INFO][10:09:22]: [Client #4] Epoch: [1/5][0/10]	Loss: 0.012727
[INFO][10:09:22]: [Client #307] Epoch: [1/5][0/10]	Loss: 0.042044
[INFO][10:09:22]: [Client #337] Epoch: [1/5][0/10]	Loss: 0.004696
[INFO][10:09:22]: [Client #4] Epoch: [2/5][0/10]	Loss: 0.001561
[INFO][10:09:22]: [Client #307] Epoch: [2/5][0/10]	Loss: 0.000790
[INFO][10:09:22]: [Client #337] Epoch: [2/5][0/10]	Loss: 0.000911
[INFO][10:09:22]: [Client #4] Epoch: [3/5][0/10]	Loss: 0.000188
[INFO][10:09:22]: [Client #337] Epoch: [3/5][0/10]	Loss: 0.001061
[INFO][10:09:22]: [Client #307] Epoch: [3/5][0/10]	Loss: 0.009843
[INFO][10:09:22]: [Client #4] Epoch: [4/5][0/10]	Loss: 0.000001
[INFO][10:09:22]: [Client #337] Epoch: [4/5][0/10]	Loss: 0.000014
[INFO][10:09:22]: [Client #337] Epoch: [5/5][0/10]	Loss: 0.001392
[INFO][10:09:22]: [Client #307] Epoch: [4/5][0/10]	Loss: 0.048768
[INFO][10:09:22]: [Client #4] Epoch: [5/5][0/10]	Loss: 0.000005
[INFO][10:09:22]: [Client #337] Model saved to /data/ykang/plato/results/test/model/lenet5_337_1127978.pth.
[INFO][10:09:22]: [Client #4] Model saved to /data/ykang/plato/results/test/model/lenet5_4_1127979.pth.
[INFO][10:09:22]: [Client #307] Epoch: [5/5][0/10]	Loss: 0.043590
[INFO][10:09:22]: [Client #307] Model saved to /data/ykang/plato/results/test/model/lenet5_307_1127977.pth.
[INFO][10:09:23]: [Client #337] Loading a model from /data/ykang/plato/results/test/model/lenet5_337_1127978.pth.
[INFO][10:09:23]: [Client #337] Model trained.
[INFO][10:09:23]: [Client #337] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:09:23]: [Server #1127936] Received 0.24 MB of payload data from client #337 (simulated).
[INFO][10:09:23]: [Client #4] Loading a model from /data/ykang/plato/results/test/model/lenet5_4_1127979.pth.
[INFO][10:09:23]: [Client #4] Model trained.
[INFO][10:09:23]: [Client #4] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:09:23]: [Server #1127936] Received 0.24 MB of payload data from client #4 (simulated).
[INFO][10:09:23]: [Client #307] Loading a model from /data/ykang/plato/results/test/model/lenet5_307_1127977.pth.
[INFO][10:09:23]: [Client #307] Model trained.
[INFO][10:09:23]: [Client #307] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:09:23]: [Server #1127936] Received 0.24 MB of payload data from client #307 (simulated).
[INFO][10:09:23]: [Server #1127936] Selecting client #340 for training.
[INFO][10:09:23]: [Server #1127936] Sending the current model to client #340 (simulated).
[INFO][10:09:23]: [Server #1127936] Sending 0.24 MB of payload data to client #340 (simulated).
[INFO][10:09:23]: [Client #340] Selected by the server.
[INFO][10:09:23]: [Client #340] Loading its data source...
[INFO][10:09:23]: [Client #340] Dataset size: 60000
[INFO][10:09:23]: [Client #340] Sampler: noniid
[INFO][10:09:23]: [Client #340] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:09:23]: [93m[1m[Client #340] Started training in communication round #43.[0m
[INFO][10:09:25]: [Client #340] Loading the dataset.
[INFO][10:09:30]: [Client #340] Epoch: [1/5][0/10]	Loss: 0.024201
[INFO][10:09:30]: [Client #340] Epoch: [2/5][0/10]	Loss: 0.001683
[INFO][10:09:30]: [Client #340] Epoch: [3/5][0/10]	Loss: 0.001510
[INFO][10:09:30]: [Client #340] Epoch: [4/5][0/10]	Loss: 0.042237
[INFO][10:09:30]: [Client #340] Epoch: [5/5][0/10]	Loss: 0.000034
[INFO][10:09:30]: [Client #340] Model saved to /data/ykang/plato/results/test/model/lenet5_340_1127977.pth.
[INFO][10:09:31]: [Client #340] Loading a model from /data/ykang/plato/results/test/model/lenet5_340_1127977.pth.
[INFO][10:09:31]: [Client #340] Model trained.
[INFO][10:09:31]: [Client #340] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:09:31]: [Server #1127936] Received 0.24 MB of payload data from client #340 (simulated).
[INFO][10:09:31]: [Server #1127936] Adding client #298 to the list of clients for aggregation.
[INFO][10:09:31]: [Server #1127936] Adding client #314 to the list of clients for aggregation.
[INFO][10:09:31]: [Server #1127936] Adding client #362 to the list of clients for aggregation.
[INFO][10:09:31]: [Server #1127936] Adding client #421 to the list of clients for aggregation.
[INFO][10:09:31]: [Server #1127936] Adding client #96 to the list of clients for aggregation.
[INFO][10:09:31]: [Server #1127936] Adding client #322 to the list of clients for aggregation.
[INFO][10:09:31]: [Server #1127936] Adding client #60 to the list of clients for aggregation.
[INFO][10:09:31]: [Server #1127936] Adding client #212 to the list of clients for aggregation.
[INFO][10:09:31]: [Server #1127936] Adding client #39 to the list of clients for aggregation.
[INFO][10:09:31]: [Server #1127936] Adding client #124 to the list of clients for aggregation.
[INFO][10:09:31]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00914531 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00858758
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01179969
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00611918 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.0307115  0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01465849 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00417446 0.         0.         0.         0.
 0.         0.         0.         0.01645594 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01695887 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00649742 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 3. 0. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 3. 1. 0. 0. 1. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 0. 0. 0. 0. 1.
 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 2. 0. 6. 0. 1. 2. 1. 0. 1. 1. 0. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.
 1. 0. 1. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 4. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 1. 0. 0. 6. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 0. 0. 0. 0. 1. 0. 0. 6. 1. 1.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 2. 0. 1. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 2. 2. 0. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00914531 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00858758
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01179969
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00611918 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.0307115  0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01465849 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00417446 0.         0.         0.         0.
 0.         0.         0.         0.01645594 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01695887 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00649742 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:09:33]: [Server #1127936] Global model accuracy: 94.80%

[INFO][10:09:33]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_43.pth.
[INFO][10:09:33]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_43.pth.
[INFO][10:09:33]: [93m[1m
[Server #1127936] Starting round 44/100.[0m
[0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1
 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.002
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8871e+00  4e-04  7e-09  7e-09
 5:  6.8876e+00  6.8875e+00  1e-04  2e-09  2e-09
 6:  6.8875e+00  6.8875e+00  9e-05  6e-10  4e-10
 7:  6.8875e+00  6.8875e+00  8e-05  4e-09  4e-10
 8:  6.8875e+00  6.8875e+00  6e-05  6e-09  6e-10
 9:  6.8875e+00  6.8875e+00  2e-05  3e-08  3e-09
10:  6.8875e+00  6.8875e+00  5e-06  1e-08  1e-09
Optimal solution found.
The calculated probability is:  [1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09406590e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.23453034e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.29658545e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407261e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 9.46360239e-01
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.35718247e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.15820371e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.39820060e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.41011466e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.19718719e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04 1.09407805e-04 1.09407805e-04
 1.09407805e-04 1.09407805e-04]
current clients pool:  [INFO][10:09:34]: [Server #1127936] Selected clients: [212 338 409 242 294 395  52 268 183 301]
[INFO][10:09:34]: [Server #1127936] Selecting client #212 for training.
[INFO][10:09:34]: [Server #1127936] Sending the current model to client #212 (simulated).
[INFO][10:09:34]: [Server #1127936] Sending 0.24 MB of payload data to client #212 (simulated).
[INFO][10:09:34]: [Server #1127936] Selecting client #338 for training.
[INFO][10:09:34]: [Server #1127936] Sending the current model to client #338 (simulated).
[INFO][10:09:34]: [Server #1127936] Sending 0.24 MB of payload data to client #338 (simulated).
[INFO][10:09:34]: [Server #1127936] Selecting client #409 for training.
[INFO][10:09:34]: [Server #1127936] Sending the current model to client #409 (simulated).
[INFO][10:09:34]: [Client #212] Selected by the server.
[INFO][10:09:34]: [Client #212] Loading its data source...
[INFO][10:09:34]: [Client #212] Dataset size: 60000
[INFO][10:09:34]: [Client #212] Sampler: noniid
[INFO][10:09:34]: [Server #1127936] Sending 0.24 MB of payload data to client #409 (simulated).
[INFO][10:09:34]: [Client #338] Selected by the server.
[INFO][10:09:34]: [Client #338] Loading its data source...
[INFO][10:09:34]: [Client #338] Dataset size: 60000
[INFO][10:09:34]: [Client #338] Sampler: noniid
[INFO][10:09:34]: [Client #409] Selected by the server.
[INFO][10:09:34]: [Client #212] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:09:34]: [Client #409] Loading its data source...
[INFO][10:09:34]: [Client #409] Dataset size: 60000
[INFO][10:09:34]: [Client #409] Sampler: noniid
[INFO][10:09:34]: [Client #338] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:09:34]: [Client #409] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:09:34]: [93m[1m[Client #212] Started training in communication round #44.[0m
[INFO][10:09:34]: [93m[1m[Client #338] Started training in communication round #44.[0m
[INFO][10:09:34]: [93m[1m[Client #409] Started training in communication round #44.[0m
[INFO][10:09:36]: [Client #338] Loading the dataset.
[INFO][10:09:36]: [Client #212] Loading the dataset.
[INFO][10:09:36]: [Client #409] Loading the dataset.
[INFO][10:09:42]: [Client #212] Epoch: [1/5][0/10]	Loss: 0.022025
[INFO][10:09:42]: [Client #338] Epoch: [1/5][0/10]	Loss: 0.030925
[INFO][10:09:42]: [Client #409] Epoch: [1/5][0/10]	Loss: 0.045314
[INFO][10:09:42]: [Client #212] Epoch: [2/5][0/10]	Loss: 0.001153
[INFO][10:09:42]: [Client #338] Epoch: [2/5][0/10]	Loss: 0.193459
[INFO][10:09:42]: [Client #409] Epoch: [2/5][0/10]	Loss: 0.059964
[INFO][10:09:42]: [Client #212] Epoch: [3/5][0/10]	Loss: 0.001613
[INFO][10:09:42]: [Client #338] Epoch: [3/5][0/10]	Loss: 0.011248
[INFO][10:09:42]: [Client #409] Epoch: [3/5][0/10]	Loss: 0.001149
[INFO][10:09:42]: [Client #212] Epoch: [4/5][0/10]	Loss: 0.002532
[INFO][10:09:42]: [Client #338] Epoch: [4/5][0/10]	Loss: 0.002004
[INFO][10:09:42]: [Client #409] Epoch: [4/5][0/10]	Loss: 0.006831
[INFO][10:09:42]: [Client #212] Epoch: [5/5][0/10]	Loss: 0.003332
[INFO][10:09:42]: [Client #212] Model saved to /data/ykang/plato/results/test/model/lenet5_212_1127977.pth.
[INFO][10:09:42]: [Client #409] Epoch: [5/5][0/10]	Loss: 0.040669
[INFO][10:09:42]: [Client #338] Epoch: [5/5][0/10]	Loss: 0.027928
[INFO][10:09:42]: [Client #409] Model saved to /data/ykang/plato/results/test/model/lenet5_409_1127979.pth.
[INFO][10:09:42]: [Client #338] Model saved to /data/ykang/plato/results/test/model/lenet5_338_1127978.pth.
[INFO][10:09:43]: [Client #212] Loading a model from /data/ykang/plato/results/test/model/lenet5_212_1127977.pth.
[INFO][10:09:43]: [Client #212] Model trained.
[INFO][10:09:43]: [Client #212] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:09:43]: [Server #1127936] Received 0.24 MB of payload data from client #212 (simulated).
[INFO][10:09:43]: [Client #338] Loading a model from /data/ykang/plato/results/test/model/lenet5_338_1127978.pth.
[INFO][10:09:43]: [Client #338] Model trained.
[INFO][10:09:43]: [Client #338] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:09:43]: [Server #1127936] Received 0.24 MB of payload data from client #338 (simulated).
[INFO][10:09:43]: [Client #409] Loading a model from /data/ykang/plato/results/test/model/lenet5_409_1127979.pth.
[INFO][10:09:43]: [Client #409] Model trained.
[INFO][10:09:43]: [Client #409] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:09:43]: [Server #1127936] Received 0.24 MB of payload data from client #409 (simulated).
[INFO][10:09:43]: [Server #1127936] Selecting client #242 for training.
[INFO][10:09:43]: [Server #1127936] Sending the current model to client #242 (simulated).
[INFO][10:09:43]: [Server #1127936] Sending 0.24 MB of payload data to client #242 (simulated).
[INFO][10:09:43]: [Server #1127936] Selecting client #294 for training.
[INFO][10:09:43]: [Server #1127936] Sending the current model to client #294 (simulated).
[INFO][10:09:43]: [Server #1127936] Sending 0.24 MB of payload data to client #294 (simulated).
[INFO][10:09:43]: [Server #1127936] Selecting client #395 for training.
[INFO][10:09:43]: [Server #1127936] Sending the current model to client #395 (simulated).
[INFO][10:09:43]: [Client #242] Selected by the server.
[INFO][10:09:43]: [Client #242] Loading its data source...
[INFO][10:09:43]: [Client #242] Dataset size: 60000
[INFO][10:09:43]: [Client #242] Sampler: noniid
[INFO][10:09:43]: [Server #1127936] Sending 0.24 MB of payload data to client #395 (simulated).
[INFO][10:09:43]: [Client #294] Selected by the server.
[INFO][10:09:43]: [Client #294] Loading its data source...
[INFO][10:09:43]: [Client #395] Selected by the server.
[INFO][10:09:43]: [Client #294] Dataset size: 60000
[INFO][10:09:43]: [Client #395] Loading its data source...
[INFO][10:09:43]: [Client #294] Sampler: noniid
[INFO][10:09:43]: [Client #395] Dataset size: 60000
[INFO][10:09:43]: [Client #395] Sampler: noniid
[INFO][10:09:43]: [Client #294] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:09:43]: [Client #395] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:09:43]: [Client #242] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:09:43]: [93m[1m[Client #294] Started training in communication round #44.[0m
[INFO][10:09:43]: [93m[1m[Client #242] Started training in communication round #44.[0m
[INFO][10:09:43]: [93m[1m[Client #395] Started training in communication round #44.[0m
[INFO][10:09:45]: [Client #294] Loading the dataset.
[INFO][10:09:45]: [Client #395] Loading the dataset.
[INFO][10:09:45]: [Client #242] Loading the dataset.
[INFO][10:09:51]: [Client #395] Epoch: [1/5][0/10]	Loss: 0.021736
[INFO][10:09:51]: [Client #294] Epoch: [1/5][0/10]	Loss: 0.003037
[INFO][10:09:51]: [Client #242] Epoch: [1/5][0/10]	Loss: 0.012745
[INFO][10:09:51]: [Client #395] Epoch: [2/5][0/10]	Loss: 0.000712
[INFO][10:09:51]: [Client #395] Epoch: [3/5][0/10]	Loss: 0.119750
[INFO][10:09:51]: [Client #294] Epoch: [2/5][0/10]	Loss: 0.002057
[INFO][10:09:51]: [Client #242] Epoch: [2/5][0/10]	Loss: 0.001343
[INFO][10:09:51]: [Client #395] Epoch: [4/5][0/10]	Loss: 0.001608
[INFO][10:09:51]: [Client #294] Epoch: [3/5][0/10]	Loss: 0.000036
[INFO][10:09:51]: [Client #242] Epoch: [3/5][0/10]	Loss: 0.000020
[INFO][10:09:51]: [Client #395] Epoch: [5/5][0/10]	Loss: 0.005013
[INFO][10:09:51]: [Client #242] Epoch: [4/5][0/10]	Loss: 0.000038
[INFO][10:09:51]: [Client #294] Epoch: [4/5][0/10]	Loss: 0.000037
[INFO][10:09:51]: [Client #395] Model saved to /data/ykang/plato/results/test/model/lenet5_395_1127979.pth.
[INFO][10:09:51]: [Client #242] Epoch: [5/5][0/10]	Loss: 0.000645
[INFO][10:09:51]: [Client #242] Model saved to /data/ykang/plato/results/test/model/lenet5_242_1127977.pth.
[INFO][10:09:52]: [Client #294] Epoch: [5/5][0/10]	Loss: 0.001612
[INFO][10:09:52]: [Client #294] Model saved to /data/ykang/plato/results/test/model/lenet5_294_1127978.pth.
[INFO][10:09:52]: [Client #395] Loading a model from /data/ykang/plato/results/test/model/lenet5_395_1127979.pth.
[INFO][10:09:52]: [Client #395] Model trained.
[INFO][10:09:52]: [Client #395] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:09:52]: [Server #1127936] Received 0.24 MB of payload data from client #395 (simulated).
[INFO][10:09:52]: [Client #242] Loading a model from /data/ykang/plato/results/test/model/lenet5_242_1127977.pth.
[INFO][10:09:52]: [Client #242] Model trained.
[INFO][10:09:52]: [Client #242] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:09:52]: [Server #1127936] Received 0.24 MB of payload data from client #242 (simulated).
[INFO][10:09:52]: [Client #294] Loading a model from /data/ykang/plato/results/test/model/lenet5_294_1127978.pth.
[INFO][10:09:52]: [Client #294] Model trained.
[INFO][10:09:52]: [Client #294] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:09:52]: [Server #1127936] Received 0.24 MB of payload data from client #294 (simulated).
[INFO][10:09:52]: [Server #1127936] Selecting client #52 for training.
[INFO][10:09:52]: [Server #1127936] Sending the current model to client #52 (simulated).
[INFO][10:09:52]: [Server #1127936] Sending 0.24 MB of payload data to client #52 (simulated).
[INFO][10:09:52]: [Server #1127936] Selecting client #268 for training.
[INFO][10:09:52]: [Server #1127936] Sending the current model to client #268 (simulated).
[INFO][10:09:52]: [Server #1127936] Sending 0.24 MB of payload data to client #268 (simulated).
[INFO][10:09:52]: [Server #1127936] Selecting client #183 for training.
[INFO][10:09:52]: [Server #1127936] Sending the current model to client #183 (simulated).
[INFO][10:09:52]: [Client #52] Selected by the server.
[INFO][10:09:52]: [Client #52] Loading its data source...
[INFO][10:09:52]: [Client #52] Dataset size: 60000
[INFO][10:09:52]: [Client #52] Sampler: noniid
[INFO][10:09:52]: [Server #1127936] Sending 0.24 MB of payload data to client #183 (simulated).
[INFO][10:09:52]: [Client #268] Selected by the server.
[INFO][10:09:52]: [Client #268] Loading its data source...
[INFO][10:09:52]: [Client #268] Dataset size: 60000
[INFO][10:09:52]: [Client #268] Sampler: noniid
[INFO][10:09:52]: [Client #183] Selected by the server.
[INFO][10:09:52]: [Client #183] Loading its data source...
[INFO][10:09:52]: [Client #183] Dataset size: 60000
[INFO][10:09:52]: [Client #183] Sampler: noniid
[INFO][10:09:52]: [Client #52] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:09:52]: [Client #268] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:09:52]: [Client #183] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:09:52]: [93m[1m[Client #52] Started training in communication round #44.[0m
[INFO][10:09:52]: [93m[1m[Client #183] Started training in communication round #44.[0m
[INFO][10:09:52]: [93m[1m[Client #268] Started training in communication round #44.[0m
[INFO][10:09:54]: [Client #183] Loading the dataset.
[INFO][10:09:54]: [Client #268] Loading the dataset.
[INFO][10:09:54]: [Client #52] Loading the dataset.
[INFO][10:10:00]: [Client #268] Epoch: [1/5][0/10]	Loss: 0.001283
[INFO][10:10:00]: [Client #52] Epoch: [1/5][0/10]	Loss: 0.077542
[INFO][10:10:00]: [Client #183] Epoch: [1/5][0/10]	Loss: 0.064287
[INFO][10:10:00]: [Client #268] Epoch: [2/5][0/10]	Loss: 0.000447
[INFO][10:10:00]: [Client #52] Epoch: [2/5][0/10]	Loss: 0.000327
[INFO][10:10:01]: [Client #183] Epoch: [2/5][0/10]	Loss: 0.000755
[INFO][10:10:01]: [Client #268] Epoch: [3/5][0/10]	Loss: 0.000870
[INFO][10:10:01]: [Client #183] Epoch: [3/5][0/10]	Loss: 0.002180
[INFO][10:10:01]: [Client #52] Epoch: [3/5][0/10]	Loss: 0.000294
[INFO][10:10:01]: [Client #268] Epoch: [4/5][0/10]	Loss: 0.000033
[INFO][10:10:01]: [Client #52] Epoch: [4/5][0/10]	Loss: 0.022301
[INFO][10:10:01]: [Client #183] Epoch: [4/5][0/10]	Loss: 0.077190
[INFO][10:10:01]: [Client #268] Epoch: [5/5][0/10]	Loss: 0.000018
[INFO][10:10:01]: [Client #52] Epoch: [5/5][0/10]	Loss: 0.000019
[INFO][10:10:01]: [Client #183] Epoch: [5/5][0/10]	Loss: 0.143789
[INFO][10:10:01]: [Client #268] Model saved to /data/ykang/plato/results/test/model/lenet5_268_1127978.pth.
[INFO][10:10:01]: [Client #52] Model saved to /data/ykang/plato/results/test/model/lenet5_52_1127977.pth.
[INFO][10:10:01]: [Client #183] Model saved to /data/ykang/plato/results/test/model/lenet5_183_1127979.pth.
[INFO][10:10:02]: [Client #52] Loading a model from /data/ykang/plato/results/test/model/lenet5_52_1127977.pth.
[INFO][10:10:02]: [Client #52] Model trained.
[INFO][10:10:02]: [Client #52] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:10:02]: [Server #1127936] Received 0.24 MB of payload data from client #52 (simulated).
[INFO][10:10:02]: [Client #268] Loading a model from /data/ykang/plato/results/test/model/lenet5_268_1127978.pth.
[INFO][10:10:02]: [Client #268] Model trained.
[INFO][10:10:02]: [Client #268] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:10:02]: [Server #1127936] Received 0.24 MB of payload data from client #268 (simulated).
[INFO][10:10:02]: [Client #183] Loading a model from /data/ykang/plato/results/test/model/lenet5_183_1127979.pth.
[INFO][10:10:02]: [Client #183] Model trained.
[INFO][10:10:02]: [Client #183] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:10:02]: [Server #1127936] Received 0.24 MB of payload data from client #183 (simulated).
[INFO][10:10:02]: [Server #1127936] Selecting client #301 for training.
[INFO][10:10:02]: [Server #1127936] Sending the current model to client #301 (simulated).
[INFO][10:10:02]: [Server #1127936] Sending 0.24 MB of payload data to client #301 (simulated).
[INFO][10:10:02]: [Client #301] Selected by the server.
[INFO][10:10:02]: [Client #301] Loading its data source...
[INFO][10:10:02]: [Client #301] Dataset size: 60000
[INFO][10:10:02]: [Client #301] Sampler: noniid
[INFO][10:10:02]: [Client #301] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:10:02]: [93m[1m[Client #301] Started training in communication round #44.[0m
[INFO][10:10:04]: [Client #301] Loading the dataset.
[INFO][10:10:09]: [Client #301] Epoch: [1/5][0/10]	Loss: 0.062896
[INFO][10:10:09]: [Client #301] Epoch: [2/5][0/10]	Loss: 0.009325
[INFO][10:10:09]: [Client #301] Epoch: [3/5][0/10]	Loss: 0.003788
[INFO][10:10:09]: [Client #301] Epoch: [4/5][0/10]	Loss: 0.001634
[INFO][10:10:09]: [Client #301] Epoch: [5/5][0/10]	Loss: 0.057620
[INFO][10:10:09]: [Client #301] Model saved to /data/ykang/plato/results/test/model/lenet5_301_1127977.pth.
[INFO][10:10:10]: [Client #301] Loading a model from /data/ykang/plato/results/test/model/lenet5_301_1127977.pth.
[INFO][10:10:10]: [Client #301] Model trained.
[INFO][10:10:10]: [Client #301] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:10:10]: [Server #1127936] Received 0.24 MB of payload data from client #301 (simulated).
[INFO][10:10:10]: [Server #1127936] Adding client #4 to the list of clients for aggregation.
[INFO][10:10:10]: [Server #1127936] Adding client #337 to the list of clients for aggregation.
[INFO][10:10:10]: [Server #1127936] Adding client #307 to the list of clients for aggregation.
[INFO][10:10:10]: [Server #1127936] Adding client #371 to the list of clients for aggregation.
[INFO][10:10:10]: [Server #1127936] Adding client #340 to the list of clients for aggregation.
[INFO][10:10:10]: [Server #1127936] Adding client #496 to the list of clients for aggregation.
[INFO][10:10:10]: [Server #1127936] Adding client #110 to the list of clients for aggregation.
[INFO][10:10:10]: [Server #1127936] Adding client #409 to the list of clients for aggregation.
[INFO][10:10:10]: [Server #1127936] Adding client #268 to the list of clients for aggregation.
[INFO][10:10:10]: [Server #1127936] Adding client #294 to the list of clients for aggregation.
[INFO][10:10:10]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 338, 339, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.00311942 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01504708 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00345679 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00803728
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01247888 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01155474 0.         0.         0.0138083  0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01524759 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00797157 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.0056888  0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 3. 0. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 3. 1. 0. 0. 1. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 0. 0. 0. 0. 1.
 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 2. 0. 6. 0. 1. 2. 1. 0. 1. 1. 0. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.
 1. 0. 1. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 4. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0.
 1. 1. 1. 1. 0. 2. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 1. 0. 0. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 0. 0. 0. 0. 1. 0. 0. 6. 1. 1.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 2. 0. 1. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 2. 2. 0. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.00311942 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01504708 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00345679 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00803728
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01247888 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01155474 0.         0.         0.0138083  0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01524759 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00797157 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.0056888  0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:10:12]: [Server #1127936] Global model accuracy: 95.59%

[INFO][10:10:12]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_44.pth.
[INFO][10:10:12]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_44.pth.
[INFO][10:10:12]: [93m[1m
[Server #1127936] Starting round 45/100.[0m
[0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1
 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8871e+00  4e-04  7e-09  7e-09
 5:  6.8876e+00  6.8875e+00  9e-05  2e-09  2e-09
 6:  6.8875e+00  6.8875e+00  9e-05  6e-10  4e-10
 7:  6.8875e+00  6.8875e+00  8e-05  4e-09  4e-10
 8:  6.8875e+00  6.8875e+00  6e-05  5e-09  5e-10
 9:  6.8875e+00  6.8875e+00  2e-05  3e-08  3e-09
10:  6.8875e+00  6.8875e+00  5e-06  1e-08  1e-09
Optimal solution found.
The calculated probability is:  [1.21900550e-04 1.21900550e-04 1.21900550e-04 1.27221558e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 9.40257884e-01
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900355e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21899494e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.46327902e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.44194633e-04 1.21900550e-04 1.49507565e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.53106275e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21899511e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.43794857e-04 1.21900550e-04 1.21900550e-04
 1.21900550e-04 1.21900550e-04]
current clients pool:  [INFO][10:10:12]: [Server #1127936] Selected clients: [157 110 169 109 464 347 302 375 274 352]
[INFO][10:10:12]: [Server #1127936] Selecting client #157 for training.
[INFO][10:10:12]: [Server #1127936] Sending the current model to client #157 (simulated).
[INFO][10:10:12]: [Server #1127936] Sending 0.24 MB of payload data to client #157 (simulated).
[INFO][10:10:12]: [Server #1127936] Selecting client #110 for training.
[INFO][10:10:12]: [Server #1127936] Sending the current model to client #110 (simulated).
[INFO][10:10:12]: [Server #1127936] Sending 0.24 MB of payload data to client #110 (simulated).
[INFO][10:10:12]: [Server #1127936] Selecting client #169 for training.
[INFO][10:10:12]: [Server #1127936] Sending the current model to client #169 (simulated).
[INFO][10:10:12]: [Client #157] Selected by the server.
[INFO][10:10:12]: [Client #157] Loading its data source...
[INFO][10:10:12]: [Client #157] Dataset size: 60000
[INFO][10:10:12]: [Client #157] Sampler: noniid
[INFO][10:10:12]: [Server #1127936] Sending 0.24 MB of payload data to client #169 (simulated).
[INFO][10:10:12]: [Client #110] Selected by the server.
[INFO][10:10:12]: [Client #110] Loading its data source...
[INFO][10:10:12]: [Client #169] Selected by the server.
[INFO][10:10:12]: [Client #110] Dataset size: 60000
[INFO][10:10:12]: [Client #157] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:10:12]: [Client #169] Loading its data source...
[INFO][10:10:12]: [Client #110] Sampler: noniid
[INFO][10:10:12]: [Client #169] Dataset size: 60000
[INFO][10:10:12]: [Client #169] Sampler: noniid
[INFO][10:10:12]: [Client #169] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:10:12]: [Client #110] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:10:12]: [93m[1m[Client #157] Started training in communication round #45.[0m
[INFO][10:10:12]: [93m[1m[Client #110] Started training in communication round #45.[0m
[INFO][10:10:12]: [93m[1m[Client #169] Started training in communication round #45.[0m
[INFO][10:10:15]: [Client #157] Loading the dataset.
[INFO][10:10:15]: [Client #169] Loading the dataset.
[INFO][10:10:15]: [Client #110] Loading the dataset.
[INFO][10:10:20]: [Client #169] Epoch: [1/5][0/10]	Loss: 0.000958
[INFO][10:10:20]: [Client #110] Epoch: [1/5][0/10]	Loss: 0.099814
[INFO][10:10:20]: [Client #157] Epoch: [1/5][0/10]	Loss: 0.017650
[INFO][10:10:20]: [Client #169] Epoch: [2/5][0/10]	Loss: 0.000615
[INFO][10:10:21]: [Client #110] Epoch: [2/5][0/10]	Loss: 0.000571
[INFO][10:10:21]: [Client #157] Epoch: [2/5][0/10]	Loss: 0.105771
[INFO][10:10:21]: [Client #169] Epoch: [3/5][0/10]	Loss: 0.000089
[INFO][10:10:21]: [Client #157] Epoch: [3/5][0/10]	Loss: 0.001162
[INFO][10:10:21]: [Client #110] Epoch: [3/5][0/10]	Loss: 0.000030
[INFO][10:10:21]: [Client #169] Epoch: [4/5][0/10]	Loss: 0.000009
[INFO][10:10:21]: [Client #157] Epoch: [4/5][0/10]	Loss: 0.000080
[INFO][10:10:21]: [Client #169] Epoch: [5/5][0/10]	Loss: 0.000028
[INFO][10:10:21]: [Client #110] Epoch: [4/5][0/10]	Loss: 0.000890
[INFO][10:10:21]: [Client #157] Epoch: [5/5][0/10]	Loss: 0.000001
[INFO][10:10:21]: [Client #169] Model saved to /data/ykang/plato/results/test/model/lenet5_169_1127979.pth.
[INFO][10:10:21]: [Client #157] Model saved to /data/ykang/plato/results/test/model/lenet5_157_1127977.pth.
[INFO][10:10:21]: [Client #110] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:10:21]: [Client #110] Model saved to /data/ykang/plato/results/test/model/lenet5_110_1127978.pth.
[INFO][10:10:22]: [Client #169] Loading a model from /data/ykang/plato/results/test/model/lenet5_169_1127979.pth.
[INFO][10:10:22]: [Client #169] Model trained.
[INFO][10:10:22]: [Client #169] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:10:22]: [Server #1127936] Received 0.24 MB of payload data from client #169 (simulated).
[INFO][10:10:22]: [Client #157] Loading a model from /data/ykang/plato/results/test/model/lenet5_157_1127977.pth.
[INFO][10:10:22]: [Client #157] Model trained.
[INFO][10:10:22]: [Client #157] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:10:22]: [Server #1127936] Received 0.24 MB of payload data from client #157 (simulated).
[INFO][10:10:22]: [Client #110] Loading a model from /data/ykang/plato/results/test/model/lenet5_110_1127978.pth.
[INFO][10:10:22]: [Client #110] Model trained.
[INFO][10:10:22]: [Client #110] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:10:22]: [Server #1127936] Received 0.24 MB of payload data from client #110 (simulated).
[INFO][10:10:22]: [Server #1127936] Selecting client #109 for training.
[INFO][10:10:22]: [Server #1127936] Sending the current model to client #109 (simulated).
[INFO][10:10:22]: [Server #1127936] Sending 0.24 MB of payload data to client #109 (simulated).
[INFO][10:10:22]: [Server #1127936] Selecting client #464 for training.
[INFO][10:10:22]: [Server #1127936] Sending the current model to client #464 (simulated).
[INFO][10:10:22]: [Server #1127936] Sending 0.24 MB of payload data to client #464 (simulated).
[INFO][10:10:22]: [Server #1127936] Selecting client #347 for training.
[INFO][10:10:22]: [Server #1127936] Sending the current model to client #347 (simulated).
[INFO][10:10:22]: [Client #109] Selected by the server.
[INFO][10:10:22]: [Client #109] Loading its data source...
[INFO][10:10:22]: [Client #109] Dataset size: 60000
[INFO][10:10:22]: [Client #109] Sampler: noniid
[INFO][10:10:22]: [Server #1127936] Sending 0.24 MB of payload data to client #347 (simulated).
[INFO][10:10:22]: [Client #464] Selected by the server.
[INFO][10:10:22]: [Client #464] Loading its data source...
[INFO][10:10:22]: [Client #464] Dataset size: 60000
[INFO][10:10:22]: [Client #464] Sampler: noniid
[INFO][10:10:22]: [Client #347] Selected by the server.
[INFO][10:10:22]: [Client #347] Loading its data source...
[INFO][10:10:22]: [Client #347] Dataset size: 60000
[INFO][10:10:22]: [Client #347] Sampler: noniid
[INFO][10:10:22]: [Client #109] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:10:22]: [Client #347] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:10:22]: [Client #464] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:10:22]: [93m[1m[Client #109] Started training in communication round #45.[0m
[INFO][10:10:22]: [93m[1m[Client #347] Started training in communication round #45.[0m
[INFO][10:10:22]: [93m[1m[Client #464] Started training in communication round #45.[0m
[INFO][10:10:24]: [Client #347] Loading the dataset.
[INFO][10:10:24]: [Client #109] Loading the dataset.
[INFO][10:10:24]: [Client #464] Loading the dataset.
[INFO][10:10:30]: [Client #109] Epoch: [1/5][0/10]	Loss: 0.051078
[INFO][10:10:30]: [Client #347] Epoch: [1/5][0/10]	Loss: 0.003460
[INFO][10:10:30]: [Client #109] Epoch: [2/5][0/10]	Loss: 0.000245
[INFO][10:10:30]: [Client #464] Epoch: [1/5][0/10]	Loss: 0.002835
[INFO][10:10:30]: [Client #347] Epoch: [2/5][0/10]	Loss: 0.001661
[INFO][10:10:30]: [Client #109] Epoch: [3/5][0/10]	Loss: 0.000001
[INFO][10:10:30]: [Client #464] Epoch: [2/5][0/10]	Loss: 0.000745
[INFO][10:10:30]: [Client #347] Epoch: [3/5][0/10]	Loss: 0.020610
[INFO][10:10:30]: [Client #109] Epoch: [4/5][0/10]	Loss: 0.001658
[INFO][10:10:30]: [Client #464] Epoch: [3/5][0/10]	Loss: 0.000037
[INFO][10:10:30]: [Client #347] Epoch: [4/5][0/10]	Loss: 0.000771
[INFO][10:10:30]: [Client #109] Epoch: [5/5][0/10]	Loss: 0.000262
[INFO][10:10:30]: [Client #347] Epoch: [5/5][0/10]	Loss: 0.051382
[INFO][10:10:30]: [Client #464] Epoch: [4/5][0/10]	Loss: 0.000005
[INFO][10:10:30]: [Client #109] Model saved to /data/ykang/plato/results/test/model/lenet5_109_1127977.pth.
[INFO][10:10:30]: [Client #347] Model saved to /data/ykang/plato/results/test/model/lenet5_347_1127979.pth.
[INFO][10:10:30]: [Client #464] Epoch: [5/5][0/10]	Loss: 0.000236
[INFO][10:10:30]: [Client #464] Model saved to /data/ykang/plato/results/test/model/lenet5_464_1127978.pth.
[INFO][10:10:31]: [Client #347] Loading a model from /data/ykang/plato/results/test/model/lenet5_347_1127979.pth.
[INFO][10:10:31]: [Client #347] Model trained.
[INFO][10:10:31]: [Client #347] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:10:31]: [Server #1127936] Received 0.24 MB of payload data from client #347 (simulated).
[INFO][10:10:31]: [Client #109] Loading a model from /data/ykang/plato/results/test/model/lenet5_109_1127977.pth.
[INFO][10:10:31]: [Client #109] Model trained.
[INFO][10:10:31]: [Client #109] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:10:31]: [Server #1127936] Received 0.24 MB of payload data from client #109 (simulated).
[INFO][10:10:31]: [Client #464] Loading a model from /data/ykang/plato/results/test/model/lenet5_464_1127978.pth.
[INFO][10:10:31]: [Client #464] Model trained.
[INFO][10:10:31]: [Client #464] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:10:31]: [Server #1127936] Received 0.24 MB of payload data from client #464 (simulated).
[INFO][10:10:31]: [Server #1127936] Selecting client #302 for training.
[INFO][10:10:31]: [Server #1127936] Sending the current model to client #302 (simulated).
[INFO][10:10:31]: [Server #1127936] Sending 0.24 MB of payload data to client #302 (simulated).
[INFO][10:10:31]: [Server #1127936] Selecting client #375 for training.
[INFO][10:10:31]: [Server #1127936] Sending the current model to client #375 (simulated).
[INFO][10:10:31]: [Server #1127936] Sending 0.24 MB of payload data to client #375 (simulated).
[INFO][10:10:31]: [Server #1127936] Selecting client #274 for training.
[INFO][10:10:31]: [Server #1127936] Sending the current model to client #274 (simulated).
[INFO][10:10:31]: [Client #302] Selected by the server.
[INFO][10:10:31]: [Client #302] Loading its data source...
[INFO][10:10:31]: [Client #302] Dataset size: 60000
[INFO][10:10:31]: [Client #302] Sampler: noniid
[INFO][10:10:31]: [Server #1127936] Sending 0.24 MB of payload data to client #274 (simulated).
[INFO][10:10:31]: [Client #302] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:10:31]: [Client #375] Selected by the server.
[INFO][10:10:31]: [Client #375] Loading its data source...
[INFO][10:10:31]: [Client #375] Dataset size: 60000
[INFO][10:10:31]: [Client #375] Sampler: noniid
[INFO][10:10:31]: [Client #274] Selected by the server.
[INFO][10:10:31]: [Client #274] Loading its data source...
[INFO][10:10:31]: [Client #274] Dataset size: 60000
[INFO][10:10:31]: [Client #274] Sampler: noniid
[INFO][10:10:31]: [Client #375] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:10:31]: [Client #274] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:10:31]: [93m[1m[Client #302] Started training in communication round #45.[0m
[INFO][10:10:31]: [93m[1m[Client #375] Started training in communication round #45.[0m
[INFO][10:10:31]: [93m[1m[Client #274] Started training in communication round #45.[0m
[INFO][10:10:33]: [Client #302] Loading the dataset.
[INFO][10:10:33]: [Client #375] Loading the dataset.
[INFO][10:10:33]: [Client #274] Loading the dataset.
[INFO][10:10:39]: [Client #302] Epoch: [1/5][0/10]	Loss: 0.018241
[INFO][10:10:39]: [Client #375] Epoch: [1/5][0/10]	Loss: 0.093480
[INFO][10:10:39]: [Client #274] Epoch: [1/5][0/10]	Loss: 0.112845
[INFO][10:10:39]: [Client #302] Epoch: [2/5][0/10]	Loss: 0.000811
[INFO][10:10:39]: [Client #274] Epoch: [2/5][0/10]	Loss: 0.000042
[INFO][10:10:39]: [Client #302] Epoch: [3/5][0/10]	Loss: 0.000143
[INFO][10:10:39]: [Client #375] Epoch: [2/5][0/10]	Loss: 0.001939
[INFO][10:10:39]: [Client #274] Epoch: [3/5][0/10]	Loss: 0.000146
[INFO][10:10:39]: [Client #302] Epoch: [4/5][0/10]	Loss: 0.000010
[INFO][10:10:39]: [Client #375] Epoch: [3/5][0/10]	Loss: 0.000626
[INFO][10:10:39]: [Client #274] Epoch: [4/5][0/10]	Loss: 0.000043
[INFO][10:10:39]: [Client #302] Epoch: [5/5][0/10]	Loss: 0.000001
[INFO][10:10:39]: [Client #302] Model saved to /data/ykang/plato/results/test/model/lenet5_302_1127977.pth.
[INFO][10:10:39]: [Client #375] Epoch: [4/5][0/10]	Loss: 0.000210
[INFO][10:10:40]: [Client #274] Epoch: [5/5][0/10]	Loss: 0.000019
[INFO][10:10:40]: [Client #274] Model saved to /data/ykang/plato/results/test/model/lenet5_274_1127979.pth.
[INFO][10:10:40]: [Client #375] Epoch: [5/5][0/10]	Loss: 0.096552
[INFO][10:10:40]: [Client #375] Model saved to /data/ykang/plato/results/test/model/lenet5_375_1127978.pth.
[INFO][10:10:40]: [Client #302] Loading a model from /data/ykang/plato/results/test/model/lenet5_302_1127977.pth.
[INFO][10:10:40]: [Client #302] Model trained.
[INFO][10:10:40]: [Client #302] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:10:40]: [Server #1127936] Received 0.24 MB of payload data from client #302 (simulated).
[INFO][10:10:40]: [Client #274] Loading a model from /data/ykang/plato/results/test/model/lenet5_274_1127979.pth.
[INFO][10:10:40]: [Client #274] Model trained.
[INFO][10:10:40]: [Client #274] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:10:40]: [Server #1127936] Received 0.24 MB of payload data from client #274 (simulated).
[INFO][10:10:40]: [Client #375] Loading a model from /data/ykang/plato/results/test/model/lenet5_375_1127978.pth.
[INFO][10:10:40]: [Client #375] Model trained.
[INFO][10:10:40]: [Client #375] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:10:40]: [Server #1127936] Received 0.24 MB of payload data from client #375 (simulated).
[INFO][10:10:40]: [Server #1127936] Selecting client #352 for training.
[INFO][10:10:40]: [Server #1127936] Sending the current model to client #352 (simulated).
[INFO][10:10:40]: [Server #1127936] Sending 0.24 MB of payload data to client #352 (simulated).
[INFO][10:10:40]: [Client #352] Selected by the server.
[INFO][10:10:40]: [Client #352] Loading its data source...
[INFO][10:10:40]: [Client #352] Dataset size: 60000
[INFO][10:10:40]: [Client #352] Sampler: noniid
[INFO][10:10:40]: [Client #352] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:10:40]: [93m[1m[Client #352] Started training in communication round #45.[0m
[INFO][10:10:42]: [Client #352] Loading the dataset.
[INFO][10:10:48]: [Client #352] Epoch: [1/5][0/10]	Loss: 0.004310
[INFO][10:10:48]: [Client #352] Epoch: [2/5][0/10]	Loss: 0.017537
[INFO][10:10:48]: [Client #352] Epoch: [3/5][0/10]	Loss: 0.000638
[INFO][10:10:48]: [Client #352] Epoch: [4/5][0/10]	Loss: 0.000405
[INFO][10:10:48]: [Client #352] Epoch: [5/5][0/10]	Loss: 0.000073
[INFO][10:10:48]: [Client #352] Model saved to /data/ykang/plato/results/test/model/lenet5_352_1127977.pth.
[INFO][10:10:48]: [Client #352] Loading a model from /data/ykang/plato/results/test/model/lenet5_352_1127977.pth.
[INFO][10:10:48]: [Client #352] Model trained.
[INFO][10:10:49]: [Client #352] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:10:49]: [Server #1127936] Received 0.24 MB of payload data from client #352 (simulated).
[INFO][10:10:49]: [Server #1127936] Adding client #395 to the list of clients for aggregation.
[INFO][10:10:49]: [Server #1127936] Adding client #52 to the list of clients for aggregation.
[INFO][10:10:49]: [Server #1127936] Adding client #183 to the list of clients for aggregation.
[INFO][10:10:49]: [Server #1127936] Adding client #301 to the list of clients for aggregation.
[INFO][10:10:49]: [Server #1127936] Adding client #338 to the list of clients for aggregation.
[INFO][10:10:49]: [Server #1127936] Adding client #242 to the list of clients for aggregation.
[INFO][10:10:49]: [Server #1127936] Adding client #375 to the list of clients for aggregation.
[INFO][10:10:49]: [Server #1127936] Adding client #157 to the list of clients for aggregation.
[INFO][10:10:49]: [Server #1127936] Adding client #464 to the list of clients for aggregation.
[INFO][10:10:49]: [Server #1127936] Adding client #274 to the list of clients for aggregation.
[INFO][10:10:49]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01031728 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00962537 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01355442 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00355162 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00825271 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01234238 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.0115373  0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01493162 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01408994 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00490077 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 3. 0. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 3. 1. 0. 0. 1. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 0. 0. 0. 0. 1.
 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 2. 0. 6. 0. 1. 2. 1. 0. 1. 1. 0. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0.
 1. 0. 1. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 4. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0.
 1. 1. 1. 1. 0. 2. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 1. 0. 0. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 0. 0. 0. 0. 1. 0. 0. 6. 1. 1.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 2. 0. 1. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 2. 2. 0. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01031728 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00962537 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01355442 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00355162 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00825271 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01234238 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.0115373  0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01493162 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01408994 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00490077 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:10:51]: [Server #1127936] Global model accuracy: 95.16%

[INFO][10:10:51]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_45.pth.
[INFO][10:10:51]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_45.pth.
[INFO][10:10:51]: [93m[1m
[Server #1127936] Starting round 46/100.[0m
[INFO][10:10:51]: [Server #1127936] Selected clients: [404 395 205 330  97 471 300 124 265 242]
[INFO][10:10:51]: [Server #1127936] Selecting client #404 for training.
[INFO][10:10:51]: [Server #1127936] Sending the current model to client #404 (simulated).
[INFO][10:10:51]: [Server #1127936] Sending 0.24 MB of payload data to client #404 (simulated).
[INFO][10:10:51]: [Server #1127936] Selecting client #395 for training.
[INFO][10:10:51]: [Server #1127936] Sending the current model to client #395 (simulated).
[INFO][10:10:51]: [Server #1127936] Sending 0.24 MB of payload data to client #395 (simulated).
[INFO][10:10:51]: [Client #404] Selected by the server.
[INFO][10:10:51]: [Client #404] Loading its data source...
[INFO][10:10:51]: [Client #404] Dataset size: 60000
[INFO][10:10:51]: [Client #404] Sampler: noniid
[INFO][10:10:51]: [Client #404] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:10:51]: [93m[1m[Client #404] Started training in communication round #46.[0m
[INFO][10:10:51]: [Server #1127936] Selecting client #205 for training.
[INFO][10:10:51]: [Server #1127936] Sending the current model to client #205 (simulated).
[INFO][10:10:51]: [Server #1127936] Sending 0.24 MB of payload data to client #205 (simulated).
[INFO][10:10:51]: [Client #395] Selected by the server.
[INFO][10:10:51]: [Client #395] Loading its data source...
[INFO][10:10:51]: [Client #395] Dataset size: 60000
[INFO][10:10:51]: [Client #395] Sampler: noniid
[INFO][10:10:51]: [Client #205] Selected by the server.
[INFO][10:10:51]: [Client #205] Loading its data source...
[INFO][10:10:51]: [Client #205] Dataset size: 60000
[INFO][10:10:51]: [Client #205] Sampler: noniid
[INFO][10:10:51]: [Client #395] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:10:51]: [Client #205] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:10:51]: [93m[1m[Client #395] Started training in communication round #46.[0m
[INFO][10:10:51]: [93m[1m[Client #205] Started training in communication round #46.[0m
[INFO][10:10:53]: [Client #395] Loading the dataset.
[INFO][10:10:53]: [Client #404] Loading the dataset.
[INFO][10:10:53]: [Client #205] Loading the dataset.
[INFO][10:10:59]: [Client #395] Epoch: [1/5][0/10]	Loss: 0.038060
[INFO][10:10:59]: [Client #404] Epoch: [1/5][0/10]	Loss: 0.002370
[INFO][10:10:59]: [Client #205] Epoch: [1/5][0/10]	Loss: 0.001151
[INFO][10:10:59]: [Client #395] Epoch: [2/5][0/10]	Loss: 0.000119
[INFO][10:10:59]: [Client #205] Epoch: [2/5][0/10]	Loss: 0.000969
[INFO][10:10:59]: [Client #404] Epoch: [2/5][0/10]	Loss: 0.007836
[INFO][10:10:59]: [Client #395] Epoch: [3/5][0/10]	Loss: 0.002007
[INFO][10:10:59]: [Client #205] Epoch: [3/5][0/10]	Loss: 0.000579
[INFO][10:10:59]: [Client #395] Epoch: [4/5][0/10]	Loss: 0.000209
[INFO][10:10:59]: [Client #404] Epoch: [3/5][0/10]	Loss: 0.000044
[INFO][10:10:59]: [Client #205] Epoch: [4/5][0/10]	Loss: 0.000360
[INFO][10:10:59]: [Client #395] Epoch: [5/5][0/10]	Loss: 0.006329
[INFO][10:10:59]: [Client #395] Model saved to /data/ykang/plato/results/test/model/lenet5_395_1127978.pth.
[INFO][10:10:59]: [Client #205] Epoch: [5/5][0/10]	Loss: 0.000162
[INFO][10:10:59]: [Client #205] Model saved to /data/ykang/plato/results/test/model/lenet5_205_1127979.pth.
[INFO][10:10:59]: [Client #404] Epoch: [4/5][0/10]	Loss: 0.000039
[INFO][10:11:00]: [Client #404] Epoch: [5/5][0/10]	Loss: 0.001960
[INFO][10:11:00]: [Client #404] Model saved to /data/ykang/plato/results/test/model/lenet5_404_1127977.pth.
[INFO][10:11:00]: [Client #395] Loading a model from /data/ykang/plato/results/test/model/lenet5_395_1127978.pth.
[INFO][10:11:00]: [Client #395] Model trained.
[INFO][10:11:00]: [Client #395] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:11:00]: [Server #1127936] Received 0.24 MB of payload data from client #395 (simulated).
[INFO][10:11:00]: [Client #205] Loading a model from /data/ykang/plato/results/test/model/lenet5_205_1127979.pth.
[INFO][10:11:00]: [Client #205] Model trained.
[INFO][10:11:00]: [Client #205] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:11:00]: [Server #1127936] Received 0.24 MB of payload data from client #205 (simulated).
[INFO][10:11:00]: [Client #404] Loading a model from /data/ykang/plato/results/test/model/lenet5_404_1127977.pth.
[INFO][10:11:00]: [Client #404] Model trained.
[INFO][10:11:00]: [Client #404] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:11:00]: [Server #1127936] Received 0.24 MB of payload data from client #404 (simulated).
[INFO][10:11:00]: [Server #1127936] Selecting client #330 for training.
[INFO][10:11:00]: [Server #1127936] Sending the current model to client #330 (simulated).
[INFO][10:11:00]: [Server #1127936] Sending 0.24 MB of payload data to client #330 (simulated).
[INFO][10:11:00]: [Server #1127936] Selecting client #97 for training.
[INFO][10:11:00]: [Server #1127936] Sending the current model to client #97 (simulated).
[INFO][10:11:00]: [Server #1127936] Sending 0.24 MB of payload data to client #97 (simulated).
[INFO][10:11:00]: [Server #1127936] Selecting client #471 for training.
[INFO][10:11:00]: [Server #1127936] Sending the current model to client #471 (simulated).
[INFO][10:11:00]: [Client #330] Selected by the server.
[INFO][10:11:00]: [Client #330] Loading its data source...
[INFO][10:11:00]: [Client #330] Dataset size: 60000
[INFO][10:11:00]: [Client #330] Sampler: noniid
[INFO][10:11:00]: [Server #1127936] Sending 0.24 MB of payload data to client #471 (simulated).
[INFO][10:11:00]: [Client #97] Selected by the server.
[INFO][10:11:00]: [Client #97] Loading its data source...
[INFO][10:11:00]: [Client #471] Selected by the server.
[INFO][10:11:00]: [Client #97] Dataset size: 60000
[INFO][10:11:00]: [Client #97] Sampler: noniid
[INFO][10:11:00]: [Client #471] Loading its data source...
[INFO][10:11:00]: [Client #471] Dataset size: 60000
[INFO][10:11:00]: [Client #471] Sampler: noniid
[INFO][10:11:00]: [Client #330] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:11:00]: [Client #97] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:11:00]: [Client #471] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:11:00]: [93m[1m[Client #97] Started training in communication round #46.[0m
[INFO][10:11:00]: [93m[1m[Client #330] Started training in communication round #46.[0m
[INFO][10:11:00]: [93m[1m[Client #471] Started training in communication round #46.[0m
[INFO][10:11:03]: [Client #330] Loading the dataset.
[INFO][10:11:03]: [Client #471] Loading the dataset.
[INFO][10:11:03]: [Client #97] Loading the dataset.
[INFO][10:11:08]: [Client #330] Epoch: [1/5][0/10]	Loss: 0.019751
[INFO][10:11:08]: [Client #330] Epoch: [2/5][0/10]	Loss: 0.004501
[INFO][10:11:08]: [Client #97] Epoch: [1/5][0/10]	Loss: 0.005327
[INFO][10:11:08]: [Client #471] Epoch: [1/5][0/10]	Loss: 0.005505
[INFO][10:11:08]: [Client #330] Epoch: [3/5][0/10]	Loss: 0.002157
[INFO][10:11:08]: [Client #471] Epoch: [2/5][0/10]	Loss: 0.012378
[INFO][10:11:08]: [Client #97] Epoch: [2/5][0/10]	Loss: 0.002042
[INFO][10:11:09]: [Client #330] Epoch: [4/5][0/10]	Loss: 0.170683
[INFO][10:11:09]: [Client #471] Epoch: [3/5][0/10]	Loss: 0.000219
[INFO][10:11:09]: [Client #97] Epoch: [3/5][0/10]	Loss: 0.000113
[INFO][10:11:09]: [Client #330] Epoch: [5/5][0/10]	Loss: 0.004954
[INFO][10:11:09]: [Client #471] Epoch: [4/5][0/10]	Loss: 0.000234
[INFO][10:11:09]: [Client #97] Epoch: [4/5][0/10]	Loss: 0.000036
[INFO][10:11:09]: [Client #330] Model saved to /data/ykang/plato/results/test/model/lenet5_330_1127977.pth.
[INFO][10:11:09]: [Client #471] Epoch: [5/5][0/10]	Loss: 0.015221
[INFO][10:11:09]: [Client #97] Epoch: [5/5][0/10]	Loss: 0.000077
[INFO][10:11:09]: [Client #471] Model saved to /data/ykang/plato/results/test/model/lenet5_471_1127979.pth.
[INFO][10:11:09]: [Client #97] Model saved to /data/ykang/plato/results/test/model/lenet5_97_1127978.pth.
[INFO][10:11:10]: [Client #330] Loading a model from /data/ykang/plato/results/test/model/lenet5_330_1127977.pth.
[INFO][10:11:10]: [Client #330] Model trained.
[INFO][10:11:10]: [Client #330] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:11:10]: [Server #1127936] Received 0.24 MB of payload data from client #330 (simulated).
[INFO][10:11:10]: [Client #471] Loading a model from /data/ykang/plato/results/test/model/lenet5_471_1127979.pth.
[INFO][10:11:10]: [Client #471] Model trained.
[INFO][10:11:10]: [Client #471] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:11:10]: [Server #1127936] Received 0.24 MB of payload data from client #471 (simulated).
[INFO][10:11:10]: [Client #97] Loading a model from /data/ykang/plato/results/test/model/lenet5_97_1127978.pth.
[INFO][10:11:10]: [Client #97] Model trained.
[INFO][10:11:10]: [Client #97] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:11:10]: [Server #1127936] Received 0.24 MB of payload data from client #97 (simulated).
[INFO][10:11:10]: [Server #1127936] Selecting client #300 for training.
[INFO][10:11:10]: [Server #1127936] Sending the current model to client #300 (simulated).
[INFO][10:11:10]: [Server #1127936] Sending 0.24 MB of payload data to client #300 (simulated).
[INFO][10:11:10]: [Server #1127936] Selecting client #124 for training.
[INFO][10:11:10]: [Server #1127936] Sending the current model to client #124 (simulated).
[INFO][10:11:10]: [Server #1127936] Sending 0.24 MB of payload data to client #124 (simulated).
[INFO][10:11:10]: [Server #1127936] Selecting client #265 for training.
[INFO][10:11:10]: [Server #1127936] Sending the current model to client #265 (simulated).
[INFO][10:11:10]: [Client #300] Selected by the server.
[INFO][10:11:10]: [Client #300] Loading its data source...
[INFO][10:11:10]: [Client #300] Dataset size: 60000
[INFO][10:11:10]: [Client #300] Sampler: noniid
[INFO][10:11:10]: [Server #1127936] Sending 0.24 MB of payload data to client #265 (simulated).
[INFO][10:11:10]: [Client #124] Selected by the server.
[INFO][10:11:10]: [Client #124] Loading its data source...
[INFO][10:11:10]: [Client #124] Dataset size: 60000
[INFO][10:11:10]: [Client #265] Selected by the server.
[INFO][10:11:10]: [Client #124] Sampler: noniid
[INFO][10:11:10]: [Client #265] Loading its data source...
[INFO][10:11:10]: [Client #265] Dataset size: 60000
[INFO][10:11:10]: [Client #265] Sampler: noniid
[INFO][10:11:10]: [Client #300] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:11:10]: [Client #265] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:11:10]: [93m[1m[Client #300] Started training in communication round #46.[0m
[INFO][10:11:10]: [93m[1m[Client #265] Started training in communication round #46.[0m
[INFO][10:11:10]: [Client #124] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:11:10]: [93m[1m[Client #124] Started training in communication round #46.[0m
[INFO][10:11:12]: [Client #124] Loading the dataset.
[INFO][10:11:12]: [Client #300] Loading the dataset.
[INFO][10:11:12]: [Client #265] Loading the dataset.
[INFO][10:11:18]: [Client #300] Epoch: [1/5][0/10]	Loss: 0.016605
[INFO][10:11:18]: [Client #265] Epoch: [1/5][0/10]	Loss: 0.049511
[INFO][10:11:18]: [Client #124] Epoch: [1/5][0/10]	Loss: 0.007621
[INFO][10:11:18]: [Client #300] Epoch: [2/5][0/10]	Loss: 0.000047
[INFO][10:11:18]: [Client #265] Epoch: [2/5][0/10]	Loss: 0.000392
[INFO][10:11:18]: [Client #124] Epoch: [2/5][0/10]	Loss: 0.005827
[INFO][10:11:18]: [Client #300] Epoch: [3/5][0/10]	Loss: 0.000379
[INFO][10:11:18]: [Client #265] Epoch: [3/5][0/10]	Loss: 0.000075
[INFO][10:11:18]: [Client #124] Epoch: [3/5][0/10]	Loss: 0.000199
[INFO][10:11:18]: [Client #300] Epoch: [4/5][0/10]	Loss: 0.000126
[INFO][10:11:18]: [Client #265] Epoch: [4/5][0/10]	Loss: 0.000068
[INFO][10:11:18]: [Client #124] Epoch: [4/5][0/10]	Loss: 0.193323
[INFO][10:11:18]: [Client #265] Epoch: [5/5][0/10]	Loss: 0.000024
[INFO][10:11:18]: [Client #300] Epoch: [5/5][0/10]	Loss: 0.000522
[INFO][10:11:18]: [Client #124] Epoch: [5/5][0/10]	Loss: 0.001906
[INFO][10:11:18]: [Client #300] Model saved to /data/ykang/plato/results/test/model/lenet5_300_1127977.pth.
[INFO][10:11:18]: [Client #265] Model saved to /data/ykang/plato/results/test/model/lenet5_265_1127979.pth.
[INFO][10:11:18]: [Client #124] Model saved to /data/ykang/plato/results/test/model/lenet5_124_1127978.pth.
[INFO][10:11:19]: [Client #300] Loading a model from /data/ykang/plato/results/test/model/lenet5_300_1127977.pth.
[INFO][10:11:19]: [Client #300] Model trained.
[INFO][10:11:19]: [Client #300] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:11:19]: [Server #1127936] Received 0.24 MB of payload data from client #300 (simulated).
[INFO][10:11:19]: [Client #265] Loading a model from /data/ykang/plato/results/test/model/lenet5_265_1127979.pth.
[INFO][10:11:19]: [Client #265] Model trained.
[INFO][10:11:19]: [Client #265] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:11:19]: [Server #1127936] Received 0.24 MB of payload data from client #265 (simulated).
[INFO][10:11:19]: [Client #124] Loading a model from /data/ykang/plato/results/test/model/lenet5_124_1127978.pth.
[INFO][10:11:19]: [Client #124] Model trained.
[INFO][10:11:19]: [Client #124] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:11:19]: [Server #1127936] Received 0.24 MB of payload data from client #124 (simulated).
[INFO][10:11:19]: [Server #1127936] Selecting client #242 for training.
[INFO][10:11:19]: [Server #1127936] Sending the current model to client #242 (simulated).
[INFO][10:11:19]: [Server #1127936] Sending 0.24 MB of payload data to client #242 (simulated).
[INFO][10:11:19]: [Client #242] Selected by the server.
[INFO][10:11:19]: [Client #242] Loading its data source...
[INFO][10:11:19]: [Client #242] Dataset size: 60000
[INFO][10:11:19]: [Client #242] Sampler: noniid
[INFO][10:11:19]: [Client #242] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:11:19]: [93m[1m[Client #242] Started training in communication round #46.[0m
[INFO][10:11:21]: [Client #242] Loading the dataset.
[INFO][10:11:26]: [Client #242] Epoch: [1/5][0/10]	Loss: 0.001722
[INFO][10:11:26]: [Client #242] Epoch: [2/5][0/10]	Loss: 0.001571
[INFO][10:11:26]: [Client #242] Epoch: [3/5][0/10]	Loss: 0.000020
[INFO][10:11:26]: [Client #242] Epoch: [4/5][0/10]	Loss: 0.000046
[INFO][10:11:26]: [Client #242] Epoch: [5/5][0/10]	Loss: 0.000599
[INFO][10:11:26]: [Client #242] Model saved to /data/ykang/plato/results/test/model/lenet5_242_1127977.pth.
[INFO][10:11:27]: [Client #242] Loading a model from /data/ykang/plato/results/test/model/lenet5_242_1127977.pth.
[INFO][10:11:27]: [Client #242] Model trained.
[INFO][10:11:27]: [Client #242] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:11:27]: [Server #1127936] Received 0.24 MB of payload data from client #242 (simulated).
[INFO][10:11:27]: [Server #1127936] Adding client #169 to the list of clients for aggregation.
[INFO][10:11:27]: [Server #1127936] Adding client #109 to the list of clients for aggregation.
[INFO][10:11:27]: [Server #1127936] Adding client #347 to the list of clients for aggregation.
[INFO][10:11:27]: [Server #1127936] Adding client #352 to the list of clients for aggregation.
[INFO][10:11:27]: [Server #1127936] Adding client #302 to the list of clients for aggregation.
[INFO][10:11:27]: [Server #1127936] Adding client #152 to the list of clients for aggregation.
[INFO][10:11:27]: [Server #1127936] Adding client #300 to the list of clients for aggregation.
[INFO][10:11:27]: [Server #1127936] Adding client #124 to the list of clients for aggregation.
[INFO][10:11:27]: [Server #1127936] Adding client #205 to the list of clients for aggregation.
[INFO][10:11:27]: [Server #1127936] Adding client #395 to the list of clients for aggregation.
[INFO][10:11:27]: [Server #1127936] Aggregating 10 clients in total.
[0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1
 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  2e-05  3e-10  3e-10
 6:  6.8876e+00  6.8875e+00  1e-05  2e-10  2e-10
 7:  6.8875e+00  6.8875e+00  1e-05  3e-10  4e-12
 8:  6.8875e+00  6.8875e+00  8e-06  2e-10  3e-12
 9:  6.8875e+00  6.8875e+00  5e-06  3e-10  5e-12
Optimal solution found.
The calculated probability is:  [0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00289748 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.0007283  0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.02074708 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00099151 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072832 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00623638 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00429011 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072823
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.61231354 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072834 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836 0.00072836
 0.00072836 0.00072836 0.00072836 0.00072836]
current clients pool:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 348, 349, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.02375594 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00799522 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00683874 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00212049 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01055287 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00554328
 0.         0.01332004 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00853731 0.
 0.         0.         0.         0.00457276 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01309101 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 3. 0. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 3. 1. 0. 0. 1. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 0. 0. 0. 0. 1.
 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 2. 1. 6. 0. 1. 2. 1. 0. 1. 1. 0. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0.
 1. 0. 1. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 4. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0.
 1. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 1. 0. 0. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 0. 0. 0. 0. 1. 0. 0. 6. 1. 1.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 2. 0. 1. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 2. 2. 0. 1. 0.]
local_gradient_bounds:  [INFO][10:11:29]: [Server #1127936] Global model accuracy: 94.92%

[INFO][10:11:29]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_46.pth.
[INFO][10:11:29]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_46.pth.
[INFO][10:11:29]: [93m[1m
[Server #1127936] Starting round 47/100.[0m
[0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.02375594 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00799522 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00683874 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00212049 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01055287 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00554328
 0.         0.01332004 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00853731 0.
 0.         0.         0.         0.00457276 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01309101 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1
 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002
 0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  3e-05  5e-10  5e-10
 6:  6.8876e+00  6.8875e+00  2e-05  3e-10  3e-10
 7:  6.8875e+00  6.8875e+00  2e-05  6e-10  2e-11
 8:  6.8875e+00  6.8875e+00  2e-05  5e-10  1e-11
 9:  6.8875e+00  6.8875e+00  9e-06  1e-09  3e-11
10:  6.8875e+00  6.8875e+00  7e-07  2e-09  4e-11
Optimal solution found.
The calculated probability is:  [5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 9.70776712e-01 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78019955e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 8.54272441e-04 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 6.43415862e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78006801e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78029159e-05 5.78037679e-05 1.56195379e-04
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 9.74739490e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 7.39916877e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.77990163e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05 5.78037679e-05 5.78037679e-05
 5.78037679e-05 5.78037679e-05]
current clients pool:  [INFO][10:11:30]: [Server #1127936] Selected clients: [109 277 172 302 467 295 490 215 366  20]
[INFO][10:11:30]: [Server #1127936] Selecting client #109 for training.
[INFO][10:11:30]: [Server #1127936] Sending the current model to client #109 (simulated).
[INFO][10:11:30]: [Server #1127936] Sending 0.24 MB of payload data to client #109 (simulated).
[INFO][10:11:30]: [Server #1127936] Selecting client #277 for training.
[INFO][10:11:30]: [Server #1127936] Sending the current model to client #277 (simulated).
[INFO][10:11:30]: [Server #1127936] Sending 0.24 MB of payload data to client #277 (simulated).
[INFO][10:11:30]: [Server #1127936] Selecting client #172 for training.
[INFO][10:11:30]: [Server #1127936] Sending the current model to client #172 (simulated).
[INFO][10:11:30]: [Client #109] Selected by the server.
[INFO][10:11:30]: [Client #109] Loading its data source...
[INFO][10:11:30]: [Client #109] Dataset size: 60000
[INFO][10:11:30]: [Client #109] Sampler: noniid
[INFO][10:11:30]: [Server #1127936] Sending 0.24 MB of payload data to client #172 (simulated).
[INFO][10:11:30]: [Client #277] Selected by the server.
[INFO][10:11:30]: [Client #277] Loading its data source...
[INFO][10:11:30]: [Client #277] Dataset size: 60000
[INFO][10:11:30]: [Client #277] Sampler: noniid
[INFO][10:11:30]: [Client #172] Selected by the server.
[INFO][10:11:30]: [Client #172] Loading its data source...
[INFO][10:11:30]: [Client #109] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:11:30]: [Client #172] Dataset size: 60000
[INFO][10:11:30]: [Client #172] Sampler: noniid
[INFO][10:11:30]: [Client #277] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:11:30]: [Client #172] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:11:30]: [93m[1m[Client #277] Started training in communication round #47.[0m
[INFO][10:11:30]: [93m[1m[Client #172] Started training in communication round #47.[0m
[INFO][10:11:30]: [93m[1m[Client #109] Started training in communication round #47.[0m
[INFO][10:11:32]: [Client #277] Loading the dataset.
[INFO][10:11:32]: [Client #109] Loading the dataset.
[INFO][10:11:32]: [Client #172] Loading the dataset.
[INFO][10:11:38]: [Client #277] Epoch: [1/5][0/10]	Loss: 0.052972
[INFO][10:11:38]: [Client #172] Epoch: [1/5][0/10]	Loss: 0.021823
[INFO][10:11:38]: [Client #109] Epoch: [1/5][0/10]	Loss: 0.002548
[INFO][10:11:38]: [Client #277] Epoch: [2/5][0/10]	Loss: 0.001708
[INFO][10:11:38]: [Client #172] Epoch: [2/5][0/10]	Loss: 0.000033
[INFO][10:11:38]: [Client #109] Epoch: [2/5][0/10]	Loss: 0.000294
[INFO][10:11:38]: [Client #172] Epoch: [3/5][0/10]	Loss: 0.000000
[INFO][10:11:38]: [Client #277] Epoch: [3/5][0/10]	Loss: 0.000363
[INFO][10:11:38]: [Client #109] Epoch: [3/5][0/10]	Loss: 0.000005
[INFO][10:11:38]: [Client #172] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:11:38]: [Client #109] Epoch: [4/5][0/10]	Loss: 0.000035
[INFO][10:11:38]: [Client #277] Epoch: [4/5][0/10]	Loss: 0.000003
[INFO][10:11:38]: [Client #172] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:11:38]: [Client #109] Epoch: [5/5][0/10]	Loss: 0.000209
[INFO][10:11:38]: [Client #172] Model saved to /data/ykang/plato/results/test/model/lenet5_172_1127979.pth.
[INFO][10:11:38]: [Client #277] Epoch: [5/5][0/10]	Loss: 0.021955
[INFO][10:11:38]: [Client #109] Model saved to /data/ykang/plato/results/test/model/lenet5_109_1127977.pth.
[INFO][10:11:38]: [Client #277] Model saved to /data/ykang/plato/results/test/model/lenet5_277_1127978.pth.
[INFO][10:11:39]: [Client #172] Loading a model from /data/ykang/plato/results/test/model/lenet5_172_1127979.pth.
[INFO][10:11:39]: [Client #172] Model trained.
[INFO][10:11:39]: [Client #172] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:11:39]: [Server #1127936] Received 0.24 MB of payload data from client #172 (simulated).
[INFO][10:11:39]: [Client #109] Loading a model from /data/ykang/plato/results/test/model/lenet5_109_1127977.pth.
[INFO][10:11:39]: [Client #109] Model trained.
[INFO][10:11:39]: [Client #109] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:11:39]: [Server #1127936] Received 0.24 MB of payload data from client #109 (simulated).
[INFO][10:11:39]: [Client #277] Loading a model from /data/ykang/plato/results/test/model/lenet5_277_1127978.pth.
[INFO][10:11:39]: [Client #277] Model trained.
[INFO][10:11:39]: [Client #277] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:11:39]: [Server #1127936] Received 0.24 MB of payload data from client #277 (simulated).
[INFO][10:11:39]: [Server #1127936] Selecting client #302 for training.
[INFO][10:11:39]: [Server #1127936] Sending the current model to client #302 (simulated).
[INFO][10:11:39]: [Server #1127936] Sending 0.24 MB of payload data to client #302 (simulated).
[INFO][10:11:39]: [Server #1127936] Selecting client #467 for training.
[INFO][10:11:39]: [Server #1127936] Sending the current model to client #467 (simulated).
[INFO][10:11:39]: [Server #1127936] Sending 0.24 MB of payload data to client #467 (simulated).
[INFO][10:11:39]: [Server #1127936] Selecting client #295 for training.
[INFO][10:11:39]: [Server #1127936] Sending the current model to client #295 (simulated).
[INFO][10:11:39]: [Client #302] Selected by the server.
[INFO][10:11:39]: [Client #302] Loading its data source...
[INFO][10:11:39]: [Client #302] Dataset size: 60000
[INFO][10:11:39]: [Client #302] Sampler: noniid
[INFO][10:11:39]: [Server #1127936] Sending 0.24 MB of payload data to client #295 (simulated).
[INFO][10:11:39]: [Client #467] Selected by the server.
[INFO][10:11:39]: [Client #295] Selected by the server.
[INFO][10:11:39]: [Client #467] Loading its data source...
[INFO][10:11:39]: [Client #295] Loading its data source...
[INFO][10:11:39]: [Client #467] Dataset size: 60000
[INFO][10:11:39]: [Client #295] Dataset size: 60000
[INFO][10:11:39]: [Client #467] Sampler: noniid
[INFO][10:11:39]: [Client #295] Sampler: noniid
[INFO][10:11:39]: [Client #302] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:11:39]: [Client #295] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:11:39]: [93m[1m[Client #295] Started training in communication round #47.[0m
[INFO][10:11:39]: [93m[1m[Client #302] Started training in communication round #47.[0m
[INFO][10:11:39]: [Client #467] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:11:39]: [93m[1m[Client #467] Started training in communication round #47.[0m
[INFO][10:11:41]: [Client #467] Loading the dataset.
[INFO][10:11:41]: [Client #295] Loading the dataset.
[INFO][10:11:41]: [Client #302] Loading the dataset.
[INFO][10:11:47]: [Client #295] Epoch: [1/5][0/10]	Loss: 0.202175
[INFO][10:11:47]: [Client #467] Epoch: [1/5][0/10]	Loss: 0.014343
[INFO][10:11:47]: [Client #295] Epoch: [2/5][0/10]	Loss: 0.000518
[INFO][10:11:47]: [Client #302] Epoch: [1/5][0/10]	Loss: 0.002993
[INFO][10:11:47]: [Client #295] Epoch: [3/5][0/10]	Loss: 0.000030
[INFO][10:11:47]: [Client #302] Epoch: [2/5][0/10]	Loss: 0.000366
[INFO][10:11:47]: [Client #467] Epoch: [2/5][0/10]	Loss: 0.000231
[INFO][10:11:47]: [Client #295] Epoch: [4/5][0/10]	Loss: 0.000079
[INFO][10:11:47]: [Client #467] Epoch: [3/5][0/10]	Loss: 0.000386
[INFO][10:11:47]: [Client #295] Epoch: [5/5][0/10]	Loss: 0.000445
[INFO][10:11:47]: [Client #302] Epoch: [3/5][0/10]	Loss: 0.000013
[INFO][10:11:47]: [Client #467] Epoch: [4/5][0/10]	Loss: 0.000281
[INFO][10:11:47]: [Client #295] Model saved to /data/ykang/plato/results/test/model/lenet5_295_1127979.pth.
[INFO][10:11:47]: [Client #302] Epoch: [4/5][0/10]	Loss: 0.000188
[INFO][10:11:47]: [Client #467] Epoch: [5/5][0/10]	Loss: 0.000868
[INFO][10:11:47]: [Client #302] Epoch: [5/5][0/10]	Loss: 0.000029
[INFO][10:11:47]: [Client #467] Model saved to /data/ykang/plato/results/test/model/lenet5_467_1127978.pth.
[INFO][10:11:47]: [Client #302] Model saved to /data/ykang/plato/results/test/model/lenet5_302_1127977.pth.
[INFO][10:11:48]: [Client #295] Loading a model from /data/ykang/plato/results/test/model/lenet5_295_1127979.pth.
[INFO][10:11:48]: [Client #295] Model trained.
[INFO][10:11:48]: [Client #295] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:11:48]: [Server #1127936] Received 0.24 MB of payload data from client #295 (simulated).
[INFO][10:11:48]: [Client #467] Loading a model from /data/ykang/plato/results/test/model/lenet5_467_1127978.pth.
[INFO][10:11:48]: [Client #467] Model trained.
[INFO][10:11:48]: [Client #467] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:11:48]: [Server #1127936] Received 0.24 MB of payload data from client #467 (simulated).
[INFO][10:11:48]: [Client #302] Loading a model from /data/ykang/plato/results/test/model/lenet5_302_1127977.pth.
[INFO][10:11:48]: [Client #302] Model trained.
[INFO][10:11:48]: [Client #302] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:11:48]: [Server #1127936] Received 0.24 MB of payload data from client #302 (simulated).
[INFO][10:11:48]: [Server #1127936] Selecting client #490 for training.
[INFO][10:11:48]: [Server #1127936] Sending the current model to client #490 (simulated).
[INFO][10:11:48]: [Server #1127936] Sending 0.24 MB of payload data to client #490 (simulated).
[INFO][10:11:48]: [Server #1127936] Selecting client #215 for training.
[INFO][10:11:48]: [Server #1127936] Sending the current model to client #215 (simulated).
[INFO][10:11:48]: [Server #1127936] Sending 0.24 MB of payload data to client #215 (simulated).
[INFO][10:11:48]: [Server #1127936] Selecting client #366 for training.
[INFO][10:11:48]: [Server #1127936] Sending the current model to client #366 (simulated).
[INFO][10:11:48]: [Client #490] Selected by the server.
[INFO][10:11:48]: [Client #490] Loading its data source...
[INFO][10:11:48]: [Client #490] Dataset size: 60000
[INFO][10:11:48]: [Client #490] Sampler: noniid
[INFO][10:11:48]: [Server #1127936] Sending 0.24 MB of payload data to client #366 (simulated).
[INFO][10:11:48]: [Client #215] Selected by the server.
[INFO][10:11:48]: [Client #215] Loading its data source...
[INFO][10:11:48]: [Client #215] Dataset size: 60000
[INFO][10:11:48]: [Client #366] Selected by the server.
[INFO][10:11:48]: [Client #215] Sampler: noniid
[INFO][10:11:48]: [Client #366] Loading its data source...
[INFO][10:11:48]: [Client #366] Dataset size: 60000
[INFO][10:11:48]: [Client #366] Sampler: noniid
[INFO][10:11:48]: [Client #490] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:11:48]: [Client #215] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:11:48]: [Client #366] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:11:48]: [93m[1m[Client #215] Started training in communication round #47.[0m
[INFO][10:11:48]: [93m[1m[Client #490] Started training in communication round #47.[0m
[INFO][10:11:48]: [93m[1m[Client #366] Started training in communication round #47.[0m
[INFO][10:11:50]: [Client #366] Loading the dataset.
[INFO][10:11:51]: [Client #490] Loading the dataset.
[INFO][10:11:51]: [Client #215] Loading the dataset.
[INFO][10:11:56]: [Client #215] Epoch: [1/5][0/10]	Loss: 0.057036
[INFO][10:11:56]: [Client #366] Epoch: [1/5][0/10]	Loss: 0.017230
[INFO][10:11:56]: [Client #490] Epoch: [1/5][0/10]	Loss: 0.013804
[INFO][10:11:56]: [Client #215] Epoch: [2/5][0/10]	Loss: 0.004893
[INFO][10:11:57]: [Client #215] Epoch: [3/5][0/10]	Loss: 0.000238
[INFO][10:11:57]: [Client #366] Epoch: [2/5][0/10]	Loss: 0.000764
[INFO][10:11:57]: [Client #490] Epoch: [2/5][0/10]	Loss: 0.040601
[INFO][10:11:57]: [Client #215] Epoch: [4/5][0/10]	Loss: 0.000051
[INFO][10:11:57]: [Client #490] Epoch: [3/5][0/10]	Loss: 0.000594
[INFO][10:11:57]: [Client #366] Epoch: [3/5][0/10]	Loss: 0.000079
[INFO][10:11:57]: [Client #215] Epoch: [5/5][0/10]	Loss: 0.000378
[INFO][10:11:57]: [Client #215] Model saved to /data/ykang/plato/results/test/model/lenet5_215_1127978.pth.
[INFO][10:11:57]: [Client #490] Epoch: [4/5][0/10]	Loss: 0.003588
[INFO][10:11:57]: [Client #366] Epoch: [4/5][0/10]	Loss: 0.000103
[INFO][10:11:57]: [Client #490] Epoch: [5/5][0/10]	Loss: 0.015523
[INFO][10:11:57]: [Client #366] Epoch: [5/5][0/10]	Loss: 0.015231
[INFO][10:11:57]: [Client #490] Model saved to /data/ykang/plato/results/test/model/lenet5_490_1127977.pth.
[INFO][10:11:57]: [Client #366] Model saved to /data/ykang/plato/results/test/model/lenet5_366_1127979.pth.
[INFO][10:11:58]: [Client #215] Loading a model from /data/ykang/plato/results/test/model/lenet5_215_1127978.pth.
[INFO][10:11:58]: [Client #215] Model trained.
[INFO][10:11:58]: [Client #215] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:11:58]: [Server #1127936] Received 0.24 MB of payload data from client #215 (simulated).
[INFO][10:11:58]: [Client #490] Loading a model from /data/ykang/plato/results/test/model/lenet5_490_1127977.pth.
[INFO][10:11:58]: [Client #490] Model trained.
[INFO][10:11:58]: [Client #490] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:11:58]: [Server #1127936] Received 0.24 MB of payload data from client #490 (simulated).
[INFO][10:11:58]: [Client #366] Loading a model from /data/ykang/plato/results/test/model/lenet5_366_1127979.pth.
[INFO][10:11:58]: [Client #366] Model trained.
[INFO][10:11:58]: [Client #366] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:11:58]: [Server #1127936] Received 0.24 MB of payload data from client #366 (simulated).
[INFO][10:11:58]: [Server #1127936] Selecting client #20 for training.
[INFO][10:11:58]: [Server #1127936] Sending the current model to client #20 (simulated).
[INFO][10:11:58]: [Server #1127936] Sending 0.24 MB of payload data to client #20 (simulated).
[INFO][10:11:58]: [Client #20] Selected by the server.
[INFO][10:11:58]: [Client #20] Loading its data source...
[INFO][10:11:58]: [Client #20] Dataset size: 60000
[INFO][10:11:58]: [Client #20] Sampler: noniid
[INFO][10:11:58]: [Client #20] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:11:58]: [93m[1m[Client #20] Started training in communication round #47.[0m
[INFO][10:12:00]: [Client #20] Loading the dataset.
[INFO][10:12:05]: [Client #20] Epoch: [1/5][0/10]	Loss: 0.000278
[INFO][10:12:05]: [Client #20] Epoch: [2/5][0/10]	Loss: 0.000529
[INFO][10:12:05]: [Client #20] Epoch: [3/5][0/10]	Loss: 0.000506
[INFO][10:12:05]: [Client #20] Epoch: [4/5][0/10]	Loss: 0.000821
[INFO][10:12:05]: [Client #20] Epoch: [5/5][0/10]	Loss: 0.000669
[INFO][10:12:05]: [Client #20] Model saved to /data/ykang/plato/results/test/model/lenet5_20_1127977.pth.
[INFO][10:12:06]: [Client #20] Loading a model from /data/ykang/plato/results/test/model/lenet5_20_1127977.pth.
[INFO][10:12:06]: [Client #20] Model trained.
[INFO][10:12:06]: [Client #20] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:12:06]: [Server #1127936] Received 0.24 MB of payload data from client #20 (simulated).
[INFO][10:12:06]: [Server #1127936] Adding client #330 to the list of clients for aggregation.
[INFO][10:12:06]: [Server #1127936] Adding client #404 to the list of clients for aggregation.
[INFO][10:12:06]: [Server #1127936] Adding client #212 to the list of clients for aggregation.
[INFO][10:12:06]: [Server #1127936] Adding client #265 to the list of clients for aggregation.
[INFO][10:12:06]: [Server #1127936] Adding client #242 to the list of clients for aggregation.
[INFO][10:12:06]: [Server #1127936] Adding client #97 to the list of clients for aggregation.
[INFO][10:12:06]: [Server #1127936] Adding client #471 to the list of clients for aggregation.
[INFO][10:12:06]: [Server #1127936] Adding client #215 to the list of clients for aggregation.
[INFO][10:12:06]: [Server #1127936] Adding client #20 to the list of clients for aggregation.
[INFO][10:12:06]: [Server #1127936] Adding client #277 to the list of clients for aggregation.
[INFO][10:12:06]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01390219 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00762453 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01319733 0.         0.         0.00432173 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00245186 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00838201 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00611558 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01077078
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.02307621 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00547519 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 3. 0. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 3. 1. 0. 0. 1. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 0. 0. 0. 0. 1.
 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 2. 1. 6. 0. 1. 2. 1. 0. 1. 1. 0. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0.
 1. 0. 1. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 4. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0.
 1. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 1. 0. 0. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 0. 0. 0. 0. 1. 0. 0. 6. 1. 1.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 2. 0. 1. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 2. 2. 0. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01390219 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00762453 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01319733 0.         0.         0.00432173 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00245186 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00838201 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00611558 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01077078
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.02307621 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00547519 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:12:08]: [Server #1127936] Global model accuracy: 94.55%

[INFO][10:12:08]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_47.pth.
[INFO][10:12:08]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_47.pth.
[INFO][10:12:08]: [93m[1m
[Server #1127936] Starting round 48/100.[0m
[0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1
 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.002 0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  4e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  4e-05  7e-10  7e-10
 6:  6.8876e+00  6.8875e+00  4e-05  4e-10  4e-10
 7:  6.8875e+00  6.8875e+00  4e-05  1e-09  4e-11
 8:  6.8875e+00  6.8875e+00  3e-05  1e-09  4e-11
 9:  6.8875e+00  6.8875e+00  1e-05  4e-09  2e-10
10:  6.8875e+00  6.8875e+00  1e-06  4e-09  2e-10
Optimal solution found.
The calculated probability is:  [5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53830018e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 7.23959246e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 9.72712881e-01
 5.53863229e-05 5.53863229e-05 5.53860020e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.99372034e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 7.46524282e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53856802e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 8.27484550e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 1.79137883e-04 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 6.66576031e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05 5.53863229e-05 5.53863229e-05
 5.53863229e-05 5.53863229e-05]
current clients pool:  [INFO][10:12:09]: [Server #1127936] Selected clients: [212  97 465  37 141 459 384 303 416 447]
[INFO][10:12:09]: [Server #1127936] Selecting client #212 for training.
[INFO][10:12:09]: [Server #1127936] Sending the current model to client #212 (simulated).
[INFO][10:12:09]: [Server #1127936] Sending 0.24 MB of payload data to client #212 (simulated).
[INFO][10:12:09]: [Server #1127936] Selecting client #97 for training.
[INFO][10:12:09]: [Server #1127936] Sending the current model to client #97 (simulated).
[INFO][10:12:09]: [Server #1127936] Sending 0.24 MB of payload data to client #97 (simulated).
[INFO][10:12:09]: [Server #1127936] Selecting client #465 for training.
[INFO][10:12:09]: [Server #1127936] Sending the current model to client #465 (simulated).
[INFO][10:12:09]: [Client #212] Selected by the server.
[INFO][10:12:09]: [Client #212] Loading its data source...
[INFO][10:12:09]: [Client #212] Dataset size: 60000
[INFO][10:12:09]: [Client #212] Sampler: noniid
[INFO][10:12:09]: [Server #1127936] Sending 0.24 MB of payload data to client #465 (simulated).
[INFO][10:12:09]: [Client #97] Selected by the server.
[INFO][10:12:09]: [Client #97] Loading its data source...
[INFO][10:12:09]: [Client #465] Selected by the server.
[INFO][10:12:09]: [Client #97] Dataset size: 60000
[INFO][10:12:09]: [Client #465] Loading its data source...
[INFO][10:12:09]: [Client #97] Sampler: noniid
[INFO][10:12:09]: [Client #465] Dataset size: 60000
[INFO][10:12:09]: [Client #465] Sampler: noniid
[INFO][10:12:09]: [Client #212] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:12:09]: [Client #97] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:12:09]: [Client #465] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:12:09]: [93m[1m[Client #97] Started training in communication round #48.[0m
[INFO][10:12:09]: [93m[1m[Client #465] Started training in communication round #48.[0m
[INFO][10:12:09]: [93m[1m[Client #212] Started training in communication round #48.[0m
[INFO][10:12:11]: [Client #465] Loading the dataset.
[INFO][10:12:11]: [Client #97] Loading the dataset.
[INFO][10:12:11]: [Client #212] Loading the dataset.
[INFO][10:12:16]: [Client #212] Epoch: [1/5][0/10]	Loss: 0.025622
[INFO][10:12:16]: [Client #465] Epoch: [1/5][0/10]	Loss: 0.001714
[INFO][10:12:16]: [Client #97] Epoch: [1/5][0/10]	Loss: 0.006648
[INFO][10:12:17]: [Client #212] Epoch: [2/5][0/10]	Loss: 0.000903
[INFO][10:12:17]: [Client #465] Epoch: [2/5][0/10]	Loss: 0.000420
[INFO][10:12:17]: [Client #212] Epoch: [3/5][0/10]	Loss: 0.000554
[INFO][10:12:17]: [Client #97] Epoch: [2/5][0/10]	Loss: 0.002770
[INFO][10:12:17]: [Client #465] Epoch: [3/5][0/10]	Loss: 0.000053
[INFO][10:12:17]: [Client #212] Epoch: [4/5][0/10]	Loss: 0.002056
[INFO][10:12:17]: [Client #97] Epoch: [3/5][0/10]	Loss: 0.000238
[INFO][10:12:17]: [Client #465] Epoch: [4/5][0/10]	Loss: 0.014203
[INFO][10:12:17]: [Client #97] Epoch: [4/5][0/10]	Loss: 0.000021
[INFO][10:12:17]: [Client #212] Epoch: [5/5][0/10]	Loss: 0.001190
[INFO][10:12:17]: [Client #465] Epoch: [5/5][0/10]	Loss: 0.015857
[INFO][10:12:17]: [Client #212] Model saved to /data/ykang/plato/results/test/model/lenet5_212_1127977.pth.
[INFO][10:12:17]: [Client #465] Model saved to /data/ykang/plato/results/test/model/lenet5_465_1127979.pth.
[INFO][10:12:17]: [Client #97] Epoch: [5/5][0/10]	Loss: 0.000037
[INFO][10:12:17]: [Client #97] Model saved to /data/ykang/plato/results/test/model/lenet5_97_1127978.pth.
[INFO][10:12:18]: [Client #465] Loading a model from /data/ykang/plato/results/test/model/lenet5_465_1127979.pth.
[INFO][10:12:18]: [Client #465] Model trained.
[INFO][10:12:18]: [Client #465] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:12:18]: [Server #1127936] Received 0.24 MB of payload data from client #465 (simulated).
[INFO][10:12:18]: [Client #212] Loading a model from /data/ykang/plato/results/test/model/lenet5_212_1127977.pth.
[INFO][10:12:18]: [Client #212] Model trained.
[INFO][10:12:18]: [Client #212] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:12:18]: [Server #1127936] Received 0.24 MB of payload data from client #212 (simulated).
[INFO][10:12:18]: [Client #97] Loading a model from /data/ykang/plato/results/test/model/lenet5_97_1127978.pth.
[INFO][10:12:18]: [Client #97] Model trained.
[INFO][10:12:18]: [Client #97] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:12:18]: [Server #1127936] Received 0.24 MB of payload data from client #97 (simulated).
[INFO][10:12:18]: [Server #1127936] Selecting client #37 for training.
[INFO][10:12:18]: [Server #1127936] Sending the current model to client #37 (simulated).
[INFO][10:12:18]: [Server #1127936] Sending 0.24 MB of payload data to client #37 (simulated).
[INFO][10:12:18]: [Server #1127936] Selecting client #141 for training.
[INFO][10:12:18]: [Server #1127936] Sending the current model to client #141 (simulated).
[INFO][10:12:18]: [Server #1127936] Sending 0.24 MB of payload data to client #141 (simulated).
[INFO][10:12:18]: [Server #1127936] Selecting client #459 for training.
[INFO][10:12:18]: [Server #1127936] Sending the current model to client #459 (simulated).
[INFO][10:12:18]: [Client #37] Selected by the server.
[INFO][10:12:18]: [Client #37] Loading its data source...
[INFO][10:12:18]: [Client #37] Dataset size: 60000
[INFO][10:12:18]: [Client #37] Sampler: noniid
[INFO][10:12:18]: [Server #1127936] Sending 0.24 MB of payload data to client #459 (simulated).
[INFO][10:12:18]: [Client #141] Selected by the server.
[INFO][10:12:18]: [Client #141] Loading its data source...
[INFO][10:12:18]: [Client #141] Dataset size: 60000
[INFO][10:12:18]: [Client #141] Sampler: noniid
[INFO][10:12:18]: [Client #459] Selected by the server.
[INFO][10:12:18]: [Client #459] Loading its data source...
[INFO][10:12:18]: [Client #459] Dataset size: 60000
[INFO][10:12:18]: [Client #459] Sampler: noniid
[INFO][10:12:18]: [Client #37] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:12:18]: [Client #141] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:12:18]: [Client #459] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:12:18]: [93m[1m[Client #37] Started training in communication round #48.[0m
[INFO][10:12:18]: [93m[1m[Client #141] Started training in communication round #48.[0m
[INFO][10:12:18]: [93m[1m[Client #459] Started training in communication round #48.[0m
[INFO][10:12:20]: [Client #459] Loading the dataset.
[INFO][10:12:20]: [Client #141] Loading the dataset.
[INFO][10:12:20]: [Client #37] Loading the dataset.
[INFO][10:12:26]: [Client #141] Epoch: [1/5][0/10]	Loss: 0.010049
[INFO][10:12:26]: [Client #37] Epoch: [1/5][0/10]	Loss: 0.009663
[INFO][10:12:26]: [Client #141] Epoch: [2/5][0/10]	Loss: 0.004835
[INFO][10:12:26]: [Client #459] Epoch: [1/5][0/10]	Loss: 0.004863
[INFO][10:12:26]: [Client #37] Epoch: [2/5][0/10]	Loss: 0.006175
[INFO][10:12:26]: [Client #141] Epoch: [3/5][0/10]	Loss: 0.000187
[INFO][10:12:26]: [Client #459] Epoch: [2/5][0/10]	Loss: 0.003143
[INFO][10:12:26]: [Client #37] Epoch: [3/5][0/10]	Loss: 0.000120
[INFO][10:12:26]: [Client #141] Epoch: [4/5][0/10]	Loss: 0.000335
[INFO][10:12:26]: [Client #459] Epoch: [3/5][0/10]	Loss: 0.000058
[INFO][10:12:26]: [Client #37] Epoch: [4/5][0/10]	Loss: 0.000004
[INFO][10:12:26]: [Client #141] Epoch: [5/5][0/10]	Loss: 0.015829
[INFO][10:12:26]: [Client #459] Epoch: [4/5][0/10]	Loss: 0.000009
[INFO][10:12:26]: [Client #141] Model saved to /data/ykang/plato/results/test/model/lenet5_141_1127978.pth.
[INFO][10:12:26]: [Client #37] Epoch: [5/5][0/10]	Loss: 0.002020
[INFO][10:12:26]: [Client #37] Model saved to /data/ykang/plato/results/test/model/lenet5_37_1127977.pth.
[INFO][10:12:26]: [Client #459] Epoch: [5/5][0/10]	Loss: 0.000198
[INFO][10:12:26]: [Client #459] Model saved to /data/ykang/plato/results/test/model/lenet5_459_1127979.pth.
[INFO][10:12:27]: [Client #141] Loading a model from /data/ykang/plato/results/test/model/lenet5_141_1127978.pth.
[INFO][10:12:27]: [Client #141] Model trained.
[INFO][10:12:27]: [Client #141] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:12:27]: [Server #1127936] Received 0.24 MB of payload data from client #141 (simulated).
[INFO][10:12:27]: [Client #37] Loading a model from /data/ykang/plato/results/test/model/lenet5_37_1127977.pth.
[INFO][10:12:27]: [Client #37] Model trained.
[INFO][10:12:27]: [Client #37] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:12:27]: [Server #1127936] Received 0.24 MB of payload data from client #37 (simulated).
[INFO][10:12:27]: [Client #459] Loading a model from /data/ykang/plato/results/test/model/lenet5_459_1127979.pth.
[INFO][10:12:27]: [Client #459] Model trained.
[INFO][10:12:27]: [Client #459] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:12:27]: [Server #1127936] Received 0.24 MB of payload data from client #459 (simulated).
[INFO][10:12:27]: [Server #1127936] Selecting client #384 for training.
[INFO][10:12:27]: [Server #1127936] Sending the current model to client #384 (simulated).
[INFO][10:12:27]: [Server #1127936] Sending 0.24 MB of payload data to client #384 (simulated).
[INFO][10:12:27]: [Server #1127936] Selecting client #303 for training.
[INFO][10:12:27]: [Server #1127936] Sending the current model to client #303 (simulated).
[INFO][10:12:27]: [Server #1127936] Sending 0.24 MB of payload data to client #303 (simulated).
[INFO][10:12:27]: [Server #1127936] Selecting client #416 for training.
[INFO][10:12:27]: [Server #1127936] Sending the current model to client #416 (simulated).
[INFO][10:12:27]: [Client #384] Selected by the server.
[INFO][10:12:27]: [Client #384] Loading its data source...
[INFO][10:12:27]: [Client #384] Dataset size: 60000
[INFO][10:12:27]: [Client #384] Sampler: noniid
[INFO][10:12:27]: [Server #1127936] Sending 0.24 MB of payload data to client #416 (simulated).
[INFO][10:12:27]: [Client #303] Selected by the server.
[INFO][10:12:27]: [Client #303] Loading its data source...
[INFO][10:12:27]: [Client #416] Selected by the server.
[INFO][10:12:27]: [Client #303] Dataset size: 60000
[INFO][10:12:27]: [Client #416] Loading its data source...
[INFO][10:12:27]: [Client #303] Sampler: noniid
[INFO][10:12:27]: [Client #416] Dataset size: 60000
[INFO][10:12:27]: [Client #416] Sampler: noniid
[INFO][10:12:27]: [Client #384] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:12:27]: [Client #303] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:12:27]: [Client #416] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:12:27]: [93m[1m[Client #384] Started training in communication round #48.[0m
[INFO][10:12:27]: [93m[1m[Client #303] Started training in communication round #48.[0m
[INFO][10:12:27]: [93m[1m[Client #416] Started training in communication round #48.[0m
[INFO][10:12:29]: [Client #384] Loading the dataset.
[INFO][10:12:29]: [Client #416] Loading the dataset.
[INFO][10:12:29]: [Client #303] Loading the dataset.
[INFO][10:12:35]: [Client #384] Epoch: [1/5][0/10]	Loss: 0.016602
[INFO][10:12:35]: [Client #416] Epoch: [1/5][0/10]	Loss: 0.003778
[INFO][10:12:35]: [Client #303] Epoch: [1/5][0/10]	Loss: 0.002167
[INFO][10:12:35]: [Client #384] Epoch: [2/5][0/10]	Loss: 0.000926
[INFO][10:12:35]: [Client #416] Epoch: [2/5][0/10]	Loss: 0.000244
[INFO][10:12:35]: [Client #303] Epoch: [2/5][0/10]	Loss: 0.007711
[INFO][10:12:35]: [Client #416] Epoch: [3/5][0/10]	Loss: 0.000959
[INFO][10:12:35]: [Client #384] Epoch: [3/5][0/10]	Loss: 0.000754
[INFO][10:12:35]: [Client #303] Epoch: [3/5][0/10]	Loss: 0.000077
[INFO][10:12:35]: [Client #384] Epoch: [4/5][0/10]	Loss: 0.000962
[INFO][10:12:36]: [Client #416] Epoch: [4/5][0/10]	Loss: 0.003653
[INFO][10:12:36]: [Client #303] Epoch: [4/5][0/10]	Loss: 0.002187
[INFO][10:12:36]: [Client #384] Epoch: [5/5][0/10]	Loss: 0.023375
[INFO][10:12:36]: [Client #303] Epoch: [5/5][0/10]	Loss: 0.030712
[INFO][10:12:36]: [Client #416] Epoch: [5/5][0/10]	Loss: 0.000018
[INFO][10:12:36]: [Client #384] Model saved to /data/ykang/plato/results/test/model/lenet5_384_1127977.pth.
[INFO][10:12:36]: [Client #416] Model saved to /data/ykang/plato/results/test/model/lenet5_416_1127979.pth.
[INFO][10:12:36]: [Client #303] Model saved to /data/ykang/plato/results/test/model/lenet5_303_1127978.pth.
[INFO][10:12:36]: [Client #384] Loading a model from /data/ykang/plato/results/test/model/lenet5_384_1127977.pth.
[INFO][10:12:36]: [Client #384] Model trained.
[INFO][10:12:36]: [Client #384] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:12:36]: [Server #1127936] Received 0.24 MB of payload data from client #384 (simulated).
[INFO][10:12:37]: [Client #416] Loading a model from /data/ykang/plato/results/test/model/lenet5_416_1127979.pth.
[INFO][10:12:37]: [Client #416] Model trained.
[INFO][10:12:37]: [Client #416] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:12:37]: [Server #1127936] Received 0.24 MB of payload data from client #416 (simulated).
[INFO][10:12:37]: [Client #303] Loading a model from /data/ykang/plato/results/test/model/lenet5_303_1127978.pth.
[INFO][10:12:37]: [Client #303] Model trained.
[INFO][10:12:37]: [Client #303] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:12:37]: [Server #1127936] Received 0.24 MB of payload data from client #303 (simulated).
[INFO][10:12:37]: [Server #1127936] Selecting client #447 for training.
[INFO][10:12:37]: [Server #1127936] Sending the current model to client #447 (simulated).
[INFO][10:12:37]: [Server #1127936] Sending 0.24 MB of payload data to client #447 (simulated).
[INFO][10:12:37]: [Client #447] Selected by the server.
[INFO][10:12:37]: [Client #447] Loading its data source...
[INFO][10:12:37]: [Client #447] Dataset size: 60000
[INFO][10:12:37]: [Client #447] Sampler: noniid
[INFO][10:12:37]: [Client #447] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:12:37]: [93m[1m[Client #447] Started training in communication round #48.[0m
[INFO][10:12:38]: [Client #447] Loading the dataset.
[INFO][10:12:44]: [Client #447] Epoch: [1/5][0/10]	Loss: 0.040830
[INFO][10:12:44]: [Client #447] Epoch: [2/5][0/10]	Loss: 0.002642
[INFO][10:12:44]: [Client #447] Epoch: [3/5][0/10]	Loss: 0.000087
[INFO][10:12:44]: [Client #447] Epoch: [4/5][0/10]	Loss: 0.003938
[INFO][10:12:44]: [Client #447] Epoch: [5/5][0/10]	Loss: 0.097276
[INFO][10:12:44]: [Client #447] Model saved to /data/ykang/plato/results/test/model/lenet5_447_1127977.pth.
[INFO][10:12:45]: [Client #447] Loading a model from /data/ykang/plato/results/test/model/lenet5_447_1127977.pth.
[INFO][10:12:45]: [Client #447] Model trained.
[INFO][10:12:45]: [Client #447] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:12:45]: [Server #1127936] Received 0.24 MB of payload data from client #447 (simulated).
[INFO][10:12:45]: [Server #1127936] Adding client #490 to the list of clients for aggregation.
[INFO][10:12:45]: [Server #1127936] Adding client #295 to the list of clients for aggregation.
[INFO][10:12:45]: [Server #1127936] Adding client #366 to the list of clients for aggregation.
[INFO][10:12:45]: [Server #1127936] Adding client #109 to the list of clients for aggregation.
[INFO][10:12:45]: [Server #1127936] Adding client #302 to the list of clients for aggregation.
[INFO][10:12:45]: [Server #1127936] Adding client #384 to the list of clients for aggregation.
[INFO][10:12:45]: [Server #1127936] Adding client #459 to the list of clients for aggregation.
[INFO][10:12:45]: [Server #1127936] Adding client #416 to the list of clients for aggregation.
[INFO][10:12:45]: [Server #1127936] Adding client #141 to the list of clients for aggregation.
[INFO][10:12:45]: [Server #1127936] Adding client #37 to the list of clients for aggregation.
[INFO][10:12:45]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 296, 297, 298, 299, 300, 301, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00292296 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00668083 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00563917 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.0077468  0.         0.         0.         0.         0.
 0.         0.01097872 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01381725
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00606004
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01102472 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00260669 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00833425 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 3. 0. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 3. 1. 0. 0. 0. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 0. 0. 0. 0. 1.
 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 2. 1. 6. 0. 1. 2. 1. 0. 1. 1. 0. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0.
 1. 0. 1. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 4. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0.
 1. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 1. 0. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 0. 0. 0. 0. 1. 0. 0. 6. 1. 1.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 2. 0. 1. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 2. 2. 0. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00292296 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00668083 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00563917 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.0077468  0.         0.         0.         0.         0.
 0.         0.01097872 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01381725
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00606004
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01102472 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00260669 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00833425 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:12:47]: [Server #1127936] Global model accuracy: 95.94%

[INFO][10:12:47]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_48.pth.
[INFO][10:12:47]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_48.pth.
[INFO][10:12:47]: [93m[1m
[Server #1127936] Starting round 49/100.[0m
[INFO][10:12:47]: [Server #1127936] Selected clients: [463 498 196 100 104  21 461 133 366 248]
[INFO][10:12:47]: [Server #1127936] Selecting client #463 for training.
[INFO][10:12:47]: [Server #1127936] Sending the current model to client #463 (simulated).
[INFO][10:12:47]: [Server #1127936] Sending 0.24 MB of payload data to client #463 (simulated).
[INFO][10:12:47]: [Server #1127936] Selecting client #498 for training.
[INFO][10:12:47]: [Server #1127936] Sending the current model to client #498 (simulated).
[INFO][10:12:47]: [Server #1127936] Sending 0.24 MB of payload data to client #498 (simulated).
[INFO][10:12:47]: [Client #463] Selected by the server.
[INFO][10:12:47]: [Client #463] Loading its data source...
[INFO][10:12:47]: [Client #463] Dataset size: 60000
[INFO][10:12:47]: [Client #463] Sampler: noniid
[INFO][10:12:47]: [Client #463] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:12:47]: [93m[1m[Client #463] Started training in communication round #49.[0m
[INFO][10:12:47]: [Server #1127936] Selecting client #196 for training.
[INFO][10:12:47]: [Server #1127936] Sending the current model to client #196 (simulated).
[INFO][10:12:47]: [Server #1127936] Sending 0.24 MB of payload data to client #196 (simulated).
[INFO][10:12:47]: [Client #498] Selected by the server.
[INFO][10:12:47]: [Client #498] Loading its data source...
[INFO][10:12:47]: [Client #498] Dataset size: 60000
[INFO][10:12:47]: [Client #498] Sampler: noniid
[INFO][10:12:47]: [Client #196] Selected by the server.
[INFO][10:12:47]: [Client #196] Loading its data source...
[INFO][10:12:47]: [Client #196] Dataset size: 60000
[INFO][10:12:47]: [Client #196] Sampler: noniid
[INFO][10:12:47]: [Client #498] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:12:47]: [Client #196] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:12:47]: [93m[1m[Client #498] Started training in communication round #49.[0m
[INFO][10:12:47]: [93m[1m[Client #196] Started training in communication round #49.[0m
[INFO][10:12:49]: [Client #498] Loading the dataset.
[INFO][10:12:49]: [Client #196] Loading the dataset.
[INFO][10:12:49]: [Client #463] Loading the dataset.
[INFO][10:12:55]: [Client #463] Epoch: [1/5][0/10]	Loss: 0.009724
[INFO][10:12:55]: [Client #498] Epoch: [1/5][0/10]	Loss: 0.001168
[INFO][10:12:55]: [Client #196] Epoch: [1/5][0/10]	Loss: 0.014524
[INFO][10:12:55]: [Client #463] Epoch: [2/5][0/10]	Loss: 0.015372
[INFO][10:12:55]: [Client #498] Epoch: [2/5][0/10]	Loss: 0.002525
[INFO][10:12:55]: [Client #463] Epoch: [3/5][0/10]	Loss: 0.000539
[INFO][10:12:55]: [Client #196] Epoch: [2/5][0/10]	Loss: 0.000219
[INFO][10:12:56]: [Client #498] Epoch: [3/5][0/10]	Loss: 0.000078
[INFO][10:12:56]: [Client #463] Epoch: [4/5][0/10]	Loss: 0.001660
[INFO][10:12:56]: [Client #498] Epoch: [4/5][0/10]	Loss: 0.000019
[INFO][10:12:56]: [Client #196] Epoch: [3/5][0/10]	Loss: 0.000006
[INFO][10:12:56]: [Client #463] Epoch: [5/5][0/10]	Loss: 0.004559
[INFO][10:12:56]: [Client #463] Model saved to /data/ykang/plato/results/test/model/lenet5_463_1127977.pth.
[INFO][10:12:56]: [Client #498] Epoch: [5/5][0/10]	Loss: 0.000026
[INFO][10:12:56]: [Client #196] Epoch: [4/5][0/10]	Loss: 0.000004
[INFO][10:12:56]: [Client #498] Model saved to /data/ykang/plato/results/test/model/lenet5_498_1127978.pth.
[INFO][10:12:56]: [Client #196] Epoch: [5/5][0/10]	Loss: 0.006413
[INFO][10:12:56]: [Client #196] Model saved to /data/ykang/plato/results/test/model/lenet5_196_1127979.pth.
[INFO][10:12:56]: [Client #463] Loading a model from /data/ykang/plato/results/test/model/lenet5_463_1127977.pth.
[INFO][10:12:56]: [Client #463] Model trained.
[INFO][10:12:56]: [Client #463] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:12:56]: [Server #1127936] Received 0.24 MB of payload data from client #463 (simulated).
[INFO][10:12:57]: [Client #498] Loading a model from /data/ykang/plato/results/test/model/lenet5_498_1127978.pth.
[INFO][10:12:57]: [Client #498] Model trained.
[INFO][10:12:57]: [Client #498] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:12:57]: [Server #1127936] Received 0.24 MB of payload data from client #498 (simulated).
[INFO][10:12:57]: [Client #196] Loading a model from /data/ykang/plato/results/test/model/lenet5_196_1127979.pth.
[INFO][10:12:57]: [Client #196] Model trained.
[INFO][10:12:57]: [Client #196] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:12:57]: [Server #1127936] Received 0.24 MB of payload data from client #196 (simulated).
[INFO][10:12:57]: [Server #1127936] Selecting client #100 for training.
[INFO][10:12:57]: [Server #1127936] Sending the current model to client #100 (simulated).
[INFO][10:12:57]: [Server #1127936] Sending 0.24 MB of payload data to client #100 (simulated).
[INFO][10:12:57]: [Server #1127936] Selecting client #104 for training.
[INFO][10:12:57]: [Server #1127936] Sending the current model to client #104 (simulated).
[INFO][10:12:57]: [Server #1127936] Sending 0.24 MB of payload data to client #104 (simulated).
[INFO][10:12:57]: [Server #1127936] Selecting client #21 for training.
[INFO][10:12:57]: [Server #1127936] Sending the current model to client #21 (simulated).
[INFO][10:12:57]: [Client #100] Selected by the server.
[INFO][10:12:57]: [Client #100] Loading its data source...
[INFO][10:12:57]: [Client #100] Dataset size: 60000
[INFO][10:12:57]: [Client #100] Sampler: noniid
[INFO][10:12:57]: [Server #1127936] Sending 0.24 MB of payload data to client #21 (simulated).
[INFO][10:12:57]: [Client #21] Selected by the server.
[INFO][10:12:57]: [Client #21] Loading its data source...
[INFO][10:12:57]: [Client #104] Selected by the server.
[INFO][10:12:57]: [Client #21] Dataset size: 60000
[INFO][10:12:57]: [Client #21] Sampler: noniid
[INFO][10:12:57]: [Client #104] Loading its data source...
[INFO][10:12:57]: [Client #104] Dataset size: 60000
[INFO][10:12:57]: [Client #104] Sampler: noniid
[INFO][10:12:57]: [Client #100] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:12:57]: [Client #21] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:12:57]: [Client #104] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:12:57]: [93m[1m[Client #100] Started training in communication round #49.[0m
[INFO][10:12:57]: [93m[1m[Client #104] Started training in communication round #49.[0m
[INFO][10:12:57]: [93m[1m[Client #21] Started training in communication round #49.[0m
[INFO][10:12:59]: [Client #104] Loading the dataset.
[INFO][10:12:59]: [Client #100] Loading the dataset.
[INFO][10:12:59]: [Client #21] Loading the dataset.
[INFO][10:13:05]: [Client #104] Epoch: [1/5][0/10]	Loss: 0.000840
[INFO][10:13:05]: [Client #100] Epoch: [1/5][0/10]	Loss: 0.010855
[INFO][10:13:05]: [Client #104] Epoch: [2/5][0/10]	Loss: 0.000098
[INFO][10:13:05]: [Client #21] Epoch: [1/5][0/10]	Loss: 0.010855
[INFO][10:13:05]: [Client #100] Epoch: [2/5][0/10]	Loss: 0.001765
[INFO][10:13:05]: [Client #21] Epoch: [2/5][0/10]	Loss: 0.001185
[INFO][10:13:05]: [Client #104] Epoch: [3/5][0/10]	Loss: 0.000000
[INFO][10:13:05]: [Client #100] Epoch: [3/5][0/10]	Loss: 0.000040
[INFO][10:13:05]: [Client #21] Epoch: [3/5][0/10]	Loss: 0.000002
[INFO][10:13:05]: [Client #100] Epoch: [4/5][0/10]	Loss: 0.000483
[INFO][10:13:05]: [Client #104] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:13:05]: [Client #100] Epoch: [5/5][0/10]	Loss: 0.000400
[INFO][10:13:05]: [Client #21] Epoch: [4/5][0/10]	Loss: 0.000184
[INFO][10:13:05]: [Client #104] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:13:05]: [Client #100] Model saved to /data/ykang/plato/results/test/model/lenet5_100_1127977.pth.
[INFO][10:13:05]: [Client #104] Model saved to /data/ykang/plato/results/test/model/lenet5_104_1127978.pth.
[INFO][10:13:05]: [Client #21] Epoch: [5/5][0/10]	Loss: 0.000001
[INFO][10:13:05]: [Client #21] Model saved to /data/ykang/plato/results/test/model/lenet5_21_1127979.pth.
[INFO][10:13:06]: [Client #104] Loading a model from /data/ykang/plato/results/test/model/lenet5_104_1127978.pth.
[INFO][10:13:06]: [Client #104] Model trained.
[INFO][10:13:06]: [Client #104] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:13:06]: [Server #1127936] Received 0.24 MB of payload data from client #104 (simulated).
[INFO][10:13:06]: [Client #100] Loading a model from /data/ykang/plato/results/test/model/lenet5_100_1127977.pth.
[INFO][10:13:06]: [Client #100] Model trained.
[INFO][10:13:06]: [Client #100] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:13:06]: [Server #1127936] Received 0.24 MB of payload data from client #100 (simulated).
[INFO][10:13:06]: [Client #21] Loading a model from /data/ykang/plato/results/test/model/lenet5_21_1127979.pth.
[INFO][10:13:06]: [Client #21] Model trained.
[INFO][10:13:06]: [Client #21] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:13:06]: [Server #1127936] Received 0.24 MB of payload data from client #21 (simulated).
[INFO][10:13:06]: [Server #1127936] Selecting client #461 for training.
[INFO][10:13:06]: [Server #1127936] Sending the current model to client #461 (simulated).
[INFO][10:13:06]: [Server #1127936] Sending 0.24 MB of payload data to client #461 (simulated).
[INFO][10:13:06]: [Server #1127936] Selecting client #133 for training.
[INFO][10:13:06]: [Server #1127936] Sending the current model to client #133 (simulated).
[INFO][10:13:06]: [Server #1127936] Sending 0.24 MB of payload data to client #133 (simulated).
[INFO][10:13:06]: [Server #1127936] Selecting client #366 for training.
[INFO][10:13:06]: [Server #1127936] Sending the current model to client #366 (simulated).
[INFO][10:13:06]: [Client #461] Selected by the server.
[INFO][10:13:06]: [Client #461] Loading its data source...
[INFO][10:13:06]: [Client #461] Dataset size: 60000
[INFO][10:13:06]: [Client #461] Sampler: noniid
[INFO][10:13:06]: [Server #1127936] Sending 0.24 MB of payload data to client #366 (simulated).
[INFO][10:13:06]: [Client #133] Selected by the server.
[INFO][10:13:06]: [Client #366] Selected by the server.
[INFO][10:13:06]: [Client #133] Loading its data source...
[INFO][10:13:06]: [Client #366] Loading its data source...
[INFO][10:13:06]: [Client #366] Dataset size: 60000
[INFO][10:13:06]: [Client #133] Dataset size: 60000
[INFO][10:13:06]: [Client #366] Sampler: noniid
[INFO][10:13:06]: [Client #133] Sampler: noniid
[INFO][10:13:06]: [Client #461] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:13:06]: [Client #133] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:13:06]: [93m[1m[Client #133] Started training in communication round #49.[0m
[INFO][10:13:06]: [93m[1m[Client #461] Started training in communication round #49.[0m
[INFO][10:13:06]: [Client #366] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:13:06]: [93m[1m[Client #366] Started training in communication round #49.[0m
[INFO][10:13:08]: [Client #461] Loading the dataset.
[INFO][10:13:08]: [Client #133] Loading the dataset.
[INFO][10:13:08]: [Client #366] Loading the dataset.
[INFO][10:13:15]: [Client #366] Epoch: [1/5][0/10]	Loss: 0.011509
[INFO][10:13:15]: [Client #133] Epoch: [1/5][0/10]	Loss: 0.009994
[INFO][10:13:15]: [Client #461] Epoch: [1/5][0/10]	Loss: 0.008169
[INFO][10:13:15]: [Client #366] Epoch: [2/5][0/10]	Loss: 0.000502
[INFO][10:13:15]: [Client #133] Epoch: [2/5][0/10]	Loss: 0.000002
[INFO][10:13:15]: [Client #461] Epoch: [2/5][0/10]	Loss: 0.000084
[INFO][10:13:15]: [Client #366] Epoch: [3/5][0/10]	Loss: 0.000170
[INFO][10:13:16]: [Client #133] Epoch: [3/5][0/10]	Loss: 0.000544
[INFO][10:13:16]: [Client #461] Epoch: [3/5][0/10]	Loss: 0.003809
[INFO][10:13:16]: [Client #366] Epoch: [4/5][0/10]	Loss: 0.000017
[INFO][10:13:16]: [Client #461] Epoch: [4/5][0/10]	Loss: 0.008791
[INFO][10:13:16]: [Client #133] Epoch: [4/5][0/10]	Loss: 0.010241
[INFO][10:13:16]: [Client #366] Epoch: [5/5][0/10]	Loss: 0.004914
[INFO][10:13:16]: [Client #366] Model saved to /data/ykang/plato/results/test/model/lenet5_366_1127979.pth.
[INFO][10:13:16]: [Client #461] Epoch: [5/5][0/10]	Loss: 0.018177
[INFO][10:13:16]: [Client #133] Epoch: [5/5][0/10]	Loss: 0.000395
[INFO][10:13:16]: [Client #461] Model saved to /data/ykang/plato/results/test/model/lenet5_461_1127977.pth.
[INFO][10:13:16]: [Client #133] Model saved to /data/ykang/plato/results/test/model/lenet5_133_1127978.pth.
[INFO][10:13:17]: [Client #366] Loading a model from /data/ykang/plato/results/test/model/lenet5_366_1127979.pth.
[INFO][10:13:17]: [Client #366] Model trained.
[INFO][10:13:17]: [Client #366] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:13:17]: [Server #1127936] Received 0.24 MB of payload data from client #366 (simulated).
[INFO][10:13:17]: [Client #133] Loading a model from /data/ykang/plato/results/test/model/lenet5_133_1127978.pth.
[INFO][10:13:17]: [Client #133] Model trained.
[INFO][10:13:17]: [Client #133] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:13:17]: [Server #1127936] Received 0.24 MB of payload data from client #133 (simulated).
[INFO][10:13:17]: [Client #461] Loading a model from /data/ykang/plato/results/test/model/lenet5_461_1127977.pth.
[INFO][10:13:17]: [Client #461] Model trained.
[INFO][10:13:17]: [Client #461] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:13:17]: [Server #1127936] Received 0.24 MB of payload data from client #461 (simulated).
[INFO][10:13:17]: [Server #1127936] Selecting client #248 for training.
[INFO][10:13:17]: [Server #1127936] Sending the current model to client #248 (simulated).
[INFO][10:13:17]: [Server #1127936] Sending 0.24 MB of payload data to client #248 (simulated).
[INFO][10:13:17]: [Client #248] Selected by the server.
[INFO][10:13:17]: [Client #248] Loading its data source...
[INFO][10:13:17]: [Client #248] Dataset size: 60000
[INFO][10:13:17]: [Client #248] Sampler: noniid
[INFO][10:13:17]: [Client #248] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:13:17]: [93m[1m[Client #248] Started training in communication round #49.[0m
[INFO][10:13:19]: [Client #248] Loading the dataset.
[INFO][10:13:25]: [Client #248] Epoch: [1/5][0/10]	Loss: 0.007203
[INFO][10:13:25]: [Client #248] Epoch: [2/5][0/10]	Loss: 0.000821
[INFO][10:13:25]: [Client #248] Epoch: [3/5][0/10]	Loss: 0.000037
[INFO][10:13:25]: [Client #248] Epoch: [4/5][0/10]	Loss: 0.000001
[INFO][10:13:25]: [Client #248] Epoch: [5/5][0/10]	Loss: 0.000919
[INFO][10:13:25]: [Client #248] Model saved to /data/ykang/plato/results/test/model/lenet5_248_1127977.pth.
[INFO][10:13:26]: [Client #248] Loading a model from /data/ykang/plato/results/test/model/lenet5_248_1127977.pth.
[INFO][10:13:26]: [Client #248] Model trained.
[INFO][10:13:26]: [Client #248] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:13:26]: [Server #1127936] Received 0.24 MB of payload data from client #248 (simulated).
[INFO][10:13:26]: [Server #1127936] Adding client #303 to the list of clients for aggregation.
[INFO][10:13:26]: [Server #1127936] Adding client #447 to the list of clients for aggregation.
[INFO][10:13:26]: [Server #1127936] Adding client #465 to the list of clients for aggregation.
[INFO][10:13:26]: [Server #1127936] Adding client #41 to the list of clients for aggregation.
[INFO][10:13:26]: [Server #1127936] Adding client #454 to the list of clients for aggregation.
[INFO][10:13:26]: [Server #1127936] Adding client #97 to the list of clients for aggregation.
[INFO][10:13:26]: [Server #1127936] Adding client #461 to the list of clients for aggregation.
[INFO][10:13:26]: [Server #1127936] Adding client #463 to the list of clients for aggregation.
[INFO][10:13:26]: [Server #1127936] Adding client #104 to the list of clients for aggregation.
[INFO][10:13:26]: [Server #1127936] Adding client #248 to the list of clients for aggregation.
[INFO][10:13:26]: [Server #1127936] Aggregating 10 clients in total.
[0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1
 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.002
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  2e-05  3e-10  3e-10
 6:  6.8876e+00  6.8875e+00  1e-05  2e-10  2e-10
 7:  6.8875e+00  6.8875e+00  1e-05  3e-10  4e-12
 8:  6.8875e+00  6.8875e+00  8e-06  2e-10  3e-12
 9:  6.8875e+00  6.8875e+00  5e-06  3e-10  5e-12
Optimal solution found.
The calculated probability is:  [0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072621 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00146447 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.0007262
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00173637 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00379744 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.63885369
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.0007262
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072615 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00193165
 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622 0.00072622
 0.00072622 0.00072622 0.00072622 0.00072622]
current clients pool:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 448, 449, 450, 451, 452, 453, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 466, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01335813 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00619523 0.         0.         0.         0.         0.
 0.         0.0149704  0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00341383 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00407432 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01752619 0.         0.         0.
 0.         0.         0.         0.00838375 0.         0.
 0.         0.         0.         0.         0.00898333 0.
 0.00877118 0.         0.0058507  0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 3. 0. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 3. 1. 0. 0. 0. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 0. 0. 0. 0. 1.
 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 2. 1. 6. 0. 1. 2. 1. 0. 1. 1. 0. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0.
 1. 0. 1. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 4. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0.
 1. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 1. 0. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 0. 0. 0. 1. 0. 0. 6. 1. 1.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 2. 0. 1. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 2. 2. 0. 1. 0.]
local_gradient_bounds:  [INFO][10:13:28]: [Server #1127936] Global model accuracy: 95.42%

[INFO][10:13:28]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_49.pth.
[INFO][10:13:28]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_49.pth.
[INFO][10:13:28]: [93m[1m
[Server #1127936] Starting round 50/100.[0m
[0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01335813 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00619523 0.         0.         0.         0.         0.
 0.         0.0149704  0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00341383 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00407432 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01752619 0.         0.         0.
 0.         0.         0.         0.00838375 0.         0.
 0.         0.         0.         0.         0.00898333 0.
 0.00877118 0.         0.0058507  0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1
 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.002
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  4e-04  7e-09  7e-09
 5:  6.8876e+00  6.8875e+00  8e-05  1e-09  1e-09
 6:  6.8875e+00  6.8875e+00  8e-05  6e-10  6e-10
 7:  6.8875e+00  6.8875e+00  7e-05  3e-09  2e-10
 8:  6.8875e+00  6.8875e+00  6e-05  3e-09  3e-10
 9:  6.8875e+00  6.8875e+00  2e-05  3e-08  2e-09
10:  6.8875e+00  6.8875e+00  7e-06  1e-08  1e-09
Optimal solution found.
The calculated probability is:  [1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 9.13777666e-01
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.92523483e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75186684e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192097e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.86220864e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 2.34879181e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 6.24130925e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75190338e-04 1.75192394e-04 1.75190434e-04 1.75192394e-04
 1.91471107e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04 1.75192394e-04 1.75192394e-04
 1.75192394e-04 1.75192394e-04]
current clients pool:  [INFO][10:13:28]: [Server #1127936] Selected clients: [ 41 389 168 401 165 157 463 488 266 476]
[INFO][10:13:28]: [Server #1127936] Selecting client #41 for training.
[INFO][10:13:28]: [Server #1127936] Sending the current model to client #41 (simulated).
[INFO][10:13:28]: [Server #1127936] Sending 0.24 MB of payload data to client #41 (simulated).
[INFO][10:13:28]: [Server #1127936] Selecting client #389 for training.
[INFO][10:13:28]: [Server #1127936] Sending the current model to client #389 (simulated).
[INFO][10:13:28]: [Server #1127936] Sending 0.24 MB of payload data to client #389 (simulated).
[INFO][10:13:28]: [Server #1127936] Selecting client #168 for training.
[INFO][10:13:28]: [Server #1127936] Sending the current model to client #168 (simulated).
[INFO][10:13:28]: [Client #41] Selected by the server.
[INFO][10:13:28]: [Client #41] Loading its data source...
[INFO][10:13:28]: [Client #41] Dataset size: 60000
[INFO][10:13:28]: [Client #41] Sampler: noniid
[INFO][10:13:28]: [Server #1127936] Sending 0.24 MB of payload data to client #168 (simulated).
[INFO][10:13:28]: [Client #389] Selected by the server.
[INFO][10:13:28]: [Client #389] Loading its data source...
[INFO][10:13:28]: [Client #389] Dataset size: 60000
[INFO][10:13:28]: [Client #389] Sampler: noniid
[INFO][10:13:28]: [Client #168] Selected by the server.
[INFO][10:13:28]: [Client #41] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:13:28]: [Client #168] Loading its data source...
[INFO][10:13:28]: [Client #168] Dataset size: 60000
[INFO][10:13:28]: [Client #168] Sampler: noniid
[INFO][10:13:28]: [Client #389] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:13:28]: [Client #168] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:13:29]: [93m[1m[Client #389] Started training in communication round #50.[0m
[INFO][10:13:29]: [93m[1m[Client #41] Started training in communication round #50.[0m
[INFO][10:13:29]: [93m[1m[Client #168] Started training in communication round #50.[0m
[INFO][10:13:31]: [Client #168] Loading the dataset.
[INFO][10:13:31]: [Client #41] Loading the dataset.
[INFO][10:13:31]: [Client #389] Loading the dataset.
[INFO][10:13:37]: [Client #41] Epoch: [1/5][0/10]	Loss: 0.027475
[INFO][10:13:37]: [Client #41] Epoch: [2/5][0/10]	Loss: 0.000521
[INFO][10:13:37]: [Client #389] Epoch: [1/5][0/10]	Loss: 0.011226
[INFO][10:13:37]: [Client #168] Epoch: [1/5][0/10]	Loss: 0.015555
[INFO][10:13:37]: [Client #41] Epoch: [3/5][0/10]	Loss: 0.003878
[INFO][10:13:37]: [Client #389] Epoch: [2/5][0/10]	Loss: 0.002937
[INFO][10:13:37]: [Client #41] Epoch: [4/5][0/10]	Loss: 0.019629
[INFO][10:13:37]: [Client #168] Epoch: [2/5][0/10]	Loss: 0.008626
[INFO][10:13:37]: [Client #41] Epoch: [5/5][0/10]	Loss: 0.021221
[INFO][10:13:37]: [Client #389] Epoch: [3/5][0/10]	Loss: 0.000590
[INFO][10:13:37]: [Client #168] Epoch: [3/5][0/10]	Loss: 0.000223
[INFO][10:13:37]: [Client #41] Model saved to /data/ykang/plato/results/test/model/lenet5_41_1127977.pth.
[INFO][10:13:37]: [Client #389] Epoch: [4/5][0/10]	Loss: 0.001532
[INFO][10:13:37]: [Client #168] Epoch: [4/5][0/10]	Loss: 0.010175
[INFO][10:13:37]: [Client #389] Epoch: [5/5][0/10]	Loss: 0.001953
[INFO][10:13:37]: [Client #389] Model saved to /data/ykang/plato/results/test/model/lenet5_389_1127978.pth.
[INFO][10:13:37]: [Client #168] Epoch: [5/5][0/10]	Loss: 0.020799
[INFO][10:13:37]: [Client #168] Model saved to /data/ykang/plato/results/test/model/lenet5_168_1127979.pth.
[INFO][10:13:38]: [Client #41] Loading a model from /data/ykang/plato/results/test/model/lenet5_41_1127977.pth.
[INFO][10:13:38]: [Client #41] Model trained.
[INFO][10:13:38]: [Client #41] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:13:38]: [Server #1127936] Received 0.24 MB of payload data from client #41 (simulated).
[INFO][10:13:38]: [Client #389] Loading a model from /data/ykang/plato/results/test/model/lenet5_389_1127978.pth.
[INFO][10:13:38]: [Client #389] Model trained.
[INFO][10:13:38]: [Client #389] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:13:38]: [Server #1127936] Received 0.24 MB of payload data from client #389 (simulated).
[INFO][10:13:38]: [Client #168] Loading a model from /data/ykang/plato/results/test/model/lenet5_168_1127979.pth.
[INFO][10:13:38]: [Client #168] Model trained.
[INFO][10:13:38]: [Client #168] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:13:38]: [Server #1127936] Received 0.24 MB of payload data from client #168 (simulated).
[INFO][10:13:38]: [Server #1127936] Selecting client #401 for training.
[INFO][10:13:38]: [Server #1127936] Sending the current model to client #401 (simulated).
[INFO][10:13:38]: [Server #1127936] Sending 0.24 MB of payload data to client #401 (simulated).
[INFO][10:13:38]: [Server #1127936] Selecting client #165 for training.
[INFO][10:13:38]: [Server #1127936] Sending the current model to client #165 (simulated).
[INFO][10:13:38]: [Server #1127936] Sending 0.24 MB of payload data to client #165 (simulated).
[INFO][10:13:38]: [Server #1127936] Selecting client #157 for training.
[INFO][10:13:38]: [Server #1127936] Sending the current model to client #157 (simulated).
[INFO][10:13:38]: [Client #401] Selected by the server.
[INFO][10:13:38]: [Client #401] Loading its data source...
[INFO][10:13:38]: [Client #401] Dataset size: 60000
[INFO][10:13:38]: [Client #401] Sampler: noniid
[INFO][10:13:38]: [Server #1127936] Sending 0.24 MB of payload data to client #157 (simulated).
[INFO][10:13:38]: [Client #165] Selected by the server.
[INFO][10:13:38]: [Client #157] Selected by the server.
[INFO][10:13:38]: [Client #165] Loading its data source...
[INFO][10:13:38]: [Client #157] Loading its data source...
[INFO][10:13:38]: [Client #165] Dataset size: 60000
[INFO][10:13:38]: [Client #157] Dataset size: 60000
[INFO][10:13:38]: [Client #165] Sampler: noniid
[INFO][10:13:38]: [Client #157] Sampler: noniid
[INFO][10:13:38]: [Client #401] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:13:38]: [93m[1m[Client #401] Started training in communication round #50.[0m
[INFO][10:13:38]: [Client #157] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:13:38]: [Client #165] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:13:38]: [93m[1m[Client #157] Started training in communication round #50.[0m
[INFO][10:13:38]: [93m[1m[Client #165] Started training in communication round #50.[0m
[INFO][10:13:40]: [Client #157] Loading the dataset.
[INFO][10:13:40]: [Client #401] Loading the dataset.
[INFO][10:13:40]: [Client #165] Loading the dataset.
[INFO][10:13:46]: [Client #401] Epoch: [1/5][0/10]	Loss: 0.000458
[INFO][10:13:46]: [Client #157] Epoch: [1/5][0/10]	Loss: 0.003546
[INFO][10:13:46]: [Client #401] Epoch: [2/5][0/10]	Loss: 0.003519
[INFO][10:13:46]: [Client #165] Epoch: [1/5][0/10]	Loss: 0.016154
[INFO][10:13:46]: [Client #157] Epoch: [2/5][0/10]	Loss: 0.242428
[INFO][10:13:46]: [Client #165] Epoch: [2/5][0/10]	Loss: 0.000524
[INFO][10:13:46]: [Client #401] Epoch: [3/5][0/10]	Loss: 0.282667
[INFO][10:13:47]: [Client #157] Epoch: [3/5][0/10]	Loss: 0.015794
[INFO][10:13:47]: [Client #165] Epoch: [3/5][0/10]	Loss: 0.001142
[INFO][10:13:47]: [Client #401] Epoch: [4/5][0/10]	Loss: 0.194267
[INFO][10:13:47]: [Client #157] Epoch: [4/5][0/10]	Loss: 0.000218
[INFO][10:13:47]: [Client #165] Epoch: [4/5][0/10]	Loss: 0.000716
[INFO][10:13:47]: [Client #157] Epoch: [5/5][0/10]	Loss: 0.000032
[INFO][10:13:47]: [Client #401] Epoch: [5/5][0/10]	Loss: 0.011954
[INFO][10:13:47]: [Client #165] Epoch: [5/5][0/10]	Loss: 0.000018
[INFO][10:13:47]: [Client #157] Model saved to /data/ykang/plato/results/test/model/lenet5_157_1127979.pth.
[INFO][10:13:47]: [Client #165] Model saved to /data/ykang/plato/results/test/model/lenet5_165_1127978.pth.
[INFO][10:13:47]: [Client #401] Model saved to /data/ykang/plato/results/test/model/lenet5_401_1127977.pth.
[INFO][10:13:48]: [Client #157] Loading a model from /data/ykang/plato/results/test/model/lenet5_157_1127979.pth.
[INFO][10:13:48]: [Client #157] Model trained.
[INFO][10:13:48]: [Client #157] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:13:48]: [Server #1127936] Received 0.24 MB of payload data from client #157 (simulated).
[INFO][10:13:48]: [Client #165] Loading a model from /data/ykang/plato/results/test/model/lenet5_165_1127978.pth.
[INFO][10:13:48]: [Client #165] Model trained.
[INFO][10:13:48]: [Client #165] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:13:48]: [Server #1127936] Received 0.24 MB of payload data from client #165 (simulated).
[INFO][10:13:48]: [Client #401] Loading a model from /data/ykang/plato/results/test/model/lenet5_401_1127977.pth.
[INFO][10:13:48]: [Client #401] Model trained.
[INFO][10:13:48]: [Client #401] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:13:48]: [Server #1127936] Received 0.24 MB of payload data from client #401 (simulated).
[INFO][10:13:48]: [Server #1127936] Selecting client #463 for training.
[INFO][10:13:48]: [Server #1127936] Sending the current model to client #463 (simulated).
[INFO][10:13:48]: [Server #1127936] Sending 0.24 MB of payload data to client #463 (simulated).
[INFO][10:13:48]: [Server #1127936] Selecting client #488 for training.
[INFO][10:13:48]: [Server #1127936] Sending the current model to client #488 (simulated).
[INFO][10:13:48]: [Server #1127936] Sending 0.24 MB of payload data to client #488 (simulated).
[INFO][10:13:48]: [Server #1127936] Selecting client #266 for training.
[INFO][10:13:48]: [Server #1127936] Sending the current model to client #266 (simulated).
[INFO][10:13:48]: [Client #463] Selected by the server.
[INFO][10:13:48]: [Client #463] Loading its data source...
[INFO][10:13:48]: [Client #463] Dataset size: 60000
[INFO][10:13:48]: [Client #463] Sampler: noniid
[INFO][10:13:48]: [Server #1127936] Sending 0.24 MB of payload data to client #266 (simulated).
[INFO][10:13:48]: [Client #488] Selected by the server.
[INFO][10:13:48]: [Client #488] Loading its data source...
[INFO][10:13:48]: [Client #488] Dataset size: 60000
[INFO][10:13:48]: [Client #266] Selected by the server.
[INFO][10:13:48]: [Client #488] Sampler: noniid
[INFO][10:13:48]: [Client #266] Loading its data source...
[INFO][10:13:48]: [Client #266] Dataset size: 60000
[INFO][10:13:48]: [Client #266] Sampler: noniid
[INFO][10:13:48]: [Client #463] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:13:48]: [Client #266] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:13:48]: [Client #488] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:13:48]: [93m[1m[Client #266] Started training in communication round #50.[0m
[INFO][10:13:48]: [93m[1m[Client #488] Started training in communication round #50.[0m
[INFO][10:13:48]: [93m[1m[Client #463] Started training in communication round #50.[0m
[INFO][10:13:50]: [Client #266] Loading the dataset.
[INFO][10:13:50]: [Client #463] Loading the dataset.
[INFO][10:13:50]: [Client #488] Loading the dataset.
[INFO][10:13:56]: [Client #266] Epoch: [1/5][0/10]	Loss: 0.007199
[INFO][10:13:56]: [Client #488] Epoch: [1/5][0/10]	Loss: 0.004370
[INFO][10:13:56]: [Client #266] Epoch: [2/5][0/10]	Loss: 0.007433
[INFO][10:13:56]: [Client #463] Epoch: [1/5][0/10]	Loss: 0.013507
[INFO][10:13:56]: [Client #488] Epoch: [2/5][0/10]	Loss: 0.002148
[INFO][10:13:56]: [Client #266] Epoch: [3/5][0/10]	Loss: 0.000003
[INFO][10:13:56]: [Client #463] Epoch: [2/5][0/10]	Loss: 0.012463
[INFO][10:13:56]: [Client #488] Epoch: [3/5][0/10]	Loss: 0.001842
[INFO][10:13:56]: [Client #266] Epoch: [4/5][0/10]	Loss: 0.000021
[INFO][10:13:56]: [Client #488] Epoch: [4/5][0/10]	Loss: 0.001426
[INFO][10:13:56]: [Client #463] Epoch: [3/5][0/10]	Loss: 0.000383
[INFO][10:13:56]: [Client #266] Epoch: [5/5][0/10]	Loss: 0.451595
[INFO][10:13:56]: [Client #463] Epoch: [4/5][0/10]	Loss: 0.000465
[INFO][10:13:56]: [Client #488] Epoch: [5/5][0/10]	Loss: 0.000920
[INFO][10:13:56]: [Client #266] Model saved to /data/ykang/plato/results/test/model/lenet5_266_1127979.pth.
[INFO][10:13:56]: [Client #488] Model saved to /data/ykang/plato/results/test/model/lenet5_488_1127978.pth.
[INFO][10:13:57]: [Client #463] Epoch: [5/5][0/10]	Loss: 0.006519
[INFO][10:13:57]: [Client #463] Model saved to /data/ykang/plato/results/test/model/lenet5_463_1127977.pth.
[INFO][10:13:57]: [Client #266] Loading a model from /data/ykang/plato/results/test/model/lenet5_266_1127979.pth.
[INFO][10:13:57]: [Client #266] Model trained.
[INFO][10:13:57]: [Client #266] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:13:57]: [Server #1127936] Received 0.24 MB of payload data from client #266 (simulated).
[INFO][10:13:57]: [Client #488] Loading a model from /data/ykang/plato/results/test/model/lenet5_488_1127978.pth.
[INFO][10:13:57]: [Client #488] Model trained.
[INFO][10:13:57]: [Client #488] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:13:57]: [Server #1127936] Received 0.24 MB of payload data from client #488 (simulated).
[INFO][10:13:57]: [Client #463] Loading a model from /data/ykang/plato/results/test/model/lenet5_463_1127977.pth.
[INFO][10:13:57]: [Client #463] Model trained.
[INFO][10:13:57]: [Client #463] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:13:57]: [Server #1127936] Received 0.24 MB of payload data from client #463 (simulated).
[INFO][10:13:57]: [Server #1127936] Selecting client #476 for training.
[INFO][10:13:57]: [Server #1127936] Sending the current model to client #476 (simulated).
[INFO][10:13:57]: [Server #1127936] Sending 0.24 MB of payload data to client #476 (simulated).
[INFO][10:13:57]: [Client #476] Selected by the server.
[INFO][10:13:57]: [Client #476] Loading its data source...
[INFO][10:13:57]: [Client #476] Dataset size: 60000
[INFO][10:13:57]: [Client #476] Sampler: noniid
[INFO][10:13:57]: [Client #476] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:13:57]: [93m[1m[Client #476] Started training in communication round #50.[0m
[INFO][10:13:59]: [Client #476] Loading the dataset.
[INFO][10:14:05]: [Client #476] Epoch: [1/5][0/10]	Loss: 0.027463
[INFO][10:14:05]: [Client #476] Epoch: [2/5][0/10]	Loss: 0.000037
[INFO][10:14:05]: [Client #476] Epoch: [3/5][0/10]	Loss: 0.000007
[INFO][10:14:05]: [Client #476] Epoch: [4/5][0/10]	Loss: 0.000485
[INFO][10:14:05]: [Client #476] Epoch: [5/5][0/10]	Loss: 0.001086
[INFO][10:14:05]: [Client #476] Model saved to /data/ykang/plato/results/test/model/lenet5_476_1127977.pth.
[INFO][10:14:06]: [Client #476] Loading a model from /data/ykang/plato/results/test/model/lenet5_476_1127977.pth.
[INFO][10:14:06]: [Client #476] Model trained.
[INFO][10:14:06]: [Client #476] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:14:06]: [Server #1127936] Received 0.24 MB of payload data from client #476 (simulated).
[INFO][10:14:06]: [Server #1127936] Adding client #133 to the list of clients for aggregation.
[INFO][10:14:06]: [Server #1127936] Adding client #100 to the list of clients for aggregation.
[INFO][10:14:06]: [Server #1127936] Adding client #366 to the list of clients for aggregation.
[INFO][10:14:06]: [Server #1127936] Adding client #21 to the list of clients for aggregation.
[INFO][10:14:06]: [Server #1127936] Adding client #498 to the list of clients for aggregation.
[INFO][10:14:06]: [Server #1127936] Adding client #488 to the list of clients for aggregation.
[INFO][10:14:06]: [Server #1127936] Adding client #476 to the list of clients for aggregation.
[INFO][10:14:06]: [Server #1127936] Adding client #389 to the list of clients for aggregation.
[INFO][10:14:06]: [Server #1127936] Adding client #463 to the list of clients for aggregation.
[INFO][10:14:06]: [Server #1127936] Adding client #165 to the list of clients for aggregation.
[INFO][10:14:06]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.02003209 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01162307 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00534969 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00718516 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01616871
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00410149 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00389596 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00561609 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00563328 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00468098
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 3. 0. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 3. 1. 0. 0. 0. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 0. 0. 0. 0. 1.
 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 2. 1. 6. 0. 1. 2. 1. 0. 1. 1. 0. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0.
 1. 0. 1. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 4. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0.
 1. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 1. 0. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 0. 0. 0. 1. 0. 0. 6. 1. 1.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 2. 0. 1. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 2. 2. 1. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.02003209 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01162307 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00534969 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00718516 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01616871
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00410149 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00389596 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00561609 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00563328 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00468098
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:14:08]: [Server #1127936] Global model accuracy: 95.67%

[INFO][10:14:08]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_50.pth.
[INFO][10:14:08]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_50.pth.
[INFO][10:14:08]: [93m[1m
[Server #1127936] Starting round 51/100.[0m
[0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.002
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  2e-05  4e-10  4e-10
 6:  6.8876e+00  6.8875e+00  2e-05  2e-10  2e-10
 7:  6.8875e+00  6.8875e+00  2e-05  5e-10  1e-11
 8:  6.8875e+00  6.8875e+00  1e-05  4e-10  9e-12
 9:  6.8875e+00  6.8875e+00  8e-06  7e-10  2e-11
10:  6.8875e+00  6.8875e+00  5e-07  1e-09  2e-11
Optimal solution found.
The calculated probability is:  [4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 9.75179991e-01 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 1.40497904e-04 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 7.11061208e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97558045e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.08641421e-04 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97567798e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97568258e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97563673e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97563619e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 4.97572511e-05
 4.97572511e-05 4.97572511e-05 4.97572511e-05 6.75004695e-05
 4.97572511e-05 4.97572511e-05]
current clients pool:  [INFO][10:14:09]: [Server #1127936] Selected clients: [ 21 313  95 270 214 400 468  91 430 480]
[INFO][10:14:09]: [Server #1127936] Selecting client #21 for training.
[INFO][10:14:09]: [Server #1127936] Sending the current model to client #21 (simulated).
[INFO][10:14:09]: [Server #1127936] Sending 0.24 MB of payload data to client #21 (simulated).
[INFO][10:14:09]: [Server #1127936] Selecting client #313 for training.
[INFO][10:14:09]: [Server #1127936] Sending the current model to client #313 (simulated).
[INFO][10:14:09]: [Server #1127936] Sending 0.24 MB of payload data to client #313 (simulated).
[INFO][10:14:09]: [Server #1127936] Selecting client #95 for training.
[INFO][10:14:09]: [Server #1127936] Sending the current model to client #95 (simulated).
[INFO][10:14:09]: [Client #21] Selected by the server.
[INFO][10:14:09]: [Client #21] Loading its data source...
[INFO][10:14:09]: [Client #21] Dataset size: 60000
[INFO][10:14:09]: [Client #21] Sampler: noniid
[INFO][10:14:09]: [Server #1127936] Sending 0.24 MB of payload data to client #95 (simulated).
[INFO][10:14:09]: [Client #313] Selected by the server.
[INFO][10:14:09]: [Client #313] Loading its data source...
[INFO][10:14:09]: [Client #21] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:14:09]: [Client #313] Dataset size: 60000
[INFO][10:14:09]: [Client #95] Selected by the server.
[INFO][10:14:09]: [Client #313] Sampler: noniid
[INFO][10:14:09]: [Client #95] Loading its data source...
[INFO][10:14:09]: [Client #95] Dataset size: 60000
[INFO][10:14:09]: [Client #95] Sampler: noniid
[INFO][10:14:09]: [Client #313] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:14:09]: [93m[1m[Client #21] Started training in communication round #51.[0m
[INFO][10:14:09]: [93m[1m[Client #313] Started training in communication round #51.[0m
[INFO][10:14:09]: [Client #95] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:14:09]: [93m[1m[Client #95] Started training in communication round #51.[0m
[INFO][10:14:11]: [Client #313] Loading the dataset.
[INFO][10:14:11]: [Client #21] Loading the dataset.
[INFO][10:14:11]: [Client #95] Loading the dataset.
[INFO][10:14:17]: [Client #313] Epoch: [1/5][0/10]	Loss: 0.002469
[INFO][10:14:17]: [Client #95] Epoch: [1/5][0/10]	Loss: 0.001340
[INFO][10:14:17]: [Client #21] Epoch: [1/5][0/10]	Loss: 0.013065
[INFO][10:14:17]: [Client #313] Epoch: [2/5][0/10]	Loss: 0.001943
[INFO][10:14:17]: [Client #95] Epoch: [2/5][0/10]	Loss: 0.001205
[INFO][10:14:17]: [Client #21] Epoch: [2/5][0/10]	Loss: 0.001764
[INFO][10:14:17]: [Client #313] Epoch: [3/5][0/10]	Loss: 0.000010
[INFO][10:14:18]: [Client #95] Epoch: [3/5][0/10]	Loss: 0.000871
[INFO][10:14:18]: [Client #21] Epoch: [3/5][0/10]	Loss: 0.000012
[INFO][10:14:18]: [Client #313] Epoch: [4/5][0/10]	Loss: 0.000010
[INFO][10:14:18]: [Client #95] Epoch: [4/5][0/10]	Loss: 0.001399
[INFO][10:14:18]: [Client #21] Epoch: [4/5][0/10]	Loss: 0.000207
[INFO][10:14:18]: [Client #313] Epoch: [5/5][0/10]	Loss: 0.000019
[INFO][10:14:18]: [Client #95] Epoch: [5/5][0/10]	Loss: 0.005163
[INFO][10:14:18]: [Client #313] Model saved to /data/ykang/plato/results/test/model/lenet5_313_1127978.pth.
[INFO][10:14:18]: [Client #21] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:14:18]: [Client #95] Model saved to /data/ykang/plato/results/test/model/lenet5_95_1127979.pth.
[INFO][10:14:18]: [Client #21] Model saved to /data/ykang/plato/results/test/model/lenet5_21_1127977.pth.
[INFO][10:14:19]: [Client #313] Loading a model from /data/ykang/plato/results/test/model/lenet5_313_1127978.pth.
[INFO][10:14:19]: [Client #313] Model trained.
[INFO][10:14:19]: [Client #313] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:14:19]: [Server #1127936] Received 0.24 MB of payload data from client #313 (simulated).
[INFO][10:14:19]: [Client #21] Loading a model from /data/ykang/plato/results/test/model/lenet5_21_1127977.pth.
[INFO][10:14:19]: [Client #21] Model trained.
[INFO][10:14:19]: [Client #21] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:14:19]: [Server #1127936] Received 0.24 MB of payload data from client #21 (simulated).
[INFO][10:14:19]: [Client #95] Loading a model from /data/ykang/plato/results/test/model/lenet5_95_1127979.pth.
[INFO][10:14:19]: [Client #95] Model trained.
[INFO][10:14:19]: [Client #95] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:14:19]: [Server #1127936] Received 0.24 MB of payload data from client #95 (simulated).
[INFO][10:14:19]: [Server #1127936] Selecting client #270 for training.
[INFO][10:14:19]: [Server #1127936] Sending the current model to client #270 (simulated).
[INFO][10:14:19]: [Server #1127936] Sending 0.24 MB of payload data to client #270 (simulated).
[INFO][10:14:19]: [Server #1127936] Selecting client #214 for training.
[INFO][10:14:19]: [Server #1127936] Sending the current model to client #214 (simulated).
[INFO][10:14:19]: [Server #1127936] Sending 0.24 MB of payload data to client #214 (simulated).
[INFO][10:14:19]: [Server #1127936] Selecting client #400 for training.
[INFO][10:14:19]: [Server #1127936] Sending the current model to client #400 (simulated).
[INFO][10:14:19]: [Client #270] Selected by the server.
[INFO][10:14:19]: [Client #270] Loading its data source...
[INFO][10:14:19]: [Client #270] Dataset size: 60000
[INFO][10:14:19]: [Client #270] Sampler: noniid
[INFO][10:14:19]: [Server #1127936] Sending 0.24 MB of payload data to client #400 (simulated).
[INFO][10:14:19]: [Client #214] Selected by the server.
[INFO][10:14:19]: [Client #214] Loading its data source...
[INFO][10:14:19]: [Client #400] Selected by the server.
[INFO][10:14:19]: [Client #214] Dataset size: 60000
[INFO][10:14:19]: [Client #214] Sampler: noniid
[INFO][10:14:19]: [Client #400] Loading its data source...
[INFO][10:14:19]: [Client #400] Dataset size: 60000
[INFO][10:14:19]: [Client #400] Sampler: noniid
[INFO][10:14:19]: [Client #270] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:14:19]: [Client #214] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:14:19]: [93m[1m[Client #214] Started training in communication round #51.[0m
[INFO][10:14:19]: [Client #400] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:14:19]: [93m[1m[Client #270] Started training in communication round #51.[0m
[INFO][10:14:19]: [93m[1m[Client #400] Started training in communication round #51.[0m
[INFO][10:14:21]: [Client #400] Loading the dataset.
[INFO][10:14:21]: [Client #214] Loading the dataset.
[INFO][10:14:21]: [Client #270] Loading the dataset.
[INFO][10:14:27]: [Client #400] Epoch: [1/5][0/10]	Loss: 0.001054
[INFO][10:14:27]: [Client #214] Epoch: [1/5][0/10]	Loss: 0.001090
[INFO][10:14:27]: [Client #214] Epoch: [2/5][0/10]	Loss: 0.000312
[INFO][10:14:27]: [Client #400] Epoch: [2/5][0/10]	Loss: 0.012095
[INFO][10:14:27]: [Client #270] Epoch: [1/5][0/10]	Loss: 0.014781
[INFO][10:14:27]: [Client #214] Epoch: [3/5][0/10]	Loss: 0.000423
[INFO][10:14:27]: [Client #270] Epoch: [2/5][0/10]	Loss: 0.000008
[INFO][10:14:27]: [Client #400] Epoch: [3/5][0/10]	Loss: 0.002574
[INFO][10:14:27]: [Client #214] Epoch: [4/5][0/10]	Loss: 0.034384
[INFO][10:14:27]: [Client #400] Epoch: [4/5][0/10]	Loss: 0.002725
[INFO][10:14:27]: [Client #214] Epoch: [5/5][0/10]	Loss: 0.001239
[INFO][10:14:27]: [Client #270] Epoch: [3/5][0/10]	Loss: 0.000002
[INFO][10:14:27]: [Client #214] Model saved to /data/ykang/plato/results/test/model/lenet5_214_1127978.pth.
[INFO][10:14:27]: [Client #400] Epoch: [5/5][0/10]	Loss: 0.001558
[INFO][10:14:27]: [Client #270] Epoch: [4/5][0/10]	Loss: 0.001577
[INFO][10:14:28]: [Client #400] Model saved to /data/ykang/plato/results/test/model/lenet5_400_1127979.pth.
[INFO][10:14:28]: [Client #270] Epoch: [5/5][0/10]	Loss: 0.000006
[INFO][10:14:28]: [Client #270] Model saved to /data/ykang/plato/results/test/model/lenet5_270_1127977.pth.
[INFO][10:14:28]: [Client #214] Loading a model from /data/ykang/plato/results/test/model/lenet5_214_1127978.pth.
[INFO][10:14:28]: [Client #214] Model trained.
[INFO][10:14:28]: [Client #214] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:14:28]: [Server #1127936] Received 0.24 MB of payload data from client #214 (simulated).
[INFO][10:14:28]: [Client #400] Loading a model from /data/ykang/plato/results/test/model/lenet5_400_1127979.pth.
[INFO][10:14:28]: [Client #400] Model trained.
[INFO][10:14:28]: [Client #400] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:14:28]: [Server #1127936] Received 0.24 MB of payload data from client #400 (simulated).
[INFO][10:14:28]: [Client #270] Loading a model from /data/ykang/plato/results/test/model/lenet5_270_1127977.pth.
[INFO][10:14:28]: [Client #270] Model trained.
[INFO][10:14:28]: [Client #270] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:14:28]: [Server #1127936] Received 0.24 MB of payload data from client #270 (simulated).
[INFO][10:14:28]: [Server #1127936] Selecting client #468 for training.
[INFO][10:14:28]: [Server #1127936] Sending the current model to client #468 (simulated).
[INFO][10:14:28]: [Server #1127936] Sending 0.24 MB of payload data to client #468 (simulated).
[INFO][10:14:28]: [Server #1127936] Selecting client #91 for training.
[INFO][10:14:28]: [Server #1127936] Sending the current model to client #91 (simulated).
[INFO][10:14:28]: [Server #1127936] Sending 0.24 MB of payload data to client #91 (simulated).
[INFO][10:14:28]: [Server #1127936] Selecting client #430 for training.
[INFO][10:14:28]: [Server #1127936] Sending the current model to client #430 (simulated).
[INFO][10:14:28]: [Client #468] Selected by the server.
[INFO][10:14:28]: [Client #468] Loading its data source...
[INFO][10:14:28]: [Client #468] Dataset size: 60000
[INFO][10:14:28]: [Client #468] Sampler: noniid
[INFO][10:14:28]: [Server #1127936] Sending 0.24 MB of payload data to client #430 (simulated).
[INFO][10:14:28]: [Client #91] Selected by the server.
[INFO][10:14:28]: [Client #91] Loading its data source...
[INFO][10:14:28]: [Client #430] Selected by the server.
[INFO][10:14:28]: [Client #91] Dataset size: 60000
[INFO][10:14:28]: [Client #430] Loading its data source...
[INFO][10:14:28]: [Client #91] Sampler: noniid
[INFO][10:14:28]: [Client #430] Dataset size: 60000
[INFO][10:14:28]: [Client #430] Sampler: noniid
[INFO][10:14:28]: [Client #468] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:14:28]: [Client #430] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:14:28]: [Client #91] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:14:28]: [93m[1m[Client #468] Started training in communication round #51.[0m
[INFO][10:14:28]: [93m[1m[Client #91] Started training in communication round #51.[0m
[INFO][10:14:28]: [93m[1m[Client #430] Started training in communication round #51.[0m
[INFO][10:14:31]: [Client #468] Loading the dataset.
[INFO][10:14:31]: [Client #91] Loading the dataset.
[INFO][10:14:31]: [Client #430] Loading the dataset.
[INFO][10:14:37]: [Client #91] Epoch: [1/5][0/10]	Loss: 0.004460
[INFO][10:14:37]: [Client #468] Epoch: [1/5][0/10]	Loss: 0.002589
[INFO][10:14:37]: [Client #430] Epoch: [1/5][0/10]	Loss: 0.001291
[INFO][10:14:37]: [Client #91] Epoch: [2/5][0/10]	Loss: 0.002131
[INFO][10:14:37]: [Client #468] Epoch: [2/5][0/10]	Loss: 0.007014
[INFO][10:14:37]: [Client #430] Epoch: [2/5][0/10]	Loss: 0.001069
[INFO][10:14:37]: [Client #91] Epoch: [3/5][0/10]	Loss: 0.000068
[INFO][10:14:37]: [Client #468] Epoch: [3/5][0/10]	Loss: 0.000189
[INFO][10:14:37]: [Client #430] Epoch: [3/5][0/10]	Loss: 0.025096
[INFO][10:14:37]: [Client #468] Epoch: [4/5][0/10]	Loss: 0.000073
[INFO][10:14:37]: [Client #91] Epoch: [4/5][0/10]	Loss: 0.000002
[INFO][10:14:37]: [Client #430] Epoch: [4/5][0/10]	Loss: 0.000547
[INFO][10:14:37]: [Client #468] Epoch: [5/5][0/10]	Loss: 0.000001
[INFO][10:14:37]: [Client #91] Epoch: [5/5][0/10]	Loss: 0.000886
[INFO][10:14:37]: [Client #468] Model saved to /data/ykang/plato/results/test/model/lenet5_468_1127977.pth.
[INFO][10:14:37]: [Client #430] Epoch: [5/5][0/10]	Loss: 0.001933
[INFO][10:14:37]: [Client #91] Model saved to /data/ykang/plato/results/test/model/lenet5_91_1127978.pth.
[INFO][10:14:37]: [Client #430] Model saved to /data/ykang/plato/results/test/model/lenet5_430_1127979.pth.
[INFO][10:14:38]: [Client #468] Loading a model from /data/ykang/plato/results/test/model/lenet5_468_1127977.pth.
[INFO][10:14:38]: [Client #468] Model trained.
[INFO][10:14:38]: [Client #468] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:14:38]: [Server #1127936] Received 0.24 MB of payload data from client #468 (simulated).
[INFO][10:14:38]: [Client #91] Loading a model from /data/ykang/plato/results/test/model/lenet5_91_1127978.pth.
[INFO][10:14:38]: [Client #91] Model trained.
[INFO][10:14:38]: [Client #91] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:14:38]: [Server #1127936] Received 0.24 MB of payload data from client #91 (simulated).
[INFO][10:14:38]: [Client #430] Loading a model from /data/ykang/plato/results/test/model/lenet5_430_1127979.pth.
[INFO][10:14:38]: [Client #430] Model trained.
[INFO][10:14:38]: [Client #430] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:14:38]: [Server #1127936] Received 0.24 MB of payload data from client #430 (simulated).
[INFO][10:14:38]: [Server #1127936] Selecting client #480 for training.
[INFO][10:14:38]: [Server #1127936] Sending the current model to client #480 (simulated).
[INFO][10:14:38]: [Server #1127936] Sending 0.24 MB of payload data to client #480 (simulated).
[INFO][10:14:38]: [Client #480] Selected by the server.
[INFO][10:14:38]: [Client #480] Loading its data source...
[INFO][10:14:38]: [Client #480] Dataset size: 60000
[INFO][10:14:38]: [Client #480] Sampler: noniid
[INFO][10:14:38]: [Client #480] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:14:38]: [93m[1m[Client #480] Started training in communication round #51.[0m
[INFO][10:14:40]: [Client #480] Loading the dataset.
[INFO][10:14:46]: [Client #480] Epoch: [1/5][0/10]	Loss: 0.139566
[INFO][10:14:46]: [Client #480] Epoch: [2/5][0/10]	Loss: 0.000258
[INFO][10:14:46]: [Client #480] Epoch: [3/5][0/10]	Loss: 0.000002
[INFO][10:14:46]: [Client #480] Epoch: [4/5][0/10]	Loss: 0.000001
[INFO][10:14:46]: [Client #480] Epoch: [5/5][0/10]	Loss: 0.000437
[INFO][10:14:46]: [Client #480] Model saved to /data/ykang/plato/results/test/model/lenet5_480_1127977.pth.
[INFO][10:14:47]: [Client #480] Loading a model from /data/ykang/plato/results/test/model/lenet5_480_1127977.pth.
[INFO][10:14:47]: [Client #480] Model trained.
[INFO][10:14:47]: [Client #480] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:14:47]: [Server #1127936] Received 0.24 MB of payload data from client #480 (simulated).
[INFO][10:14:47]: [Server #1127936] Adding client #157 to the list of clients for aggregation.
[INFO][10:14:47]: [Server #1127936] Adding client #266 to the list of clients for aggregation.
[INFO][10:14:47]: [Server #1127936] Adding client #168 to the list of clients for aggregation.
[INFO][10:14:47]: [Server #1127936] Adding client #196 to the list of clients for aggregation.
[INFO][10:14:47]: [Server #1127936] Adding client #212 to the list of clients for aggregation.
[INFO][10:14:47]: [Server #1127936] Adding client #110 to the list of clients for aggregation.
[INFO][10:14:47]: [Server #1127936] Adding client #91 to the list of clients for aggregation.
[INFO][10:14:47]: [Server #1127936] Adding client #214 to the list of clients for aggregation.
[INFO][10:14:47]: [Server #1127936] Adding client #313 to the list of clients for aggregation.
[INFO][10:14:47]: [Server #1127936] Adding client #468 to the list of clients for aggregation.
[INFO][10:14:47]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00391654 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01638853 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00896416 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00734914
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00757111 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01378761 0.         0.01489665 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00828029 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00581598 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00552699
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 3. 0. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 3. 1. 0. 0. 0. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 0. 0. 0. 0. 1.
 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 2. 1. 6. 0. 1. 2. 1. 0. 1. 1. 0. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0.
 1. 0. 1. 2. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 4. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0.
 1. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 1. 0. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 0. 0. 0. 1. 0. 0. 6. 1. 1.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 2. 0. 1. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 2. 2. 1. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00391654 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01638853 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00896416 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00734914
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00757111 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01378761 0.         0.01489665 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00828029 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00581598 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00552699
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:14:49]: [Server #1127936] Global model accuracy: 95.88%

[INFO][10:14:49]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_51.pth.
[INFO][10:14:49]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_51.pth.
[INFO][10:14:49]: [93m[1m
[Server #1127936] Starting round 52/100.[0m
[0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.002
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8871e+00  4e-04  7e-09  7e-09
 5:  6.8876e+00  6.8874e+00  1e-04  2e-09  2e-09
 6:  6.8875e+00  6.8875e+00  9e-05  7e-10  1e-10
 7:  6.8875e+00  6.8875e+00  8e-05  6e-09  7e-10
 8:  6.8875e+00  6.8875e+00  6e-05  8e-09  9e-10
 9:  6.8875e+00  6.8875e+00  2e-05  4e-08  4e-09
10:  6.8875e+00  6.8875e+00  3e-06  1e-08  1e-09
Optimal solution found.
The calculated probability is:  [6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41502422e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 9.68520744e-01 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 7.25766246e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 7.09033966e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 7.97536685e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 1.35386725e-04
 6.41503701e-05 6.41485205e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 7.18588372e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41500882e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41501155e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05 6.41503701e-05 6.41503701e-05
 6.41503701e-05 6.41503701e-05]
current clients pool:  [INFO][10:14:50]: [Server #1127936] Selected clients: [110  93 142 251 321  57 377 425 421 445]
[INFO][10:14:50]: [Server #1127936] Selecting client #110 for training.
[INFO][10:14:50]: [Server #1127936] Sending the current model to client #110 (simulated).
[INFO][10:14:50]: [Server #1127936] Sending 0.24 MB of payload data to client #110 (simulated).
[INFO][10:14:50]: [Server #1127936] Selecting client #93 for training.
[INFO][10:14:50]: [Server #1127936] Sending the current model to client #93 (simulated).
[INFO][10:14:50]: [Server #1127936] Sending 0.24 MB of payload data to client #93 (simulated).
[INFO][10:14:50]: [Server #1127936] Selecting client #142 for training.
[INFO][10:14:50]: [Server #1127936] Sending the current model to client #142 (simulated).
[INFO][10:14:50]: [Client #110] Selected by the server.
[INFO][10:14:50]: [Client #110] Loading its data source...
[INFO][10:14:50]: [Client #110] Dataset size: 60000
[INFO][10:14:50]: [Client #110] Sampler: noniid
[INFO][10:14:50]: [Server #1127936] Sending 0.24 MB of payload data to client #142 (simulated).
[INFO][10:14:50]: [Client #93] Selected by the server.
[INFO][10:14:50]: [Client #93] Loading its data source...
[INFO][10:14:50]: [Client #93] Dataset size: 60000
[INFO][10:14:50]: [Client #93] Sampler: noniid
[INFO][10:14:50]: [Client #110] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:14:50]: [Client #142] Selected by the server.
[INFO][10:14:50]: [Client #142] Loading its data source...
[INFO][10:14:50]: [Client #142] Dataset size: 60000
[INFO][10:14:50]: [Client #142] Sampler: noniid
[INFO][10:14:50]: [Client #93] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:14:50]: [Client #142] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:14:50]: [93m[1m[Client #142] Started training in communication round #52.[0m
[INFO][10:14:50]: [93m[1m[Client #93] Started training in communication round #52.[0m
[INFO][10:14:50]: [93m[1m[Client #110] Started training in communication round #52.[0m
[INFO][10:14:52]: [Client #93] Loading the dataset.
[INFO][10:14:52]: [Client #142] Loading the dataset.
[INFO][10:14:52]: [Client #110] Loading the dataset.
[INFO][10:14:58]: [Client #93] Epoch: [1/5][0/10]	Loss: 0.013565
[INFO][10:14:58]: [Client #110] Epoch: [1/5][0/10]	Loss: 0.027996
[INFO][10:14:58]: [Client #142] Epoch: [1/5][0/10]	Loss: 0.028308
[INFO][10:14:58]: [Client #93] Epoch: [2/5][0/10]	Loss: 0.000069
[INFO][10:14:58]: [Client #142] Epoch: [2/5][0/10]	Loss: 0.013438
[INFO][10:14:58]: [Client #110] Epoch: [2/5][0/10]	Loss: 0.000732
[INFO][10:14:58]: [Client #93] Epoch: [3/5][0/10]	Loss: 0.015092
[INFO][10:14:58]: [Client #142] Epoch: [3/5][0/10]	Loss: 0.000152
[INFO][10:14:58]: [Client #110] Epoch: [3/5][0/10]	Loss: 0.000012
[INFO][10:14:58]: [Client #93] Epoch: [4/5][0/10]	Loss: 0.000001
[INFO][10:14:58]: [Client #110] Epoch: [4/5][0/10]	Loss: 0.000379
[INFO][10:14:58]: [Client #142] Epoch: [4/5][0/10]	Loss: 0.000091
[INFO][10:14:58]: [Client #93] Epoch: [5/5][0/10]	Loss: 0.081755
[INFO][10:14:58]: [Client #110] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:14:58]: [Client #93] Model saved to /data/ykang/plato/results/test/model/lenet5_93_1127978.pth.
[INFO][10:14:58]: [Client #142] Epoch: [5/5][0/10]	Loss: 0.001177
[INFO][10:14:59]: [Client #142] Model saved to /data/ykang/plato/results/test/model/lenet5_142_1127979.pth.
[INFO][10:14:59]: [Client #110] Model saved to /data/ykang/plato/results/test/model/lenet5_110_1127977.pth.
[INFO][10:14:59]: [Client #93] Loading a model from /data/ykang/plato/results/test/model/lenet5_93_1127978.pth.
[INFO][10:14:59]: [Client #93] Model trained.
[INFO][10:14:59]: [Client #93] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:14:59]: [Server #1127936] Received 0.24 MB of payload data from client #93 (simulated).
[INFO][10:14:59]: [Client #142] Loading a model from /data/ykang/plato/results/test/model/lenet5_142_1127979.pth.
[INFO][10:14:59]: [Client #142] Model trained.
[INFO][10:14:59]: [Client #142] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:14:59]: [Server #1127936] Received 0.24 MB of payload data from client #142 (simulated).
[INFO][10:14:59]: [Client #110] Loading a model from /data/ykang/plato/results/test/model/lenet5_110_1127977.pth.
[INFO][10:14:59]: [Client #110] Model trained.
[INFO][10:14:59]: [Client #110] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:14:59]: [Server #1127936] Received 0.24 MB of payload data from client #110 (simulated).
[INFO][10:14:59]: [Server #1127936] Selecting client #251 for training.
[INFO][10:14:59]: [Server #1127936] Sending the current model to client #251 (simulated).
[INFO][10:14:59]: [Server #1127936] Sending 0.24 MB of payload data to client #251 (simulated).
[INFO][10:14:59]: [Server #1127936] Selecting client #321 for training.
[INFO][10:14:59]: [Server #1127936] Sending the current model to client #321 (simulated).
[INFO][10:14:59]: [Server #1127936] Sending 0.24 MB of payload data to client #321 (simulated).
[INFO][10:14:59]: [Server #1127936] Selecting client #57 for training.
[INFO][10:14:59]: [Server #1127936] Sending the current model to client #57 (simulated).
[INFO][10:14:59]: [Client #251] Selected by the server.
[INFO][10:14:59]: [Client #251] Loading its data source...
[INFO][10:14:59]: [Client #251] Dataset size: 60000
[INFO][10:14:59]: [Client #251] Sampler: noniid
[INFO][10:14:59]: [Server #1127936] Sending 0.24 MB of payload data to client #57 (simulated).
[INFO][10:14:59]: [Client #321] Selected by the server.
[INFO][10:14:59]: [Client #57] Selected by the server.
[INFO][10:14:59]: [Client #321] Loading its data source...
[INFO][10:14:59]: [Client #57] Loading its data source...
[INFO][10:14:59]: [Client #321] Dataset size: 60000
[INFO][10:14:59]: [Client #321] Sampler: noniid
[INFO][10:14:59]: [Client #57] Dataset size: 60000
[INFO][10:14:59]: [Client #57] Sampler: noniid
[INFO][10:14:59]: [Client #251] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:14:59]: [Client #57] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:14:59]: [Client #321] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:14:59]: [93m[1m[Client #321] Started training in communication round #52.[0m
[INFO][10:14:59]: [93m[1m[Client #57] Started training in communication round #52.[0m
[INFO][10:14:59]: [93m[1m[Client #251] Started training in communication round #52.[0m
[INFO][10:15:02]: [Client #251] Loading the dataset.
[INFO][10:15:02]: [Client #57] Loading the dataset.
[INFO][10:15:02]: [Client #321] Loading the dataset.
[INFO][10:15:08]: [Client #251] Epoch: [1/5][0/10]	Loss: 0.004441
[INFO][10:15:08]: [Client #321] Epoch: [1/5][0/10]	Loss: 0.023667
[INFO][10:15:08]: [Client #251] Epoch: [2/5][0/10]	Loss: 0.006259
[INFO][10:15:08]: [Client #57] Epoch: [1/5][0/10]	Loss: 0.022954
[INFO][10:15:08]: [Client #321] Epoch: [2/5][0/10]	Loss: 0.001165
[INFO][10:15:08]: [Client #251] Epoch: [3/5][0/10]	Loss: 0.000170
[INFO][10:15:08]: [Client #57] Epoch: [2/5][0/10]	Loss: 0.000123
[INFO][10:15:08]: [Client #321] Epoch: [3/5][0/10]	Loss: 0.000010
[INFO][10:15:08]: [Client #251] Epoch: [4/5][0/10]	Loss: 0.000067
[INFO][10:15:08]: [Client #57] Epoch: [3/5][0/10]	Loss: 0.000101
[INFO][10:15:08]: [Client #321] Epoch: [4/5][0/10]	Loss: 0.000252
[INFO][10:15:08]: [Client #251] Epoch: [5/5][0/10]	Loss: 0.000415
[INFO][10:15:08]: [Client #57] Epoch: [4/5][0/10]	Loss: 0.008050
[INFO][10:15:08]: [Client #251] Model saved to /data/ykang/plato/results/test/model/lenet5_251_1127977.pth.
[INFO][10:15:08]: [Client #321] Epoch: [5/5][0/10]	Loss: 0.007938
[INFO][10:15:08]: [Client #57] Epoch: [5/5][0/10]	Loss: 0.130493
[INFO][10:15:08]: [Client #321] Model saved to /data/ykang/plato/results/test/model/lenet5_321_1127978.pth.
[INFO][10:15:08]: [Client #57] Model saved to /data/ykang/plato/results/test/model/lenet5_57_1127979.pth.
[INFO][10:15:09]: [Client #251] Loading a model from /data/ykang/plato/results/test/model/lenet5_251_1127977.pth.
[INFO][10:15:09]: [Client #251] Model trained.
[INFO][10:15:09]: [Client #251] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:15:09]: [Server #1127936] Received 0.24 MB of payload data from client #251 (simulated).
[INFO][10:15:09]: [Client #321] Loading a model from /data/ykang/plato/results/test/model/lenet5_321_1127978.pth.
[INFO][10:15:09]: [Client #321] Model trained.
[INFO][10:15:09]: [Client #321] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:15:09]: [Server #1127936] Received 0.24 MB of payload data from client #321 (simulated).
[INFO][10:15:09]: [Client #57] Loading a model from /data/ykang/plato/results/test/model/lenet5_57_1127979.pth.
[INFO][10:15:09]: [Client #57] Model trained.
[INFO][10:15:09]: [Client #57] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:15:09]: [Server #1127936] Received 0.24 MB of payload data from client #57 (simulated).
[INFO][10:15:09]: [Server #1127936] Selecting client #377 for training.
[INFO][10:15:09]: [Server #1127936] Sending the current model to client #377 (simulated).
[INFO][10:15:09]: [Server #1127936] Sending 0.24 MB of payload data to client #377 (simulated).
[INFO][10:15:09]: [Server #1127936] Selecting client #425 for training.
[INFO][10:15:09]: [Server #1127936] Sending the current model to client #425 (simulated).
[INFO][10:15:09]: [Server #1127936] Sending 0.24 MB of payload data to client #425 (simulated).
[INFO][10:15:09]: [Server #1127936] Selecting client #421 for training.
[INFO][10:15:09]: [Server #1127936] Sending the current model to client #421 (simulated).
[INFO][10:15:09]: [Client #377] Selected by the server.
[INFO][10:15:09]: [Client #377] Loading its data source...
[INFO][10:15:09]: [Client #377] Dataset size: 60000
[INFO][10:15:09]: [Client #377] Sampler: noniid
[INFO][10:15:09]: [Server #1127936] Sending 0.24 MB of payload data to client #421 (simulated).
[INFO][10:15:09]: [Client #425] Selected by the server.
[INFO][10:15:09]: [Client #425] Loading its data source...
[INFO][10:15:09]: [Client #421] Selected by the server.
[INFO][10:15:09]: [Client #425] Dataset size: 60000
[INFO][10:15:09]: [Client #421] Loading its data source...
[INFO][10:15:09]: [Client #425] Sampler: noniid
[INFO][10:15:09]: [Client #421] Dataset size: 60000
[INFO][10:15:09]: [Client #421] Sampler: noniid
[INFO][10:15:09]: [Client #377] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:15:09]: [Client #425] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:15:09]: [93m[1m[Client #377] Started training in communication round #52.[0m
[INFO][10:15:09]: [Client #421] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:15:09]: [93m[1m[Client #425] Started training in communication round #52.[0m
[INFO][10:15:09]: [93m[1m[Client #421] Started training in communication round #52.[0m
[INFO][10:15:11]: [Client #425] Loading the dataset.
[INFO][10:15:11]: [Client #377] Loading the dataset.
[INFO][10:15:11]: [Client #421] Loading the dataset.
[INFO][10:15:17]: [Client #425] Epoch: [1/5][0/10]	Loss: 0.016484
[INFO][10:15:17]: [Client #421] Epoch: [1/5][0/10]	Loss: 0.010745
[INFO][10:15:18]: [Client #425] Epoch: [2/5][0/10]	Loss: 0.001823
[INFO][10:15:18]: [Client #377] Epoch: [1/5][0/10]	Loss: 0.031671
[INFO][10:15:18]: [Client #377] Epoch: [2/5][0/10]	Loss: 0.002434
[INFO][10:15:18]: [Client #421] Epoch: [2/5][0/10]	Loss: 0.002233
[INFO][10:15:18]: [Client #425] Epoch: [3/5][0/10]	Loss: 0.000089
[INFO][10:15:18]: [Client #377] Epoch: [3/5][0/10]	Loss: 0.000024
[INFO][10:15:18]: [Client #421] Epoch: [3/5][0/10]	Loss: 0.000005
[INFO][10:15:18]: [Client #425] Epoch: [4/5][0/10]	Loss: 0.000642
[INFO][10:15:18]: [Client #377] Epoch: [4/5][0/10]	Loss: 0.000250
[INFO][10:15:18]: [Client #425] Epoch: [5/5][0/10]	Loss: 0.011162
[INFO][10:15:18]: [Client #421] Epoch: [4/5][0/10]	Loss: 0.000006
[INFO][10:15:18]: [Client #425] Model saved to /data/ykang/plato/results/test/model/lenet5_425_1127978.pth.
[INFO][10:15:18]: [Client #377] Epoch: [5/5][0/10]	Loss: 0.010715
[INFO][10:15:18]: [Client #421] Epoch: [5/5][0/10]	Loss: 0.000262
[INFO][10:15:18]: [Client #377] Model saved to /data/ykang/plato/results/test/model/lenet5_377_1127977.pth.
[INFO][10:15:18]: [Client #421] Model saved to /data/ykang/plato/results/test/model/lenet5_421_1127979.pth.
[INFO][10:15:19]: [Client #425] Loading a model from /data/ykang/plato/results/test/model/lenet5_425_1127978.pth.
[INFO][10:15:19]: [Client #425] Model trained.
[INFO][10:15:19]: [Client #425] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:15:19]: [Server #1127936] Received 0.24 MB of payload data from client #425 (simulated).
[INFO][10:15:19]: [Client #377] Loading a model from /data/ykang/plato/results/test/model/lenet5_377_1127977.pth.
[INFO][10:15:19]: [Client #377] Model trained.
[INFO][10:15:19]: [Client #377] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:15:19]: [Server #1127936] Received 0.24 MB of payload data from client #377 (simulated).
[INFO][10:15:19]: [Client #421] Loading a model from /data/ykang/plato/results/test/model/lenet5_421_1127979.pth.
[INFO][10:15:19]: [Client #421] Model trained.
[INFO][10:15:19]: [Client #421] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:15:19]: [Server #1127936] Received 0.24 MB of payload data from client #421 (simulated).
[INFO][10:15:19]: [Server #1127936] Selecting client #445 for training.
[INFO][10:15:19]: [Server #1127936] Sending the current model to client #445 (simulated).
[INFO][10:15:19]: [Server #1127936] Sending 0.24 MB of payload data to client #445 (simulated).
[INFO][10:15:19]: [Client #445] Selected by the server.
[INFO][10:15:19]: [Client #445] Loading its data source...
[INFO][10:15:19]: [Client #445] Dataset size: 60000
[INFO][10:15:19]: [Client #445] Sampler: noniid
[INFO][10:15:19]: [Client #445] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:15:19]: [93m[1m[Client #445] Started training in communication round #52.[0m
[INFO][10:15:21]: [Client #445] Loading the dataset.
[INFO][10:15:27]: [Client #445] Epoch: [1/5][0/10]	Loss: 0.000395
[INFO][10:15:27]: [Client #445] Epoch: [2/5][0/10]	Loss: 0.061280
[INFO][10:15:27]: [Client #445] Epoch: [3/5][0/10]	Loss: 0.000481
[INFO][10:15:27]: [Client #445] Epoch: [4/5][0/10]	Loss: 0.000009
[INFO][10:15:27]: [Client #445] Epoch: [5/5][0/10]	Loss: 0.000007
[INFO][10:15:27]: [Client #445] Model saved to /data/ykang/plato/results/test/model/lenet5_445_1127977.pth.
[INFO][10:15:27]: [Client #445] Loading a model from /data/ykang/plato/results/test/model/lenet5_445_1127977.pth.
[INFO][10:15:27]: [Client #445] Model trained.
[INFO][10:15:27]: [Client #445] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:15:27]: [Server #1127936] Received 0.24 MB of payload data from client #445 (simulated).
[INFO][10:15:27]: [Server #1127936] Adding client #95 to the list of clients for aggregation.
[INFO][10:15:27]: [Server #1127936] Adding client #270 to the list of clients for aggregation.
[INFO][10:15:27]: [Server #1127936] Adding client #480 to the list of clients for aggregation.
[INFO][10:15:27]: [Server #1127936] Adding client #400 to the list of clients for aggregation.
[INFO][10:15:27]: [Server #1127936] Adding client #430 to the list of clients for aggregation.
[INFO][10:15:27]: [Server #1127936] Adding client #21 to the list of clients for aggregation.
[INFO][10:15:27]: [Server #1127936] Adding client #251 to the list of clients for aggregation.
[INFO][10:15:27]: [Server #1127936] Adding client #142 to the list of clients for aggregation.
[INFO][10:15:27]: [Server #1127936] Adding client #421 to the list of clients for aggregation.
[INFO][10:15:27]: [Server #1127936] Adding client #321 to the list of clients for aggregation.
[INFO][10:15:27]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00528051 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00561303 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00556993 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00637609 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.0064528
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00514502 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01992631 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00333929 0.         0.         0.         0.         0.
 0.         0.         0.         0.01125273 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.0058782
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 3. 0. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 3. 1. 0. 0. 0. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 0. 0. 0. 1. 1.
 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 2. 1. 6. 0. 1. 2. 1. 0. 1. 1. 0. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0.
 1. 0. 1. 2. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 4. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0.
 1. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 1. 0. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 0. 0. 0. 1. 0. 0. 6. 1. 1.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 2. 0. 1. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 2. 2. 1. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00528051 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00561303 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00556993 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00637609 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.0064528
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00514502 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01992631 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00333929 0.         0.         0.         0.         0.
 0.         0.         0.         0.01125273 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.0058782
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:15:30]: [Server #1127936] Global model accuracy: 95.35%

[INFO][10:15:30]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_52.pth.
[INFO][10:15:30]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_52.pth.
[INFO][10:15:30]: [93m[1m
[Server #1127936] Starting round 53/100.[0m
[0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.002
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  2e-05  4e-10  4e-10
 6:  6.8876e+00  6.8875e+00  2e-05  2e-10  2e-10
 7:  6.8875e+00  6.8875e+00  2e-05  5e-10  1e-11
 8:  6.8875e+00  6.8875e+00  1e-05  4e-10  9e-12
 9:  6.8875e+00  6.8875e+00  8e-06  7e-10  1e-11
10:  6.8875e+00  6.8875e+00  5e-07  1e-09  2e-11
Optimal solution found.
The calculated probability is:  [4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 6.89674147e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 7.08543860e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83916260e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83913618e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 7.61040174e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83917510e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 9.76158969e-01 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83921713e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 1.30715526e-04 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 7.24334314e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05 4.83924772e-05 4.83924772e-05
 4.83924772e-05 4.83924772e-05]
current clients pool:  [INFO][10:15:30]: [Server #1127936] Selected clients: [400 440 315 106 175  81 224  42  84 100]
[INFO][10:15:30]: [Server #1127936] Selecting client #400 for training.
[INFO][10:15:30]: [Server #1127936] Sending the current model to client #400 (simulated).
[INFO][10:15:30]: [Server #1127936] Sending 0.24 MB of payload data to client #400 (simulated).
[INFO][10:15:30]: [Server #1127936] Selecting client #440 for training.
[INFO][10:15:30]: [Server #1127936] Sending the current model to client #440 (simulated).
[INFO][10:15:30]: [Server #1127936] Sending 0.24 MB of payload data to client #440 (simulated).
[INFO][10:15:30]: [Server #1127936] Selecting client #315 for training.
[INFO][10:15:30]: [Server #1127936] Sending the current model to client #315 (simulated).
[INFO][10:15:30]: [Client #400] Selected by the server.
[INFO][10:15:30]: [Client #400] Loading its data source...
[INFO][10:15:30]: [Client #400] Dataset size: 60000
[INFO][10:15:30]: [Client #400] Sampler: noniid
[INFO][10:15:30]: [Server #1127936] Sending 0.24 MB of payload data to client #315 (simulated).
[INFO][10:15:30]: [Client #440] Selected by the server.
[INFO][10:15:30]: [Client #440] Loading its data source...
[INFO][10:15:30]: [Client #440] Dataset size: 60000
[INFO][10:15:30]: [Client #440] Sampler: noniid
[INFO][10:15:30]: [Client #400] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:15:30]: [Client #315] Selected by the server.
[INFO][10:15:30]: [Client #315] Loading its data source...
[INFO][10:15:30]: [Client #315] Dataset size: 60000
[INFO][10:15:30]: [Client #315] Sampler: noniid
[INFO][10:15:30]: [Client #440] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:15:30]: [Client #315] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:15:30]: [93m[1m[Client #400] Started training in communication round #53.[0m
[INFO][10:15:30]: [93m[1m[Client #440] Started training in communication round #53.[0m
[INFO][10:15:30]: [93m[1m[Client #315] Started training in communication round #53.[0m
[INFO][10:15:33]: [Client #440] Loading the dataset.
[INFO][10:15:33]: [Client #400] Loading the dataset.
[INFO][10:15:33]: [Client #315] Loading the dataset.
[INFO][10:15:39]: [Client #440] Epoch: [1/5][0/10]	Loss: 0.000861
[INFO][10:15:39]: [Client #400] Epoch: [1/5][0/10]	Loss: 0.002747
[INFO][10:15:39]: [Client #315] Epoch: [1/5][0/10]	Loss: 0.005189
[INFO][10:15:39]: [Client #440] Epoch: [2/5][0/10]	Loss: 0.001676
[INFO][10:15:39]: [Client #315] Epoch: [2/5][0/10]	Loss: 0.000846
[INFO][10:15:39]: [Client #400] Epoch: [2/5][0/10]	Loss: 0.011982
[INFO][10:15:39]: [Client #440] Epoch: [3/5][0/10]	Loss: 0.000048
[INFO][10:15:39]: [Client #315] Epoch: [3/5][0/10]	Loss: 0.002911
[INFO][10:15:39]: [Client #440] Epoch: [4/5][0/10]	Loss: 0.000228
[INFO][10:15:39]: [Client #400] Epoch: [3/5][0/10]	Loss: 0.000553
[INFO][10:15:39]: [Client #440] Epoch: [5/5][0/10]	Loss: 0.003114
[INFO][10:15:39]: [Client #440] Model saved to /data/ykang/plato/results/test/model/lenet5_440_1127978.pth.
[INFO][10:15:39]: [Client #315] Epoch: [4/5][0/10]	Loss: 0.625308
[INFO][10:15:39]: [Client #400] Epoch: [4/5][0/10]	Loss: 0.000188
[INFO][10:15:39]: [Client #400] Epoch: [5/5][0/10]	Loss: 0.003730
[INFO][10:15:39]: [Client #315] Epoch: [5/5][0/10]	Loss: 0.208180
[INFO][10:15:39]: [Client #315] Model saved to /data/ykang/plato/results/test/model/lenet5_315_1127979.pth.
[INFO][10:15:39]: [Client #400] Model saved to /data/ykang/plato/results/test/model/lenet5_400_1127977.pth.
[INFO][10:15:40]: [Client #440] Loading a model from /data/ykang/plato/results/test/model/lenet5_440_1127978.pth.
[INFO][10:15:40]: [Client #440] Model trained.
[INFO][10:15:40]: [Client #440] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:15:40]: [Server #1127936] Received 0.24 MB of payload data from client #440 (simulated).
[INFO][10:15:40]: [Client #315] Loading a model from /data/ykang/plato/results/test/model/lenet5_315_1127979.pth.
[INFO][10:15:40]: [Client #315] Model trained.
[INFO][10:15:40]: [Client #315] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:15:40]: [Server #1127936] Received 0.24 MB of payload data from client #315 (simulated).
[INFO][10:15:40]: [Client #400] Loading a model from /data/ykang/plato/results/test/model/lenet5_400_1127977.pth.
[INFO][10:15:40]: [Client #400] Model trained.
[INFO][10:15:40]: [Client #400] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:15:40]: [Server #1127936] Received 0.24 MB of payload data from client #400 (simulated).
[INFO][10:15:40]: [Server #1127936] Selecting client #106 for training.
[INFO][10:15:40]: [Server #1127936] Sending the current model to client #106 (simulated).
[INFO][10:15:40]: [Server #1127936] Sending 0.24 MB of payload data to client #106 (simulated).
[INFO][10:15:40]: [Server #1127936] Selecting client #175 for training.
[INFO][10:15:40]: [Server #1127936] Sending the current model to client #175 (simulated).
[INFO][10:15:40]: [Server #1127936] Sending 0.24 MB of payload data to client #175 (simulated).
[INFO][10:15:40]: [Server #1127936] Selecting client #81 for training.
[INFO][10:15:40]: [Server #1127936] Sending the current model to client #81 (simulated).
[INFO][10:15:40]: [Client #106] Selected by the server.
[INFO][10:15:40]: [Client #106] Loading its data source...
[INFO][10:15:40]: [Client #106] Dataset size: 60000
[INFO][10:15:40]: [Client #106] Sampler: noniid
[INFO][10:15:40]: [Server #1127936] Sending 0.24 MB of payload data to client #81 (simulated).
[INFO][10:15:40]: [Client #175] Selected by the server.
[INFO][10:15:40]: [Client #175] Loading its data source...
[INFO][10:15:40]: [Client #175] Dataset size: 60000
[INFO][10:15:40]: [Client #175] Sampler: noniid
[INFO][10:15:40]: [Client #81] Selected by the server.
[INFO][10:15:40]: [Client #81] Loading its data source...
[INFO][10:15:40]: [Client #81] Dataset size: 60000
[INFO][10:15:40]: [Client #81] Sampler: noniid
[INFO][10:15:40]: [Client #106] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:15:40]: [Client #81] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:15:40]: [Client #175] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:15:40]: [93m[1m[Client #81] Started training in communication round #53.[0m
[INFO][10:15:40]: [93m[1m[Client #175] Started training in communication round #53.[0m
[INFO][10:15:40]: [93m[1m[Client #106] Started training in communication round #53.[0m
[INFO][10:15:42]: [Client #175] Loading the dataset.
[INFO][10:15:42]: [Client #106] Loading the dataset.
[INFO][10:15:42]: [Client #81] Loading the dataset.
[INFO][10:15:48]: [Client #106] Epoch: [1/5][0/10]	Loss: 0.010942
[INFO][10:15:48]: [Client #81] Epoch: [1/5][0/10]	Loss: 0.000912
[INFO][10:15:48]: [Client #175] Epoch: [1/5][0/10]	Loss: 0.001884
[INFO][10:15:49]: [Client #106] Epoch: [2/5][0/10]	Loss: 0.002994
[INFO][10:15:49]: [Client #81] Epoch: [2/5][0/10]	Loss: 0.000617
[INFO][10:15:49]: [Client #175] Epoch: [2/5][0/10]	Loss: 0.004453
[INFO][10:15:49]: [Client #106] Epoch: [3/5][0/10]	Loss: 0.000373
[INFO][10:15:49]: [Client #81] Epoch: [3/5][0/10]	Loss: 0.000219
[INFO][10:15:49]: [Client #106] Epoch: [4/5][0/10]	Loss: 0.000034
[INFO][10:15:49]: [Client #175] Epoch: [3/5][0/10]	Loss: 0.000010
[INFO][10:15:49]: [Client #106] Epoch: [5/5][0/10]	Loss: 0.000237
[INFO][10:15:49]: [Client #175] Epoch: [4/5][0/10]	Loss: 0.000050
[INFO][10:15:49]: [Client #81] Epoch: [4/5][0/10]	Loss: 0.001563
[INFO][10:15:49]: [Client #106] Model saved to /data/ykang/plato/results/test/model/lenet5_106_1127977.pth.
[INFO][10:15:49]: [Client #175] Epoch: [5/5][0/10]	Loss: 0.000502
[INFO][10:15:49]: [Client #81] Epoch: [5/5][0/10]	Loss: 0.000075
[INFO][10:15:49]: [Client #81] Model saved to /data/ykang/plato/results/test/model/lenet5_81_1127979.pth.
[INFO][10:15:49]: [Client #175] Model saved to /data/ykang/plato/results/test/model/lenet5_175_1127978.pth.
[INFO][10:15:50]: [Client #106] Loading a model from /data/ykang/plato/results/test/model/lenet5_106_1127977.pth.
[INFO][10:15:50]: [Client #106] Model trained.
[INFO][10:15:50]: [Client #106] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:15:50]: [Server #1127936] Received 0.24 MB of payload data from client #106 (simulated).
[INFO][10:15:50]: [Client #81] Loading a model from /data/ykang/plato/results/test/model/lenet5_81_1127979.pth.
[INFO][10:15:50]: [Client #81] Model trained.
[INFO][10:15:50]: [Client #81] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:15:50]: [Server #1127936] Received 0.24 MB of payload data from client #81 (simulated).
[INFO][10:15:50]: [Client #175] Loading a model from /data/ykang/plato/results/test/model/lenet5_175_1127978.pth.
[INFO][10:15:50]: [Client #175] Model trained.
[INFO][10:15:50]: [Client #175] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:15:50]: [Server #1127936] Received 0.24 MB of payload data from client #175 (simulated).
[INFO][10:15:50]: [Server #1127936] Selecting client #224 for training.
[INFO][10:15:50]: [Server #1127936] Sending the current model to client #224 (simulated).
[INFO][10:15:50]: [Server #1127936] Sending 0.24 MB of payload data to client #224 (simulated).
[INFO][10:15:50]: [Server #1127936] Selecting client #42 for training.
[INFO][10:15:50]: [Server #1127936] Sending the current model to client #42 (simulated).
[INFO][10:15:50]: [Server #1127936] Sending 0.24 MB of payload data to client #42 (simulated).
[INFO][10:15:50]: [Server #1127936] Selecting client #84 for training.
[INFO][10:15:50]: [Server #1127936] Sending the current model to client #84 (simulated).
[INFO][10:15:50]: [Client #224] Selected by the server.
[INFO][10:15:50]: [Client #224] Loading its data source...
[INFO][10:15:50]: [Client #224] Dataset size: 60000
[INFO][10:15:50]: [Client #224] Sampler: noniid
[INFO][10:15:50]: [Server #1127936] Sending 0.24 MB of payload data to client #84 (simulated).
[INFO][10:15:50]: [Client #84] Selected by the server.
[INFO][10:15:50]: [Client #84] Loading its data source...
[INFO][10:15:50]: [Client #84] Dataset size: 60000
[INFO][10:15:50]: [Client #84] Sampler: noniid
[INFO][10:15:50]: [Client #42] Selected by the server.
[INFO][10:15:50]: [Client #42] Loading its data source...
[INFO][10:15:50]: [Client #42] Dataset size: 60000
[INFO][10:15:50]: [Client #42] Sampler: noniid
[INFO][10:15:50]: [Client #224] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:15:50]: [Client #84] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:15:50]: [93m[1m[Client #224] Started training in communication round #53.[0m
[INFO][10:15:50]: [Client #42] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:15:50]: [93m[1m[Client #84] Started training in communication round #53.[0m
[INFO][10:15:50]: [93m[1m[Client #42] Started training in communication round #53.[0m
[INFO][10:15:52]: [Client #224] Loading the dataset.
[INFO][10:15:52]: [Client #42] Loading the dataset.
[INFO][10:15:52]: [Client #84] Loading the dataset.
[INFO][10:15:58]: [Client #224] Epoch: [1/5][0/10]	Loss: 0.007631
[INFO][10:15:58]: [Client #84] Epoch: [1/5][0/10]	Loss: 0.007616
[INFO][10:15:58]: [Client #42] Epoch: [1/5][0/10]	Loss: 0.000254
[INFO][10:15:58]: [Client #224] Epoch: [2/5][0/10]	Loss: 0.000002
[INFO][10:15:58]: [Client #42] Epoch: [2/5][0/10]	Loss: 0.000810
[INFO][10:15:58]: [Client #84] Epoch: [2/5][0/10]	Loss: 0.182484
[INFO][10:15:58]: [Client #224] Epoch: [3/5][0/10]	Loss: 0.000002
[INFO][10:15:59]: [Client #42] Epoch: [3/5][0/10]	Loss: 0.000037
[INFO][10:15:59]: [Client #224] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:15:59]: [Client #84] Epoch: [3/5][0/10]	Loss: 0.000046
[INFO][10:15:59]: [Client #42] Epoch: [4/5][0/10]	Loss: 0.000003
[INFO][10:15:59]: [Client #84] Epoch: [4/5][0/10]	Loss: 0.032456
[INFO][10:15:59]: [Client #224] Epoch: [5/5][0/10]	Loss: 0.000056
[INFO][10:15:59]: [Client #42] Epoch: [5/5][0/10]	Loss: 0.000780
[INFO][10:15:59]: [Client #84] Epoch: [5/5][0/10]	Loss: 0.186590
[INFO][10:15:59]: [Client #42] Model saved to /data/ykang/plato/results/test/model/lenet5_42_1127978.pth.
[INFO][10:15:59]: [Client #224] Model saved to /data/ykang/plato/results/test/model/lenet5_224_1127977.pth.
[INFO][10:15:59]: [Client #84] Model saved to /data/ykang/plato/results/test/model/lenet5_84_1127979.pth.
[INFO][10:16:00]: [Client #42] Loading a model from /data/ykang/plato/results/test/model/lenet5_42_1127978.pth.
[INFO][10:16:00]: [Client #42] Model trained.
[INFO][10:16:00]: [Client #42] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:16:00]: [Server #1127936] Received 0.24 MB of payload data from client #42 (simulated).
[INFO][10:16:00]: [Client #224] Loading a model from /data/ykang/plato/results/test/model/lenet5_224_1127977.pth.
[INFO][10:16:00]: [Client #224] Model trained.
[INFO][10:16:00]: [Client #224] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:16:00]: [Server #1127936] Received 0.24 MB of payload data from client #224 (simulated).
[INFO][10:16:00]: [Client #84] Loading a model from /data/ykang/plato/results/test/model/lenet5_84_1127979.pth.
[INFO][10:16:00]: [Client #84] Model trained.
[INFO][10:16:00]: [Client #84] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:16:00]: [Server #1127936] Received 0.24 MB of payload data from client #84 (simulated).
[INFO][10:16:00]: [Server #1127936] Selecting client #100 for training.
[INFO][10:16:00]: [Server #1127936] Sending the current model to client #100 (simulated).
[INFO][10:16:00]: [Server #1127936] Sending 0.24 MB of payload data to client #100 (simulated).
[INFO][10:16:00]: [Client #100] Selected by the server.
[INFO][10:16:00]: [Client #100] Loading its data source...
[INFO][10:16:00]: [Client #100] Dataset size: 60000
[INFO][10:16:00]: [Client #100] Sampler: noniid
[INFO][10:16:00]: [Client #100] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:16:00]: [93m[1m[Client #100] Started training in communication round #53.[0m
[INFO][10:16:02]: [Client #100] Loading the dataset.
[INFO][10:16:07]: [Client #100] Epoch: [1/5][0/10]	Loss: 0.006535
[INFO][10:16:08]: [Client #100] Epoch: [2/5][0/10]	Loss: 0.002352
[INFO][10:16:08]: [Client #100] Epoch: [3/5][0/10]	Loss: 0.000192
[INFO][10:16:08]: [Client #100] Epoch: [4/5][0/10]	Loss: 0.001165
[INFO][10:16:08]: [Client #100] Epoch: [5/5][0/10]	Loss: 0.000001
[INFO][10:16:08]: [Client #100] Model saved to /data/ykang/plato/results/test/model/lenet5_100_1127977.pth.
[INFO][10:16:08]: [Client #100] Loading a model from /data/ykang/plato/results/test/model/lenet5_100_1127977.pth.
[INFO][10:16:08]: [Client #100] Model trained.
[INFO][10:16:08]: [Client #100] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:16:08]: [Server #1127936] Received 0.24 MB of payload data from client #100 (simulated).
[INFO][10:16:08]: [Server #1127936] Adding client #93 to the list of clients for aggregation.
[INFO][10:16:08]: [Server #1127936] Adding client #425 to the list of clients for aggregation.
[INFO][10:16:08]: [Server #1127936] Adding client #57 to the list of clients for aggregation.
[INFO][10:16:08]: [Server #1127936] Adding client #467 to the list of clients for aggregation.
[INFO][10:16:08]: [Server #1127936] Adding client #401 to the list of clients for aggregation.
[INFO][10:16:08]: [Server #1127936] Adding client #377 to the list of clients for aggregation.
[INFO][10:16:08]: [Server #1127936] Adding client #172 to the list of clients for aggregation.
[INFO][10:16:08]: [Server #1127936] Adding client #440 to the list of clients for aggregation.
[INFO][10:16:08]: [Server #1127936] Adding client #315 to the list of clients for aggregation.
[INFO][10:16:08]: [Server #1127936] Adding client #84 to the list of clients for aggregation.
[INFO][10:16:08]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.02155354 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.02350189
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00793229 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01684202 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01807052 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00554167 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00956741 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00649628 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.0016281  0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00840151 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 3. 0. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 3. 1. 0. 0. 0. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 0. 1. 0. 1. 1.
 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 2. 1. 6. 0. 1. 2. 1. 0. 1. 1. 0. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 0. 6. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0.
 1. 0. 1. 2. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 4. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0.
 1. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 1. 0. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 3. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 0. 0. 0. 1. 0. 0. 6. 1. 1.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 6. 0. 1. 1. 1. 0. 0. 0. 2. 0. 1. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 2. 2. 1. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.02155354 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.02350189
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00793229 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01684202 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01807052 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00554167 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00956741 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00649628 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.0016281  0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00840151 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:16:11]: [Server #1127936] Global model accuracy: 95.67%

[INFO][10:16:11]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_53.pth.
[INFO][10:16:11]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_53.pth.
[INFO][10:16:11]: [93m[1m
[Server #1127936] Starting round 54/100.[0m
[0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.002
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8871e+00  4e-04  7e-09  7e-09
 5:  6.8876e+00  6.8874e+00  1e-04  2e-09  2e-09
 6:  6.8875e+00  6.8874e+00  1e-04  8e-10  9e-11
 7:  6.8875e+00  6.8874e+00  8e-05  7e-09  8e-10
 8:  6.8875e+00  6.8875e+00  6e-05  1e-08  1e-09
 9:  6.8875e+00  6.8875e+00  1e-05  4e-08  4e-09
10:  6.8875e+00  6.8875e+00  2e-06  9e-09  1e-09
Optimal solution found.
The calculated probability is:  [5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 7.06151578e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11871588e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.70001295e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 9.74817843e-01 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11886485e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.51193738e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 8.05898108e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.58557876e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11907855e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 1.37825017e-04 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05 5.11908030e-05 5.11908030e-05
 5.11908030e-05 5.11908030e-05]
current clients pool:  [INFO][10:16:11]: [Server #1127936] Selected clients: [172 190 130 116  24   5 251  23 143 324]
[INFO][10:16:11]: [Server #1127936] Selecting client #172 for training.
[INFO][10:16:11]: [Server #1127936] Sending the current model to client #172 (simulated).
[INFO][10:16:11]: [Server #1127936] Sending 0.24 MB of payload data to client #172 (simulated).
[INFO][10:16:11]: [Server #1127936] Selecting client #190 for training.
[INFO][10:16:11]: [Server #1127936] Sending the current model to client #190 (simulated).
[INFO][10:16:11]: [Server #1127936] Sending 0.24 MB of payload data to client #190 (simulated).
[INFO][10:16:11]: [Server #1127936] Selecting client #130 for training.
[INFO][10:16:11]: [Server #1127936] Sending the current model to client #130 (simulated).
[INFO][10:16:11]: [Client #172] Selected by the server.
[INFO][10:16:11]: [Client #172] Loading its data source...
[INFO][10:16:11]: [Client #172] Dataset size: 60000
[INFO][10:16:11]: [Client #172] Sampler: noniid
[INFO][10:16:11]: [Server #1127936] Sending 0.24 MB of payload data to client #130 (simulated).
[INFO][10:16:11]: [Client #190] Selected by the server.
[INFO][10:16:11]: [Client #190] Loading its data source...
[INFO][10:16:11]: [Client #190] Dataset size: 60000
[INFO][10:16:11]: [Client #190] Sampler: noniid
[INFO][10:16:11]: [Client #130] Selected by the server.
[INFO][10:16:11]: [Client #172] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:16:11]: [Client #130] Loading its data source...
[INFO][10:16:11]: [Client #130] Dataset size: 60000
[INFO][10:16:11]: [Client #130] Sampler: noniid
[INFO][10:16:11]: [Client #190] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:16:11]: [93m[1m[Client #172] Started training in communication round #54.[0m
[INFO][10:16:11]: [93m[1m[Client #190] Started training in communication round #54.[0m
[INFO][10:16:11]: [Client #130] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:16:11]: [93m[1m[Client #130] Started training in communication round #54.[0m
[INFO][10:16:13]: [Client #130] Loading the dataset.
[INFO][10:16:13]: [Client #172] Loading the dataset.
[INFO][10:16:13]: [Client #190] Loading the dataset.
[INFO][10:16:20]: [Client #190] Epoch: [1/5][0/10]	Loss: 0.020363
[INFO][10:16:20]: [Client #172] Epoch: [1/5][0/10]	Loss: 0.002205
[INFO][10:16:20]: [Client #130] Epoch: [1/5][0/10]	Loss: 0.020137
[INFO][10:16:20]: [Client #190] Epoch: [2/5][0/10]	Loss: 0.000002
[INFO][10:16:20]: [Client #172] Epoch: [2/5][0/10]	Loss: 0.000422
[INFO][10:16:20]: [Client #130] Epoch: [2/5][0/10]	Loss: 0.000256
[INFO][10:16:20]: [Client #172] Epoch: [3/5][0/10]	Loss: 0.000014
[INFO][10:16:20]: [Client #190] Epoch: [3/5][0/10]	Loss: 0.000002
[INFO][10:16:20]: [Client #130] Epoch: [3/5][0/10]	Loss: 0.000009
[INFO][10:16:20]: [Client #190] Epoch: [4/5][0/10]	Loss: 0.003988
[INFO][10:16:20]: [Client #172] Epoch: [4/5][0/10]	Loss: 0.000001
[INFO][10:16:20]: [Client #130] Epoch: [4/5][0/10]	Loss: 0.000003
[INFO][10:16:20]: [Client #190] Epoch: [5/5][0/10]	Loss: 0.001377
[INFO][10:16:20]: [Client #190] Model saved to /data/ykang/plato/results/test/model/lenet5_190_1127978.pth.
[INFO][10:16:20]: [Client #172] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:16:20]: [Client #130] Epoch: [5/5][0/10]	Loss: 0.000010
[INFO][10:16:20]: [Client #172] Model saved to /data/ykang/plato/results/test/model/lenet5_172_1127977.pth.
[INFO][10:16:20]: [Client #130] Model saved to /data/ykang/plato/results/test/model/lenet5_130_1127979.pth.
[INFO][10:16:21]: [Client #190] Loading a model from /data/ykang/plato/results/test/model/lenet5_190_1127978.pth.
[INFO][10:16:21]: [Client #190] Model trained.
[INFO][10:16:21]: [Client #190] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:16:21]: [Server #1127936] Received 0.24 MB of payload data from client #190 (simulated).
[INFO][10:16:21]: [Client #172] Loading a model from /data/ykang/plato/results/test/model/lenet5_172_1127977.pth.
[INFO][10:16:21]: [Client #172] Model trained.
[INFO][10:16:21]: [Client #172] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:16:21]: [Server #1127936] Received 0.24 MB of payload data from client #172 (simulated).
[INFO][10:16:21]: [Client #130] Loading a model from /data/ykang/plato/results/test/model/lenet5_130_1127979.pth.
[INFO][10:16:21]: [Client #130] Model trained.
[INFO][10:16:21]: [Client #130] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:16:21]: [Server #1127936] Received 0.24 MB of payload data from client #130 (simulated).
[INFO][10:16:21]: [Server #1127936] Selecting client #116 for training.
[INFO][10:16:21]: [Server #1127936] Sending the current model to client #116 (simulated).
[INFO][10:16:21]: [Server #1127936] Sending 0.24 MB of payload data to client #116 (simulated).
[INFO][10:16:21]: [Server #1127936] Selecting client #24 for training.
[INFO][10:16:21]: [Server #1127936] Sending the current model to client #24 (simulated).
[INFO][10:16:21]: [Server #1127936] Sending 0.24 MB of payload data to client #24 (simulated).
[INFO][10:16:21]: [Server #1127936] Selecting client #5 for training.
[INFO][10:16:21]: [Server #1127936] Sending the current model to client #5 (simulated).
[INFO][10:16:21]: [Client #116] Selected by the server.
[INFO][10:16:21]: [Client #116] Loading its data source...
[INFO][10:16:21]: [Client #116] Dataset size: 60000
[INFO][10:16:21]: [Client #116] Sampler: noniid
[INFO][10:16:21]: [Server #1127936] Sending 0.24 MB of payload data to client #5 (simulated).
[INFO][10:16:21]: [Client #116] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:16:21]: [Client #24] Selected by the server.
[INFO][10:16:21]: [Client #24] Loading its data source...
[INFO][10:16:21]: [Client #24] Dataset size: 60000
[INFO][10:16:21]: [Client #24] Sampler: noniid
[INFO][10:16:21]: [Client #5] Selected by the server.
[INFO][10:16:21]: [Client #5] Loading its data source...
[INFO][10:16:21]: [Client #5] Dataset size: 60000
[INFO][10:16:21]: [Client #5] Sampler: noniid
[INFO][10:16:21]: [Client #24] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:16:21]: [93m[1m[Client #116] Started training in communication round #54.[0m
[INFO][10:16:21]: [Client #5] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:16:21]: [93m[1m[Client #24] Started training in communication round #54.[0m
[INFO][10:16:21]: [93m[1m[Client #5] Started training in communication round #54.[0m
[INFO][10:16:23]: [Client #24] Loading the dataset.
[INFO][10:16:23]: [Client #5] Loading the dataset.
[INFO][10:16:23]: [Client #116] Loading the dataset.
[INFO][10:16:29]: [Client #24] Epoch: [1/5][0/10]	Loss: 0.007582
[INFO][10:16:29]: [Client #5] Epoch: [1/5][0/10]	Loss: 0.032547
[INFO][10:16:29]: [Client #116] Epoch: [1/5][0/10]	Loss: 0.014378
[INFO][10:16:29]: [Client #5] Epoch: [2/5][0/10]	Loss: 0.000002
[INFO][10:16:29]: [Client #24] Epoch: [2/5][0/10]	Loss: 0.006064
[INFO][10:16:29]: [Client #116] Epoch: [2/5][0/10]	Loss: 0.003872
[INFO][10:16:29]: [Client #5] Epoch: [3/5][0/10]	Loss: 0.000356
[INFO][10:16:29]: [Client #24] Epoch: [3/5][0/10]	Loss: 0.000195
[INFO][10:16:29]: [Client #116] Epoch: [3/5][0/10]	Loss: 0.000033
[INFO][10:16:29]: [Client #5] Epoch: [4/5][0/10]	Loss: 0.002683
[INFO][10:16:29]: [Client #24] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:16:30]: [Client #5] Epoch: [5/5][0/10]	Loss: 0.010518
[INFO][10:16:30]: [Client #116] Epoch: [4/5][0/10]	Loss: 0.000183
[INFO][10:16:30]: [Client #5] Model saved to /data/ykang/plato/results/test/model/lenet5_5_1127979.pth.
[INFO][10:16:30]: [Client #24] Epoch: [5/5][0/10]	Loss: 0.026892
[INFO][10:16:30]: [Client #24] Model saved to /data/ykang/plato/results/test/model/lenet5_24_1127978.pth.
[INFO][10:16:30]: [Client #116] Epoch: [5/5][0/10]	Loss: 0.000164
[INFO][10:16:30]: [Client #116] Model saved to /data/ykang/plato/results/test/model/lenet5_116_1127977.pth.
[INFO][10:16:30]: [Client #5] Loading a model from /data/ykang/plato/results/test/model/lenet5_5_1127979.pth.
[INFO][10:16:30]: [Client #5] Model trained.
[INFO][10:16:30]: [Client #5] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:16:30]: [Server #1127936] Received 0.24 MB of payload data from client #5 (simulated).
[INFO][10:16:30]: [Client #24] Loading a model from /data/ykang/plato/results/test/model/lenet5_24_1127978.pth.
[INFO][10:16:30]: [Client #24] Model trained.
[INFO][10:16:30]: [Client #24] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:16:30]: [Server #1127936] Received 0.24 MB of payload data from client #24 (simulated).
[INFO][10:16:30]: [Client #116] Loading a model from /data/ykang/plato/results/test/model/lenet5_116_1127977.pth.
[INFO][10:16:30]: [Client #116] Model trained.
[INFO][10:16:30]: [Client #116] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:16:31]: [Server #1127936] Received 0.24 MB of payload data from client #116 (simulated).
[INFO][10:16:31]: [Server #1127936] Selecting client #251 for training.
[INFO][10:16:31]: [Server #1127936] Sending the current model to client #251 (simulated).
[INFO][10:16:31]: [Server #1127936] Sending 0.24 MB of payload data to client #251 (simulated).
[INFO][10:16:31]: [Server #1127936] Selecting client #23 for training.
[INFO][10:16:31]: [Server #1127936] Sending the current model to client #23 (simulated).
[INFO][10:16:31]: [Server #1127936] Sending 0.24 MB of payload data to client #23 (simulated).
[INFO][10:16:31]: [Server #1127936] Selecting client #143 for training.
[INFO][10:16:31]: [Server #1127936] Sending the current model to client #143 (simulated).
[INFO][10:16:31]: [Client #251] Selected by the server.
[INFO][10:16:31]: [Client #251] Loading its data source...
[INFO][10:16:31]: [Client #251] Dataset size: 60000
[INFO][10:16:31]: [Client #251] Sampler: noniid
[INFO][10:16:31]: [Server #1127936] Sending 0.24 MB of payload data to client #143 (simulated).
[INFO][10:16:31]: [Client #23] Selected by the server.
[INFO][10:16:31]: [Client #23] Loading its data source...
[INFO][10:16:31]: [Client #23] Dataset size: 60000
[INFO][10:16:31]: [Client #143] Selected by the server.
[INFO][10:16:31]: [Client #23] Sampler: noniid
[INFO][10:16:31]: [Client #143] Loading its data source...
[INFO][10:16:31]: [Client #143] Dataset size: 60000
[INFO][10:16:31]: [Client #143] Sampler: noniid
[INFO][10:16:31]: [Client #251] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:16:31]: [Client #23] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:16:31]: [Client #143] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:16:31]: [93m[1m[Client #251] Started training in communication round #54.[0m
[INFO][10:16:31]: [93m[1m[Client #23] Started training in communication round #54.[0m
[INFO][10:16:31]: [93m[1m[Client #143] Started training in communication round #54.[0m
[INFO][10:16:33]: [Client #143] Loading the dataset.
[INFO][10:16:33]: [Client #23] Loading the dataset.
[INFO][10:16:33]: [Client #251] Loading the dataset.
[INFO][10:16:38]: [Client #143] Epoch: [1/5][0/10]	Loss: 0.002205
[INFO][10:16:39]: [Client #143] Epoch: [2/5][0/10]	Loss: 0.000239
[INFO][10:16:39]: [Client #23] Epoch: [1/5][0/10]	Loss: 0.000617
[INFO][10:16:39]: [Client #251] Epoch: [1/5][0/10]	Loss: 0.000836
[INFO][10:16:39]: [Client #143] Epoch: [3/5][0/10]	Loss: 0.000010
[INFO][10:16:39]: [Client #23] Epoch: [2/5][0/10]	Loss: 0.001645
[INFO][10:16:39]: [Client #251] Epoch: [2/5][0/10]	Loss: 0.013707
[INFO][10:16:39]: [Client #143] Epoch: [4/5][0/10]	Loss: 0.000174
[INFO][10:16:39]: [Client #23] Epoch: [3/5][0/10]	Loss: 0.000238
[INFO][10:16:39]: [Client #251] Epoch: [3/5][0/10]	Loss: 0.000195
[INFO][10:16:39]: [Client #143] Epoch: [5/5][0/10]	Loss: 0.016827
[INFO][10:16:39]: [Client #23] Epoch: [4/5][0/10]	Loss: 0.000994
[INFO][10:16:39]: [Client #143] Model saved to /data/ykang/plato/results/test/model/lenet5_143_1127979.pth.
[INFO][10:16:39]: [Client #251] Epoch: [4/5][0/10]	Loss: 0.000165
[INFO][10:16:39]: [Client #23] Epoch: [5/5][0/10]	Loss: 0.003401
[INFO][10:16:39]: [Client #251] Epoch: [5/5][0/10]	Loss: 0.001175
[INFO][10:16:39]: [Client #23] Model saved to /data/ykang/plato/results/test/model/lenet5_23_1127978.pth.
[INFO][10:16:39]: [Client #251] Model saved to /data/ykang/plato/results/test/model/lenet5_251_1127977.pth.
[INFO][10:16:40]: [Client #143] Loading a model from /data/ykang/plato/results/test/model/lenet5_143_1127979.pth.
[INFO][10:16:40]: [Client #143] Model trained.
[INFO][10:16:40]: [Client #143] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:16:40]: [Server #1127936] Received 0.24 MB of payload data from client #143 (simulated).
[INFO][10:16:40]: [Client #23] Loading a model from /data/ykang/plato/results/test/model/lenet5_23_1127978.pth.
[INFO][10:16:40]: [Client #23] Model trained.
[INFO][10:16:40]: [Client #23] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:16:40]: [Server #1127936] Received 0.24 MB of payload data from client #23 (simulated).
[INFO][10:16:40]: [Client #251] Loading a model from /data/ykang/plato/results/test/model/lenet5_251_1127977.pth.
[INFO][10:16:40]: [Client #251] Model trained.
[INFO][10:16:40]: [Client #251] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:16:40]: [Server #1127936] Received 0.24 MB of payload data from client #251 (simulated).
[INFO][10:16:40]: [Server #1127936] Selecting client #324 for training.
[INFO][10:16:40]: [Server #1127936] Sending the current model to client #324 (simulated).
[INFO][10:16:40]: [Server #1127936] Sending 0.24 MB of payload data to client #324 (simulated).
[INFO][10:16:40]: [Client #324] Selected by the server.
[INFO][10:16:40]: [Client #324] Loading its data source...
[INFO][10:16:40]: [Client #324] Dataset size: 60000
[INFO][10:16:40]: [Client #324] Sampler: noniid
[INFO][10:16:40]: [Client #324] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:16:40]: [93m[1m[Client #324] Started training in communication round #54.[0m
[INFO][10:16:42]: [Client #324] Loading the dataset.
[INFO][10:16:47]: [Client #324] Epoch: [1/5][0/10]	Loss: 0.012884
[INFO][10:16:47]: [Client #324] Epoch: [2/5][0/10]	Loss: 0.000088
[INFO][10:16:47]: [Client #324] Epoch: [3/5][0/10]	Loss: 0.000023
[INFO][10:16:47]: [Client #324] Epoch: [4/5][0/10]	Loss: 0.000371
[INFO][10:16:47]: [Client #324] Epoch: [5/5][0/10]	Loss: 0.000002
[INFO][10:16:47]: [Client #324] Model saved to /data/ykang/plato/results/test/model/lenet5_324_1127977.pth.
[INFO][10:16:48]: [Client #324] Loading a model from /data/ykang/plato/results/test/model/lenet5_324_1127977.pth.
[INFO][10:16:48]: [Client #324] Model trained.
[INFO][10:16:48]: [Client #324] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:16:48]: [Server #1127936] Received 0.24 MB of payload data from client #324 (simulated).
[INFO][10:16:48]: [Server #1127936] Adding client #400 to the list of clients for aggregation.
[INFO][10:16:48]: [Server #1127936] Adding client #175 to the list of clients for aggregation.
[INFO][10:16:48]: [Server #1127936] Adding client #100 to the list of clients for aggregation.
[INFO][10:16:48]: [Server #1127936] Adding client #106 to the list of clients for aggregation.
[INFO][10:16:48]: [Server #1127936] Adding client #42 to the list of clients for aggregation.
[INFO][10:16:48]: [Server #1127936] Adding client #224 to the list of clients for aggregation.
[INFO][10:16:48]: [Server #1127936] Adding client #81 to the list of clients for aggregation.
[INFO][10:16:48]: [Server #1127936] Adding client #445 to the list of clients for aggregation.
[INFO][10:16:48]: [Server #1127936] Adding client #251 to the list of clients for aggregation.
[INFO][10:16:48]: [Server #1127936] Adding client #143 to the list of clients for aggregation.
[INFO][10:16:48]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 102, 103, 104, 105, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00955545
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.0050347  0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00792664 0.         0.
 0.         0.         0.         0.00771086 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.03755958 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00317876 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00700814 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00339375 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01208859 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00307604 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 3. 0. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 3. 1. 0. 0. 0. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 0. 1. 0. 1. 1.
 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 2. 1. 6. 0. 1. 2. 1. 0. 1. 1. 0. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 0. 6. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0.
 1. 0. 1. 2. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 4. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0.
 1. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 1. 0. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 3. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 0. 0. 0. 1. 0. 0. 6. 1. 1.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 6. 0. 1. 1. 1. 0. 0. 0. 2. 0. 1. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 2. 2. 1. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00955545
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.0050347  0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00792664 0.         0.
 0.         0.         0.         0.00771086 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.03755958 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00317876 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00700814 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00339375 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01208859 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00307604 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:16:50]: [Server #1127936] Global model accuracy: 95.91%

[INFO][10:16:50]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_54.pth.
[INFO][10:16:50]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_54.pth.
[INFO][10:16:50]: [93m[1m
[Server #1127936] Starting round 55/100.[0m
[INFO][10:16:51]: [Server #1127936] Selected clients: [400 374 398 184 469 404 470  92 467 139]
[INFO][10:16:51]: [Server #1127936] Selecting client #400 for training.
[INFO][10:16:51]: [Server #1127936] Sending the current model to client #400 (simulated).
[INFO][10:16:51]: [Server #1127936] Sending 0.24 MB of payload data to client #400 (simulated).
[INFO][10:16:51]: [Server #1127936] Selecting client #374 for training.
[INFO][10:16:51]: [Server #1127936] Sending the current model to client #374 (simulated).
[INFO][10:16:51]: [Server #1127936] Sending 0.24 MB of payload data to client #374 (simulated).
[INFO][10:16:51]: [Server #1127936] Selecting client #398 for training.
[INFO][10:16:51]: [Server #1127936] Sending the current model to client #398 (simulated).
[INFO][10:16:51]: [Client #400] Selected by the server.
[INFO][10:16:51]: [Client #400] Loading its data source...
[INFO][10:16:51]: [Client #400] Dataset size: 60000
[INFO][10:16:51]: [Client #400] Sampler: noniid
[INFO][10:16:51]: [Server #1127936] Sending 0.24 MB of payload data to client #398 (simulated).
[INFO][10:16:51]: [Client #400] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:16:51]: [Client #398] Selected by the server.
[INFO][10:16:51]: [Client #398] Loading its data source...
[INFO][10:16:51]: [Client #398] Dataset size: 60000
[INFO][10:16:51]: [Client #398] Sampler: noniid
[INFO][10:16:51]: [Client #374] Selected by the server.
[INFO][10:16:51]: [Client #374] Loading its data source...
[INFO][10:16:51]: [Client #374] Dataset size: 60000
[INFO][10:16:51]: [Client #374] Sampler: noniid
[INFO][10:16:51]: [Client #398] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:16:51]: [Client #374] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:16:51]: [93m[1m[Client #374] Started training in communication round #55.[0m
[INFO][10:16:51]: [93m[1m[Client #400] Started training in communication round #55.[0m
[INFO][10:16:51]: [93m[1m[Client #398] Started training in communication round #55.[0m
[INFO][10:16:53]: [Client #374] Loading the dataset.
[INFO][10:16:53]: [Client #400] Loading the dataset.
[INFO][10:16:53]: [Client #398] Loading the dataset.
[INFO][10:17:00]: [Client #374] Epoch: [1/5][0/10]	Loss: 0.014538
[INFO][10:17:00]: [Client #400] Epoch: [1/5][0/10]	Loss: 0.000641
[INFO][10:17:00]: [Client #398] Epoch: [1/5][0/10]	Loss: 0.000124
[INFO][10:17:00]: [Client #374] Epoch: [2/5][0/10]	Loss: 0.000548
[INFO][10:17:00]: [Client #400] Epoch: [2/5][0/10]	Loss: 0.008571
[INFO][10:17:00]: [Client #398] Epoch: [2/5][0/10]	Loss: 0.000261
[INFO][10:17:00]: [Client #374] Epoch: [3/5][0/10]	Loss: 0.001709
[INFO][10:17:00]: [Client #400] Epoch: [3/5][0/10]	Loss: 0.000322
[INFO][10:17:00]: [Client #400] Epoch: [4/5][0/10]	Loss: 0.000241
[INFO][10:17:00]: [Client #398] Epoch: [3/5][0/10]	Loss: 0.000643
[INFO][10:17:00]: [Client #374] Epoch: [4/5][0/10]	Loss: 0.002934
[INFO][10:17:00]: [Client #400] Epoch: [5/5][0/10]	Loss: 0.000008
[INFO][10:17:00]: [Client #398] Epoch: [4/5][0/10]	Loss: 0.000460
[INFO][10:17:00]: [Client #374] Epoch: [5/5][0/10]	Loss: 0.183986
[INFO][10:17:00]: [Client #400] Model saved to /data/ykang/plato/results/test/model/lenet5_400_1127977.pth.
[INFO][10:17:00]: [Client #374] Model saved to /data/ykang/plato/results/test/model/lenet5_374_1127978.pth.
[INFO][10:17:00]: [Client #398] Epoch: [5/5][0/10]	Loss: 0.000001
[INFO][10:17:00]: [Client #398] Model saved to /data/ykang/plato/results/test/model/lenet5_398_1127979.pth.
[INFO][10:17:01]: [Client #400] Loading a model from /data/ykang/plato/results/test/model/lenet5_400_1127977.pth.
[INFO][10:17:01]: [Client #400] Model trained.
[INFO][10:17:01]: [Client #400] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:17:01]: [Server #1127936] Received 0.24 MB of payload data from client #400 (simulated).
[INFO][10:17:01]: [Client #374] Loading a model from /data/ykang/plato/results/test/model/lenet5_374_1127978.pth.
[INFO][10:17:01]: [Client #374] Model trained.
[INFO][10:17:01]: [Client #374] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:17:01]: [Server #1127936] Received 0.24 MB of payload data from client #374 (simulated).
[INFO][10:17:01]: [Client #398] Loading a model from /data/ykang/plato/results/test/model/lenet5_398_1127979.pth.
[INFO][10:17:01]: [Client #398] Model trained.
[INFO][10:17:01]: [Client #398] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:17:01]: [Server #1127936] Received 0.24 MB of payload data from client #398 (simulated).
[INFO][10:17:01]: [Server #1127936] Selecting client #184 for training.
[INFO][10:17:01]: [Server #1127936] Sending the current model to client #184 (simulated).
[INFO][10:17:01]: [Server #1127936] Sending 0.24 MB of payload data to client #184 (simulated).
[INFO][10:17:01]: [Server #1127936] Selecting client #469 for training.
[INFO][10:17:01]: [Server #1127936] Sending the current model to client #469 (simulated).
[INFO][10:17:01]: [Server #1127936] Sending 0.24 MB of payload data to client #469 (simulated).
[INFO][10:17:01]: [Server #1127936] Selecting client #404 for training.
[INFO][10:17:01]: [Server #1127936] Sending the current model to client #404 (simulated).
[INFO][10:17:01]: [Client #184] Selected by the server.
[INFO][10:17:01]: [Client #184] Loading its data source...
[INFO][10:17:01]: [Client #184] Dataset size: 60000
[INFO][10:17:01]: [Client #184] Sampler: noniid
[INFO][10:17:01]: [Server #1127936] Sending 0.24 MB of payload data to client #404 (simulated).
[INFO][10:17:01]: [Client #469] Selected by the server.
[INFO][10:17:01]: [Client #469] Loading its data source...
[INFO][10:17:01]: [Client #404] Selected by the server.
[INFO][10:17:01]: [Client #469] Dataset size: 60000
[INFO][10:17:01]: [Client #469] Sampler: noniid
[INFO][10:17:01]: [Client #404] Loading its data source...
[INFO][10:17:01]: [Client #404] Dataset size: 60000
[INFO][10:17:01]: [Client #404] Sampler: noniid
[INFO][10:17:01]: [Client #184] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:17:01]: [Client #469] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:17:01]: [Client #404] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:17:01]: [93m[1m[Client #184] Started training in communication round #55.[0m
[INFO][10:17:01]: [93m[1m[Client #469] Started training in communication round #55.[0m
[INFO][10:17:01]: [93m[1m[Client #404] Started training in communication round #55.[0m
[INFO][10:17:03]: [Client #184] Loading the dataset.
[INFO][10:17:03]: [Client #404] Loading the dataset.
[INFO][10:17:03]: [Client #469] Loading the dataset.
[INFO][10:17:09]: [Client #184] Epoch: [1/5][0/10]	Loss: 0.004240
[INFO][10:17:09]: [Client #404] Epoch: [1/5][0/10]	Loss: 0.000289
[INFO][10:17:09]: [Client #469] Epoch: [1/5][0/10]	Loss: 0.005746
[INFO][10:17:09]: [Client #184] Epoch: [2/5][0/10]	Loss: 0.004746
[INFO][10:17:09]: [Client #404] Epoch: [2/5][0/10]	Loss: 0.001625
[INFO][10:17:09]: [Client #184] Epoch: [3/5][0/10]	Loss: 0.000087
[INFO][10:17:10]: [Client #469] Epoch: [2/5][0/10]	Loss: 0.004608
[INFO][10:17:10]: [Client #404] Epoch: [3/5][0/10]	Loss: 0.000053
[INFO][10:17:10]: [Client #184] Epoch: [4/5][0/10]	Loss: 0.000460
[INFO][10:17:10]: [Client #469] Epoch: [3/5][0/10]	Loss: 0.003944
[INFO][10:17:10]: [Client #404] Epoch: [4/5][0/10]	Loss: 0.000086
[INFO][10:17:10]: [Client #469] Epoch: [4/5][0/10]	Loss: 0.005223
[INFO][10:17:10]: [Client #184] Epoch: [5/5][0/10]	Loss: 0.000085
[INFO][10:17:10]: [Client #184] Model saved to /data/ykang/plato/results/test/model/lenet5_184_1127977.pth.
[INFO][10:17:10]: [Client #404] Epoch: [5/5][0/10]	Loss: 0.002580
[INFO][10:17:10]: [Client #469] Epoch: [5/5][0/10]	Loss: 0.048590
[INFO][10:17:10]: [Client #404] Model saved to /data/ykang/plato/results/test/model/lenet5_404_1127979.pth.
[INFO][10:17:10]: [Client #469] Model saved to /data/ykang/plato/results/test/model/lenet5_469_1127978.pth.
[INFO][10:17:11]: [Client #184] Loading a model from /data/ykang/plato/results/test/model/lenet5_184_1127977.pth.
[INFO][10:17:11]: [Client #184] Model trained.
[INFO][10:17:11]: [Client #184] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:17:11]: [Server #1127936] Received 0.24 MB of payload data from client #184 (simulated).
[INFO][10:17:11]: [Client #469] Loading a model from /data/ykang/plato/results/test/model/lenet5_469_1127978.pth.
[INFO][10:17:11]: [Client #469] Model trained.
[INFO][10:17:11]: [Client #469] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:17:11]: [Server #1127936] Received 0.24 MB of payload data from client #469 (simulated).
[INFO][10:17:11]: [Client #404] Loading a model from /data/ykang/plato/results/test/model/lenet5_404_1127979.pth.
[INFO][10:17:11]: [Client #404] Model trained.
[INFO][10:17:11]: [Client #404] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:17:11]: [Server #1127936] Received 0.24 MB of payload data from client #404 (simulated).
[INFO][10:17:11]: [Server #1127936] Selecting client #470 for training.
[INFO][10:17:11]: [Server #1127936] Sending the current model to client #470 (simulated).
[INFO][10:17:11]: [Server #1127936] Sending 0.24 MB of payload data to client #470 (simulated).
[INFO][10:17:11]: [Server #1127936] Selecting client #92 for training.
[INFO][10:17:11]: [Server #1127936] Sending the current model to client #92 (simulated).
[INFO][10:17:11]: [Server #1127936] Sending 0.24 MB of payload data to client #92 (simulated).
[INFO][10:17:11]: [Server #1127936] Selecting client #467 for training.
[INFO][10:17:11]: [Server #1127936] Sending the current model to client #467 (simulated).
[INFO][10:17:11]: [Client #470] Selected by the server.
[INFO][10:17:11]: [Client #470] Loading its data source...
[INFO][10:17:11]: [Client #470] Dataset size: 60000
[INFO][10:17:11]: [Client #470] Sampler: noniid
[INFO][10:17:11]: [Server #1127936] Sending 0.24 MB of payload data to client #467 (simulated).
[INFO][10:17:11]: [Client #92] Selected by the server.
[INFO][10:17:11]: [Client #92] Loading its data source...
[INFO][10:17:11]: [Client #92] Dataset size: 60000
[INFO][10:17:11]: [Client #467] Selected by the server.
[INFO][10:17:11]: [Client #92] Sampler: noniid
[INFO][10:17:11]: [Client #467] Loading its data source...
[INFO][10:17:11]: [Client #467] Dataset size: 60000
[INFO][10:17:11]: [Client #467] Sampler: noniid
[INFO][10:17:11]: [Client #470] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:17:11]: [Client #92] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:17:11]: [Client #467] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:17:11]: [93m[1m[Client #470] Started training in communication round #55.[0m
[INFO][10:17:11]: [93m[1m[Client #92] Started training in communication round #55.[0m
[INFO][10:17:11]: [93m[1m[Client #467] Started training in communication round #55.[0m
[INFO][10:17:13]: [Client #467] Loading the dataset.
[INFO][10:17:13]: [Client #470] Loading the dataset.
[INFO][10:17:13]: [Client #92] Loading the dataset.
[INFO][10:17:19]: [Client #467] Epoch: [1/5][0/10]	Loss: 0.025328
[INFO][10:17:19]: [Client #470] Epoch: [1/5][0/10]	Loss: 0.000613
[INFO][10:17:19]: [Client #467] Epoch: [2/5][0/10]	Loss: 0.000315
[INFO][10:17:19]: [Client #92] Epoch: [1/5][0/10]	Loss: 0.003307
[INFO][10:17:19]: [Client #470] Epoch: [2/5][0/10]	Loss: 0.000651
[INFO][10:17:19]: [Client #467] Epoch: [3/5][0/10]	Loss: 0.000427
[INFO][10:17:19]: [Client #92] Epoch: [2/5][0/10]	Loss: 0.003427
[INFO][10:17:19]: [Client #470] Epoch: [3/5][0/10]	Loss: 0.000127
[INFO][10:17:19]: [Client #467] Epoch: [4/5][0/10]	Loss: 0.000119
[INFO][10:17:19]: [Client #92] Epoch: [3/5][0/10]	Loss: 0.000116
[INFO][10:17:19]: [Client #470] Epoch: [4/5][0/10]	Loss: 0.000133
[INFO][10:17:19]: [Client #467] Epoch: [5/5][0/10]	Loss: 0.001161
[INFO][10:17:20]: [Client #92] Epoch: [4/5][0/10]	Loss: 0.000126
[INFO][10:17:20]: [Client #467] Model saved to /data/ykang/plato/results/test/model/lenet5_467_1127979.pth.
[INFO][10:17:20]: [Client #470] Epoch: [5/5][0/10]	Loss: 0.042082
[INFO][10:17:20]: [Client #92] Epoch: [5/5][0/10]	Loss: 0.003400
[INFO][10:17:20]: [Client #470] Model saved to /data/ykang/plato/results/test/model/lenet5_470_1127977.pth.
[INFO][10:17:20]: [Client #92] Model saved to /data/ykang/plato/results/test/model/lenet5_92_1127978.pth.
[INFO][10:17:20]: [Client #467] Loading a model from /data/ykang/plato/results/test/model/lenet5_467_1127979.pth.
[INFO][10:17:20]: [Client #467] Model trained.
[INFO][10:17:20]: [Client #467] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:17:20]: [Server #1127936] Received 0.24 MB of payload data from client #467 (simulated).
[INFO][10:17:20]: [Client #470] Loading a model from /data/ykang/plato/results/test/model/lenet5_470_1127977.pth.
[INFO][10:17:20]: [Client #470] Model trained.
[INFO][10:17:20]: [Client #470] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:17:20]: [Server #1127936] Received 0.24 MB of payload data from client #470 (simulated).
[INFO][10:17:21]: [Client #92] Loading a model from /data/ykang/plato/results/test/model/lenet5_92_1127978.pth.
[INFO][10:17:21]: [Client #92] Model trained.
[INFO][10:17:21]: [Client #92] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:17:21]: [Server #1127936] Received 0.24 MB of payload data from client #92 (simulated).
[INFO][10:17:21]: [Server #1127936] Selecting client #139 for training.
[INFO][10:17:21]: [Server #1127936] Sending the current model to client #139 (simulated).
[INFO][10:17:21]: [Server #1127936] Sending 0.24 MB of payload data to client #139 (simulated).
[INFO][10:17:21]: [Client #139] Selected by the server.
[INFO][10:17:21]: [Client #139] Loading its data source...
[INFO][10:17:21]: [Client #139] Dataset size: 60000
[INFO][10:17:21]: [Client #139] Sampler: noniid
[INFO][10:17:21]: [Client #139] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:17:21]: [93m[1m[Client #139] Started training in communication round #55.[0m
[INFO][10:17:23]: [Client #139] Loading the dataset.
[INFO][10:17:28]: [Client #139] Epoch: [1/5][0/10]	Loss: 0.000138
[INFO][10:17:28]: [Client #139] Epoch: [2/5][0/10]	Loss: 0.000190
[INFO][10:17:28]: [Client #139] Epoch: [3/5][0/10]	Loss: 0.000005
[INFO][10:17:28]: [Client #139] Epoch: [4/5][0/10]	Loss: 0.000001
[INFO][10:17:28]: [Client #139] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:17:28]: [Client #139] Model saved to /data/ykang/plato/results/test/model/lenet5_139_1127977.pth.
[INFO][10:17:29]: [Client #139] Loading a model from /data/ykang/plato/results/test/model/lenet5_139_1127977.pth.
[INFO][10:17:29]: [Client #139] Model trained.
[INFO][10:17:29]: [Client #139] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:17:29]: [Server #1127936] Received 0.24 MB of payload data from client #139 (simulated).
[INFO][10:17:29]: [Server #1127936] Adding client #324 to the list of clients for aggregation.
[INFO][10:17:29]: [Server #1127936] Adding client #5 to the list of clients for aggregation.
[INFO][10:17:29]: [Server #1127936] Adding client #130 to the list of clients for aggregation.
[INFO][10:17:29]: [Server #1127936] Adding client #23 to the list of clients for aggregation.
[INFO][10:17:29]: [Server #1127936] Adding client #24 to the list of clients for aggregation.
[INFO][10:17:29]: [Server #1127936] Adding client #116 to the list of clients for aggregation.
[INFO][10:17:29]: [Server #1127936] Adding client #184 to the list of clients for aggregation.
[INFO][10:17:29]: [Server #1127936] Adding client #470 to the list of clients for aggregation.
[INFO][10:17:29]: [Server #1127936] Adding client #469 to the list of clients for aggregation.
[INFO][10:17:29]: [Server #1127936] Adding client #400 to the list of clients for aggregation.
[INFO][10:17:29]: [Server #1127936] Aggregating 10 clients in total.
[0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.002
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  2e-05  3e-10  3e-10
 6:  6.8876e+00  6.8875e+00  1e-05  2e-10  2e-10
 7:  6.8875e+00  6.8875e+00  1e-05  2e-10  3e-12
 8:  6.8875e+00  6.8875e+00  7e-06  2e-10  2e-12
 9:  6.8875e+00  6.8875e+00  4e-06  3e-10  3e-12
Optimal solution found.
The calculated probability is:  [0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00371811 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00127812 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00222745
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00211295
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072009 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00099728 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00180768 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072097 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.63881327
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00153419 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098 0.00072098
 0.00072098 0.00072098 0.00072098 0.00072098]
current clients pool:  [1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.00587267 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00600062 0.00994504
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00996165 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00418    0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00333774 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01575851
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00390472 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00914968 0.00812923 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 3. 1. 0. 0. 0. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 0. 1. 0. 1. 1.
 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 2. 1. 6. 0. 1. 2. 1. 0. 1. 1. 0. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 0. 6. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0.
 1. 0. 1. 2. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 4. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0.
 1. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 1. 0. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 3. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 0. 0. 0. 1. 0. 0. 6. 1. 1.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 6. 0. 0. 0. 1. 0. 0. 0. 2. 0. 1. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 2. 2. 1. 1. 0.]
local_gradient_bounds:  [INFO][10:17:31]: [Server #1127936] Global model accuracy: 95.77%

[INFO][10:17:31]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_55.pth.
[INFO][10:17:31]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_55.pth.
[INFO][10:17:31]: [93m[1m
[Server #1127936] Starting round 56/100.[0m
[0.         0.         0.         0.         0.00587267 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00600062 0.00994504
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00996165 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00418    0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00333774 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01575851
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00390472 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00914968 0.00812923 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.002
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  2e-05  3e-10  3e-10
 6:  6.8876e+00  6.8875e+00  2e-05  2e-10  2e-10
 7:  6.8875e+00  6.8875e+00  2e-05  3e-10  6e-12
 8:  6.8875e+00  6.8875e+00  1e-05  3e-10  5e-12
 9:  6.8875e+00  6.8875e+00  6e-06  4e-10  7e-12
Optimal solution found.
The calculated probability is:  [0.00073537 0.00073537 0.00073537 0.00073537 0.0012072  0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.001224   0.00211011
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00211639 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00102063 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.63640097
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073533 0.00073534 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537 0.00073537
 0.00073537 0.00073537 0.00073537 0.00073537]
current clients pool:  [INFO][10:17:32]: [Server #1127936] Selected clients: [324 253 428 450 485  69 313 328 214  26]
[INFO][10:17:32]: [Server #1127936] Selecting client #324 for training.
[INFO][10:17:32]: [Server #1127936] Sending the current model to client #324 (simulated).
[INFO][10:17:32]: [Server #1127936] Sending 0.24 MB of payload data to client #324 (simulated).
[INFO][10:17:32]: [Server #1127936] Selecting client #253 for training.
[INFO][10:17:32]: [Server #1127936] Sending the current model to client #253 (simulated).
[INFO][10:17:32]: [Server #1127936] Sending 0.24 MB of payload data to client #253 (simulated).
[INFO][10:17:32]: [Server #1127936] Selecting client #428 for training.
[INFO][10:17:32]: [Server #1127936] Sending the current model to client #428 (simulated).
[INFO][10:17:32]: [Client #324] Selected by the server.
[INFO][10:17:32]: [Client #324] Loading its data source...
[INFO][10:17:32]: [Client #324] Dataset size: 60000
[INFO][10:17:32]: [Client #324] Sampler: noniid
[INFO][10:17:32]: [Server #1127936] Sending 0.24 MB of payload data to client #428 (simulated).
[INFO][10:17:32]: [Client #253] Selected by the server.
[INFO][10:17:32]: [Client #253] Loading its data source...
[INFO][10:17:32]: [Client #253] Dataset size: 60000
[INFO][10:17:32]: [Client #253] Sampler: noniid
[INFO][10:17:32]: [Client #324] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:17:32]: [Client #428] Selected by the server.
[INFO][10:17:32]: [Client #428] Loading its data source...
[INFO][10:17:32]: [Client #428] Dataset size: 60000
[INFO][10:17:32]: [Client #428] Sampler: noniid
[INFO][10:17:32]: [Client #253] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:17:32]: [Client #428] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:17:32]: [93m[1m[Client #324] Started training in communication round #56.[0m
[INFO][10:17:32]: [93m[1m[Client #253] Started training in communication round #56.[0m
[INFO][10:17:32]: [93m[1m[Client #428] Started training in communication round #56.[0m
[INFO][10:17:34]: [Client #324] Loading the dataset.
[INFO][10:17:34]: [Client #253] Loading the dataset.
[INFO][10:17:34]: [Client #428] Loading the dataset.
[INFO][10:17:40]: [Client #324] Epoch: [1/5][0/10]	Loss: 0.014165
[INFO][10:17:40]: [Client #253] Epoch: [1/5][0/10]	Loss: 0.005639
[INFO][10:17:40]: [Client #428] Epoch: [1/5][0/10]	Loss: 0.006694
[INFO][10:17:40]: [Client #253] Epoch: [2/5][0/10]	Loss: 0.000265
[INFO][10:17:40]: [Client #324] Epoch: [2/5][0/10]	Loss: 0.000068
[INFO][10:17:40]: [Client #428] Epoch: [2/5][0/10]	Loss: 0.000188
[INFO][10:17:40]: [Client #253] Epoch: [3/5][0/10]	Loss: 0.000101
[INFO][10:17:40]: [Client #324] Epoch: [3/5][0/10]	Loss: 0.000021
[INFO][10:17:40]: [Client #428] Epoch: [3/5][0/10]	Loss: 0.000004
[INFO][10:17:40]: [Client #253] Epoch: [4/5][0/10]	Loss: 0.012863
[INFO][10:17:40]: [Client #428] Epoch: [4/5][0/10]	Loss: 0.000023
[INFO][10:17:40]: [Client #324] Epoch: [4/5][0/10]	Loss: 0.000560
[INFO][10:17:41]: [Client #253] Epoch: [5/5][0/10]	Loss: 0.057608
[INFO][10:17:41]: [Client #428] Epoch: [5/5][0/10]	Loss: 0.001442
[INFO][10:17:41]: [Client #324] Epoch: [5/5][0/10]	Loss: 0.000005
[INFO][10:17:41]: [Client #253] Model saved to /data/ykang/plato/results/test/model/lenet5_253_1127978.pth.
[INFO][10:17:41]: [Client #428] Model saved to /data/ykang/plato/results/test/model/lenet5_428_1127979.pth.
[INFO][10:17:41]: [Client #324] Model saved to /data/ykang/plato/results/test/model/lenet5_324_1127977.pth.
[INFO][10:17:41]: [Client #253] Loading a model from /data/ykang/plato/results/test/model/lenet5_253_1127978.pth.
[INFO][10:17:41]: [Client #253] Model trained.
[INFO][10:17:41]: [Client #253] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:17:41]: [Server #1127936] Received 0.24 MB of payload data from client #253 (simulated).
[INFO][10:17:41]: [Client #428] Loading a model from /data/ykang/plato/results/test/model/lenet5_428_1127979.pth.
[INFO][10:17:41]: [Client #428] Model trained.
[INFO][10:17:41]: [Client #428] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:17:41]: [Server #1127936] Received 0.24 MB of payload data from client #428 (simulated).
[INFO][10:17:42]: [Client #324] Loading a model from /data/ykang/plato/results/test/model/lenet5_324_1127977.pth.
[INFO][10:17:42]: [Client #324] Model trained.
[INFO][10:17:42]: [Client #324] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:17:42]: [Server #1127936] Received 0.24 MB of payload data from client #324 (simulated).
[INFO][10:17:42]: [Server #1127936] Selecting client #450 for training.
[INFO][10:17:42]: [Server #1127936] Sending the current model to client #450 (simulated).
[INFO][10:17:42]: [Server #1127936] Sending 0.24 MB of payload data to client #450 (simulated).
[INFO][10:17:42]: [Server #1127936] Selecting client #485 for training.
[INFO][10:17:42]: [Server #1127936] Sending the current model to client #485 (simulated).
[INFO][10:17:42]: [Server #1127936] Sending 0.24 MB of payload data to client #485 (simulated).
[INFO][10:17:42]: [Server #1127936] Selecting client #69 for training.
[INFO][10:17:42]: [Server #1127936] Sending the current model to client #69 (simulated).
[INFO][10:17:42]: [Client #450] Selected by the server.
[INFO][10:17:42]: [Client #450] Loading its data source...
[INFO][10:17:42]: [Client #450] Dataset size: 60000
[INFO][10:17:42]: [Client #450] Sampler: noniid
[INFO][10:17:42]: [Server #1127936] Sending 0.24 MB of payload data to client #69 (simulated).
[INFO][10:17:42]: [Client #485] Selected by the server.
[INFO][10:17:42]: [Client #485] Loading its data source...
[INFO][10:17:42]: [Client #485] Dataset size: 60000
[INFO][10:17:42]: [Client #485] Sampler: noniid
[INFO][10:17:42]: [Client #69] Selected by the server.
[INFO][10:17:42]: [Client #69] Loading its data source...
[INFO][10:17:42]: [Client #69] Dataset size: 60000
[INFO][10:17:42]: [Client #69] Sampler: noniid
[INFO][10:17:42]: [Client #450] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:17:42]: [Client #485] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:17:42]: [Client #69] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:17:42]: [93m[1m[Client #450] Started training in communication round #56.[0m
[INFO][10:17:42]: [93m[1m[Client #485] Started training in communication round #56.[0m
[INFO][10:17:42]: [93m[1m[Client #69] Started training in communication round #56.[0m
[INFO][10:17:44]: [Client #69] Loading the dataset.
[INFO][10:17:44]: [Client #485] Loading the dataset.
[INFO][10:17:44]: [Client #450] Loading the dataset.
[INFO][10:17:50]: [Client #450] Epoch: [1/5][0/10]	Loss: 0.010100
[INFO][10:17:50]: [Client #69] Epoch: [1/5][0/10]	Loss: 0.009197
[INFO][10:17:50]: [Client #485] Epoch: [1/5][0/10]	Loss: 0.008241
[INFO][10:17:50]: [Client #69] Epoch: [2/5][0/10]	Loss: 0.000350
[INFO][10:17:50]: [Client #450] Epoch: [2/5][0/10]	Loss: 0.000443
[INFO][10:17:50]: [Client #485] Epoch: [2/5][0/10]	Loss: 0.010884
[INFO][10:17:50]: [Client #450] Epoch: [3/5][0/10]	Loss: 0.003808
[INFO][10:17:50]: [Client #69] Epoch: [3/5][0/10]	Loss: 0.000390
[INFO][10:17:50]: [Client #485] Epoch: [3/5][0/10]	Loss: 0.000621
[INFO][10:17:50]: [Client #485] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:17:50]: [Client #450] Epoch: [4/5][0/10]	Loss: 0.000194
[INFO][10:17:50]: [Client #69] Epoch: [4/5][0/10]	Loss: 0.000224
[INFO][10:17:50]: [Client #485] Epoch: [5/5][0/10]	Loss: 0.001013
[INFO][10:17:50]: [Client #485] Model saved to /data/ykang/plato/results/test/model/lenet5_485_1127978.pth.
[INFO][10:17:50]: [Client #450] Epoch: [5/5][0/10]	Loss: 0.579697
[INFO][10:17:50]: [Client #69] Epoch: [5/5][0/10]	Loss: 0.000001
[INFO][10:17:50]: [Client #450] Model saved to /data/ykang/plato/results/test/model/lenet5_450_1127977.pth.
[INFO][10:17:50]: [Client #69] Model saved to /data/ykang/plato/results/test/model/lenet5_69_1127979.pth.
[INFO][10:17:51]: [Client #485] Loading a model from /data/ykang/plato/results/test/model/lenet5_485_1127978.pth.
[INFO][10:17:51]: [Client #485] Model trained.
[INFO][10:17:51]: [Client #485] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:17:51]: [Server #1127936] Received 0.24 MB of payload data from client #485 (simulated).
[INFO][10:17:51]: [Client #450] Loading a model from /data/ykang/plato/results/test/model/lenet5_450_1127977.pth.
[INFO][10:17:51]: [Client #450] Model trained.
[INFO][10:17:51]: [Client #450] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:17:51]: [Server #1127936] Received 0.24 MB of payload data from client #450 (simulated).
[INFO][10:17:51]: [Client #69] Loading a model from /data/ykang/plato/results/test/model/lenet5_69_1127979.pth.
[INFO][10:17:51]: [Client #69] Model trained.
[INFO][10:17:51]: [Client #69] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:17:51]: [Server #1127936] Received 0.24 MB of payload data from client #69 (simulated).
[INFO][10:17:51]: [Server #1127936] Selecting client #313 for training.
[INFO][10:17:51]: [Server #1127936] Sending the current model to client #313 (simulated).
[INFO][10:17:51]: [Server #1127936] Sending 0.24 MB of payload data to client #313 (simulated).
[INFO][10:17:51]: [Server #1127936] Selecting client #328 for training.
[INFO][10:17:51]: [Server #1127936] Sending the current model to client #328 (simulated).
[INFO][10:17:51]: [Server #1127936] Sending 0.24 MB of payload data to client #328 (simulated).
[INFO][10:17:51]: [Server #1127936] Selecting client #214 for training.
[INFO][10:17:51]: [Server #1127936] Sending the current model to client #214 (simulated).
[INFO][10:17:51]: [Client #313] Selected by the server.
[INFO][10:17:51]: [Client #313] Loading its data source...
[INFO][10:17:51]: [Client #313] Dataset size: 60000
[INFO][10:17:51]: [Client #313] Sampler: noniid
[INFO][10:17:51]: [Server #1127936] Sending 0.24 MB of payload data to client #214 (simulated).
[INFO][10:17:51]: [Client #328] Selected by the server.
[INFO][10:17:51]: [Client #328] Loading its data source...
[INFO][10:17:51]: [Client #328] Dataset size: 60000
[INFO][10:17:51]: [Client #328] Sampler: noniid
[INFO][10:17:51]: [Client #214] Selected by the server.
[INFO][10:17:51]: [Client #214] Loading its data source...
[INFO][10:17:51]: [Client #214] Dataset size: 60000
[INFO][10:17:51]: [Client #214] Sampler: noniid
[INFO][10:17:51]: [Client #313] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:17:51]: [Client #328] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:17:51]: [Client #214] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:17:51]: [93m[1m[Client #328] Started training in communication round #56.[0m
[INFO][10:17:51]: [93m[1m[Client #313] Started training in communication round #56.[0m
[INFO][10:17:51]: [93m[1m[Client #214] Started training in communication round #56.[0m
[INFO][10:17:53]: [Client #328] Loading the dataset.
[INFO][10:17:53]: [Client #313] Loading the dataset.
[INFO][10:17:53]: [Client #214] Loading the dataset.
[INFO][10:17:59]: [Client #313] Epoch: [1/5][0/10]	Loss: 0.018234
[INFO][10:18:00]: [Client #328] Epoch: [1/5][0/10]	Loss: 0.034405
[INFO][10:18:00]: [Client #214] Epoch: [1/5][0/10]	Loss: 0.000348
[INFO][10:18:00]: [Client #313] Epoch: [2/5][0/10]	Loss: 0.000872
[INFO][10:18:00]: [Client #328] Epoch: [2/5][0/10]	Loss: 0.003922
[INFO][10:18:00]: [Client #328] Epoch: [3/5][0/10]	Loss: 0.000554
[INFO][10:18:00]: [Client #313] Epoch: [3/5][0/10]	Loss: 0.000119
[INFO][10:18:00]: [Client #214] Epoch: [2/5][0/10]	Loss: 0.000293
[INFO][10:18:00]: [Client #328] Epoch: [4/5][0/10]	Loss: 0.000135
[INFO][10:18:00]: [Client #214] Epoch: [3/5][0/10]	Loss: 0.000093
[INFO][10:18:00]: [Client #313] Epoch: [4/5][0/10]	Loss: 0.000014
[INFO][10:18:00]: [Client #328] Epoch: [5/5][0/10]	Loss: 0.004641
[INFO][10:18:00]: [Client #313] Epoch: [5/5][0/10]	Loss: 0.000111
[INFO][10:18:00]: [Client #328] Model saved to /data/ykang/plato/results/test/model/lenet5_328_1127978.pth.
[INFO][10:18:00]: [Client #214] Epoch: [4/5][0/10]	Loss: 0.002768
[INFO][10:18:00]: [Client #313] Model saved to /data/ykang/plato/results/test/model/lenet5_313_1127977.pth.
[INFO][10:18:00]: [Client #214] Epoch: [5/5][0/10]	Loss: 0.000245
[INFO][10:18:00]: [Client #214] Model saved to /data/ykang/plato/results/test/model/lenet5_214_1127979.pth.
[INFO][10:18:01]: [Client #328] Loading a model from /data/ykang/plato/results/test/model/lenet5_328_1127978.pth.
[INFO][10:18:01]: [Client #328] Model trained.
[INFO][10:18:01]: [Client #328] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:18:01]: [Server #1127936] Received 0.24 MB of payload data from client #328 (simulated).
[INFO][10:18:01]: [Client #313] Loading a model from /data/ykang/plato/results/test/model/lenet5_313_1127977.pth.
[INFO][10:18:01]: [Client #313] Model trained.
[INFO][10:18:01]: [Client #313] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:18:01]: [Server #1127936] Received 0.24 MB of payload data from client #313 (simulated).
[INFO][10:18:01]: [Client #214] Loading a model from /data/ykang/plato/results/test/model/lenet5_214_1127979.pth.
[INFO][10:18:01]: [Client #214] Model trained.
[INFO][10:18:01]: [Client #214] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:18:01]: [Server #1127936] Received 0.24 MB of payload data from client #214 (simulated).
[INFO][10:18:01]: [Server #1127936] Selecting client #26 for training.
[INFO][10:18:01]: [Server #1127936] Sending the current model to client #26 (simulated).
[INFO][10:18:01]: [Server #1127936] Sending 0.24 MB of payload data to client #26 (simulated).
[INFO][10:18:01]: [Client #26] Selected by the server.
[INFO][10:18:01]: [Client #26] Loading its data source...
[INFO][10:18:01]: [Client #26] Dataset size: 60000
[INFO][10:18:01]: [Client #26] Sampler: noniid
[INFO][10:18:01]: [Client #26] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:18:01]: [93m[1m[Client #26] Started training in communication round #56.[0m
[INFO][10:18:03]: [Client #26] Loading the dataset.
[INFO][10:18:09]: [Client #26] Epoch: [1/5][0/10]	Loss: 0.008056
[INFO][10:18:09]: [Client #26] Epoch: [2/5][0/10]	Loss: 0.000168
[INFO][10:18:09]: [Client #26] Epoch: [3/5][0/10]	Loss: 0.000177
[INFO][10:18:09]: [Client #26] Epoch: [4/5][0/10]	Loss: 0.035788
[INFO][10:18:09]: [Client #26] Epoch: [5/5][0/10]	Loss: 0.001699
[INFO][10:18:09]: [Client #26] Model saved to /data/ykang/plato/results/test/model/lenet5_26_1127977.pth.
[INFO][10:18:10]: [Client #26] Loading a model from /data/ykang/plato/results/test/model/lenet5_26_1127977.pth.
[INFO][10:18:10]: [Client #26] Model trained.
[INFO][10:18:10]: [Client #26] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:18:10]: [Server #1127936] Received 0.24 MB of payload data from client #26 (simulated).
[INFO][10:18:10]: [Server #1127936] Adding client #190 to the list of clients for aggregation.
[INFO][10:18:10]: [Server #1127936] Adding client #404 to the list of clients for aggregation.
[INFO][10:18:10]: [Server #1127936] Adding client #374 to the list of clients for aggregation.
[INFO][10:18:10]: [Server #1127936] Adding client #398 to the list of clients for aggregation.
[INFO][10:18:10]: [Server #1127936] Adding client #139 to the list of clients for aggregation.
[INFO][10:18:10]: [Server #1127936] Adding client #41 to the list of clients for aggregation.
[INFO][10:18:10]: [Server #1127936] Adding client #324 to the list of clients for aggregation.
[INFO][10:18:10]: [Server #1127936] Adding client #214 to the list of clients for aggregation.
[INFO][10:18:10]: [Server #1127936] Adding client #450 to the list of clients for aggregation.
[INFO][10:18:10]: [Server #1127936] Adding client #313 to the list of clients for aggregation.
[INFO][10:18:10]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 399, 400, 401, 402, 403, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01581305 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00462818 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00557317 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00444297 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00952659 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.02096583
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01264691 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00410658 0.         0.         0.         0.
 0.         0.0121697  0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01376051
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 3. 1. 0. 0. 0. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 0. 1. 0. 1. 1.
 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 2. 1. 6. 0. 1. 2. 1. 0. 1. 1. 0. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 0. 6. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 2. 0. 0.
 1. 0. 1. 2. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 4. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0.
 1. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 1. 0. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 3. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 0. 0. 0. 1. 0. 0. 6. 1. 1.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 6. 0. 0. 0. 1. 0. 0. 0. 2. 0. 1. 0. 0. 1.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 2. 2. 1. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01581305 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00462818 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00557317 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00444297 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00952659 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.02096583
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01264691 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00410658 0.         0.         0.         0.
 0.         0.0121697  0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01376051
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:18:12]: [Server #1127936] Global model accuracy: 94.87%

[INFO][10:18:12]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_56.pth.
[INFO][10:18:12]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_56.pth.
[INFO][10:18:12]: [93m[1m
[Server #1127936] Starting round 57/100.[0m
[0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.002
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8871e+00  4e-04  7e-09  7e-09
 5:  6.8876e+00  6.8875e+00  1e-04  2e-09  2e-09
 6:  6.8875e+00  6.8875e+00  9e-05  6e-10  3e-10
 7:  6.8875e+00  6.8875e+00  8e-05  5e-09  5e-10
 8:  6.8875e+00  6.8875e+00  6e-05  7e-09  7e-10
 9:  6.8875e+00  6.8875e+00  2e-05  4e-08  4e-09
10:  6.8875e+00  6.8875e+00  4e-06  1e-08  1e-09
Optimal solution found.
The calculated probability is:  [8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 9.55934416e-01
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 9.57931545e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 1.05330374e-04 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99885630e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99877253e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99836109e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 1.07792254e-04 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 9.51026360e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 1.06996749e-04 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99865623e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05 8.99887959e-05 8.99887959e-05
 8.99887959e-05 8.99887959e-05]
current clients pool:  [INFO][10:18:12]: [Server #1127936] Selected clients: [ 41 385 118  30  71 468 444 147 474 424]
[INFO][10:18:12]: [Server #1127936] Selecting client #41 for training.
[INFO][10:18:12]: [Server #1127936] Sending the current model to client #41 (simulated).
[INFO][10:18:12]: [Server #1127936] Sending 0.24 MB of payload data to client #41 (simulated).
[INFO][10:18:12]: [Server #1127936] Selecting client #385 for training.
[INFO][10:18:12]: [Server #1127936] Sending the current model to client #385 (simulated).
[INFO][10:18:12]: [Server #1127936] Sending 0.24 MB of payload data to client #385 (simulated).
[INFO][10:18:12]: [Server #1127936] Selecting client #118 for training.
[INFO][10:18:12]: [Server #1127936] Sending the current model to client #118 (simulated).
[INFO][10:18:12]: [Client #41] Selected by the server.
[INFO][10:18:12]: [Client #41] Loading its data source...
[INFO][10:18:12]: [Client #41] Dataset size: 60000
[INFO][10:18:12]: [Client #41] Sampler: noniid
[INFO][10:18:12]: [Server #1127936] Sending 0.24 MB of payload data to client #118 (simulated).
[INFO][10:18:12]: [Client #385] Selected by the server.
[INFO][10:18:12]: [Client #385] Loading its data source...
[INFO][10:18:12]: [Client #385] Dataset size: 60000
[INFO][10:18:12]: [Client #385] Sampler: noniid
[INFO][10:18:12]: [Client #41] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:18:12]: [Client #118] Selected by the server.
[INFO][10:18:12]: [Client #118] Loading its data source...
[INFO][10:18:12]: [Client #118] Dataset size: 60000
[INFO][10:18:12]: [Client #118] Sampler: noniid
[INFO][10:18:12]: [Client #118] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:18:12]: [Client #385] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:18:12]: [93m[1m[Client #385] Started training in communication round #57.[0m
[INFO][10:18:12]: [93m[1m[Client #41] Started training in communication round #57.[0m
[INFO][10:18:12]: [93m[1m[Client #118] Started training in communication round #57.[0m
[INFO][10:18:15]: [Client #41] Loading the dataset.
[INFO][10:18:15]: [Client #118] Loading the dataset.
[INFO][10:18:15]: [Client #385] Loading the dataset.
[INFO][10:18:21]: [Client #41] Epoch: [1/5][0/10]	Loss: 0.005405
[INFO][10:18:21]: [Client #118] Epoch: [1/5][0/10]	Loss: 0.006768
[INFO][10:18:21]: [Client #41] Epoch: [2/5][0/10]	Loss: 0.001520
[INFO][10:18:21]: [Client #385] Epoch: [1/5][0/10]	Loss: 0.055014
[INFO][10:18:21]: [Client #41] Epoch: [3/5][0/10]	Loss: 0.006619
[INFO][10:18:21]: [Client #118] Epoch: [2/5][0/10]	Loss: 0.035097
[INFO][10:18:21]: [Client #385] Epoch: [2/5][0/10]	Loss: 0.014351
[INFO][10:18:21]: [Client #41] Epoch: [4/5][0/10]	Loss: 0.010487
[INFO][10:18:21]: [Client #118] Epoch: [3/5][0/10]	Loss: 0.001066
[INFO][10:18:21]: [Client #385] Epoch: [3/5][0/10]	Loss: 0.000000
[INFO][10:18:21]: [Client #41] Epoch: [5/5][0/10]	Loss: 0.029922
[INFO][10:18:21]: [Client #41] Model saved to /data/ykang/plato/results/test/model/lenet5_41_1127977.pth.
[INFO][10:18:21]: [Client #118] Epoch: [4/5][0/10]	Loss: 0.000498
[INFO][10:18:21]: [Client #385] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:18:21]: [Client #118] Epoch: [5/5][0/10]	Loss: 0.001381
[INFO][10:18:21]: [Client #385] Epoch: [5/5][0/10]	Loss: 0.000004
[INFO][10:18:21]: [Client #118] Model saved to /data/ykang/plato/results/test/model/lenet5_118_1127979.pth.
[INFO][10:18:21]: [Client #385] Model saved to /data/ykang/plato/results/test/model/lenet5_385_1127978.pth.
[INFO][10:18:22]: [Client #41] Loading a model from /data/ykang/plato/results/test/model/lenet5_41_1127977.pth.
[INFO][10:18:22]: [Client #41] Model trained.
[INFO][10:18:22]: [Client #41] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:18:22]: [Server #1127936] Received 0.24 MB of payload data from client #41 (simulated).
[INFO][10:18:22]: [Client #118] Loading a model from /data/ykang/plato/results/test/model/lenet5_118_1127979.pth.
[INFO][10:18:22]: [Client #118] Model trained.
[INFO][10:18:22]: [Client #118] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:18:22]: [Server #1127936] Received 0.24 MB of payload data from client #118 (simulated).
[INFO][10:18:22]: [Client #385] Loading a model from /data/ykang/plato/results/test/model/lenet5_385_1127978.pth.
[INFO][10:18:22]: [Client #385] Model trained.
[INFO][10:18:22]: [Client #385] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:18:22]: [Server #1127936] Received 0.24 MB of payload data from client #385 (simulated).
[INFO][10:18:22]: [Server #1127936] Selecting client #30 for training.
[INFO][10:18:22]: [Server #1127936] Sending the current model to client #30 (simulated).
[INFO][10:18:22]: [Server #1127936] Sending 0.24 MB of payload data to client #30 (simulated).
[INFO][10:18:22]: [Server #1127936] Selecting client #71 for training.
[INFO][10:18:22]: [Server #1127936] Sending the current model to client #71 (simulated).
[INFO][10:18:22]: [Server #1127936] Sending 0.24 MB of payload data to client #71 (simulated).
[INFO][10:18:22]: [Server #1127936] Selecting client #468 for training.
[INFO][10:18:22]: [Server #1127936] Sending the current model to client #468 (simulated).
[INFO][10:18:22]: [Client #30] Selected by the server.
[INFO][10:18:22]: [Client #30] Loading its data source...
[INFO][10:18:22]: [Client #30] Dataset size: 60000
[INFO][10:18:22]: [Client #30] Sampler: noniid
[INFO][10:18:22]: [Server #1127936] Sending 0.24 MB of payload data to client #468 (simulated).
[INFO][10:18:22]: [Client #71] Selected by the server.
[INFO][10:18:22]: [Client #468] Selected by the server.
[INFO][10:18:22]: [Client #468] Loading its data source...
[INFO][10:18:22]: [Client #71] Loading its data source...
[INFO][10:18:22]: [Client #468] Dataset size: 60000
[INFO][10:18:22]: [Client #71] Dataset size: 60000
[INFO][10:18:22]: [Client #468] Sampler: noniid
[INFO][10:18:22]: [Client #71] Sampler: noniid
[INFO][10:18:22]: [Client #30] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:18:22]: [Client #71] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:18:22]: [Client #468] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:18:22]: [93m[1m[Client #30] Started training in communication round #57.[0m
[INFO][10:18:22]: [93m[1m[Client #468] Started training in communication round #57.[0m
[INFO][10:18:22]: [93m[1m[Client #71] Started training in communication round #57.[0m
[INFO][10:18:24]: [Client #71] Loading the dataset.
[INFO][10:18:24]: [Client #30] Loading the dataset.
[INFO][10:18:24]: [Client #468] Loading the dataset.
[INFO][10:18:30]: [Client #30] Epoch: [1/5][0/10]	Loss: 0.156612
[INFO][10:18:30]: [Client #468] Epoch: [1/5][0/10]	Loss: 0.102908
[INFO][10:18:30]: [Client #71] Epoch: [1/5][0/10]	Loss: 0.001250
[INFO][10:18:30]: [Client #30] Epoch: [2/5][0/10]	Loss: 0.000005
[INFO][10:18:30]: [Client #468] Epoch: [2/5][0/10]	Loss: 0.007033
[INFO][10:18:30]: [Client #71] Epoch: [2/5][0/10]	Loss: 0.012305
[INFO][10:18:30]: [Client #30] Epoch: [3/5][0/10]	Loss: 0.000081
[INFO][10:18:30]: [Client #468] Epoch: [3/5][0/10]	Loss: 0.000120
[INFO][10:18:30]: [Client #71] Epoch: [3/5][0/10]	Loss: 0.003424
[INFO][10:18:30]: [Client #468] Epoch: [4/5][0/10]	Loss: 0.003643
[INFO][10:18:31]: [Client #30] Epoch: [4/5][0/10]	Loss: 0.000007
[INFO][10:18:31]: [Client #71] Epoch: [4/5][0/10]	Loss: 0.005330
[INFO][10:18:31]: [Client #468] Epoch: [5/5][0/10]	Loss: 0.000290
[INFO][10:18:31]: [Client #30] Epoch: [5/5][0/10]	Loss: 0.001435
[INFO][10:18:31]: [Client #468] Model saved to /data/ykang/plato/results/test/model/lenet5_468_1127979.pth.
[INFO][10:18:31]: [Client #30] Model saved to /data/ykang/plato/results/test/model/lenet5_30_1127977.pth.
[INFO][10:18:31]: [Client #71] Epoch: [5/5][0/10]	Loss: 0.242210
[INFO][10:18:31]: [Client #71] Model saved to /data/ykang/plato/results/test/model/lenet5_71_1127978.pth.
[INFO][10:18:31]: [Client #30] Loading a model from /data/ykang/plato/results/test/model/lenet5_30_1127977.pth.
[INFO][10:18:31]: [Client #30] Model trained.
[INFO][10:18:31]: [Client #30] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:18:31]: [Server #1127936] Received 0.24 MB of payload data from client #30 (simulated).
[INFO][10:18:31]: [Client #468] Loading a model from /data/ykang/plato/results/test/model/lenet5_468_1127979.pth.
[INFO][10:18:32]: [Client #468] Model trained.
[INFO][10:18:32]: [Client #468] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:18:32]: [Server #1127936] Received 0.24 MB of payload data from client #468 (simulated).
[INFO][10:18:32]: [Client #71] Loading a model from /data/ykang/plato/results/test/model/lenet5_71_1127978.pth.
[INFO][10:18:32]: [Client #71] Model trained.
[INFO][10:18:32]: [Client #71] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:18:32]: [Server #1127936] Received 0.24 MB of payload data from client #71 (simulated).
[INFO][10:18:32]: [Server #1127936] Selecting client #444 for training.
[INFO][10:18:32]: [Server #1127936] Sending the current model to client #444 (simulated).
[INFO][10:18:32]: [Server #1127936] Sending 0.24 MB of payload data to client #444 (simulated).
[INFO][10:18:32]: [Server #1127936] Selecting client #147 for training.
[INFO][10:18:32]: [Server #1127936] Sending the current model to client #147 (simulated).
[INFO][10:18:32]: [Server #1127936] Sending 0.24 MB of payload data to client #147 (simulated).
[INFO][10:18:32]: [Server #1127936] Selecting client #474 for training.
[INFO][10:18:32]: [Server #1127936] Sending the current model to client #474 (simulated).
[INFO][10:18:32]: [Client #444] Selected by the server.
[INFO][10:18:32]: [Client #444] Loading its data source...
[INFO][10:18:32]: [Client #444] Dataset size: 60000
[INFO][10:18:32]: [Client #444] Sampler: noniid
[INFO][10:18:32]: [Server #1127936] Sending 0.24 MB of payload data to client #474 (simulated).
[INFO][10:18:32]: [Client #147] Selected by the server.
[INFO][10:18:32]: [Client #147] Loading its data source...
[INFO][10:18:32]: [Client #147] Dataset size: 60000
[INFO][10:18:32]: [Client #147] Sampler: noniid
[INFO][10:18:32]: [Client #474] Selected by the server.
[INFO][10:18:32]: [Client #474] Loading its data source...
[INFO][10:18:32]: [Client #474] Dataset size: 60000
[INFO][10:18:32]: [Client #474] Sampler: noniid
[INFO][10:18:32]: [Client #444] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:18:32]: [Client #147] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:18:32]: [Client #474] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:18:32]: [93m[1m[Client #474] Started training in communication round #57.[0m
[INFO][10:18:32]: [93m[1m[Client #444] Started training in communication round #57.[0m
[INFO][10:18:32]: [93m[1m[Client #147] Started training in communication round #57.[0m
[INFO][10:18:34]: [Client #444] Loading the dataset.
[INFO][10:18:34]: [Client #474] Loading the dataset.
[INFO][10:18:34]: [Client #147] Loading the dataset.
[INFO][10:18:40]: [Client #444] Epoch: [1/5][0/10]	Loss: 0.021024
[INFO][10:18:40]: [Client #147] Epoch: [1/5][0/10]	Loss: 0.201937
[INFO][10:18:40]: [Client #474] Epoch: [1/5][0/10]	Loss: 0.156260
[INFO][10:18:40]: [Client #444] Epoch: [2/5][0/10]	Loss: 0.002852
[INFO][10:18:40]: [Client #147] Epoch: [2/5][0/10]	Loss: 0.000002
[INFO][10:18:40]: [Client #444] Epoch: [3/5][0/10]	Loss: 0.000124
[INFO][10:18:40]: [Client #474] Epoch: [2/5][0/10]	Loss: 0.000016
[INFO][10:18:40]: [Client #147] Epoch: [3/5][0/10]	Loss: 0.000000
[INFO][10:18:40]: [Client #444] Epoch: [4/5][0/10]	Loss: 0.000206
[INFO][10:18:40]: [Client #474] Epoch: [3/5][0/10]	Loss: 0.000238
[INFO][10:18:40]: [Client #147] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:18:40]: [Client #444] Epoch: [5/5][0/10]	Loss: 0.000290
[INFO][10:18:40]: [Client #474] Epoch: [4/5][0/10]	Loss: 0.002843
[INFO][10:18:40]: [Client #147] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:18:40]: [Client #444] Model saved to /data/ykang/plato/results/test/model/lenet5_444_1127977.pth.
[INFO][10:18:40]: [Client #147] Model saved to /data/ykang/plato/results/test/model/lenet5_147_1127978.pth.
[INFO][10:18:40]: [Client #474] Epoch: [5/5][0/10]	Loss: 0.029587
[INFO][10:18:40]: [Client #474] Model saved to /data/ykang/plato/results/test/model/lenet5_474_1127979.pth.
[INFO][10:18:41]: [Client #444] Loading a model from /data/ykang/plato/results/test/model/lenet5_444_1127977.pth.
[INFO][10:18:41]: [Client #444] Model trained.
[INFO][10:18:41]: [Client #444] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:18:41]: [Server #1127936] Received 0.24 MB of payload data from client #444 (simulated).
[INFO][10:18:41]: [Client #147] Loading a model from /data/ykang/plato/results/test/model/lenet5_147_1127978.pth.
[INFO][10:18:41]: [Client #147] Model trained.
[INFO][10:18:41]: [Client #147] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:18:41]: [Server #1127936] Received 0.24 MB of payload data from client #147 (simulated).
[INFO][10:18:41]: [Client #474] Loading a model from /data/ykang/plato/results/test/model/lenet5_474_1127979.pth.
[INFO][10:18:41]: [Client #474] Model trained.
[INFO][10:18:41]: [Client #474] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:18:41]: [Server #1127936] Received 0.24 MB of payload data from client #474 (simulated).
[INFO][10:18:41]: [Server #1127936] Selecting client #424 for training.
[INFO][10:18:41]: [Server #1127936] Sending the current model to client #424 (simulated).
[INFO][10:18:41]: [Server #1127936] Sending 0.24 MB of payload data to client #424 (simulated).
[INFO][10:18:41]: [Client #424] Selected by the server.
[INFO][10:18:41]: [Client #424] Loading its data source...
[INFO][10:18:41]: [Client #424] Dataset size: 60000
[INFO][10:18:41]: [Client #424] Sampler: noniid
[INFO][10:18:41]: [Client #424] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:18:41]: [93m[1m[Client #424] Started training in communication round #57.[0m
[INFO][10:18:43]: [Client #424] Loading the dataset.
[INFO][10:18:49]: [Client #424] Epoch: [1/5][0/10]	Loss: 0.003585
[INFO][10:18:49]: [Client #424] Epoch: [2/5][0/10]	Loss: 0.020307
[INFO][10:18:49]: [Client #424] Epoch: [3/5][0/10]	Loss: 0.000647
[INFO][10:18:49]: [Client #424] Epoch: [4/5][0/10]	Loss: 0.000002
[INFO][10:18:49]: [Client #424] Epoch: [5/5][0/10]	Loss: 0.000015
[INFO][10:18:49]: [Client #424] Model saved to /data/ykang/plato/results/test/model/lenet5_424_1127977.pth.
[INFO][10:18:50]: [Client #424] Loading a model from /data/ykang/plato/results/test/model/lenet5_424_1127977.pth.
[INFO][10:18:50]: [Client #424] Model trained.
[INFO][10:18:50]: [Client #424] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:18:50]: [Server #1127936] Received 0.24 MB of payload data from client #424 (simulated).
[INFO][10:18:50]: [Server #1127936] Adding client #253 to the list of clients for aggregation.
[INFO][10:18:50]: [Server #1127936] Adding client #485 to the list of clients for aggregation.
[INFO][10:18:50]: [Server #1127936] Adding client #328 to the list of clients for aggregation.
[INFO][10:18:50]: [Server #1127936] Adding client #428 to the list of clients for aggregation.
[INFO][10:18:50]: [Server #1127936] Adding client #92 to the list of clients for aggregation.
[INFO][10:18:50]: [Server #1127936] Adding client #26 to the list of clients for aggregation.
[INFO][10:18:50]: [Server #1127936] Adding client #444 to the list of clients for aggregation.
[INFO][10:18:50]: [Server #1127936] Adding client #424 to the list of clients for aggregation.
[INFO][10:18:50]: [Server #1127936] Adding client #147 to the list of clients for aggregation.
[INFO][10:18:50]: [Server #1127936] Adding client #385 to the list of clients for aggregation.
[INFO][10:18:50]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00913717 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00502749 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.04483194 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00655049 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00717899 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.0076672  0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00556067 0.         0.
 0.         0.00322758 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00289403
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00887571 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 0. 1. 0. 0. 0. 0. 0. 1. 3. 1. 0. 0. 0. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 2. 1. 0. 1. 1.
 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 2. 1. 6. 0. 1. 2. 1. 0. 1. 1. 0. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 0. 6. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 2. 0. 0.
 1. 0. 1. 2. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 4. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0.
 1. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 1. 0. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 3. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 0. 0. 0. 1. 0. 0. 6. 1. 1.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 6. 0. 0. 0. 1. 0. 0. 0. 2. 0. 1. 0. 0. 1.
 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 2. 2. 1. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00913717 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00502749 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.04483194 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00655049 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00717899 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.0076672  0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00556067 0.         0.
 0.         0.00322758 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00289403
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00887571 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:18:52]: [Server #1127936] Global model accuracy: 95.68%

[INFO][10:18:52]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_57.pth.
[INFO][10:18:52]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_57.pth.
[INFO][10:18:52]: [93m[1m
[Server #1127936] Starting round 58/100.[0m
[INFO][10:18:53]: [Server #1127936] Selected clients: [ 92 203  13 240 137 394  67 123 148  96]
[INFO][10:18:53]: [Server #1127936] Selecting client #92 for training.
[INFO][10:18:53]: [Server #1127936] Sending the current model to client #92 (simulated).
[INFO][10:18:53]: [Server #1127936] Sending 0.24 MB of payload data to client #92 (simulated).
[INFO][10:18:53]: [Server #1127936] Selecting client #203 for training.
[INFO][10:18:53]: [Server #1127936] Sending the current model to client #203 (simulated).
[INFO][10:18:53]: [Server #1127936] Sending 0.24 MB of payload data to client #203 (simulated).
[INFO][10:18:53]: [Client #92] Selected by the server.
[INFO][10:18:53]: [Client #92] Loading its data source...
[INFO][10:18:53]: [Client #92] Dataset size: 60000
[INFO][10:18:53]: [Client #92] Sampler: noniid
[INFO][10:18:53]: [Client #92] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:18:53]: [Server #1127936] Selecting client #13 for training.
[INFO][10:18:53]: [Server #1127936] Sending the current model to client #13 (simulated).
[INFO][10:18:53]: [Server #1127936] Sending 0.24 MB of payload data to client #13 (simulated).
[INFO][10:18:53]: [Client #203] Selected by the server.
[INFO][10:18:53]: [Client #203] Loading its data source...
[INFO][10:18:53]: [Client #203] Dataset size: 60000
[INFO][10:18:53]: [Client #203] Sampler: noniid
[INFO][10:18:53]: [Client #13] Selected by the server.
[INFO][10:18:53]: [Client #13] Loading its data source...
[INFO][10:18:53]: [Client #13] Dataset size: 60000
[INFO][10:18:53]: [Client #13] Sampler: noniid
[INFO][10:18:53]: [Client #203] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:18:53]: [Client #13] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:18:53]: [93m[1m[Client #203] Started training in communication round #58.[0m
[INFO][10:18:53]: [93m[1m[Client #92] Started training in communication round #58.[0m
[INFO][10:18:53]: [93m[1m[Client #13] Started training in communication round #58.[0m
[INFO][10:18:55]: [Client #92] Loading the dataset.
[INFO][10:18:55]: [Client #203] Loading the dataset.
[INFO][10:18:55]: [Client #13] Loading the dataset.
[INFO][10:19:01]: [Client #92] Epoch: [1/5][0/10]	Loss: 0.000543
[INFO][10:19:01]: [Client #203] Epoch: [1/5][0/10]	Loss: 0.001259
[INFO][10:19:01]: [Client #92] Epoch: [2/5][0/10]	Loss: 0.005575
[INFO][10:19:01]: [Client #13] Epoch: [1/5][0/10]	Loss: 0.001988
[INFO][10:19:01]: [Client #203] Epoch: [2/5][0/10]	Loss: 0.000117
[INFO][10:19:01]: [Client #13] Epoch: [2/5][0/10]	Loss: 0.001061
[INFO][10:19:01]: [Client #92] Epoch: [3/5][0/10]	Loss: 0.000152
[INFO][10:19:01]: [Client #203] Epoch: [3/5][0/10]	Loss: 0.000155
[INFO][10:19:01]: [Client #13] Epoch: [3/5][0/10]	Loss: 0.000000
[INFO][10:19:01]: [Client #92] Epoch: [4/5][0/10]	Loss: 0.000228
[INFO][10:19:01]: [Client #203] Epoch: [4/5][0/10]	Loss: 0.063735
[INFO][10:19:01]: [Client #92] Epoch: [5/5][0/10]	Loss: 0.001201
[INFO][10:19:01]: [Client #13] Epoch: [4/5][0/10]	Loss: 0.000018
[INFO][10:19:01]: [Client #203] Epoch: [5/5][0/10]	Loss: 0.000024
[INFO][10:19:01]: [Client #92] Model saved to /data/ykang/plato/results/test/model/lenet5_92_1127977.pth.
[INFO][10:19:01]: [Client #203] Model saved to /data/ykang/plato/results/test/model/lenet5_203_1127978.pth.
[INFO][10:19:01]: [Client #13] Epoch: [5/5][0/10]	Loss: 0.000627
[INFO][10:19:01]: [Client #13] Model saved to /data/ykang/plato/results/test/model/lenet5_13_1127979.pth.
[INFO][10:19:02]: [Client #92] Loading a model from /data/ykang/plato/results/test/model/lenet5_92_1127977.pth.
[INFO][10:19:02]: [Client #92] Model trained.
[INFO][10:19:02]: [Client #92] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:19:02]: [Server #1127936] Received 0.24 MB of payload data from client #92 (simulated).
[INFO][10:19:02]: [Client #203] Loading a model from /data/ykang/plato/results/test/model/lenet5_203_1127978.pth.
[INFO][10:19:02]: [Client #203] Model trained.
[INFO][10:19:02]: [Client #203] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:19:02]: [Server #1127936] Received 0.24 MB of payload data from client #203 (simulated).
[INFO][10:19:02]: [Client #13] Loading a model from /data/ykang/plato/results/test/model/lenet5_13_1127979.pth.
[INFO][10:19:02]: [Client #13] Model trained.
[INFO][10:19:02]: [Client #13] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:19:02]: [Server #1127936] Received 0.24 MB of payload data from client #13 (simulated).
[INFO][10:19:02]: [Server #1127936] Selecting client #240 for training.
[INFO][10:19:02]: [Server #1127936] Sending the current model to client #240 (simulated).
[INFO][10:19:02]: [Server #1127936] Sending 0.24 MB of payload data to client #240 (simulated).
[INFO][10:19:02]: [Server #1127936] Selecting client #137 for training.
[INFO][10:19:02]: [Server #1127936] Sending the current model to client #137 (simulated).
[INFO][10:19:02]: [Server #1127936] Sending 0.24 MB of payload data to client #137 (simulated).
[INFO][10:19:02]: [Server #1127936] Selecting client #394 for training.
[INFO][10:19:02]: [Server #1127936] Sending the current model to client #394 (simulated).
[INFO][10:19:02]: [Client #240] Selected by the server.
[INFO][10:19:02]: [Client #240] Loading its data source...
[INFO][10:19:02]: [Client #240] Dataset size: 60000
[INFO][10:19:02]: [Client #240] Sampler: noniid
[INFO][10:19:02]: [Server #1127936] Sending 0.24 MB of payload data to client #394 (simulated).
[INFO][10:19:02]: [Client #240] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:19:02]: [Client #137] Selected by the server.
[INFO][10:19:02]: [Client #137] Loading its data source...
[INFO][10:19:02]: [Client #394] Selected by the server.
[INFO][10:19:02]: [Client #137] Dataset size: 60000
[INFO][10:19:02]: [Client #394] Loading its data source...
[INFO][10:19:02]: [Client #137] Sampler: noniid
[INFO][10:19:02]: [Client #394] Dataset size: 60000
[INFO][10:19:02]: [Client #394] Sampler: noniid
[INFO][10:19:02]: [Client #137] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:19:02]: [Client #394] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:19:02]: [93m[1m[Client #240] Started training in communication round #58.[0m
[INFO][10:19:02]: [93m[1m[Client #137] Started training in communication round #58.[0m
[INFO][10:19:02]: [93m[1m[Client #394] Started training in communication round #58.[0m
[INFO][10:19:04]: [Client #394] Loading the dataset.
[INFO][10:19:04]: [Client #240] Loading the dataset.
[INFO][10:19:04]: [Client #137] Loading the dataset.
[INFO][10:19:10]: [Client #394] Epoch: [1/5][0/10]	Loss: 0.013968
[INFO][10:19:10]: [Client #137] Epoch: [1/5][0/10]	Loss: 0.010058
[INFO][10:19:10]: [Client #240] Epoch: [1/5][0/10]	Loss: 0.001622
[INFO][10:19:11]: [Client #394] Epoch: [2/5][0/10]	Loss: 0.001988
[INFO][10:19:11]: [Client #137] Epoch: [2/5][0/10]	Loss: 0.016011
[INFO][10:19:11]: [Client #240] Epoch: [2/5][0/10]	Loss: 0.002578
[INFO][10:19:11]: [Client #137] Epoch: [3/5][0/10]	Loss: 0.000221
[INFO][10:19:11]: [Client #394] Epoch: [3/5][0/10]	Loss: 0.000036
[INFO][10:19:11]: [Client #240] Epoch: [3/5][0/10]	Loss: 0.001866
[INFO][10:19:11]: [Client #137] Epoch: [4/5][0/10]	Loss: 0.000084
[INFO][10:19:11]: [Client #394] Epoch: [4/5][0/10]	Loss: 0.000290
[INFO][10:19:11]: [Client #240] Epoch: [4/5][0/10]	Loss: 0.539970
[INFO][10:19:11]: [Client #137] Epoch: [5/5][0/10]	Loss: 0.064901
[INFO][10:19:11]: [Client #394] Epoch: [5/5][0/10]	Loss: 0.003846
[INFO][10:19:11]: [Client #137] Model saved to /data/ykang/plato/results/test/model/lenet5_137_1127978.pth.
[INFO][10:19:11]: [Client #240] Epoch: [5/5][0/10]	Loss: 0.007823
[INFO][10:19:11]: [Client #394] Model saved to /data/ykang/plato/results/test/model/lenet5_394_1127979.pth.
[INFO][10:19:11]: [Client #240] Model saved to /data/ykang/plato/results/test/model/lenet5_240_1127977.pth.
[INFO][10:19:12]: [Client #137] Loading a model from /data/ykang/plato/results/test/model/lenet5_137_1127978.pth.
[INFO][10:19:12]: [Client #137] Model trained.
[INFO][10:19:12]: [Client #137] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:19:12]: [Server #1127936] Received 0.24 MB of payload data from client #137 (simulated).
[INFO][10:19:12]: [Client #240] Loading a model from /data/ykang/plato/results/test/model/lenet5_240_1127977.pth.
[INFO][10:19:12]: [Client #240] Model trained.
[INFO][10:19:12]: [Client #240] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:19:12]: [Server #1127936] Received 0.24 MB of payload data from client #240 (simulated).
[INFO][10:19:12]: [Client #394] Loading a model from /data/ykang/plato/results/test/model/lenet5_394_1127979.pth.
[INFO][10:19:12]: [Client #394] Model trained.
[INFO][10:19:12]: [Client #394] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:19:12]: [Server #1127936] Received 0.24 MB of payload data from client #394 (simulated).
[INFO][10:19:12]: [Server #1127936] Selecting client #67 for training.
[INFO][10:19:12]: [Server #1127936] Sending the current model to client #67 (simulated).
[INFO][10:19:12]: [Server #1127936] Sending 0.24 MB of payload data to client #67 (simulated).
[INFO][10:19:12]: [Server #1127936] Selecting client #123 for training.
[INFO][10:19:12]: [Server #1127936] Sending the current model to client #123 (simulated).
[INFO][10:19:12]: [Server #1127936] Sending 0.24 MB of payload data to client #123 (simulated).
[INFO][10:19:12]: [Server #1127936] Selecting client #148 for training.
[INFO][10:19:12]: [Server #1127936] Sending the current model to client #148 (simulated).
[INFO][10:19:12]: [Client #67] Selected by the server.
[INFO][10:19:12]: [Client #67] Loading its data source...
[INFO][10:19:12]: [Client #67] Dataset size: 60000
[INFO][10:19:12]: [Client #67] Sampler: noniid
[INFO][10:19:12]: [Server #1127936] Sending 0.24 MB of payload data to client #148 (simulated).
[INFO][10:19:12]: [Client #123] Selected by the server.
[INFO][10:19:12]: [Client #123] Loading its data source...
[INFO][10:19:12]: [Client #123] Dataset size: 60000
[INFO][10:19:12]: [Client #123] Sampler: noniid
[INFO][10:19:12]: [Client #148] Selected by the server.
[INFO][10:19:12]: [Client #148] Loading its data source...
[INFO][10:19:12]: [Client #148] Dataset size: 60000
[INFO][10:19:12]: [Client #148] Sampler: noniid
[INFO][10:19:12]: [Client #67] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:19:12]: [Client #123] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:19:12]: [93m[1m[Client #67] Started training in communication round #58.[0m
[INFO][10:19:12]: [93m[1m[Client #123] Started training in communication round #58.[0m
[INFO][10:19:12]: [Client #148] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:19:12]: [93m[1m[Client #148] Started training in communication round #58.[0m
[INFO][10:19:14]: [Client #148] Loading the dataset.
[INFO][10:19:14]: [Client #123] Loading the dataset.
[INFO][10:19:14]: [Client #67] Loading the dataset.
[INFO][10:19:20]: [Client #148] Epoch: [1/5][0/10]	Loss: 0.007378
[INFO][10:19:20]: [Client #67] Epoch: [1/5][0/10]	Loss: 0.002049
[INFO][10:19:20]: [Client #123] Epoch: [1/5][0/10]	Loss: 0.015421
[INFO][10:19:20]: [Client #148] Epoch: [2/5][0/10]	Loss: 0.006761
[INFO][10:19:20]: [Client #123] Epoch: [2/5][0/10]	Loss: 0.000323
[INFO][10:19:20]: [Client #67] Epoch: [2/5][0/10]	Loss: 0.000844
[INFO][10:19:20]: [Client #148] Epoch: [3/5][0/10]	Loss: 0.000043
[INFO][10:19:20]: [Client #123] Epoch: [3/5][0/10]	Loss: 0.000000
[INFO][10:19:20]: [Client #148] Epoch: [4/5][0/10]	Loss: 0.000489
[INFO][10:19:20]: [Client #67] Epoch: [3/5][0/10]	Loss: 0.000407
[INFO][10:19:21]: [Client #123] Epoch: [4/5][0/10]	Loss: 0.000005
[INFO][10:19:21]: [Client #148] Epoch: [5/5][0/10]	Loss: 0.000060
[INFO][10:19:21]: [Client #67] Epoch: [4/5][0/10]	Loss: 0.000022
[INFO][10:19:21]: [Client #148] Model saved to /data/ykang/plato/results/test/model/lenet5_148_1127979.pth.
[INFO][10:19:21]: [Client #123] Epoch: [5/5][0/10]	Loss: 0.000002
[INFO][10:19:21]: [Client #123] Model saved to /data/ykang/plato/results/test/model/lenet5_123_1127978.pth.
[INFO][10:19:21]: [Client #67] Epoch: [5/5][0/10]	Loss: 0.000030
[INFO][10:19:21]: [Client #67] Model saved to /data/ykang/plato/results/test/model/lenet5_67_1127977.pth.
[INFO][10:19:21]: [Client #148] Loading a model from /data/ykang/plato/results/test/model/lenet5_148_1127979.pth.
[INFO][10:19:21]: [Client #148] Model trained.
[INFO][10:19:21]: [Client #148] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:19:21]: [Server #1127936] Received 0.24 MB of payload data from client #148 (simulated).
[INFO][10:19:22]: [Client #123] Loading a model from /data/ykang/plato/results/test/model/lenet5_123_1127978.pth.
[INFO][10:19:22]: [Client #123] Model trained.
[INFO][10:19:22]: [Client #123] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:19:22]: [Server #1127936] Received 0.24 MB of payload data from client #123 (simulated).
[INFO][10:19:22]: [Client #67] Loading a model from /data/ykang/plato/results/test/model/lenet5_67_1127977.pth.
[INFO][10:19:22]: [Client #67] Model trained.
[INFO][10:19:22]: [Client #67] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:19:22]: [Server #1127936] Received 0.24 MB of payload data from client #67 (simulated).
[INFO][10:19:22]: [Server #1127936] Selecting client #96 for training.
[INFO][10:19:22]: [Server #1127936] Sending the current model to client #96 (simulated).
[INFO][10:19:22]: [Server #1127936] Sending 0.24 MB of payload data to client #96 (simulated).
[INFO][10:19:22]: [Client #96] Selected by the server.
[INFO][10:19:22]: [Client #96] Loading its data source...
[INFO][10:19:22]: [Client #96] Dataset size: 60000
[INFO][10:19:22]: [Client #96] Sampler: noniid
[INFO][10:19:22]: [Client #96] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:19:22]: [93m[1m[Client #96] Started training in communication round #58.[0m
[INFO][10:19:24]: [Client #96] Loading the dataset.
[INFO][10:19:29]: [Client #96] Epoch: [1/5][0/10]	Loss: 0.004596
[INFO][10:19:29]: [Client #96] Epoch: [2/5][0/10]	Loss: 0.000377
[INFO][10:19:29]: [Client #96] Epoch: [3/5][0/10]	Loss: 0.000045
[INFO][10:19:29]: [Client #96] Epoch: [4/5][0/10]	Loss: 0.001166
[INFO][10:19:29]: [Client #96] Epoch: [5/5][0/10]	Loss: 0.002869
[INFO][10:19:29]: [Client #96] Model saved to /data/ykang/plato/results/test/model/lenet5_96_1127977.pth.
[INFO][10:19:30]: [Client #96] Loading a model from /data/ykang/plato/results/test/model/lenet5_96_1127977.pth.
[INFO][10:19:30]: [Client #96] Model trained.
[INFO][10:19:30]: [Client #96] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:19:30]: [Server #1127936] Received 0.24 MB of payload data from client #96 (simulated).
[INFO][10:19:30]: [Server #1127936] Adding client #468 to the list of clients for aggregation.
[INFO][10:19:30]: [Server #1127936] Adding client #474 to the list of clients for aggregation.
[INFO][10:19:30]: [Server #1127936] Adding client #69 to the list of clients for aggregation.
[INFO][10:19:30]: [Server #1127936] Adding client #110 to the list of clients for aggregation.
[INFO][10:19:30]: [Server #1127936] Adding client #30 to the list of clients for aggregation.
[INFO][10:19:30]: [Server #1127936] Adding client #71 to the list of clients for aggregation.
[INFO][10:19:30]: [Server #1127936] Adding client #394 to the list of clients for aggregation.
[INFO][10:19:30]: [Server #1127936] Adding client #123 to the list of clients for aggregation.
[INFO][10:19:30]: [Server #1127936] Adding client #13 to the list of clients for aggregation.
[INFO][10:19:30]: [Server #1127936] Adding client #96 to the list of clients for aggregation.
[INFO][10:19:30]: [Server #1127936] Aggregating 10 clients in total.
[0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.002
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  1e-05  2e-10  2e-10
 6:  6.8876e+00  6.8875e+00  1e-05  1e-10  1e-10
 7:  6.8875e+00  6.8875e+00  1e-05  1e-10  1e-12
 8:  6.8875e+00  6.8875e+00  6e-06  1e-10  1e-12
Optimal solution found.
The calculated probability is:  [0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.01373945 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.38376387 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00119931 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00366255
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00448773 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120179
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120183 0.00120187 0.00120187 0.00120187
 0.00181598 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120185 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.01083029 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187 0.00120187
 0.00120187 0.00120187 0.00120187 0.00120187]
current clients pool:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 469, 470, 471, 472, 473, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00993983 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.02809744
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00240365 0.         0.01348351 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00484801
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01591458 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01344334 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01319725 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.016084
 0.         0.         0.         0.         0.         0.04384642
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 0. 1. 0. 0. 0. 1. 0. 1. 3. 1. 0. 0. 0. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 2. 0. 1. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 2. 1. 0. 1. 0.
 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 2. 1. 6. 0. 1. 2. 1. 0. 1. 1. 0. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 0. 6. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 2. 0. 0.
 1. 0. 1. 2. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 4. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0.
 1. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 1. 0. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 3. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 0. 0. 0. 1. 0. 0. 6. 1. 1.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 6. 1. 0. 0. 1. 0. 0. 1. 2. 0. 1. 0. 0. 1.
 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 2. 2. 1. 1. 0.]
local_gradient_bounds:  [INFO][10:19:32]: [Server #1127936] Global model accuracy: 94.24%

[INFO][10:19:32]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_58.pth.
[INFO][10:19:32]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_58.pth.
[INFO][10:19:32]: [93m[1m
[Server #1127936] Starting round 59/100.[0m
[0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00993983 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.02809744
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00240365 0.         0.01348351 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00484801
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01591458 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01344334 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01319725 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.016084
 0.         0.         0.         0.         0.         0.04384642
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.002
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8871e+00  4e-04  7e-09  7e-09
 5:  6.8876e+00  6.8875e+00  1e-04  2e-09  2e-09
 6:  6.8875e+00  6.8875e+00  9e-05  6e-10  2e-10
 7:  6.8875e+00  6.8875e+00  8e-05  5e-09  5e-10
 8:  6.8875e+00  6.8875e+00  6e-05  7e-09  7e-10
 9:  6.8875e+00  6.8875e+00  2e-05  4e-08  4e-09
10:  6.8875e+00  6.8875e+00  4e-06  1e-08  1e-09
Optimal solution found.
The calculated probability is:  [8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35860297e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 1.31450424e-04 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.91945508e-05 8.35871111e-05
 1.01418975e-04 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35868539e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 9.58924956e-01 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35851330e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35852048e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 1.05743943e-04 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 1.91079560e-04
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05 8.35871111e-05 8.35871111e-05
 8.35871111e-05 8.35871111e-05]
current clients pool:  [INFO][10:19:33]: [Server #1127936] Selected clients: [110  14 403 406 436 339 407 451 232 284]
[INFO][10:19:33]: [Server #1127936] Selecting client #110 for training.
[INFO][10:19:33]: [Server #1127936] Sending the current model to client #110 (simulated).
[INFO][10:19:33]: [Server #1127936] Sending 0.24 MB of payload data to client #110 (simulated).
[INFO][10:19:33]: [Server #1127936] Selecting client #14 for training.
[INFO][10:19:33]: [Server #1127936] Sending the current model to client #14 (simulated).
[INFO][10:19:33]: [Server #1127936] Sending 0.24 MB of payload data to client #14 (simulated).
[INFO][10:19:33]: [Server #1127936] Selecting client #403 for training.
[INFO][10:19:33]: [Server #1127936] Sending the current model to client #403 (simulated).
[INFO][10:19:33]: [Client #110] Selected by the server.
[INFO][10:19:33]: [Client #110] Loading its data source...
[INFO][10:19:33]: [Client #110] Dataset size: 60000
[INFO][10:19:33]: [Client #110] Sampler: noniid
[INFO][10:19:33]: [Server #1127936] Sending 0.24 MB of payload data to client #403 (simulated).
[INFO][10:19:33]: [Client #14] Selected by the server.
[INFO][10:19:33]: [Client #14] Loading its data source...
[INFO][10:19:33]: [Client #14] Dataset size: 60000
[INFO][10:19:33]: [Client #14] Sampler: noniid
[INFO][10:19:33]: [Client #110] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:19:33]: [Client #14] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:19:33]: [Client #403] Selected by the server.
[INFO][10:19:33]: [Client #403] Loading its data source...
[INFO][10:19:33]: [Client #403] Dataset size: 60000
[INFO][10:19:33]: [Client #403] Sampler: noniid
[INFO][10:19:33]: [Client #403] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:19:33]: [93m[1m[Client #110] Started training in communication round #59.[0m
[INFO][10:19:33]: [93m[1m[Client #14] Started training in communication round #59.[0m
[INFO][10:19:33]: [93m[1m[Client #403] Started training in communication round #59.[0m
[INFO][10:19:35]: [Client #403] Loading the dataset.
[INFO][10:19:35]: [Client #110] Loading the dataset.
[INFO][10:19:35]: [Client #14] Loading the dataset.
[INFO][10:19:41]: [Client #403] Epoch: [1/5][0/10]	Loss: 0.031350
[INFO][10:19:41]: [Client #110] Epoch: [1/5][0/10]	Loss: 0.029493
[INFO][10:19:41]: [Client #14] Epoch: [1/5][0/10]	Loss: 0.070865
[INFO][10:19:41]: [Client #403] Epoch: [2/5][0/10]	Loss: 0.003480
[INFO][10:19:41]: [Client #403] Epoch: [3/5][0/10]	Loss: 0.000069
[INFO][10:19:41]: [Client #110] Epoch: [2/5][0/10]	Loss: 0.000612
[INFO][10:19:41]: [Client #14] Epoch: [2/5][0/10]	Loss: 0.001560
[INFO][10:19:41]: [Client #403] Epoch: [4/5][0/10]	Loss: 0.000078
[INFO][10:19:41]: [Client #110] Epoch: [3/5][0/10]	Loss: 0.000005
[INFO][10:19:41]: [Client #14] Epoch: [3/5][0/10]	Loss: 0.001804
[INFO][10:19:42]: [Client #403] Epoch: [5/5][0/10]	Loss: 0.000030
[INFO][10:19:42]: [Client #110] Epoch: [4/5][0/10]	Loss: 0.000016
[INFO][10:19:42]: [Client #403] Model saved to /data/ykang/plato/results/test/model/lenet5_403_1127979.pth.
[INFO][10:19:42]: [Client #14] Epoch: [4/5][0/10]	Loss: 0.195866
[INFO][10:19:42]: [Client #110] Epoch: [5/5][0/10]	Loss: 0.000007
[INFO][10:19:42]: [Client #110] Model saved to /data/ykang/plato/results/test/model/lenet5_110_1127977.pth.
[INFO][10:19:42]: [Client #14] Epoch: [5/5][0/10]	Loss: 0.001739
[INFO][10:19:42]: [Client #14] Model saved to /data/ykang/plato/results/test/model/lenet5_14_1127978.pth.
[INFO][10:19:42]: [Client #403] Loading a model from /data/ykang/plato/results/test/model/lenet5_403_1127979.pth.
[INFO][10:19:42]: [Client #403] Model trained.
[INFO][10:19:42]: [Client #403] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:19:42]: [Server #1127936] Received 0.24 MB of payload data from client #403 (simulated).
[INFO][10:19:42]: [Client #110] Loading a model from /data/ykang/plato/results/test/model/lenet5_110_1127977.pth.
[INFO][10:19:43]: [Client #110] Model trained.
[INFO][10:19:43]: [Client #110] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:19:43]: [Server #1127936] Received 0.24 MB of payload data from client #110 (simulated).
[INFO][10:19:43]: [Client #14] Loading a model from /data/ykang/plato/results/test/model/lenet5_14_1127978.pth.
[INFO][10:19:43]: [Client #14] Model trained.
[INFO][10:19:43]: [Client #14] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:19:43]: [Server #1127936] Received 0.24 MB of payload data from client #14 (simulated).
[INFO][10:19:43]: [Server #1127936] Selecting client #406 for training.
[INFO][10:19:43]: [Server #1127936] Sending the current model to client #406 (simulated).
[INFO][10:19:43]: [Server #1127936] Sending 0.24 MB of payload data to client #406 (simulated).
[INFO][10:19:43]: [Server #1127936] Selecting client #436 for training.
[INFO][10:19:43]: [Server #1127936] Sending the current model to client #436 (simulated).
[INFO][10:19:43]: [Server #1127936] Sending 0.24 MB of payload data to client #436 (simulated).
[INFO][10:19:43]: [Server #1127936] Selecting client #339 for training.
[INFO][10:19:43]: [Server #1127936] Sending the current model to client #339 (simulated).
[INFO][10:19:43]: [Client #406] Selected by the server.
[INFO][10:19:43]: [Client #406] Loading its data source...
[INFO][10:19:43]: [Client #406] Dataset size: 60000
[INFO][10:19:43]: [Client #406] Sampler: noniid
[INFO][10:19:43]: [Server #1127936] Sending 0.24 MB of payload data to client #339 (simulated).
[INFO][10:19:43]: [Client #436] Selected by the server.
[INFO][10:19:43]: [Client #436] Loading its data source...
[INFO][10:19:43]: [Client #436] Dataset size: 60000
[INFO][10:19:43]: [Client #436] Sampler: noniid
[INFO][10:19:43]: [Client #339] Selected by the server.
[INFO][10:19:43]: [Client #339] Loading its data source...
[INFO][10:19:43]: [Client #339] Dataset size: 60000
[INFO][10:19:43]: [Client #339] Sampler: noniid
[INFO][10:19:43]: [Client #406] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:19:43]: [Client #436] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:19:43]: [Client #339] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:19:43]: [93m[1m[Client #339] Started training in communication round #59.[0m
[INFO][10:19:43]: [93m[1m[Client #406] Started training in communication round #59.[0m
[INFO][10:19:43]: [93m[1m[Client #436] Started training in communication round #59.[0m
[INFO][10:19:45]: [Client #339] Loading the dataset.
[INFO][10:19:45]: [Client #436] Loading the dataset.
[INFO][10:19:45]: [Client #406] Loading the dataset.
[INFO][10:19:51]: [Client #339] Epoch: [1/5][0/10]	Loss: 0.005249
[INFO][10:19:51]: [Client #436] Epoch: [1/5][0/10]	Loss: 0.042032
[INFO][10:19:51]: [Client #339] Epoch: [2/5][0/10]	Loss: 0.000327
[INFO][10:19:51]: [Client #406] Epoch: [1/5][0/10]	Loss: 0.014361
[INFO][10:19:51]: [Client #436] Epoch: [2/5][0/10]	Loss: 0.025960
[INFO][10:19:51]: [Client #339] Epoch: [3/5][0/10]	Loss: 0.000156
[INFO][10:19:51]: [Client #406] Epoch: [2/5][0/10]	Loss: 0.008128
[INFO][10:19:51]: [Client #436] Epoch: [3/5][0/10]	Loss: 0.000067
[INFO][10:19:51]: [Client #339] Epoch: [4/5][0/10]	Loss: 0.000001
[INFO][10:19:51]: [Client #436] Epoch: [4/5][0/10]	Loss: 0.000018
[INFO][10:19:51]: [Client #406] Epoch: [3/5][0/10]	Loss: 0.002615
[INFO][10:19:51]: [Client #339] Epoch: [5/5][0/10]	Loss: 0.000073
[INFO][10:19:51]: [Client #406] Epoch: [4/5][0/10]	Loss: 0.000728
[INFO][10:19:51]: [Client #436] Epoch: [5/5][0/10]	Loss: 0.000604
[INFO][10:19:51]: [Client #339] Model saved to /data/ykang/plato/results/test/model/lenet5_339_1127979.pth.
[INFO][10:19:51]: [Client #436] Model saved to /data/ykang/plato/results/test/model/lenet5_436_1127978.pth.
[INFO][10:19:51]: [Client #406] Epoch: [5/5][0/10]	Loss: 0.008526
[INFO][10:19:51]: [Client #406] Model saved to /data/ykang/plato/results/test/model/lenet5_406_1127977.pth.
[INFO][10:19:52]: [Client #339] Loading a model from /data/ykang/plato/results/test/model/lenet5_339_1127979.pth.
[INFO][10:19:52]: [Client #339] Model trained.
[INFO][10:19:52]: [Client #339] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:19:52]: [Server #1127936] Received 0.24 MB of payload data from client #339 (simulated).
[INFO][10:19:52]: [Client #436] Loading a model from /data/ykang/plato/results/test/model/lenet5_436_1127978.pth.
[INFO][10:19:52]: [Client #436] Model trained.
[INFO][10:19:52]: [Client #436] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:19:52]: [Server #1127936] Received 0.24 MB of payload data from client #436 (simulated).
[INFO][10:19:52]: [Client #406] Loading a model from /data/ykang/plato/results/test/model/lenet5_406_1127977.pth.
[INFO][10:19:52]: [Client #406] Model trained.
[INFO][10:19:52]: [Client #406] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:19:52]: [Server #1127936] Received 0.24 MB of payload data from client #406 (simulated).
[INFO][10:19:52]: [Server #1127936] Selecting client #407 for training.
[INFO][10:19:52]: [Server #1127936] Sending the current model to client #407 (simulated).
[INFO][10:19:52]: [Server #1127936] Sending 0.24 MB of payload data to client #407 (simulated).
[INFO][10:19:52]: [Server #1127936] Selecting client #451 for training.
[INFO][10:19:52]: [Server #1127936] Sending the current model to client #451 (simulated).
[INFO][10:19:52]: [Server #1127936] Sending 0.24 MB of payload data to client #451 (simulated).
[INFO][10:19:52]: [Server #1127936] Selecting client #232 for training.
[INFO][10:19:52]: [Server #1127936] Sending the current model to client #232 (simulated).
[INFO][10:19:52]: [Client #407] Selected by the server.
[INFO][10:19:52]: [Client #407] Loading its data source...
[INFO][10:19:52]: [Client #407] Dataset size: 60000
[INFO][10:19:52]: [Client #407] Sampler: noniid
[INFO][10:19:52]: [Server #1127936] Sending 0.24 MB of payload data to client #232 (simulated).
[INFO][10:19:52]: [Client #451] Selected by the server.
[INFO][10:19:52]: [Client #451] Loading its data source...
[INFO][10:19:52]: [Client #451] Dataset size: 60000
[INFO][10:19:52]: [Client #232] Selected by the server.
[INFO][10:19:52]: [Client #451] Sampler: noniid
[INFO][10:19:52]: [Client #232] Loading its data source...
[INFO][10:19:52]: [Client #232] Dataset size: 60000
[INFO][10:19:52]: [Client #232] Sampler: noniid
[INFO][10:19:52]: [Client #407] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:19:52]: [Client #451] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:19:52]: [Client #232] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:19:52]: [93m[1m[Client #451] Started training in communication round #59.[0m
[INFO][10:19:52]: [93m[1m[Client #407] Started training in communication round #59.[0m
[INFO][10:19:52]: [93m[1m[Client #232] Started training in communication round #59.[0m
[INFO][10:19:54]: [Client #451] Loading the dataset.
[INFO][10:19:54]: [Client #407] Loading the dataset.
[INFO][10:19:54]: [Client #232] Loading the dataset.
[INFO][10:20:00]: [Client #451] Epoch: [1/5][0/10]	Loss: 0.013958
[INFO][10:20:00]: [Client #407] Epoch: [1/5][0/10]	Loss: 0.017555
[INFO][10:20:00]: [Client #451] Epoch: [2/5][0/10]	Loss: 0.000254
[INFO][10:20:00]: [Client #232] Epoch: [1/5][0/10]	Loss: 0.022924
[INFO][10:20:00]: [Client #451] Epoch: [3/5][0/10]	Loss: 0.000017
[INFO][10:20:00]: [Client #407] Epoch: [2/5][0/10]	Loss: 0.000805
[INFO][10:20:00]: [Client #232] Epoch: [2/5][0/10]	Loss: 0.000316
[INFO][10:20:00]: [Client #451] Epoch: [4/5][0/10]	Loss: 0.000004
[INFO][10:20:01]: [Client #407] Epoch: [3/5][0/10]	Loss: 0.000121
[INFO][10:20:01]: [Client #451] Epoch: [5/5][0/10]	Loss: 0.000218
[INFO][10:20:01]: [Client #232] Epoch: [3/5][0/10]	Loss: 0.000465
[INFO][10:20:01]: [Client #407] Epoch: [4/5][0/10]	Loss: 0.000004
[INFO][10:20:01]: [Client #451] Model saved to /data/ykang/plato/results/test/model/lenet5_451_1127978.pth.
[INFO][10:20:01]: [Client #232] Epoch: [4/5][0/10]	Loss: 0.445958
[INFO][10:20:01]: [Client #407] Epoch: [5/5][0/10]	Loss: 0.002208
[INFO][10:20:01]: [Client #232] Epoch: [5/5][0/10]	Loss: 0.199162
[INFO][10:20:01]: [Client #407] Model saved to /data/ykang/plato/results/test/model/lenet5_407_1127977.pth.
[INFO][10:20:01]: [Client #232] Model saved to /data/ykang/plato/results/test/model/lenet5_232_1127979.pth.
[INFO][10:20:02]: [Client #451] Loading a model from /data/ykang/plato/results/test/model/lenet5_451_1127978.pth.
[INFO][10:20:02]: [Client #451] Model trained.
[INFO][10:20:02]: [Client #451] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:20:02]: [Server #1127936] Received 0.24 MB of payload data from client #451 (simulated).
[INFO][10:20:02]: [Client #407] Loading a model from /data/ykang/plato/results/test/model/lenet5_407_1127977.pth.
[INFO][10:20:02]: [Client #407] Model trained.
[INFO][10:20:02]: [Client #407] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:20:02]: [Server #1127936] Received 0.24 MB of payload data from client #407 (simulated).
[INFO][10:20:02]: [Client #232] Loading a model from /data/ykang/plato/results/test/model/lenet5_232_1127979.pth.
[INFO][10:20:02]: [Client #232] Model trained.
[INFO][10:20:02]: [Client #232] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:20:02]: [Server #1127936] Received 0.24 MB of payload data from client #232 (simulated).
[INFO][10:20:02]: [Server #1127936] Selecting client #284 for training.
[INFO][10:20:02]: [Server #1127936] Sending the current model to client #284 (simulated).
[INFO][10:20:02]: [Server #1127936] Sending 0.24 MB of payload data to client #284 (simulated).
[INFO][10:20:02]: [Client #284] Selected by the server.
[INFO][10:20:02]: [Client #284] Loading its data source...
[INFO][10:20:02]: [Client #284] Dataset size: 60000
[INFO][10:20:02]: [Client #284] Sampler: noniid
[INFO][10:20:02]: [Client #284] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:20:02]: [93m[1m[Client #284] Started training in communication round #59.[0m
[INFO][10:20:04]: [Client #284] Loading the dataset.
[INFO][10:20:11]: [Client #284] Epoch: [1/5][0/10]	Loss: 0.059305
[INFO][10:20:11]: [Client #284] Epoch: [2/5][0/10]	Loss: 0.000293
[INFO][10:20:11]: [Client #284] Epoch: [3/5][0/10]	Loss: 0.000038
[INFO][10:20:11]: [Client #284] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:20:11]: [Client #284] Epoch: [5/5][0/10]	Loss: 0.000013
[INFO][10:20:11]: [Client #284] Model saved to /data/ykang/plato/results/test/model/lenet5_284_1127977.pth.
[INFO][10:20:12]: [Client #284] Loading a model from /data/ykang/plato/results/test/model/lenet5_284_1127977.pth.
[INFO][10:20:12]: [Client #284] Model trained.
[INFO][10:20:12]: [Client #284] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:20:12]: [Server #1127936] Received 0.24 MB of payload data from client #284 (simulated).
[INFO][10:20:12]: [Server #1127936] Adding client #118 to the list of clients for aggregation.
[INFO][10:20:12]: [Server #1127936] Adding client #67 to the list of clients for aggregation.
[INFO][10:20:12]: [Server #1127936] Adding client #148 to the list of clients for aggregation.
[INFO][10:20:12]: [Server #1127936] Adding client #137 to the list of clients for aggregation.
[INFO][10:20:12]: [Server #1127936] Adding client #240 to the list of clients for aggregation.
[INFO][10:20:12]: [Server #1127936] Adding client #403 to the list of clients for aggregation.
[INFO][10:20:12]: [Server #1127936] Adding client #451 to the list of clients for aggregation.
[INFO][10:20:12]: [Server #1127936] Adding client #407 to the list of clients for aggregation.
[INFO][10:20:12]: [Server #1127936] Adding client #284 to the list of clients for aggregation.
[INFO][10:20:12]: [Server #1127936] Adding client #203 to the list of clients for aggregation.
[INFO][10:20:12]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00813626 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00862736 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.0060094  0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.008712   0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00966569 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01648236
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00843181 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00865622 0.         0.         0.         0.00332823 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00561944 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 0. 1. 0. 0. 0. 1. 0. 1. 3. 1. 0. 0. 0. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 2. 0. 1. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 2. 1. 0. 1. 0.
 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 2. 1. 6. 0. 1. 2. 1. 0. 1. 1. 2. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 0. 6. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 2. 0. 0.
 1. 0. 1. 2. 0. 0. 0. 2. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 4. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0.
 1. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 1. 0. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 3. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 0. 0. 0. 0. 0. 0. 6. 1. 1.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 6. 1. 0. 0. 1. 0. 0. 1. 2. 0. 1. 0. 0. 1.
 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 2. 2. 1. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00813626 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00862736 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.0060094  0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.008712   0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00966569 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01648236
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00843181 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00865622 0.         0.         0.         0.00332823 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00561944 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:20:14]: [Server #1127936] Global model accuracy: 95.92%

[INFO][10:20:14]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_59.pth.
[INFO][10:20:14]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_59.pth.
[INFO][10:20:14]: [93m[1m
[Server #1127936] Starting round 60/100.[0m
[INFO][10:20:14]: [Server #1127936] Selected clients: [118 266 262 459 364 137 281 478  44  45]
[INFO][10:20:14]: [Server #1127936] Selecting client #118 for training.
[INFO][10:20:14]: [Server #1127936] Sending the current model to client #118 (simulated).
[INFO][10:20:14]: [Server #1127936] Sending 0.24 MB of payload data to client #118 (simulated).
[INFO][10:20:14]: [Server #1127936] Selecting client #266 for training.
[INFO][10:20:14]: [Server #1127936] Sending the current model to client #266 (simulated).
[INFO][10:20:15]: [Server #1127936] Sending 0.24 MB of payload data to client #266 (simulated).
[INFO][10:20:15]: [Server #1127936] Selecting client #262 for training.
[INFO][10:20:15]: [Server #1127936] Sending the current model to client #262 (simulated).
[INFO][10:20:15]: [Client #118] Selected by the server.
[INFO][10:20:15]: [Client #118] Loading its data source...
[INFO][10:20:15]: [Client #118] Dataset size: 60000
[INFO][10:20:15]: [Client #118] Sampler: noniid
[INFO][10:20:15]: [Server #1127936] Sending 0.24 MB of payload data to client #262 (simulated).
[INFO][10:20:15]: [Client #266] Selected by the server.
[INFO][10:20:15]: [Client #266] Loading its data source...
[INFO][10:20:15]: [Client #266] Dataset size: 60000
[INFO][10:20:15]: [Client #266] Sampler: noniid
[INFO][10:20:15]: [Client #118] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:20:15]: [Client #262] Selected by the server.
[INFO][10:20:15]: [Client #262] Loading its data source...
[INFO][10:20:15]: [Client #262] Dataset size: 60000
[INFO][10:20:15]: [Client #262] Sampler: noniid
[INFO][10:20:15]: [Client #266] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:20:15]: [Client #262] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:20:15]: [93m[1m[Client #266] Started training in communication round #60.[0m
[INFO][10:20:15]: [93m[1m[Client #262] Started training in communication round #60.[0m
[INFO][10:20:15]: [93m[1m[Client #118] Started training in communication round #60.[0m
[INFO][10:20:17]: [Client #262] Loading the dataset.
[INFO][10:20:17]: [Client #266] Loading the dataset.
[INFO][10:20:17]: [Client #118] Loading the dataset.
[INFO][10:20:23]: [Client #262] Epoch: [1/5][0/10]	Loss: 0.014751
[INFO][10:20:23]: [Client #118] Epoch: [1/5][0/10]	Loss: 0.011041
[INFO][10:20:23]: [Client #262] Epoch: [2/5][0/10]	Loss: 0.001444
[INFO][10:20:23]: [Client #266] Epoch: [1/5][0/10]	Loss: 0.010950
[INFO][10:20:23]: [Client #118] Epoch: [2/5][0/10]	Loss: 0.008387
[INFO][10:20:23]: [Client #262] Epoch: [3/5][0/10]	Loss: 0.000350
[INFO][10:20:23]: [Client #266] Epoch: [2/5][0/10]	Loss: 0.001227
[INFO][10:20:23]: [Client #118] Epoch: [3/5][0/10]	Loss: 0.000723
[INFO][10:20:23]: [Client #262] Epoch: [4/5][0/10]	Loss: 0.000935
[INFO][10:20:23]: [Client #266] Epoch: [3/5][0/10]	Loss: 0.000015
[INFO][10:20:23]: [Client #118] Epoch: [4/5][0/10]	Loss: 0.001257
[INFO][10:20:23]: [Client #262] Epoch: [5/5][0/10]	Loss: 0.067118
[INFO][10:20:23]: [Client #262] Model saved to /data/ykang/plato/results/test/model/lenet5_262_1127979.pth.
[INFO][10:20:23]: [Client #266] Epoch: [4/5][0/10]	Loss: 0.000009
[INFO][10:20:23]: [Client #118] Epoch: [5/5][0/10]	Loss: 0.004144
[INFO][10:20:23]: [Client #266] Epoch: [5/5][0/10]	Loss: 0.001863
[INFO][10:20:23]: [Client #118] Model saved to /data/ykang/plato/results/test/model/lenet5_118_1127977.pth.
[INFO][10:20:23]: [Client #266] Model saved to /data/ykang/plato/results/test/model/lenet5_266_1127978.pth.
[INFO][10:20:24]: [Client #262] Loading a model from /data/ykang/plato/results/test/model/lenet5_262_1127979.pth.
[INFO][10:20:24]: [Client #262] Model trained.
[INFO][10:20:24]: [Client #262] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:20:24]: [Server #1127936] Received 0.24 MB of payload data from client #262 (simulated).
[INFO][10:20:24]: [Client #118] Loading a model from /data/ykang/plato/results/test/model/lenet5_118_1127977.pth.
[INFO][10:20:24]: [Client #118] Model trained.
[INFO][10:20:24]: [Client #118] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:20:24]: [Server #1127936] Received 0.24 MB of payload data from client #118 (simulated).
[INFO][10:20:24]: [Client #266] Loading a model from /data/ykang/plato/results/test/model/lenet5_266_1127978.pth.
[INFO][10:20:24]: [Client #266] Model trained.
[INFO][10:20:24]: [Client #266] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:20:24]: [Server #1127936] Received 0.24 MB of payload data from client #266 (simulated).
[INFO][10:20:24]: [Server #1127936] Selecting client #459 for training.
[INFO][10:20:24]: [Server #1127936] Sending the current model to client #459 (simulated).
[INFO][10:20:24]: [Server #1127936] Sending 0.24 MB of payload data to client #459 (simulated).
[INFO][10:20:24]: [Server #1127936] Selecting client #364 for training.
[INFO][10:20:24]: [Server #1127936] Sending the current model to client #364 (simulated).
[INFO][10:20:24]: [Server #1127936] Sending 0.24 MB of payload data to client #364 (simulated).
[INFO][10:20:24]: [Server #1127936] Selecting client #137 for training.
[INFO][10:20:24]: [Server #1127936] Sending the current model to client #137 (simulated).
[INFO][10:20:24]: [Client #459] Selected by the server.
[INFO][10:20:24]: [Client #459] Loading its data source...
[INFO][10:20:24]: [Client #459] Dataset size: 60000
[INFO][10:20:24]: [Client #459] Sampler: noniid
[INFO][10:20:24]: [Server #1127936] Sending 0.24 MB of payload data to client #137 (simulated).
[INFO][10:20:24]: [Client #459] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:20:24]: [Client #364] Selected by the server.
[INFO][10:20:24]: [Client #364] Loading its data source...
[INFO][10:20:24]: [Client #364] Dataset size: 60000
[INFO][10:20:24]: [Client #364] Sampler: noniid
[INFO][10:20:24]: [Client #137] Selected by the server.
[INFO][10:20:24]: [Client #137] Loading its data source...
[INFO][10:20:24]: [Client #137] Dataset size: 60000
[INFO][10:20:24]: [Client #137] Sampler: noniid
[INFO][10:20:24]: [Client #364] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:20:24]: [Client #137] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:20:24]: [93m[1m[Client #459] Started training in communication round #60.[0m
[INFO][10:20:24]: [93m[1m[Client #137] Started training in communication round #60.[0m
[INFO][10:20:24]: [93m[1m[Client #364] Started training in communication round #60.[0m
[INFO][10:20:26]: [Client #459] Loading the dataset.
[INFO][10:20:26]: [Client #364] Loading the dataset.
[INFO][10:20:26]: [Client #137] Loading the dataset.
[INFO][10:20:32]: [Client #459] Epoch: [1/5][0/10]	Loss: 0.009137
[INFO][10:20:32]: [Client #364] Epoch: [1/5][0/10]	Loss: 0.000741
[INFO][10:20:32]: [Client #137] Epoch: [1/5][0/10]	Loss: 0.010970
[INFO][10:20:32]: [Client #364] Epoch: [2/5][0/10]	Loss: 0.000124
[INFO][10:20:32]: [Client #459] Epoch: [2/5][0/10]	Loss: 0.003415
[INFO][10:20:33]: [Client #364] Epoch: [3/5][0/10]	Loss: 0.000037
[INFO][10:20:33]: [Client #137] Epoch: [2/5][0/10]	Loss: 0.013276
[INFO][10:20:33]: [Client #459] Epoch: [3/5][0/10]	Loss: 0.000058
[INFO][10:20:33]: [Client #459] Epoch: [4/5][0/10]	Loss: 0.000002
[INFO][10:20:33]: [Client #364] Epoch: [4/5][0/10]	Loss: 0.036336
[INFO][10:20:33]: [Client #459] Epoch: [5/5][0/10]	Loss: 0.000181
[INFO][10:20:33]: [Client #137] Epoch: [3/5][0/10]	Loss: 0.000134
[INFO][10:20:33]: [Client #459] Model saved to /data/ykang/plato/results/test/model/lenet5_459_1127977.pth.
[INFO][10:20:33]: [Client #364] Epoch: [5/5][0/10]	Loss: 0.000016
[INFO][10:20:33]: [Client #137] Epoch: [4/5][0/10]	Loss: 0.000067
[INFO][10:20:33]: [Client #364] Model saved to /data/ykang/plato/results/test/model/lenet5_364_1127978.pth.
[INFO][10:20:33]: [Client #137] Epoch: [5/5][0/10]	Loss: 0.134279
[INFO][10:20:33]: [Client #137] Model saved to /data/ykang/plato/results/test/model/lenet5_137_1127979.pth.
[INFO][10:20:34]: [Client #459] Loading a model from /data/ykang/plato/results/test/model/lenet5_459_1127977.pth.
[INFO][10:20:34]: [Client #459] Model trained.
[INFO][10:20:34]: [Client #459] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:20:34]: [Server #1127936] Received 0.24 MB of payload data from client #459 (simulated).
[INFO][10:20:34]: [Client #364] Loading a model from /data/ykang/plato/results/test/model/lenet5_364_1127978.pth.
[INFO][10:20:34]: [Client #364] Model trained.
[INFO][10:20:34]: [Client #364] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:20:34]: [Server #1127936] Received 0.24 MB of payload data from client #364 (simulated).
[INFO][10:20:34]: [Client #137] Loading a model from /data/ykang/plato/results/test/model/lenet5_137_1127979.pth.
[INFO][10:20:34]: [Client #137] Model trained.
[INFO][10:20:34]: [Client #137] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:20:34]: [Server #1127936] Received 0.24 MB of payload data from client #137 (simulated).
[INFO][10:20:34]: [Server #1127936] Selecting client #281 for training.
[INFO][10:20:34]: [Server #1127936] Sending the current model to client #281 (simulated).
[INFO][10:20:34]: [Server #1127936] Sending 0.24 MB of payload data to client #281 (simulated).
[INFO][10:20:34]: [Server #1127936] Selecting client #478 for training.
[INFO][10:20:34]: [Server #1127936] Sending the current model to client #478 (simulated).
[INFO][10:20:34]: [Server #1127936] Sending 0.24 MB of payload data to client #478 (simulated).
[INFO][10:20:34]: [Server #1127936] Selecting client #44 for training.
[INFO][10:20:34]: [Server #1127936] Sending the current model to client #44 (simulated).
[INFO][10:20:34]: [Client #281] Selected by the server.
[INFO][10:20:34]: [Client #281] Loading its data source...
[INFO][10:20:34]: [Client #281] Dataset size: 60000
[INFO][10:20:34]: [Client #281] Sampler: noniid
[INFO][10:20:34]: [Server #1127936] Sending 0.24 MB of payload data to client #44 (simulated).
[INFO][10:20:34]: [Client #478] Selected by the server.
[INFO][10:20:34]: [Client #281] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:20:34]: [Client #478] Loading its data source...
[INFO][10:20:34]: [Client #478] Dataset size: 60000
[INFO][10:20:34]: [Client #44] Selected by the server.
[INFO][10:20:34]: [Client #478] Sampler: noniid
[INFO][10:20:34]: [Client #44] Loading its data source...
[INFO][10:20:34]: [Client #44] Dataset size: 60000
[INFO][10:20:34]: [Client #44] Sampler: noniid
[INFO][10:20:34]: [Client #478] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:20:34]: [Client #44] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:20:34]: [93m[1m[Client #281] Started training in communication round #60.[0m
[INFO][10:20:34]: [93m[1m[Client #478] Started training in communication round #60.[0m
[INFO][10:20:34]: [93m[1m[Client #44] Started training in communication round #60.[0m
[INFO][10:20:36]: [Client #281] Loading the dataset.
[INFO][10:20:36]: [Client #478] Loading the dataset.
[INFO][10:20:36]: [Client #44] Loading the dataset.
[INFO][10:20:42]: [Client #478] Epoch: [1/5][0/10]	Loss: 0.005654
[INFO][10:20:42]: [Client #281] Epoch: [1/5][0/10]	Loss: 0.002682
[INFO][10:20:42]: [Client #281] Epoch: [2/5][0/10]	Loss: 0.002265
[INFO][10:20:42]: [Client #44] Epoch: [1/5][0/10]	Loss: 0.000871
[INFO][10:20:42]: [Client #478] Epoch: [2/5][0/10]	Loss: 0.003632
[INFO][10:20:42]: [Client #281] Epoch: [3/5][0/10]	Loss: 0.000143
[INFO][10:20:42]: [Client #44] Epoch: [2/5][0/10]	Loss: 0.003879
[INFO][10:20:42]: [Client #478] Epoch: [3/5][0/10]	Loss: 0.000010
[INFO][10:20:42]: [Client #281] Epoch: [4/5][0/10]	Loss: 0.000026
[INFO][10:20:42]: [Client #44] Epoch: [3/5][0/10]	Loss: 0.000206
[INFO][10:20:42]: [Client #478] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:20:42]: [Client #281] Epoch: [5/5][0/10]	Loss: 0.000141
[INFO][10:20:42]: [Client #44] Epoch: [4/5][0/10]	Loss: 0.000321
[INFO][10:20:42]: [Client #478] Epoch: [5/5][0/10]	Loss: 0.000001
[INFO][10:20:43]: [Client #281] Model saved to /data/ykang/plato/results/test/model/lenet5_281_1127977.pth.
[INFO][10:20:43]: [Client #478] Model saved to /data/ykang/plato/results/test/model/lenet5_478_1127978.pth.
[INFO][10:20:43]: [Client #44] Epoch: [5/5][0/10]	Loss: 0.001201
[INFO][10:20:43]: [Client #44] Model saved to /data/ykang/plato/results/test/model/lenet5_44_1127979.pth.
[INFO][10:20:43]: [Client #281] Loading a model from /data/ykang/plato/results/test/model/lenet5_281_1127977.pth.
[INFO][10:20:43]: [Client #281] Model trained.
[INFO][10:20:43]: [Client #281] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:20:43]: [Server #1127936] Received 0.24 MB of payload data from client #281 (simulated).
[INFO][10:20:43]: [Client #478] Loading a model from /data/ykang/plato/results/test/model/lenet5_478_1127978.pth.
[INFO][10:20:43]: [Client #478] Model trained.
[INFO][10:20:43]: [Client #478] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:20:43]: [Server #1127936] Received 0.24 MB of payload data from client #478 (simulated).
[INFO][10:20:43]: [Client #44] Loading a model from /data/ykang/plato/results/test/model/lenet5_44_1127979.pth.
[INFO][10:20:43]: [Client #44] Model trained.
[INFO][10:20:43]: [Client #44] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:20:43]: [Server #1127936] Received 0.24 MB of payload data from client #44 (simulated).
[INFO][10:20:43]: [Server #1127936] Selecting client #45 for training.
[INFO][10:20:43]: [Server #1127936] Sending the current model to client #45 (simulated).
[INFO][10:20:43]: [Server #1127936] Sending 0.24 MB of payload data to client #45 (simulated).
[INFO][10:20:43]: [Client #45] Selected by the server.
[INFO][10:20:43]: [Client #45] Loading its data source...
[INFO][10:20:43]: [Client #45] Dataset size: 60000
[INFO][10:20:43]: [Client #45] Sampler: noniid
[INFO][10:20:43]: [Client #45] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:20:43]: [93m[1m[Client #45] Started training in communication round #60.[0m
[INFO][10:20:45]: [Client #45] Loading the dataset.
[INFO][10:20:51]: [Client #45] Epoch: [1/5][0/10]	Loss: 0.000369
[INFO][10:20:51]: [Client #45] Epoch: [2/5][0/10]	Loss: 0.009790
[INFO][10:20:51]: [Client #45] Epoch: [3/5][0/10]	Loss: 0.000656
[INFO][10:20:51]: [Client #45] Epoch: [4/5][0/10]	Loss: 0.000015
[INFO][10:20:51]: [Client #45] Epoch: [5/5][0/10]	Loss: 0.000076
[INFO][10:20:51]: [Client #45] Model saved to /data/ykang/plato/results/test/model/lenet5_45_1127977.pth.
[INFO][10:20:52]: [Client #45] Loading a model from /data/ykang/plato/results/test/model/lenet5_45_1127977.pth.
[INFO][10:20:52]: [Client #45] Model trained.
[INFO][10:20:52]: [Client #45] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:20:52]: [Server #1127936] Received 0.24 MB of payload data from client #45 (simulated).
[INFO][10:20:52]: [Server #1127936] Adding client #436 to the list of clients for aggregation.
[INFO][10:20:52]: [Server #1127936] Adding client #232 to the list of clients for aggregation.
[INFO][10:20:52]: [Server #1127936] Adding client #339 to the list of clients for aggregation.
[INFO][10:20:52]: [Server #1127936] Adding client #14 to the list of clients for aggregation.
[INFO][10:20:52]: [Server #1127936] Adding client #172 to the list of clients for aggregation.
[INFO][10:20:52]: [Server #1127936] Adding client #92 to the list of clients for aggregation.
[INFO][10:20:52]: [Server #1127936] Adding client #45 to the list of clients for aggregation.
[INFO][10:20:52]: [Server #1127936] Adding client #459 to the list of clients for aggregation.
[INFO][10:20:52]: [Server #1127936] Adding client #266 to the list of clients for aggregation.
[INFO][10:20:52]: [Server #1127936] Adding client #364 to the list of clients for aggregation.
[INFO][10:20:52]: [Server #1127936] Aggregating 10 clients in total.
[0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.002
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  2e-05  4e-10  4e-10
 6:  6.8876e+00  6.8875e+00  2e-05  2e-10  2e-10
 7:  6.8875e+00  6.8875e+00  2e-05  4e-10  7e-12
 8:  6.8875e+00  6.8875e+00  1e-05  3e-10  5e-12
 9:  6.8875e+00  6.8875e+00  6e-06  5e-10  9e-12
Optimal solution found.
The calculated probability is:  [0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00146614 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.61584688
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00117445 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00157044
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00177825
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.01743619
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074527 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074527
 0.00074531 0.00074531 0.0007453  0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074529 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531 0.00074531
 0.00074531 0.00074531 0.00074531 0.00074531]
current clients pool:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.02486618 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00323057 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00274486 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00962571 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.02033102 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00455897 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00510642 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.015154   0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01388769 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00344865 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 0. 1. 0. 0. 0. 1. 0. 1. 3. 1. 0. 0. 0. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 2. 0. 1. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 2. 1. 0. 1. 0.
 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 2. 1. 6. 0. 1. 2. 1. 0. 1. 1. 2. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 0. 6. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 2. 0. 0.
 1. 0. 1. 2. 0. 0. 0. 2. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 4. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0.
 1. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 0. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 3. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 0. 0. 0. 0. 0. 0. 6. 1. 1.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 6. 1. 0. 0. 1. 0. 0. 1. 2. 0. 1. 0. 0. 1.
 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 2. 2. 1. 1. 0.]
local_gradient_bounds:  [INFO][10:20:54]: [Server #1127936] Global model accuracy: 95.91%

[INFO][10:20:54]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_60.pth.
[INFO][10:20:54]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_60.pth.
[INFO][10:20:54]: [93m[1m
[Server #1127936] Starting round 61/100.[0m
[0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.02486618 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00323057 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00274486 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00962571 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.02033102 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00455897 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00510642 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.015154   0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01388769 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00344865 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.002
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  4e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  6e-05  1e-09  1e-09
 6:  6.8876e+00  6.8875e+00  6e-05  6e-10  6e-10
 7:  6.8875e+00  6.8875e+00  5e-05  2e-09  9e-11
 8:  6.8875e+00  6.8875e+00  4e-05  2e-09  1e-10
 9:  6.8875e+00  6.8875e+00  2e-05  1e-08  8e-10
10:  6.8875e+00  6.8875e+00  5e-06  7e-09  4e-10
Optimal solution found.
The calculated probability is:  [1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 3.67142998e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91276850e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 2.13986213e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 9.06052460e-01 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 3.14648954e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91276467e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 2.12227033e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91268741e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 2.61362706e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91276796e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04 1.91277236e-04 1.91277236e-04
 1.91277236e-04 1.91277236e-04]
current clients pool:  [INFO][10:20:55]: [Server #1127936] Selected clients: [172 188 337  95 447 475 482 365 205 372]
[INFO][10:20:55]: [Server #1127936] Selecting client #172 for training.
[INFO][10:20:55]: [Server #1127936] Sending the current model to client #172 (simulated).
[INFO][10:20:55]: [Server #1127936] Sending 0.24 MB of payload data to client #172 (simulated).
[INFO][10:20:55]: [Server #1127936] Selecting client #188 for training.
[INFO][10:20:55]: [Server #1127936] Sending the current model to client #188 (simulated).
[INFO][10:20:55]: [Server #1127936] Sending 0.24 MB of payload data to client #188 (simulated).
[INFO][10:20:55]: [Server #1127936] Selecting client #337 for training.
[INFO][10:20:55]: [Server #1127936] Sending the current model to client #337 (simulated).
[INFO][10:20:55]: [Client #172] Selected by the server.
[INFO][10:20:55]: [Client #172] Loading its data source...
[INFO][10:20:55]: [Client #172] Dataset size: 60000
[INFO][10:20:55]: [Client #172] Sampler: noniid
[INFO][10:20:55]: [Server #1127936] Sending 0.24 MB of payload data to client #337 (simulated).
[INFO][10:20:55]: [Client #188] Selected by the server.
[INFO][10:20:55]: [Client #188] Loading its data source...
[INFO][10:20:55]: [Client #188] Dataset size: 60000
[INFO][10:20:55]: [Client #188] Sampler: noniid
[INFO][10:20:55]: [Client #172] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:20:55]: [Client #337] Selected by the server.
[INFO][10:20:55]: [Client #337] Loading its data source...
[INFO][10:20:55]: [Client #337] Dataset size: 60000
[INFO][10:20:55]: [Client #337] Sampler: noniid
[INFO][10:20:55]: [Client #188] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:20:55]: [Client #337] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:20:55]: [93m[1m[Client #172] Started training in communication round #61.[0m
[INFO][10:20:55]: [93m[1m[Client #188] Started training in communication round #61.[0m
[INFO][10:20:55]: [93m[1m[Client #337] Started training in communication round #61.[0m
[INFO][10:20:57]: [Client #172] Loading the dataset.
[INFO][10:20:57]: [Client #337] Loading the dataset.
[INFO][10:20:57]: [Client #188] Loading the dataset.
[INFO][10:21:03]: [Client #337] Epoch: [1/5][0/10]	Loss: 0.000788
[INFO][10:21:03]: [Client #172] Epoch: [1/5][0/10]	Loss: 0.000706
[INFO][10:21:03]: [Client #188] Epoch: [1/5][0/10]	Loss: 0.004003
[INFO][10:21:03]: [Client #337] Epoch: [2/5][0/10]	Loss: 0.000497
[INFO][10:21:03]: [Client #188] Epoch: [2/5][0/10]	Loss: 0.003355
[INFO][10:21:03]: [Client #172] Epoch: [2/5][0/10]	Loss: 0.000429
[INFO][10:21:03]: [Client #337] Epoch: [3/5][0/10]	Loss: 0.000363
[INFO][10:21:04]: [Client #172] Epoch: [3/5][0/10]	Loss: 0.000024
[INFO][10:21:04]: [Client #188] Epoch: [3/5][0/10]	Loss: 0.000147
[INFO][10:21:04]: [Client #337] Epoch: [4/5][0/10]	Loss: 0.000007
[INFO][10:21:04]: [Client #188] Epoch: [4/5][0/10]	Loss: 0.000052
[INFO][10:21:04]: [Client #172] Epoch: [4/5][0/10]	Loss: 0.000002
[INFO][10:21:04]: [Client #337] Epoch: [5/5][0/10]	Loss: 0.019667
[INFO][10:21:04]: [Client #188] Epoch: [5/5][0/10]	Loss: 0.002901
[INFO][10:21:04]: [Client #172] Epoch: [5/5][0/10]	Loss: 0.000003
[INFO][10:21:04]: [Client #337] Model saved to /data/ykang/plato/results/test/model/lenet5_337_1127979.pth.
[INFO][10:21:04]: [Client #188] Model saved to /data/ykang/plato/results/test/model/lenet5_188_1127978.pth.
[INFO][10:21:04]: [Client #172] Model saved to /data/ykang/plato/results/test/model/lenet5_172_1127977.pth.
[INFO][10:21:05]: [Client #337] Loading a model from /data/ykang/plato/results/test/model/lenet5_337_1127979.pth.
[INFO][10:21:05]: [Client #337] Model trained.
[INFO][10:21:05]: [Client #337] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:21:05]: [Server #1127936] Received 0.24 MB of payload data from client #337 (simulated).
[INFO][10:21:05]: [Client #188] Loading a model from /data/ykang/plato/results/test/model/lenet5_188_1127978.pth.
[INFO][10:21:05]: [Client #188] Model trained.
[INFO][10:21:05]: [Client #188] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:21:05]: [Server #1127936] Received 0.24 MB of payload data from client #188 (simulated).
[INFO][10:21:05]: [Client #172] Loading a model from /data/ykang/plato/results/test/model/lenet5_172_1127977.pth.
[INFO][10:21:05]: [Client #172] Model trained.
[INFO][10:21:05]: [Client #172] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:21:05]: [Server #1127936] Received 0.24 MB of payload data from client #172 (simulated).
[INFO][10:21:05]: [Server #1127936] Selecting client #95 for training.
[INFO][10:21:05]: [Server #1127936] Sending the current model to client #95 (simulated).
[INFO][10:21:05]: [Server #1127936] Sending 0.24 MB of payload data to client #95 (simulated).
[INFO][10:21:05]: [Server #1127936] Selecting client #447 for training.
[INFO][10:21:05]: [Server #1127936] Sending the current model to client #447 (simulated).
[INFO][10:21:05]: [Server #1127936] Sending 0.24 MB of payload data to client #447 (simulated).
[INFO][10:21:05]: [Server #1127936] Selecting client #475 for training.
[INFO][10:21:05]: [Server #1127936] Sending the current model to client #475 (simulated).
[INFO][10:21:05]: [Client #95] Selected by the server.
[INFO][10:21:05]: [Client #95] Loading its data source...
[INFO][10:21:05]: [Client #95] Dataset size: 60000
[INFO][10:21:05]: [Client #95] Sampler: noniid
[INFO][10:21:05]: [Server #1127936] Sending 0.24 MB of payload data to client #475 (simulated).
[INFO][10:21:05]: [Client #447] Selected by the server.
[INFO][10:21:05]: [Client #447] Loading its data source...
[INFO][10:21:05]: [Client #447] Dataset size: 60000
[INFO][10:21:05]: [Client #475] Selected by the server.
[INFO][10:21:05]: [Client #447] Sampler: noniid
[INFO][10:21:05]: [Client #475] Loading its data source...
[INFO][10:21:05]: [Client #475] Dataset size: 60000
[INFO][10:21:05]: [Client #475] Sampler: noniid
[INFO][10:21:05]: [Client #95] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:21:05]: [Client #475] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:21:05]: [Client #447] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:21:05]: [93m[1m[Client #95] Started training in communication round #61.[0m
[INFO][10:21:05]: [93m[1m[Client #475] Started training in communication round #61.[0m
[INFO][10:21:05]: [93m[1m[Client #447] Started training in communication round #61.[0m
[INFO][10:21:07]: [Client #447] Loading the dataset.
[INFO][10:21:07]: [Client #95] Loading the dataset.
[INFO][10:21:07]: [Client #475] Loading the dataset.
[INFO][10:21:13]: [Client #447] Epoch: [1/5][0/10]	Loss: 0.024473
[INFO][10:21:13]: [Client #447] Epoch: [2/5][0/10]	Loss: 0.001620
[INFO][10:21:13]: [Client #475] Epoch: [1/5][0/10]	Loss: 0.002621
[INFO][10:21:13]: [Client #95] Epoch: [1/5][0/10]	Loss: 0.007330
[INFO][10:21:13]: [Client #447] Epoch: [3/5][0/10]	Loss: 0.000035
[INFO][10:21:13]: [Client #475] Epoch: [2/5][0/10]	Loss: 0.000246
[INFO][10:21:13]: [Client #95] Epoch: [2/5][0/10]	Loss: 0.000278
[INFO][10:21:13]: [Client #447] Epoch: [4/5][0/10]	Loss: 0.004289
[INFO][10:21:13]: [Client #475] Epoch: [3/5][0/10]	Loss: 0.000000
[INFO][10:21:13]: [Client #95] Epoch: [3/5][0/10]	Loss: 0.000545
[INFO][10:21:13]: [Client #447] Epoch: [5/5][0/10]	Loss: 0.021281
[INFO][10:21:13]: [Client #475] Epoch: [4/5][0/10]	Loss: 0.000002
[INFO][10:21:13]: [Client #95] Epoch: [4/5][0/10]	Loss: 0.002166
[INFO][10:21:14]: [Client #447] Model saved to /data/ykang/plato/results/test/model/lenet5_447_1127978.pth.
[INFO][10:21:14]: [Client #475] Epoch: [5/5][0/10]	Loss: 0.000014
[INFO][10:21:14]: [Client #95] Epoch: [5/5][0/10]	Loss: 0.147233
[INFO][10:21:14]: [Client #475] Model saved to /data/ykang/plato/results/test/model/lenet5_475_1127979.pth.
[INFO][10:21:14]: [Client #95] Model saved to /data/ykang/plato/results/test/model/lenet5_95_1127977.pth.
[INFO][10:21:14]: [Client #447] Loading a model from /data/ykang/plato/results/test/model/lenet5_447_1127978.pth.
[INFO][10:21:14]: [Client #447] Model trained.
[INFO][10:21:14]: [Client #447] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:21:14]: [Server #1127936] Received 0.24 MB of payload data from client #447 (simulated).
[INFO][10:21:14]: [Client #475] Loading a model from /data/ykang/plato/results/test/model/lenet5_475_1127979.pth.
[INFO][10:21:14]: [Client #475] Model trained.
[INFO][10:21:14]: [Client #475] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:21:14]: [Server #1127936] Received 0.24 MB of payload data from client #475 (simulated).
[INFO][10:21:14]: [Client #95] Loading a model from /data/ykang/plato/results/test/model/lenet5_95_1127977.pth.
[INFO][10:21:14]: [Client #95] Model trained.
[INFO][10:21:14]: [Client #95] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:21:15]: [Server #1127936] Received 0.24 MB of payload data from client #95 (simulated).
[INFO][10:21:15]: [Server #1127936] Selecting client #482 for training.
[INFO][10:21:15]: [Server #1127936] Sending the current model to client #482 (simulated).
[INFO][10:21:15]: [Server #1127936] Sending 0.24 MB of payload data to client #482 (simulated).
[INFO][10:21:15]: [Server #1127936] Selecting client #365 for training.
[INFO][10:21:15]: [Server #1127936] Sending the current model to client #365 (simulated).
[INFO][10:21:15]: [Server #1127936] Sending 0.24 MB of payload data to client #365 (simulated).
[INFO][10:21:15]: [Server #1127936] Selecting client #205 for training.
[INFO][10:21:15]: [Server #1127936] Sending the current model to client #205 (simulated).
[INFO][10:21:15]: [Client #482] Selected by the server.
[INFO][10:21:15]: [Client #482] Loading its data source...
[INFO][10:21:15]: [Client #482] Dataset size: 60000
[INFO][10:21:15]: [Client #482] Sampler: noniid
[INFO][10:21:15]: [Server #1127936] Sending 0.24 MB of payload data to client #205 (simulated).
[INFO][10:21:15]: [Client #365] Selected by the server.
[INFO][10:21:15]: [Client #365] Loading its data source...
[INFO][10:21:15]: [Client #365] Dataset size: 60000
[INFO][10:21:15]: [Client #365] Sampler: noniid
[INFO][10:21:15]: [Client #205] Selected by the server.
[INFO][10:21:15]: [Client #205] Loading its data source...
[INFO][10:21:15]: [Client #205] Dataset size: 60000
[INFO][10:21:15]: [Client #205] Sampler: noniid
[INFO][10:21:15]: [Client #482] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:21:15]: [Client #365] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:21:15]: [93m[1m[Client #482] Started training in communication round #61.[0m
[INFO][10:21:15]: [93m[1m[Client #365] Started training in communication round #61.[0m
[INFO][10:21:15]: [Client #205] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:21:15]: [93m[1m[Client #205] Started training in communication round #61.[0m
[INFO][10:21:17]: [Client #482] Loading the dataset.
[INFO][10:21:17]: [Client #205] Loading the dataset.
[INFO][10:21:17]: [Client #365] Loading the dataset.
[INFO][10:21:23]: [Client #482] Epoch: [1/5][0/10]	Loss: 0.004375
[INFO][10:21:23]: [Client #205] Epoch: [1/5][0/10]	Loss: 0.014009
[INFO][10:21:23]: [Client #365] Epoch: [1/5][0/10]	Loss: 0.007385
[INFO][10:21:23]: [Client #482] Epoch: [2/5][0/10]	Loss: 0.002454
[INFO][10:21:23]: [Client #205] Epoch: [2/5][0/10]	Loss: 0.000131
[INFO][10:21:23]: [Client #482] Epoch: [3/5][0/10]	Loss: 0.000433
[INFO][10:21:23]: [Client #365] Epoch: [2/5][0/10]	Loss: 0.002216
[INFO][10:21:23]: [Client #482] Epoch: [4/5][0/10]	Loss: 0.000256
[INFO][10:21:23]: [Client #205] Epoch: [3/5][0/10]	Loss: 0.000190
[INFO][10:21:23]: [Client #365] Epoch: [3/5][0/10]	Loss: 0.000052
[INFO][10:21:23]: [Client #205] Epoch: [4/5][0/10]	Loss: 0.000010
[INFO][10:21:23]: [Client #482] Epoch: [5/5][0/10]	Loss: 0.000012
[INFO][10:21:23]: [Client #482] Model saved to /data/ykang/plato/results/test/model/lenet5_482_1127977.pth.
[INFO][10:21:23]: [Client #365] Epoch: [4/5][0/10]	Loss: 0.000336
[INFO][10:21:23]: [Client #205] Epoch: [5/5][0/10]	Loss: 0.002474
[INFO][10:21:23]: [Client #205] Model saved to /data/ykang/plato/results/test/model/lenet5_205_1127979.pth.
[INFO][10:21:23]: [Client #365] Epoch: [5/5][0/10]	Loss: 0.027013
[INFO][10:21:23]: [Client #365] Model saved to /data/ykang/plato/results/test/model/lenet5_365_1127978.pth.
[INFO][10:21:24]: [Client #482] Loading a model from /data/ykang/plato/results/test/model/lenet5_482_1127977.pth.
[INFO][10:21:24]: [Client #482] Model trained.
[INFO][10:21:24]: [Client #482] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:21:24]: [Server #1127936] Received 0.24 MB of payload data from client #482 (simulated).
[INFO][10:21:24]: [Client #205] Loading a model from /data/ykang/plato/results/test/model/lenet5_205_1127979.pth.
[INFO][10:21:24]: [Client #205] Model trained.
[INFO][10:21:24]: [Client #205] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:21:24]: [Server #1127936] Received 0.24 MB of payload data from client #205 (simulated).
[INFO][10:21:24]: [Client #365] Loading a model from /data/ykang/plato/results/test/model/lenet5_365_1127978.pth.
[INFO][10:21:24]: [Client #365] Model trained.
[INFO][10:21:24]: [Client #365] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:21:24]: [Server #1127936] Received 0.24 MB of payload data from client #365 (simulated).
[INFO][10:21:24]: [Server #1127936] Selecting client #372 for training.
[INFO][10:21:24]: [Server #1127936] Sending the current model to client #372 (simulated).
[INFO][10:21:24]: [Server #1127936] Sending 0.24 MB of payload data to client #372 (simulated).
[INFO][10:21:24]: [Client #372] Selected by the server.
[INFO][10:21:24]: [Client #372] Loading its data source...
[INFO][10:21:24]: [Client #372] Dataset size: 60000
[INFO][10:21:24]: [Client #372] Sampler: noniid
[INFO][10:21:24]: [Client #372] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:21:24]: [93m[1m[Client #372] Started training in communication round #61.[0m
[INFO][10:21:26]: [Client #372] Loading the dataset.
[INFO][10:21:32]: [Client #372] Epoch: [1/5][0/10]	Loss: 0.046645
[INFO][10:21:32]: [Client #372] Epoch: [2/5][0/10]	Loss: 0.004180
[INFO][10:21:32]: [Client #372] Epoch: [3/5][0/10]	Loss: 0.000212
[INFO][10:21:32]: [Client #372] Epoch: [4/5][0/10]	Loss: 0.000039
[INFO][10:21:32]: [Client #372] Epoch: [5/5][0/10]	Loss: 0.000770
[INFO][10:21:32]: [Client #372] Model saved to /data/ykang/plato/results/test/model/lenet5_372_1127977.pth.
[INFO][10:21:33]: [Client #372] Loading a model from /data/ykang/plato/results/test/model/lenet5_372_1127977.pth.
[INFO][10:21:33]: [Client #372] Model trained.
[INFO][10:21:33]: [Client #372] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:21:33]: [Server #1127936] Received 0.24 MB of payload data from client #372 (simulated).
[INFO][10:21:33]: [Server #1127936] Adding client #467 to the list of clients for aggregation.
[INFO][10:21:33]: [Server #1127936] Adding client #44 to the list of clients for aggregation.
[INFO][10:21:33]: [Server #1127936] Adding client #137 to the list of clients for aggregation.
[INFO][10:21:33]: [Server #1127936] Adding client #262 to the list of clients for aggregation.
[INFO][10:21:33]: [Server #1127936] Adding client #281 to the list of clients for aggregation.
[INFO][10:21:33]: [Server #1127936] Adding client #406 to the list of clients for aggregation.
[INFO][10:21:33]: [Server #1127936] Adding client #478 to the list of clients for aggregation.
[INFO][10:21:33]: [Server #1127936] Adding client #372 to the list of clients for aggregation.
[INFO][10:21:33]: [Server #1127936] Adding client #337 to the list of clients for aggregation.
[INFO][10:21:33]: [Server #1127936] Adding client #205 to the list of clients for aggregation.
[INFO][10:21:33]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01663864 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00511691 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01634834 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00881682 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00628865 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00885503 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00737649
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00293226 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00779077 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00591211 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 0. 1. 0. 0. 0. 1. 0. 1. 3. 1. 0. 0. 0. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 2. 0. 1. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 2. 1. 0. 1. 0.
 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 2. 1. 6. 0. 1. 2. 1. 0. 1. 1. 2. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 0. 6. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 2. 0. 0.
 1. 0. 1. 2. 0. 0. 0. 2. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 4. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0.
 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 0. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 3. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 0. 0. 0. 0. 0. 0. 6. 1. 1.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 6. 1. 0. 0. 1. 0. 0. 1. 2. 0. 1. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 2. 2. 1. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01663864 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00511691 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01634834 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00881682 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00628865 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00885503 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00737649
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00293226 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00779077 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00591211 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:21:35]: [Server #1127936] Global model accuracy: 95.65%

[INFO][10:21:35]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_61.pth.
[INFO][10:21:35]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_61.pth.
[INFO][10:21:35]: [93m[1m
[Server #1127936] Starting round 62/100.[0m
[0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  4e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  5e-05  9e-10  9e-10
 6:  6.8876e+00  6.8875e+00  5e-05  5e-10  5e-10
 7:  6.8875e+00  6.8875e+00  4e-05  1e-09  6e-11
 8:  6.8875e+00  6.8875e+00  3e-05  1e-09  6e-11
 9:  6.8875e+00  6.8875e+00  1e-05  8e-09  4e-10
10:  6.8875e+00  6.8875e+00  6e-07  4e-09  2e-10
Optimal solution found.
The calculated probability is:  [2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 5.04919663e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 3.15681758e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69123866e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 3.59920224e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 3.28540506e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69138632e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69140508e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 3.23776937e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 9.86784597e-01 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 3.24302384e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05 2.69144763e-05 2.69144763e-05
 2.69144763e-05 2.69144763e-05]
current clients pool:  [INFO][10:21:36]: [Server #1127936] Selected clients: [467  39 328 420  92 426 184 444  14  44]
[INFO][10:21:36]: [Server #1127936] Selecting client #467 for training.
[INFO][10:21:36]: [Server #1127936] Sending the current model to client #467 (simulated).
[INFO][10:21:36]: [Server #1127936] Sending 0.24 MB of payload data to client #467 (simulated).
[INFO][10:21:36]: [Server #1127936] Selecting client #39 for training.
[INFO][10:21:36]: [Server #1127936] Sending the current model to client #39 (simulated).
[INFO][10:21:36]: [Server #1127936] Sending 0.24 MB of payload data to client #39 (simulated).
[INFO][10:21:36]: [Server #1127936] Selecting client #328 for training.
[INFO][10:21:36]: [Server #1127936] Sending the current model to client #328 (simulated).
[INFO][10:21:36]: [Client #467] Selected by the server.
[INFO][10:21:36]: [Client #467] Loading its data source...
[INFO][10:21:36]: [Client #467] Dataset size: 60000
[INFO][10:21:36]: [Client #467] Sampler: noniid
[INFO][10:21:36]: [Server #1127936] Sending 0.24 MB of payload data to client #328 (simulated).
[INFO][10:21:36]: [Client #39] Selected by the server.
[INFO][10:21:36]: [Client #39] Loading its data source...
[INFO][10:21:36]: [Client #39] Dataset size: 60000
[INFO][10:21:36]: [Client #39] Sampler: noniid
[INFO][10:21:36]: [Client #328] Selected by the server.
[INFO][10:21:36]: [Client #467] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:21:36]: [Client #328] Loading its data source...
[INFO][10:21:36]: [Client #328] Dataset size: 60000
[INFO][10:21:36]: [Client #328] Sampler: noniid
[INFO][10:21:36]: [Client #39] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:21:36]: [Client #328] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:21:36]: [93m[1m[Client #39] Started training in communication round #62.[0m
[INFO][10:21:36]: [93m[1m[Client #328] Started training in communication round #62.[0m
[INFO][10:21:36]: [93m[1m[Client #467] Started training in communication round #62.[0m
[INFO][10:21:38]: [Client #39] Loading the dataset.
[INFO][10:21:38]: [Client #467] Loading the dataset.
[INFO][10:21:38]: [Client #328] Loading the dataset.
[INFO][10:21:44]: [Client #39] Epoch: [1/5][0/10]	Loss: 0.000576
[INFO][10:21:44]: [Client #328] Epoch: [1/5][0/10]	Loss: 0.043843
[INFO][10:21:44]: [Client #467] Epoch: [1/5][0/10]	Loss: 0.007366
[INFO][10:21:44]: [Client #39] Epoch: [2/5][0/10]	Loss: 0.002045
[INFO][10:21:44]: [Client #328] Epoch: [2/5][0/10]	Loss: 0.003443
[INFO][10:21:44]: [Client #467] Epoch: [2/5][0/10]	Loss: 0.000127
[INFO][10:21:44]: [Client #328] Epoch: [3/5][0/10]	Loss: 0.000381
[INFO][10:21:44]: [Client #467] Epoch: [3/5][0/10]	Loss: 0.000217
[INFO][10:21:44]: [Client #39] Epoch: [3/5][0/10]	Loss: 0.000747
[INFO][10:21:44]: [Client #328] Epoch: [4/5][0/10]	Loss: 0.000113
[INFO][10:21:44]: [Client #467] Epoch: [4/5][0/10]	Loss: 0.000041
[INFO][10:21:44]: [Client #328] Epoch: [5/5][0/10]	Loss: 0.000803
[INFO][10:21:44]: [Client #39] Epoch: [4/5][0/10]	Loss: 0.000151
[INFO][10:21:44]: [Client #328] Model saved to /data/ykang/plato/results/test/model/lenet5_328_1127979.pth.
[INFO][10:21:44]: [Client #467] Epoch: [5/5][0/10]	Loss: 0.000342
[INFO][10:21:44]: [Client #39] Epoch: [5/5][0/10]	Loss: 0.018671
[INFO][10:21:44]: [Client #467] Model saved to /data/ykang/plato/results/test/model/lenet5_467_1127977.pth.
[INFO][10:21:44]: [Client #39] Model saved to /data/ykang/plato/results/test/model/lenet5_39_1127978.pth.
[INFO][10:21:45]: [Client #328] Loading a model from /data/ykang/plato/results/test/model/lenet5_328_1127979.pth.
[INFO][10:21:45]: [Client #328] Model trained.
[INFO][10:21:45]: [Client #328] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:21:45]: [Server #1127936] Received 0.24 MB of payload data from client #328 (simulated).
[INFO][10:21:45]: [Client #467] Loading a model from /data/ykang/plato/results/test/model/lenet5_467_1127977.pth.
[INFO][10:21:45]: [Client #467] Model trained.
[INFO][10:21:45]: [Client #467] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:21:45]: [Server #1127936] Received 0.24 MB of payload data from client #467 (simulated).
[INFO][10:21:45]: [Client #39] Loading a model from /data/ykang/plato/results/test/model/lenet5_39_1127978.pth.
[INFO][10:21:45]: [Client #39] Model trained.
[INFO][10:21:45]: [Client #39] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:21:45]: [Server #1127936] Received 0.24 MB of payload data from client #39 (simulated).
[INFO][10:21:45]: [Server #1127936] Selecting client #420 for training.
[INFO][10:21:45]: [Server #1127936] Sending the current model to client #420 (simulated).
[INFO][10:21:45]: [Server #1127936] Sending 0.24 MB of payload data to client #420 (simulated).
[INFO][10:21:45]: [Server #1127936] Selecting client #92 for training.
[INFO][10:21:45]: [Server #1127936] Sending the current model to client #92 (simulated).
[INFO][10:21:45]: [Server #1127936] Sending 0.24 MB of payload data to client #92 (simulated).
[INFO][10:21:45]: [Server #1127936] Selecting client #426 for training.
[INFO][10:21:45]: [Server #1127936] Sending the current model to client #426 (simulated).
[INFO][10:21:45]: [Client #420] Selected by the server.
[INFO][10:21:45]: [Client #420] Loading its data source...
[INFO][10:21:45]: [Client #420] Dataset size: 60000
[INFO][10:21:45]: [Client #420] Sampler: noniid
[INFO][10:21:45]: [Server #1127936] Sending 0.24 MB of payload data to client #426 (simulated).
[INFO][10:21:45]: [Client #92] Selected by the server.
[INFO][10:21:45]: [Client #92] Loading its data source...
[INFO][10:21:45]: [Client #92] Dataset size: 60000
[INFO][10:21:45]: [Client #92] Sampler: noniid
[INFO][10:21:45]: [Client #426] Selected by the server.
[INFO][10:21:45]: [Client #426] Loading its data source...
[INFO][10:21:45]: [Client #426] Dataset size: 60000
[INFO][10:21:45]: [Client #426] Sampler: noniid
[INFO][10:21:45]: [Client #420] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:21:45]: [Client #92] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:21:45]: [Client #426] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:21:45]: [93m[1m[Client #426] Started training in communication round #62.[0m
[INFO][10:21:45]: [93m[1m[Client #92] Started training in communication round #62.[0m
[INFO][10:21:45]: [93m[1m[Client #420] Started training in communication round #62.[0m
[INFO][10:21:47]: [Client #426] Loading the dataset.
[INFO][10:21:48]: [Client #92] Loading the dataset.
[INFO][10:21:48]: [Client #420] Loading the dataset.
[INFO][10:21:54]: [Client #420] Epoch: [1/5][0/10]	Loss: 0.009184
[INFO][10:21:54]: [Client #426] Epoch: [1/5][0/10]	Loss: 0.001317
[INFO][10:21:54]: [Client #92] Epoch: [1/5][0/10]	Loss: 0.001965
[INFO][10:21:54]: [Client #420] Epoch: [2/5][0/10]	Loss: 0.001693
[INFO][10:21:54]: [Client #426] Epoch: [2/5][0/10]	Loss: 0.000948
[INFO][10:21:54]: [Client #92] Epoch: [2/5][0/10]	Loss: 0.005888
[INFO][10:21:54]: [Client #420] Epoch: [3/5][0/10]	Loss: 0.000212
[INFO][10:21:54]: [Client #426] Epoch: [3/5][0/10]	Loss: 0.000381
[INFO][10:21:54]: [Client #426] Epoch: [4/5][0/10]	Loss: 0.002714
[INFO][10:21:54]: [Client #420] Epoch: [4/5][0/10]	Loss: 0.000541
[INFO][10:21:54]: [Client #92] Epoch: [3/5][0/10]	Loss: 0.000099
[INFO][10:21:54]: [Client #426] Epoch: [5/5][0/10]	Loss: 0.014422
[INFO][10:21:54]: [Client #426] Model saved to /data/ykang/plato/results/test/model/lenet5_426_1127979.pth.
[INFO][10:21:54]: [Client #420] Epoch: [5/5][0/10]	Loss: 0.000459
[INFO][10:21:54]: [Client #92] Epoch: [4/5][0/10]	Loss: 0.000177
[INFO][10:21:54]: [Client #420] Model saved to /data/ykang/plato/results/test/model/lenet5_420_1127977.pth.
[INFO][10:21:54]: [Client #92] Epoch: [5/5][0/10]	Loss: 0.000729
[INFO][10:21:54]: [Client #92] Model saved to /data/ykang/plato/results/test/model/lenet5_92_1127978.pth.
[INFO][10:21:55]: [Client #426] Loading a model from /data/ykang/plato/results/test/model/lenet5_426_1127979.pth.
[INFO][10:21:55]: [Client #426] Model trained.
[INFO][10:21:55]: [Client #426] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:21:55]: [Server #1127936] Received 0.24 MB of payload data from client #426 (simulated).
[INFO][10:21:55]: [Client #420] Loading a model from /data/ykang/plato/results/test/model/lenet5_420_1127977.pth.
[INFO][10:21:55]: [Client #420] Model trained.
[INFO][10:21:55]: [Client #420] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:21:55]: [Server #1127936] Received 0.24 MB of payload data from client #420 (simulated).
[INFO][10:21:55]: [Client #92] Loading a model from /data/ykang/plato/results/test/model/lenet5_92_1127978.pth.
[INFO][10:21:55]: [Client #92] Model trained.
[INFO][10:21:55]: [Client #92] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:21:55]: [Server #1127936] Received 0.24 MB of payload data from client #92 (simulated).
[INFO][10:21:55]: [Server #1127936] Selecting client #184 for training.
[INFO][10:21:55]: [Server #1127936] Sending the current model to client #184 (simulated).
[INFO][10:21:55]: [Server #1127936] Sending 0.24 MB of payload data to client #184 (simulated).
[INFO][10:21:55]: [Server #1127936] Selecting client #444 for training.
[INFO][10:21:55]: [Server #1127936] Sending the current model to client #444 (simulated).
[INFO][10:21:55]: [Server #1127936] Sending 0.24 MB of payload data to client #444 (simulated).
[INFO][10:21:55]: [Server #1127936] Selecting client #14 for training.
[INFO][10:21:55]: [Server #1127936] Sending the current model to client #14 (simulated).
[INFO][10:21:55]: [Client #184] Selected by the server.
[INFO][10:21:55]: [Client #184] Loading its data source...
[INFO][10:21:55]: [Client #184] Dataset size: 60000
[INFO][10:21:55]: [Client #184] Sampler: noniid
[INFO][10:21:55]: [Server #1127936] Sending 0.24 MB of payload data to client #14 (simulated).
[INFO][10:21:55]: [Client #444] Selected by the server.
[INFO][10:21:55]: [Client #444] Loading its data source...
[INFO][10:21:55]: [Client #444] Dataset size: 60000
[INFO][10:21:55]: [Client #444] Sampler: noniid
[INFO][10:21:55]: [Client #14] Selected by the server.
[INFO][10:21:55]: [Client #14] Loading its data source...
[INFO][10:21:55]: [Client #14] Dataset size: 60000
[INFO][10:21:55]: [Client #14] Sampler: noniid
[INFO][10:21:55]: [Client #184] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:21:55]: [Client #14] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:21:55]: [Client #444] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:21:55]: [93m[1m[Client #14] Started training in communication round #62.[0m
[INFO][10:21:55]: [93m[1m[Client #184] Started training in communication round #62.[0m
[INFO][10:21:55]: [93m[1m[Client #444] Started training in communication round #62.[0m
[INFO][10:21:57]: [Client #444] Loading the dataset.
[INFO][10:21:57]: [Client #14] Loading the dataset.
[INFO][10:21:57]: [Client #184] Loading the dataset.
[INFO][10:22:03]: [Client #444] Epoch: [1/5][0/10]	Loss: 0.094531
[INFO][10:22:03]: [Client #14] Epoch: [1/5][0/10]	Loss: 0.005446
[INFO][10:22:03]: [Client #444] Epoch: [2/5][0/10]	Loss: 0.003036
[INFO][10:22:03]: [Client #14] Epoch: [2/5][0/10]	Loss: 0.000078
[INFO][10:22:03]: [Client #184] Epoch: [1/5][0/10]	Loss: 0.005125
[INFO][10:22:03]: [Client #444] Epoch: [3/5][0/10]	Loss: 0.000233
[INFO][10:22:04]: [Client #184] Epoch: [2/5][0/10]	Loss: 0.002639
[INFO][10:22:04]: [Client #14] Epoch: [3/5][0/10]	Loss: 0.002225
[INFO][10:22:04]: [Client #444] Epoch: [4/5][0/10]	Loss: 0.000252
[INFO][10:22:04]: [Client #184] Epoch: [3/5][0/10]	Loss: 0.000037
[INFO][10:22:04]: [Client #14] Epoch: [4/5][0/10]	Loss: 0.000181
[INFO][10:22:04]: [Client #444] Epoch: [5/5][0/10]	Loss: 0.000646
[INFO][10:22:04]: [Client #14] Epoch: [5/5][0/10]	Loss: 0.000078
[INFO][10:22:04]: [Client #184] Epoch: [4/5][0/10]	Loss: 0.002094
[INFO][10:22:04]: [Client #444] Model saved to /data/ykang/plato/results/test/model/lenet5_444_1127978.pth.
[INFO][10:22:04]: [Client #14] Model saved to /data/ykang/plato/results/test/model/lenet5_14_1127979.pth.
[INFO][10:22:04]: [Client #184] Epoch: [5/5][0/10]	Loss: 0.000058
[INFO][10:22:04]: [Client #184] Model saved to /data/ykang/plato/results/test/model/lenet5_184_1127977.pth.
[INFO][10:22:05]: [Client #444] Loading a model from /data/ykang/plato/results/test/model/lenet5_444_1127978.pth.
[INFO][10:22:05]: [Client #444] Model trained.
[INFO][10:22:05]: [Client #444] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:22:05]: [Server #1127936] Received 0.24 MB of payload data from client #444 (simulated).
[INFO][10:22:05]: [Client #14] Loading a model from /data/ykang/plato/results/test/model/lenet5_14_1127979.pth.
[INFO][10:22:05]: [Client #14] Model trained.
[INFO][10:22:05]: [Client #14] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:22:05]: [Server #1127936] Received 0.24 MB of payload data from client #14 (simulated).
[INFO][10:22:05]: [Client #184] Loading a model from /data/ykang/plato/results/test/model/lenet5_184_1127977.pth.
[INFO][10:22:05]: [Client #184] Model trained.
[INFO][10:22:05]: [Client #184] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:22:05]: [Server #1127936] Received 0.24 MB of payload data from client #184 (simulated).
[INFO][10:22:05]: [Server #1127936] Selecting client #44 for training.
[INFO][10:22:05]: [Server #1127936] Sending the current model to client #44 (simulated).
[INFO][10:22:05]: [Server #1127936] Sending 0.24 MB of payload data to client #44 (simulated).
[INFO][10:22:05]: [Client #44] Selected by the server.
[INFO][10:22:05]: [Client #44] Loading its data source...
[INFO][10:22:05]: [Client #44] Dataset size: 60000
[INFO][10:22:05]: [Client #44] Sampler: noniid
[INFO][10:22:05]: [Client #44] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:22:05]: [93m[1m[Client #44] Started training in communication round #62.[0m
[INFO][10:22:07]: [Client #44] Loading the dataset.
[INFO][10:22:12]: [Client #44] Epoch: [1/5][0/10]	Loss: 0.000186
[INFO][10:22:12]: [Client #44] Epoch: [2/5][0/10]	Loss: 0.001306
[INFO][10:22:13]: [Client #44] Epoch: [3/5][0/10]	Loss: 0.000229
[INFO][10:22:13]: [Client #44] Epoch: [4/5][0/10]	Loss: 0.000102
[INFO][10:22:13]: [Client #44] Epoch: [5/5][0/10]	Loss: 0.000007
[INFO][10:22:13]: [Client #44] Model saved to /data/ykang/plato/results/test/model/lenet5_44_1127977.pth.
[INFO][10:22:13]: [Client #44] Loading a model from /data/ykang/plato/results/test/model/lenet5_44_1127977.pth.
[INFO][10:22:13]: [Client #44] Model trained.
[INFO][10:22:13]: [Client #44] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:22:13]: [Server #1127936] Received 0.24 MB of payload data from client #44 (simulated).
[INFO][10:22:13]: [Server #1127936] Adding client #95 to the list of clients for aggregation.
[INFO][10:22:13]: [Server #1127936] Adding client #118 to the list of clients for aggregation.
[INFO][10:22:13]: [Server #1127936] Adding client #188 to the list of clients for aggregation.
[INFO][10:22:13]: [Server #1127936] Adding client #447 to the list of clients for aggregation.
[INFO][10:22:13]: [Server #1127936] Adding client #482 to the list of clients for aggregation.
[INFO][10:22:13]: [Server #1127936] Adding client #444 to the list of clients for aggregation.
[INFO][10:22:13]: [Server #1127936] Adding client #39 to the list of clients for aggregation.
[INFO][10:22:13]: [Server #1127936] Adding client #184 to the list of clients for aggregation.
[INFO][10:22:13]: [Server #1127936] Adding client #426 to the list of clients for aggregation.
[INFO][10:22:13]: [Server #1127936] Adding client #328 to the list of clients for aggregation.
[INFO][10:22:13]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 476, 477, 478, 479, 480, 481, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01770324 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01195562 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00369169 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01246483 0.         0.
 0.         0.0033257  0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00330732 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00157379
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00499998
 0.         0.         0.02519497 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00357353 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 0. 1. 0. 0. 0. 1. 0. 1. 3. 1. 0. 0. 0. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 2. 0. 1. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 2. 1. 0. 1. 0.
 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 2. 1. 6. 0. 1. 2. 1. 0. 1. 1. 2. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 0. 6. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 2. 0. 0.
 1. 0. 1. 2. 0. 0. 0. 2. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 4. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0.
 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 0. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 3. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 0. 0. 0. 0. 0. 0. 6. 1. 1.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 6. 1. 0. 0. 1. 0. 0. 1. 2. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 2. 2. 1. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01770324 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01195562 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00369169 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01246483 0.         0.
 0.         0.0033257  0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00330732 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00157379
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00499998
 0.         0.         0.02519497 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00357353 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:22:16]: [Server #1127936] Global model accuracy: 96.24%

[INFO][10:22:16]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_62.pth.
[INFO][10:22:16]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_62.pth.
[INFO][10:22:16]: [93m[1m
[Server #1127936] Starting round 63/100.[0m
[0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  3e-05  5e-10  5e-10
 6:  6.8876e+00  6.8875e+00  3e-05  3e-10  3e-10
 7:  6.8875e+00  6.8875e+00  2e-05  7e-10  2e-11
 8:  6.8875e+00  6.8875e+00  2e-05  6e-10  2e-11
 9:  6.8875e+00  6.8875e+00  1e-05  1e-09  3e-11
10:  6.8875e+00  6.8875e+00  7e-07  2e-09  5e-11
Optimal solution found.
The calculated probability is:  [5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81406850e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 1.26071521e-04 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 8.74129007e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81448721e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 6.85139422e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81486995e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489237e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81483268e-05
 5.81489894e-05 5.81489894e-05 9.71446321e-01 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 6.94340368e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05 5.81489894e-05 5.81489894e-05
 5.81489894e-05 5.81489894e-05]
current clients pool:  [INFO][10:22:16]: [Server #1127936] Selected clients: [447 422 463 329 289 200 484  79 446 157]
[INFO][10:22:16]: [Server #1127936] Selecting client #447 for training.
[INFO][10:22:16]: [Server #1127936] Sending the current model to client #447 (simulated).
[INFO][10:22:16]: [Server #1127936] Sending 0.24 MB of payload data to client #447 (simulated).
[INFO][10:22:16]: [Server #1127936] Selecting client #422 for training.
[INFO][10:22:16]: [Server #1127936] Sending the current model to client #422 (simulated).
[INFO][10:22:16]: [Server #1127936] Sending 0.24 MB of payload data to client #422 (simulated).
[INFO][10:22:16]: [Server #1127936] Selecting client #463 for training.
[INFO][10:22:16]: [Server #1127936] Sending the current model to client #463 (simulated).
[INFO][10:22:16]: [Client #447] Selected by the server.
[INFO][10:22:16]: [Client #447] Loading its data source...
[INFO][10:22:16]: [Client #447] Dataset size: 60000
[INFO][10:22:16]: [Client #447] Sampler: noniid
[INFO][10:22:16]: [Server #1127936] Sending 0.24 MB of payload data to client #463 (simulated).
[INFO][10:22:16]: [Client #422] Selected by the server.
[INFO][10:22:16]: [Client #422] Loading its data source...
[INFO][10:22:16]: [Client #422] Dataset size: 60000
[INFO][10:22:16]: [Client #422] Sampler: noniid
[INFO][10:22:16]: [Client #447] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:22:16]: [Client #422] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:22:16]: [93m[1m[Client #447] Started training in communication round #63.[0m
[INFO][10:22:16]: [Client #463] Selected by the server.
[INFO][10:22:16]: [Client #463] Loading its data source...
[INFO][10:22:16]: [Client #463] Dataset size: 60000
[INFO][10:22:16]: [Client #463] Sampler: noniid
[INFO][10:22:16]: [Client #463] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:22:16]: [93m[1m[Client #422] Started training in communication round #63.[0m
[INFO][10:22:16]: [93m[1m[Client #463] Started training in communication round #63.[0m
[INFO][10:22:18]: [Client #447] Loading the dataset.
[INFO][10:22:18]: [Client #422] Loading the dataset.
[INFO][10:22:18]: [Client #463] Loading the dataset.
[INFO][10:22:24]: [Client #463] Epoch: [1/5][0/10]	Loss: 0.001851
[INFO][10:22:24]: [Client #447] Epoch: [1/5][0/10]	Loss: 0.005120
[INFO][10:22:25]: [Client #422] Epoch: [1/5][0/10]	Loss: 0.002925
[INFO][10:22:25]: [Client #447] Epoch: [2/5][0/10]	Loss: 0.002208
[INFO][10:22:25]: [Client #463] Epoch: [2/5][0/10]	Loss: 0.006690
[INFO][10:22:25]: [Client #422] Epoch: [2/5][0/10]	Loss: 0.000391
[INFO][10:22:25]: [Client #447] Epoch: [3/5][0/10]	Loss: 0.000042
[INFO][10:22:25]: [Client #463] Epoch: [3/5][0/10]	Loss: 0.000312
[INFO][10:22:25]: [Client #463] Epoch: [4/5][0/10]	Loss: 0.000264
[INFO][10:22:25]: [Client #422] Epoch: [3/5][0/10]	Loss: 0.000253
[INFO][10:22:25]: [Client #447] Epoch: [4/5][0/10]	Loss: 0.001375
[INFO][10:22:25]: [Client #422] Epoch: [4/5][0/10]	Loss: 0.001142
[INFO][10:22:25]: [Client #463] Epoch: [5/5][0/10]	Loss: 0.007076
[INFO][10:22:25]: [Client #447] Epoch: [5/5][0/10]	Loss: 0.053363
[INFO][10:22:25]: [Client #463] Model saved to /data/ykang/plato/results/test/model/lenet5_463_1127979.pth.
[INFO][10:22:25]: [Client #422] Epoch: [5/5][0/10]	Loss: 0.005119
[INFO][10:22:25]: [Client #447] Model saved to /data/ykang/plato/results/test/model/lenet5_447_1127977.pth.
[INFO][10:22:25]: [Client #422] Model saved to /data/ykang/plato/results/test/model/lenet5_422_1127978.pth.
[INFO][10:22:26]: [Client #463] Loading a model from /data/ykang/plato/results/test/model/lenet5_463_1127979.pth.
[INFO][10:22:26]: [Client #463] Model trained.
[INFO][10:22:26]: [Client #463] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:22:26]: [Server #1127936] Received 0.24 MB of payload data from client #463 (simulated).
[INFO][10:22:26]: [Client #447] Loading a model from /data/ykang/plato/results/test/model/lenet5_447_1127977.pth.
[INFO][10:22:26]: [Client #447] Model trained.
[INFO][10:22:26]: [Client #447] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:22:26]: [Server #1127936] Received 0.24 MB of payload data from client #447 (simulated).
[INFO][10:22:26]: [Client #422] Loading a model from /data/ykang/plato/results/test/model/lenet5_422_1127978.pth.
[INFO][10:22:26]: [Client #422] Model trained.
[INFO][10:22:26]: [Client #422] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:22:26]: [Server #1127936] Received 0.24 MB of payload data from client #422 (simulated).
[INFO][10:22:26]: [Server #1127936] Selecting client #329 for training.
[INFO][10:22:26]: [Server #1127936] Sending the current model to client #329 (simulated).
[INFO][10:22:26]: [Server #1127936] Sending 0.24 MB of payload data to client #329 (simulated).
[INFO][10:22:26]: [Server #1127936] Selecting client #289 for training.
[INFO][10:22:26]: [Server #1127936] Sending the current model to client #289 (simulated).
[INFO][10:22:26]: [Server #1127936] Sending 0.24 MB of payload data to client #289 (simulated).
[INFO][10:22:26]: [Server #1127936] Selecting client #200 for training.
[INFO][10:22:26]: [Server #1127936] Sending the current model to client #200 (simulated).
[INFO][10:22:26]: [Client #329] Selected by the server.
[INFO][10:22:26]: [Client #329] Loading its data source...
[INFO][10:22:26]: [Client #329] Dataset size: 60000
[INFO][10:22:26]: [Client #329] Sampler: noniid
[INFO][10:22:26]: [Server #1127936] Sending 0.24 MB of payload data to client #200 (simulated).
[INFO][10:22:26]: [Client #289] Selected by the server.
[INFO][10:22:26]: [Client #289] Loading its data source...
[INFO][10:22:26]: [Client #289] Dataset size: 60000
[INFO][10:22:26]: [Client #289] Sampler: noniid
[INFO][10:22:26]: [Client #200] Selected by the server.
[INFO][10:22:26]: [Client #200] Loading its data source...
[INFO][10:22:26]: [Client #200] Dataset size: 60000
[INFO][10:22:26]: [Client #200] Sampler: noniid
[INFO][10:22:26]: [Client #329] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:22:26]: [Client #289] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:22:26]: [Client #200] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:22:26]: [93m[1m[Client #200] Started training in communication round #63.[0m
[INFO][10:22:26]: [93m[1m[Client #329] Started training in communication round #63.[0m
[INFO][10:22:26]: [93m[1m[Client #289] Started training in communication round #63.[0m
[INFO][10:22:28]: [Client #329] Loading the dataset.
[INFO][10:22:28]: [Client #200] Loading the dataset.
[INFO][10:22:28]: [Client #289] Loading the dataset.
[INFO][10:22:34]: [Client #200] Epoch: [1/5][0/10]	Loss: 0.000730
[INFO][10:22:34]: [Client #289] Epoch: [1/5][0/10]	Loss: 0.022283
[INFO][10:22:34]: [Client #329] Epoch: [1/5][0/10]	Loss: 0.022177
[INFO][10:22:34]: [Client #200] Epoch: [2/5][0/10]	Loss: 0.003219
[INFO][10:22:34]: [Client #289] Epoch: [2/5][0/10]	Loss: 0.003219
[INFO][10:22:34]: [Client #329] Epoch: [2/5][0/10]	Loss: 0.000170
[INFO][10:22:34]: [Client #289] Epoch: [3/5][0/10]	Loss: 0.000089
[INFO][10:22:34]: [Client #200] Epoch: [3/5][0/10]	Loss: 0.000495
[INFO][10:22:34]: [Client #329] Epoch: [3/5][0/10]	Loss: 0.000001
[INFO][10:22:35]: [Client #289] Epoch: [4/5][0/10]	Loss: 0.002694
[INFO][10:22:35]: [Client #200] Epoch: [4/5][0/10]	Loss: 0.002528
[INFO][10:22:35]: [Client #329] Epoch: [4/5][0/10]	Loss: 0.000003
[INFO][10:22:35]: [Client #289] Epoch: [5/5][0/10]	Loss: 0.000106
[INFO][10:22:35]: [Client #200] Epoch: [5/5][0/10]	Loss: 0.003087
[INFO][10:22:35]: [Client #200] Model saved to /data/ykang/plato/results/test/model/lenet5_200_1127979.pth.
[INFO][10:22:35]: [Client #289] Model saved to /data/ykang/plato/results/test/model/lenet5_289_1127978.pth.
[INFO][10:22:35]: [Client #329] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:22:35]: [Client #329] Model saved to /data/ykang/plato/results/test/model/lenet5_329_1127977.pth.
[INFO][10:22:36]: [Client #200] Loading a model from /data/ykang/plato/results/test/model/lenet5_200_1127979.pth.
[INFO][10:22:36]: [Client #200] Model trained.
[INFO][10:22:36]: [Client #200] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:22:36]: [Server #1127936] Received 0.24 MB of payload data from client #200 (simulated).
[INFO][10:22:36]: [Client #289] Loading a model from /data/ykang/plato/results/test/model/lenet5_289_1127978.pth.
[INFO][10:22:36]: [Client #289] Model trained.
[INFO][10:22:36]: [Client #289] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:22:36]: [Server #1127936] Received 0.24 MB of payload data from client #289 (simulated).
[INFO][10:22:36]: [Client #329] Loading a model from /data/ykang/plato/results/test/model/lenet5_329_1127977.pth.
[INFO][10:22:36]: [Client #329] Model trained.
[INFO][10:22:36]: [Client #329] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:22:36]: [Server #1127936] Received 0.24 MB of payload data from client #329 (simulated).
[INFO][10:22:36]: [Server #1127936] Selecting client #484 for training.
[INFO][10:22:36]: [Server #1127936] Sending the current model to client #484 (simulated).
[INFO][10:22:36]: [Server #1127936] Sending 0.24 MB of payload data to client #484 (simulated).
[INFO][10:22:36]: [Server #1127936] Selecting client #79 for training.
[INFO][10:22:36]: [Server #1127936] Sending the current model to client #79 (simulated).
[INFO][10:22:36]: [Server #1127936] Sending 0.24 MB of payload data to client #79 (simulated).
[INFO][10:22:36]: [Server #1127936] Selecting client #446 for training.
[INFO][10:22:36]: [Server #1127936] Sending the current model to client #446 (simulated).
[INFO][10:22:36]: [Client #484] Selected by the server.
[INFO][10:22:36]: [Client #484] Loading its data source...
[INFO][10:22:36]: [Client #484] Dataset size: 60000
[INFO][10:22:36]: [Client #484] Sampler: noniid
[INFO][10:22:36]: [Server #1127936] Sending 0.24 MB of payload data to client #446 (simulated).
[INFO][10:22:36]: [Client #446] Selected by the server.
[INFO][10:22:36]: [Client #446] Loading its data source...
[INFO][10:22:36]: [Client #446] Dataset size: 60000
[INFO][10:22:36]: [Client #446] Sampler: noniid
[INFO][10:22:36]: [Client #79] Selected by the server.
[INFO][10:22:36]: [Client #79] Loading its data source...
[INFO][10:22:36]: [Client #79] Dataset size: 60000
[INFO][10:22:36]: [Client #79] Sampler: noniid
[INFO][10:22:36]: [Client #484] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:22:36]: [Client #446] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:22:36]: [93m[1m[Client #484] Started training in communication round #63.[0m
[INFO][10:22:36]: [Client #79] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:22:36]: [93m[1m[Client #79] Started training in communication round #63.[0m
[INFO][10:22:36]: [93m[1m[Client #446] Started training in communication round #63.[0m
[INFO][10:22:38]: [Client #484] Loading the dataset.
[INFO][10:22:38]: [Client #79] Loading the dataset.
[INFO][10:22:38]: [Client #446] Loading the dataset.
[INFO][10:22:44]: [Client #484] Epoch: [1/5][0/10]	Loss: 0.002073
[INFO][10:22:44]: [Client #484] Epoch: [2/5][0/10]	Loss: 0.001663
[INFO][10:22:44]: [Client #79] Epoch: [1/5][0/10]	Loss: 0.004374
[INFO][10:22:44]: [Client #446] Epoch: [1/5][0/10]	Loss: 0.002190
[INFO][10:22:44]: [Client #484] Epoch: [3/5][0/10]	Loss: 0.000021
[INFO][10:22:44]: [Client #79] Epoch: [2/5][0/10]	Loss: 0.006460
[INFO][10:22:44]: [Client #446] Epoch: [2/5][0/10]	Loss: 0.001782
[INFO][10:22:44]: [Client #484] Epoch: [4/5][0/10]	Loss: 0.017429
[INFO][10:22:44]: [Client #79] Epoch: [3/5][0/10]	Loss: 0.000012
[INFO][10:22:45]: [Client #446] Epoch: [3/5][0/10]	Loss: 0.000013
[INFO][10:22:45]: [Client #484] Epoch: [5/5][0/10]	Loss: 0.216707
[INFO][10:22:45]: [Client #79] Epoch: [4/5][0/10]	Loss: 0.002220
[INFO][10:22:45]: [Client #446] Epoch: [4/5][0/10]	Loss: 0.000027
[INFO][10:22:45]: [Client #484] Model saved to /data/ykang/plato/results/test/model/lenet5_484_1127977.pth.
[INFO][10:22:45]: [Client #446] Epoch: [5/5][0/10]	Loss: 0.007500
[INFO][10:22:45]: [Client #79] Epoch: [5/5][0/10]	Loss: 0.000030
[INFO][10:22:45]: [Client #79] Model saved to /data/ykang/plato/results/test/model/lenet5_79_1127978.pth.
[INFO][10:22:45]: [Client #446] Model saved to /data/ykang/plato/results/test/model/lenet5_446_1127979.pth.
[INFO][10:22:45]: [Client #484] Loading a model from /data/ykang/plato/results/test/model/lenet5_484_1127977.pth.
[INFO][10:22:45]: [Client #484] Model trained.
[INFO][10:22:45]: [Client #484] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:22:45]: [Server #1127936] Received 0.24 MB of payload data from client #484 (simulated).
[INFO][10:22:46]: [Client #79] Loading a model from /data/ykang/plato/results/test/model/lenet5_79_1127978.pth.
[INFO][10:22:46]: [Client #79] Model trained.
[INFO][10:22:46]: [Client #79] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:22:46]: [Server #1127936] Received 0.24 MB of payload data from client #79 (simulated).
[INFO][10:22:46]: [Client #446] Loading a model from /data/ykang/plato/results/test/model/lenet5_446_1127979.pth.
[INFO][10:22:46]: [Client #446] Model trained.
[INFO][10:22:46]: [Client #446] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:22:46]: [Server #1127936] Received 0.24 MB of payload data from client #446 (simulated).
[INFO][10:22:46]: [Server #1127936] Selecting client #157 for training.
[INFO][10:22:46]: [Server #1127936] Sending the current model to client #157 (simulated).
[INFO][10:22:46]: [Server #1127936] Sending 0.24 MB of payload data to client #157 (simulated).
[INFO][10:22:46]: [Client #157] Selected by the server.
[INFO][10:22:46]: [Client #157] Loading its data source...
[INFO][10:22:46]: [Client #157] Dataset size: 60000
[INFO][10:22:46]: [Client #157] Sampler: noniid
[INFO][10:22:46]: [Client #157] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:22:46]: [93m[1m[Client #157] Started training in communication round #63.[0m
[INFO][10:22:48]: [Client #157] Loading the dataset.
[INFO][10:22:53]: [Client #157] Epoch: [1/5][0/10]	Loss: 0.002121
[INFO][10:22:53]: [Client #157] Epoch: [2/5][0/10]	Loss: 0.046533
[INFO][10:22:53]: [Client #157] Epoch: [3/5][0/10]	Loss: 0.000053
[INFO][10:22:54]: [Client #157] Epoch: [4/5][0/10]	Loss: 0.000141
[INFO][10:22:54]: [Client #157] Epoch: [5/5][0/10]	Loss: 0.000061
[INFO][10:22:54]: [Client #157] Model saved to /data/ykang/plato/results/test/model/lenet5_157_1127977.pth.
[INFO][10:22:54]: [Client #157] Loading a model from /data/ykang/plato/results/test/model/lenet5_157_1127977.pth.
[INFO][10:22:54]: [Client #157] Model trained.
[INFO][10:22:54]: [Client #157] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:22:54]: [Server #1127936] Received 0.24 MB of payload data from client #157 (simulated).
[INFO][10:22:54]: [Server #1127936] Adding client #420 to the list of clients for aggregation.
[INFO][10:22:54]: [Server #1127936] Adding client #44 to the list of clients for aggregation.
[INFO][10:22:54]: [Server #1127936] Adding client #475 to the list of clients for aggregation.
[INFO][10:22:54]: [Server #1127936] Adding client #41 to the list of clients for aggregation.
[INFO][10:22:54]: [Server #1127936] Adding client #14 to the list of clients for aggregation.
[INFO][10:22:54]: [Server #1127936] Adding client #422 to the list of clients for aggregation.
[INFO][10:22:54]: [Server #1127936] Adding client #463 to the list of clients for aggregation.
[INFO][10:22:54]: [Server #1127936] Adding client #157 to the list of clients for aggregation.
[INFO][10:22:54]: [Server #1127936] Adding client #329 to the list of clients for aggregation.
[INFO][10:22:54]: [Server #1127936] Adding client #289 to the list of clients for aggregation.
[INFO][10:22:54]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 468, 469, 470, 471, 472, 473, 474, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00299725 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01233532 0.
 0.         0.0031943  0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00766233 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00639196 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00560347 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00458952
 0.         0.0087182  0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00394003 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01141221 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 0. 1. 0. 0. 0. 1. 0. 1. 3. 1. 0. 0. 0. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 2. 0. 1. 0.
 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 2. 1. 0. 1. 0.
 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 2. 1. 6. 0. 1. 2. 1. 0. 1. 1. 2. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 0. 6. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 2. 0. 0.
 1. 0. 1. 2. 0. 0. 0. 2. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 4. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0.
 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 0. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 3. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 0. 0. 0. 0. 0. 0. 6. 1. 1.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 6. 1. 0. 0. 1. 0. 0. 1. 2. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 2. 2. 1. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00299725 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01233532 0.
 0.         0.0031943  0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00766233 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00639196 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00560347 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00458952
 0.         0.0087182  0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00394003 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01141221 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:22:57]: [Server #1127936] Global model accuracy: 96.16%

[INFO][10:22:57]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_63.pth.
[INFO][10:22:57]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_63.pth.
[INFO][10:22:57]: [93m[1m
[Server #1127936] Starting round 64/100.[0m
[0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  4e-04  7e-09  7e-09
 5:  6.8876e+00  6.8875e+00  8e-05  1e-09  1e-09
 6:  6.8876e+00  6.8875e+00  7e-05  6e-10  6e-10
 7:  6.8875e+00  6.8875e+00  7e-05  2e-09  2e-10
 8:  6.8875e+00  6.8875e+00  5e-05  3e-09  2e-10
 9:  6.8875e+00  6.8875e+00  2e-05  2e-08  2e-09
10:  6.8875e+00  6.8875e+00  7e-06  1e-08  8e-10
Optimal solution found.
The calculated probability is:  [1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 2.05090631e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 9.04227748e-01 1.95564605e-04 1.95564605e-04 2.05749263e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95562825e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95563366e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95563653e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 2.10535413e-04 1.95564605e-04 1.95562301e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564134e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 3.02049662e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04 1.95564605e-04 1.95564605e-04
 1.95564605e-04 1.95564605e-04]
current clients pool:  [INFO][10:22:57]: [Server #1127936] Selected clients: [ 41 474 277  43 103 426 352 448 166 265]
[INFO][10:22:57]: [Server #1127936] Selecting client #41 for training.
[INFO][10:22:57]: [Server #1127936] Sending the current model to client #41 (simulated).
[INFO][10:22:57]: [Server #1127936] Sending 0.24 MB of payload data to client #41 (simulated).
[INFO][10:22:57]: [Server #1127936] Selecting client #474 for training.
[INFO][10:22:57]: [Server #1127936] Sending the current model to client #474 (simulated).
[INFO][10:22:57]: [Server #1127936] Sending 0.24 MB of payload data to client #474 (simulated).
[INFO][10:22:57]: [Server #1127936] Selecting client #277 for training.
[INFO][10:22:57]: [Server #1127936] Sending the current model to client #277 (simulated).
[INFO][10:22:57]: [Client #41] Selected by the server.
[INFO][10:22:57]: [Client #41] Loading its data source...
[INFO][10:22:57]: [Client #41] Dataset size: 60000
[INFO][10:22:57]: [Client #41] Sampler: noniid
[INFO][10:22:57]: [Server #1127936] Sending 0.24 MB of payload data to client #277 (simulated).
[INFO][10:22:57]: [Client #474] Selected by the server.
[INFO][10:22:57]: [Client #474] Loading its data source...
[INFO][10:22:57]: [Client #474] Dataset size: 60000
[INFO][10:22:57]: [Client #474] Sampler: noniid
[INFO][10:22:57]: [Client #41] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:22:57]: [Client #277] Selected by the server.
[INFO][10:22:57]: [Client #277] Loading its data source...
[INFO][10:22:57]: [Client #277] Dataset size: 60000
[INFO][10:22:57]: [Client #277] Sampler: noniid
[INFO][10:22:57]: [Client #277] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:22:57]: [Client #474] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:22:57]: [93m[1m[Client #277] Started training in communication round #64.[0m
[INFO][10:22:57]: [93m[1m[Client #41] Started training in communication round #64.[0m
[INFO][10:22:57]: [93m[1m[Client #474] Started training in communication round #64.[0m
[INFO][10:22:59]: [Client #277] Loading the dataset.
[INFO][10:22:59]: [Client #474] Loading the dataset.
[INFO][10:22:59]: [Client #41] Loading the dataset.
[INFO][10:23:05]: [Client #474] Epoch: [1/5][0/10]	Loss: 0.000961
[INFO][10:23:05]: [Client #41] Epoch: [1/5][0/10]	Loss: 0.001625
[INFO][10:23:05]: [Client #277] Epoch: [1/5][0/10]	Loss: 0.060673
[INFO][10:23:06]: [Client #474] Epoch: [2/5][0/10]	Loss: 0.000382
[INFO][10:23:06]: [Client #277] Epoch: [2/5][0/10]	Loss: 0.000702
[INFO][10:23:06]: [Client #41] Epoch: [2/5][0/10]	Loss: 0.000862
[INFO][10:23:06]: [Client #474] Epoch: [3/5][0/10]	Loss: 0.000519
[INFO][10:23:06]: [Client #277] Epoch: [3/5][0/10]	Loss: 0.000180
[INFO][10:23:06]: [Client #474] Epoch: [4/5][0/10]	Loss: 0.077982
[INFO][10:23:06]: [Client #41] Epoch: [3/5][0/10]	Loss: 0.038450
[INFO][10:23:06]: [Client #474] Epoch: [5/5][0/10]	Loss: 0.004932
[INFO][10:23:06]: [Client #277] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:23:06]: [Client #474] Model saved to /data/ykang/plato/results/test/model/lenet5_474_1127978.pth.
[INFO][10:23:06]: [Client #41] Epoch: [4/5][0/10]	Loss: 0.005673
[INFO][10:23:06]: [Client #277] Epoch: [5/5][0/10]	Loss: 0.062851
[INFO][10:23:06]: [Client #277] Model saved to /data/ykang/plato/results/test/model/lenet5_277_1127979.pth.
[INFO][10:23:06]: [Client #41] Epoch: [5/5][0/10]	Loss: 0.043058
[INFO][10:23:06]: [Client #41] Model saved to /data/ykang/plato/results/test/model/lenet5_41_1127977.pth.
[INFO][10:23:07]: [Client #474] Loading a model from /data/ykang/plato/results/test/model/lenet5_474_1127978.pth.
[INFO][10:23:07]: [Client #474] Model trained.
[INFO][10:23:07]: [Client #474] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:23:07]: [Server #1127936] Received 0.24 MB of payload data from client #474 (simulated).
[INFO][10:23:07]: [Client #277] Loading a model from /data/ykang/plato/results/test/model/lenet5_277_1127979.pth.
[INFO][10:23:07]: [Client #277] Model trained.
[INFO][10:23:07]: [Client #277] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:23:07]: [Server #1127936] Received 0.24 MB of payload data from client #277 (simulated).
[INFO][10:23:07]: [Client #41] Loading a model from /data/ykang/plato/results/test/model/lenet5_41_1127977.pth.
[INFO][10:23:07]: [Client #41] Model trained.
[INFO][10:23:07]: [Client #41] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:23:07]: [Server #1127936] Received 0.24 MB of payload data from client #41 (simulated).
[INFO][10:23:07]: [Server #1127936] Selecting client #43 for training.
[INFO][10:23:07]: [Server #1127936] Sending the current model to client #43 (simulated).
[INFO][10:23:07]: [Server #1127936] Sending 0.24 MB of payload data to client #43 (simulated).
[INFO][10:23:07]: [Server #1127936] Selecting client #103 for training.
[INFO][10:23:07]: [Server #1127936] Sending the current model to client #103 (simulated).
[INFO][10:23:07]: [Server #1127936] Sending 0.24 MB of payload data to client #103 (simulated).
[INFO][10:23:07]: [Server #1127936] Selecting client #426 for training.
[INFO][10:23:07]: [Server #1127936] Sending the current model to client #426 (simulated).
[INFO][10:23:07]: [Client #43] Selected by the server.
[INFO][10:23:07]: [Client #43] Loading its data source...
[INFO][10:23:07]: [Client #43] Dataset size: 60000
[INFO][10:23:07]: [Client #43] Sampler: noniid
[INFO][10:23:07]: [Server #1127936] Sending 0.24 MB of payload data to client #426 (simulated).
[INFO][10:23:07]: [Client #103] Selected by the server.
[INFO][10:23:07]: [Client #103] Loading its data source...
[INFO][10:23:07]: [Client #426] Selected by the server.
[INFO][10:23:07]: [Client #103] Dataset size: 60000
[INFO][10:23:07]: [Client #426] Loading its data source...
[INFO][10:23:07]: [Client #103] Sampler: noniid
[INFO][10:23:07]: [Client #426] Dataset size: 60000
[INFO][10:23:07]: [Client #426] Sampler: noniid
[INFO][10:23:07]: [Client #43] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:23:07]: [Client #103] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:23:07]: [Client #426] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:23:07]: [93m[1m[Client #43] Started training in communication round #64.[0m
[INFO][10:23:07]: [93m[1m[Client #426] Started training in communication round #64.[0m
[INFO][10:23:07]: [93m[1m[Client #103] Started training in communication round #64.[0m
[INFO][10:23:09]: [Client #426] Loading the dataset.
[INFO][10:23:09]: [Client #43] Loading the dataset.
[INFO][10:23:09]: [Client #103] Loading the dataset.
[INFO][10:23:15]: [Client #426] Epoch: [1/5][0/10]	Loss: 0.001673
[INFO][10:23:15]: [Client #103] Epoch: [1/5][0/10]	Loss: 0.063096
[INFO][10:23:15]: [Client #426] Epoch: [2/5][0/10]	Loss: 0.000331
[INFO][10:23:15]: [Client #43] Epoch: [1/5][0/10]	Loss: 0.007051
[INFO][10:23:15]: [Client #103] Epoch: [2/5][0/10]	Loss: 0.000516
[INFO][10:23:15]: [Client #426] Epoch: [3/5][0/10]	Loss: 0.000262
[INFO][10:23:15]: [Client #103] Epoch: [3/5][0/10]	Loss: 0.000040
[INFO][10:23:15]: [Client #43] Epoch: [2/5][0/10]	Loss: 0.000009
[INFO][10:23:15]: [Client #426] Epoch: [4/5][0/10]	Loss: 0.000616
[INFO][10:23:15]: [Client #103] Epoch: [4/5][0/10]	Loss: 0.000373
[INFO][10:23:15]: [Client #43] Epoch: [3/5][0/10]	Loss: 0.000010
[INFO][10:23:15]: [Client #426] Epoch: [5/5][0/10]	Loss: 0.010772
[INFO][10:23:15]: [Client #103] Epoch: [5/5][0/10]	Loss: 0.033444
[INFO][10:23:15]: [Client #43] Epoch: [4/5][0/10]	Loss: 0.000022
[INFO][10:23:15]: [Client #426] Model saved to /data/ykang/plato/results/test/model/lenet5_426_1127979.pth.
[INFO][10:23:15]: [Client #103] Model saved to /data/ykang/plato/results/test/model/lenet5_103_1127978.pth.
[INFO][10:23:16]: [Client #43] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:23:16]: [Client #43] Model saved to /data/ykang/plato/results/test/model/lenet5_43_1127977.pth.
[INFO][10:23:16]: [Client #426] Loading a model from /data/ykang/plato/results/test/model/lenet5_426_1127979.pth.
[INFO][10:23:16]: [Client #426] Model trained.
[INFO][10:23:16]: [Client #426] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:23:16]: [Server #1127936] Received 0.24 MB of payload data from client #426 (simulated).
[INFO][10:23:16]: [Client #103] Loading a model from /data/ykang/plato/results/test/model/lenet5_103_1127978.pth.
[INFO][10:23:16]: [Client #103] Model trained.
[INFO][10:23:16]: [Client #103] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:23:16]: [Server #1127936] Received 0.24 MB of payload data from client #103 (simulated).
[INFO][10:23:16]: [Client #43] Loading a model from /data/ykang/plato/results/test/model/lenet5_43_1127977.pth.
[INFO][10:23:16]: [Client #43] Model trained.
[INFO][10:23:16]: [Client #43] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:23:16]: [Server #1127936] Received 0.24 MB of payload data from client #43 (simulated).
[INFO][10:23:16]: [Server #1127936] Selecting client #352 for training.
[INFO][10:23:16]: [Server #1127936] Sending the current model to client #352 (simulated).
[INFO][10:23:16]: [Server #1127936] Sending 0.24 MB of payload data to client #352 (simulated).
[INFO][10:23:16]: [Server #1127936] Selecting client #448 for training.
[INFO][10:23:16]: [Server #1127936] Sending the current model to client #448 (simulated).
[INFO][10:23:16]: [Server #1127936] Sending 0.24 MB of payload data to client #448 (simulated).
[INFO][10:23:16]: [Server #1127936] Selecting client #166 for training.
[INFO][10:23:16]: [Server #1127936] Sending the current model to client #166 (simulated).
[INFO][10:23:16]: [Client #352] Selected by the server.
[INFO][10:23:16]: [Client #352] Loading its data source...
[INFO][10:23:16]: [Client #352] Dataset size: 60000
[INFO][10:23:16]: [Client #352] Sampler: noniid
[INFO][10:23:16]: [Server #1127936] Sending 0.24 MB of payload data to client #166 (simulated).
[INFO][10:23:16]: [Client #448] Selected by the server.
[INFO][10:23:16]: [Client #448] Loading its data source...
[INFO][10:23:16]: [Client #166] Selected by the server.
[INFO][10:23:16]: [Client #448] Dataset size: 60000
[INFO][10:23:16]: [Client #448] Sampler: noniid
[INFO][10:23:16]: [Client #166] Loading its data source...
[INFO][10:23:16]: [Client #166] Dataset size: 60000
[INFO][10:23:16]: [Client #166] Sampler: noniid
[INFO][10:23:16]: [Client #352] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:23:16]: [Client #448] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:23:16]: [Client #166] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:23:16]: [93m[1m[Client #166] Started training in communication round #64.[0m
[INFO][10:23:16]: [93m[1m[Client #352] Started training in communication round #64.[0m
[INFO][10:23:16]: [93m[1m[Client #448] Started training in communication round #64.[0m
[INFO][10:23:19]: [Client #166] Loading the dataset.
[INFO][10:23:19]: [Client #352] Loading the dataset.
[INFO][10:23:19]: [Client #448] Loading the dataset.
[INFO][10:23:25]: [Client #166] Epoch: [1/5][0/10]	Loss: 0.003703
[INFO][10:23:25]: [Client #352] Epoch: [1/5][0/10]	Loss: 0.000922
[INFO][10:23:25]: [Client #448] Epoch: [1/5][0/10]	Loss: 0.003947
[INFO][10:23:25]: [Client #166] Epoch: [2/5][0/10]	Loss: 0.001348
[INFO][10:23:25]: [Client #352] Epoch: [2/5][0/10]	Loss: 0.003704
[INFO][10:23:25]: [Client #448] Epoch: [2/5][0/10]	Loss: 0.001878
[INFO][10:23:25]: [Client #166] Epoch: [3/5][0/10]	Loss: 0.000311
[INFO][10:23:25]: [Client #352] Epoch: [3/5][0/10]	Loss: 0.000189
[INFO][10:23:25]: [Client #448] Epoch: [3/5][0/10]	Loss: 0.000146
[INFO][10:23:25]: [Client #166] Epoch: [4/5][0/10]	Loss: 0.004667
[INFO][10:23:25]: [Client #448] Epoch: [4/5][0/10]	Loss: 0.079016
[INFO][10:23:25]: [Client #352] Epoch: [4/5][0/10]	Loss: 0.000069
[INFO][10:23:25]: [Client #166] Epoch: [5/5][0/10]	Loss: 0.001157
[INFO][10:23:25]: [Client #448] Epoch: [5/5][0/10]	Loss: 1.172709
[INFO][10:23:25]: [Client #166] Model saved to /data/ykang/plato/results/test/model/lenet5_166_1127979.pth.
[INFO][10:23:25]: [Client #352] Epoch: [5/5][0/10]	Loss: 0.000248
[INFO][10:23:25]: [Client #448] Model saved to /data/ykang/plato/results/test/model/lenet5_448_1127978.pth.
[INFO][10:23:25]: [Client #352] Model saved to /data/ykang/plato/results/test/model/lenet5_352_1127977.pth.
[INFO][10:23:26]: [Client #166] Loading a model from /data/ykang/plato/results/test/model/lenet5_166_1127979.pth.
[INFO][10:23:26]: [Client #166] Model trained.
[INFO][10:23:26]: [Client #166] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:23:26]: [Server #1127936] Received 0.24 MB of payload data from client #166 (simulated).
[INFO][10:23:26]: [Client #352] Loading a model from /data/ykang/plato/results/test/model/lenet5_352_1127977.pth.
[INFO][10:23:26]: [Client #352] Model trained.
[INFO][10:23:26]: [Client #352] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:23:26]: [Server #1127936] Received 0.24 MB of payload data from client #352 (simulated).
[INFO][10:23:26]: [Client #448] Loading a model from /data/ykang/plato/results/test/model/lenet5_448_1127978.pth.
[INFO][10:23:26]: [Client #448] Model trained.
[INFO][10:23:26]: [Client #448] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:23:26]: [Server #1127936] Received 0.24 MB of payload data from client #448 (simulated).
[INFO][10:23:26]: [Server #1127936] Selecting client #265 for training.
[INFO][10:23:26]: [Server #1127936] Sending the current model to client #265 (simulated).
[INFO][10:23:26]: [Server #1127936] Sending 0.24 MB of payload data to client #265 (simulated).
[INFO][10:23:26]: [Client #265] Selected by the server.
[INFO][10:23:26]: [Client #265] Loading its data source...
[INFO][10:23:26]: [Client #265] Dataset size: 60000
[INFO][10:23:26]: [Client #265] Sampler: noniid
[INFO][10:23:26]: [Client #265] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:23:26]: [93m[1m[Client #265] Started training in communication round #64.[0m
[INFO][10:23:28]: [Client #265] Loading the dataset.
[INFO][10:23:33]: [Client #265] Epoch: [1/5][0/10]	Loss: 0.001474
[INFO][10:23:33]: [Client #265] Epoch: [2/5][0/10]	Loss: 0.002014
[INFO][10:23:33]: [Client #265] Epoch: [3/5][0/10]	Loss: 0.000052
[INFO][10:23:34]: [Client #265] Epoch: [4/5][0/10]	Loss: 0.001447
[INFO][10:23:34]: [Client #265] Epoch: [5/5][0/10]	Loss: 0.000994
[INFO][10:23:34]: [Client #265] Model saved to /data/ykang/plato/results/test/model/lenet5_265_1127977.pth.
[INFO][10:23:34]: [Client #265] Loading a model from /data/ykang/plato/results/test/model/lenet5_265_1127977.pth.
[INFO][10:23:34]: [Client #265] Model trained.
[INFO][10:23:34]: [Client #265] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:23:34]: [Server #1127936] Received 0.24 MB of payload data from client #265 (simulated).
[INFO][10:23:34]: [Server #1127936] Adding client #484 to the list of clients for aggregation.
[INFO][10:23:34]: [Server #1127936] Adding client #79 to the list of clients for aggregation.
[INFO][10:23:34]: [Server #1127936] Adding client #447 to the list of clients for aggregation.
[INFO][10:23:34]: [Server #1127936] Adding client #92 to the list of clients for aggregation.
[INFO][10:23:34]: [Server #1127936] Adding client #446 to the list of clients for aggregation.
[INFO][10:23:34]: [Server #1127936] Adding client #166 to the list of clients for aggregation.
[INFO][10:23:34]: [Server #1127936] Adding client #426 to the list of clients for aggregation.
[INFO][10:23:34]: [Server #1127936] Adding client #277 to the list of clients for aggregation.
[INFO][10:23:34]: [Server #1127936] Adding client #43 to the list of clients for aggregation.
[INFO][10:23:34]: [Server #1127936] Adding client #474 to the list of clients for aggregation.
[INFO][10:23:34]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00325235 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.02108937 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00183544 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01167681 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00624505 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00240173
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00311611 0.01397514 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.03536889
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00980295 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 0. 1. 0. 0. 0. 1. 0. 1. 3. 1. 0. 0. 0. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 2. 0. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 2. 1. 0. 1. 0.
 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 2. 1. 6. 0. 1. 2. 1. 0. 1. 1. 2. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 0. 6. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 2. 0. 0.
 1. 0. 1. 2. 0. 0. 0. 2. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 4. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0.
 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 0. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 3. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 0. 0. 0. 0. 0. 0. 6. 1. 1.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 6. 1. 0. 0. 1. 0. 0. 0. 2. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 2. 2. 1. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00325235 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.02108937 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00183544 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01167681 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00624505 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00240173
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00311611 0.01397514 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.03536889
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00980295 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:23:36]: [Server #1127936] Global model accuracy: 95.97%

[INFO][10:23:36]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_64.pth.
[INFO][10:23:36]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_64.pth.
[INFO][10:23:36]: [93m[1m
[Server #1127936] Starting round 65/100.[0m
[0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.002 0.002
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  2e-05  4e-10  4e-10
 6:  6.8876e+00  6.8875e+00  2e-05  3e-10  3e-10
 7:  6.8875e+00  6.8875e+00  2e-05  5e-10  1e-11
 8:  6.8875e+00  6.8875e+00  1e-05  4e-10  1e-11
 9:  6.8875e+00  6.8875e+00  8e-06  8e-10  2e-11
10:  6.8875e+00  6.8875e+00  5e-07  1e-09  3e-11
Optimal solution found.
The calculated probability is:  [5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09583228e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 9.74860752e-01 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 6.34501966e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09548786e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09575444e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09584546e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 6.11879670e-05 1.93091021e-04 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09243751e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 1.06631333e-04 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05 5.09586125e-05 5.09586125e-05
 5.09586125e-05 5.09586125e-05]
current clients pool:  [INFO][10:23:37]: [Server #1127936] Selected clients: [ 79 238 316 131 329 188 149 457 473 310]
[INFO][10:23:37]: [Server #1127936] Selecting client #79 for training.
[INFO][10:23:37]: [Server #1127936] Sending the current model to client #79 (simulated).
[INFO][10:23:37]: [Server #1127936] Sending 0.24 MB of payload data to client #79 (simulated).
[INFO][10:23:37]: [Server #1127936] Selecting client #238 for training.
[INFO][10:23:37]: [Server #1127936] Sending the current model to client #238 (simulated).
[INFO][10:23:37]: [Server #1127936] Sending 0.24 MB of payload data to client #238 (simulated).
[INFO][10:23:37]: [Server #1127936] Selecting client #316 for training.
[INFO][10:23:37]: [Server #1127936] Sending the current model to client #316 (simulated).
[INFO][10:23:37]: [Client #79] Selected by the server.
[INFO][10:23:37]: [Client #79] Loading its data source...
[INFO][10:23:37]: [Client #79] Dataset size: 60000
[INFO][10:23:37]: [Client #79] Sampler: noniid
[INFO][10:23:37]: [Server #1127936] Sending 0.24 MB of payload data to client #316 (simulated).
[INFO][10:23:37]: [Client #238] Selected by the server.
[INFO][10:23:37]: [Client #238] Loading its data source...
[INFO][10:23:37]: [Client #238] Dataset size: 60000
[INFO][10:23:37]: [Client #238] Sampler: noniid
[INFO][10:23:37]: [Client #79] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:23:37]: [Client #238] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:23:37]: [Client #316] Selected by the server.
[INFO][10:23:37]: [Client #316] Loading its data source...
[INFO][10:23:37]: [Client #316] Dataset size: 60000
[INFO][10:23:37]: [Client #316] Sampler: noniid
[INFO][10:23:37]: [Client #316] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:23:37]: [93m[1m[Client #79] Started training in communication round #65.[0m
[INFO][10:23:37]: [93m[1m[Client #316] Started training in communication round #65.[0m
[INFO][10:23:37]: [93m[1m[Client #238] Started training in communication round #65.[0m
[INFO][10:23:39]: [Client #79] Loading the dataset.
[INFO][10:23:39]: [Client #238] Loading the dataset.
[INFO][10:23:39]: [Client #316] Loading the dataset.
[INFO][10:23:47]: [Client #79] Epoch: [1/5][0/10]	Loss: 0.004875
[INFO][10:23:47]: [Client #79] Epoch: [2/5][0/10]	Loss: 0.004983
[INFO][10:23:47]: [Client #316] Epoch: [1/5][0/10]	Loss: 0.004127
[INFO][10:23:48]: [Client #79] Epoch: [3/5][0/10]	Loss: 0.000025
[INFO][10:23:48]: [Client #316] Epoch: [2/5][0/10]	Loss: 0.004592
[INFO][10:23:48]: [Client #238] Epoch: [1/5][0/10]	Loss: 0.001910
[INFO][10:23:48]: [Client #79] Epoch: [4/5][0/10]	Loss: 0.000046
[INFO][10:23:48]: [Client #316] Epoch: [3/5][0/10]	Loss: 0.000211
[INFO][10:23:48]: [Client #238] Epoch: [2/5][0/10]	Loss: 0.001253
[INFO][10:23:48]: [Client #79] Epoch: [5/5][0/10]	Loss: 0.000196
[INFO][10:23:48]: [Client #79] Model saved to /data/ykang/plato/results/test/model/lenet5_79_1127977.pth.
[INFO][10:23:48]: [Client #316] Epoch: [4/5][0/10]	Loss: 0.000061
[INFO][10:23:48]: [Client #238] Epoch: [3/5][0/10]	Loss: 0.000078
[INFO][10:23:48]: [Client #316] Epoch: [5/5][0/10]	Loss: 0.000140
[INFO][10:23:48]: [Client #238] Epoch: [4/5][0/10]	Loss: 0.007064
[INFO][10:23:48]: [Client #316] Model saved to /data/ykang/plato/results/test/model/lenet5_316_1127979.pth.
[INFO][10:23:48]: [Client #238] Epoch: [5/5][0/10]	Loss: 0.009094
[INFO][10:23:48]: [Client #238] Model saved to /data/ykang/plato/results/test/model/lenet5_238_1127978.pth.
[INFO][10:23:49]: [Client #79] Loading a model from /data/ykang/plato/results/test/model/lenet5_79_1127977.pth.
[INFO][10:23:49]: [Client #79] Model trained.
[INFO][10:23:49]: [Client #79] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:23:49]: [Server #1127936] Received 0.24 MB of payload data from client #79 (simulated).
[INFO][10:23:49]: [Client #316] Loading a model from /data/ykang/plato/results/test/model/lenet5_316_1127979.pth.
[INFO][10:23:49]: [Client #316] Model trained.
[INFO][10:23:49]: [Client #316] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:23:49]: [Server #1127936] Received 0.24 MB of payload data from client #316 (simulated).
[INFO][10:23:49]: [Client #238] Loading a model from /data/ykang/plato/results/test/model/lenet5_238_1127978.pth.
[INFO][10:23:49]: [Client #238] Model trained.
[INFO][10:23:49]: [Client #238] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:23:49]: [Server #1127936] Received 0.24 MB of payload data from client #238 (simulated).
[INFO][10:23:49]: [Server #1127936] Selecting client #131 for training.
[INFO][10:23:49]: [Server #1127936] Sending the current model to client #131 (simulated).
[INFO][10:23:49]: [Server #1127936] Sending 0.24 MB of payload data to client #131 (simulated).
[INFO][10:23:49]: [Server #1127936] Selecting client #329 for training.
[INFO][10:23:49]: [Server #1127936] Sending the current model to client #329 (simulated).
[INFO][10:23:49]: [Server #1127936] Sending 0.24 MB of payload data to client #329 (simulated).
[INFO][10:23:49]: [Server #1127936] Selecting client #188 for training.
[INFO][10:23:49]: [Server #1127936] Sending the current model to client #188 (simulated).
[INFO][10:23:49]: [Client #131] Selected by the server.
[INFO][10:23:49]: [Client #131] Loading its data source...
[INFO][10:23:49]: [Client #131] Dataset size: 60000
[INFO][10:23:49]: [Client #131] Sampler: noniid
[INFO][10:23:49]: [Server #1127936] Sending 0.24 MB of payload data to client #188 (simulated).
[INFO][10:23:49]: [Client #329] Selected by the server.
[INFO][10:23:49]: [Client #329] Loading its data source...
[INFO][10:23:49]: [Client #188] Selected by the server.
[INFO][10:23:49]: [Client #329] Dataset size: 60000
[INFO][10:23:49]: [Client #188] Loading its data source...
[INFO][10:23:49]: [Client #329] Sampler: noniid
[INFO][10:23:49]: [Client #188] Dataset size: 60000
[INFO][10:23:49]: [Client #188] Sampler: noniid
[INFO][10:23:49]: [Client #131] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:23:49]: [Client #329] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:23:49]: [Client #188] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:23:49]: [93m[1m[Client #131] Started training in communication round #65.[0m
[INFO][10:23:49]: [93m[1m[Client #188] Started training in communication round #65.[0m
[INFO][10:23:49]: [93m[1m[Client #329] Started training in communication round #65.[0m
[INFO][10:23:51]: [Client #131] Loading the dataset.
[INFO][10:23:51]: [Client #329] Loading the dataset.
[INFO][10:23:51]: [Client #188] Loading the dataset.
[INFO][10:23:57]: [Client #131] Epoch: [1/5][0/10]	Loss: 0.002592
[INFO][10:23:57]: [Client #188] Epoch: [1/5][0/10]	Loss: 0.003991
[INFO][10:23:57]: [Client #329] Epoch: [1/5][0/10]	Loss: 0.028942
[INFO][10:23:57]: [Client #131] Epoch: [2/5][0/10]	Loss: 0.000751
[INFO][10:23:57]: [Client #188] Epoch: [2/5][0/10]	Loss: 0.001266
[INFO][10:23:57]: [Client #329] Epoch: [2/5][0/10]	Loss: 0.000696
[INFO][10:23:57]: [Client #131] Epoch: [3/5][0/10]	Loss: 0.000799
[INFO][10:23:58]: [Client #188] Epoch: [3/5][0/10]	Loss: 0.000206
[INFO][10:23:58]: [Client #131] Epoch: [4/5][0/10]	Loss: 0.000014
[INFO][10:23:58]: [Client #329] Epoch: [3/5][0/10]	Loss: 0.000004
[INFO][10:23:58]: [Client #188] Epoch: [4/5][0/10]	Loss: 0.000045
[INFO][10:23:58]: [Client #131] Epoch: [5/5][0/10]	Loss: 0.040167
[INFO][10:23:58]: [Client #329] Epoch: [4/5][0/10]	Loss: 0.000023
[INFO][10:23:58]: [Client #188] Epoch: [5/5][0/10]	Loss: 0.000932
[INFO][10:23:58]: [Client #131] Model saved to /data/ykang/plato/results/test/model/lenet5_131_1127977.pth.
[INFO][10:23:58]: [Client #188] Model saved to /data/ykang/plato/results/test/model/lenet5_188_1127979.pth.
[INFO][10:23:58]: [Client #329] Epoch: [5/5][0/10]	Loss: 0.000006
[INFO][10:23:58]: [Client #329] Model saved to /data/ykang/plato/results/test/model/lenet5_329_1127978.pth.
[INFO][10:23:58]: [Client #131] Loading a model from /data/ykang/plato/results/test/model/lenet5_131_1127977.pth.
[INFO][10:23:59]: [Client #131] Model trained.
[INFO][10:23:59]: [Client #131] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:23:59]: [Server #1127936] Received 0.24 MB of payload data from client #131 (simulated).
[INFO][10:23:59]: [Client #188] Loading a model from /data/ykang/plato/results/test/model/lenet5_188_1127979.pth.
[INFO][10:23:59]: [Client #188] Model trained.
[INFO][10:23:59]: [Client #188] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:23:59]: [Server #1127936] Received 0.24 MB of payload data from client #188 (simulated).
[INFO][10:23:59]: [Client #329] Loading a model from /data/ykang/plato/results/test/model/lenet5_329_1127978.pth.
[INFO][10:23:59]: [Client #329] Model trained.
[INFO][10:23:59]: [Client #329] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:23:59]: [Server #1127936] Received 0.24 MB of payload data from client #329 (simulated).
[INFO][10:23:59]: [Server #1127936] Selecting client #149 for training.
[INFO][10:23:59]: [Server #1127936] Sending the current model to client #149 (simulated).
[INFO][10:23:59]: [Server #1127936] Sending 0.24 MB of payload data to client #149 (simulated).
[INFO][10:23:59]: [Server #1127936] Selecting client #457 for training.
[INFO][10:23:59]: [Server #1127936] Sending the current model to client #457 (simulated).
[INFO][10:23:59]: [Server #1127936] Sending 0.24 MB of payload data to client #457 (simulated).
[INFO][10:23:59]: [Server #1127936] Selecting client #473 for training.
[INFO][10:23:59]: [Server #1127936] Sending the current model to client #473 (simulated).
[INFO][10:23:59]: [Client #149] Selected by the server.
[INFO][10:23:59]: [Client #149] Loading its data source...
[INFO][10:23:59]: [Client #149] Dataset size: 60000
[INFO][10:23:59]: [Client #149] Sampler: noniid
[INFO][10:23:59]: [Server #1127936] Sending 0.24 MB of payload data to client #473 (simulated).
[INFO][10:23:59]: [Client #457] Selected by the server.
[INFO][10:23:59]: [Client #457] Loading its data source...
[INFO][10:23:59]: [Client #457] Dataset size: 60000
[INFO][10:23:59]: [Client #457] Sampler: noniid
[INFO][10:23:59]: [Client #473] Selected by the server.
[INFO][10:23:59]: [Client #473] Loading its data source...
[INFO][10:23:59]: [Client #473] Dataset size: 60000
[INFO][10:23:59]: [Client #473] Sampler: noniid
[INFO][10:23:59]: [Client #149] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:23:59]: [Client #457] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:23:59]: [Client #473] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:23:59]: [93m[1m[Client #149] Started training in communication round #65.[0m
[INFO][10:23:59]: [93m[1m[Client #473] Started training in communication round #65.[0m
[INFO][10:23:59]: [93m[1m[Client #457] Started training in communication round #65.[0m
[INFO][10:24:01]: [Client #473] Loading the dataset.
[INFO][10:24:01]: [Client #149] Loading the dataset.
[INFO][10:24:01]: [Client #457] Loading the dataset.
[INFO][10:24:07]: [Client #473] Epoch: [1/5][0/10]	Loss: 0.005819
[INFO][10:24:07]: [Client #149] Epoch: [1/5][0/10]	Loss: 0.004844
[INFO][10:24:07]: [Client #473] Epoch: [2/5][0/10]	Loss: 0.006576
[INFO][10:24:07]: [Client #457] Epoch: [1/5][0/10]	Loss: 0.009455
[INFO][10:24:07]: [Client #149] Epoch: [2/5][0/10]	Loss: 0.002302
[INFO][10:24:07]: [Client #457] Epoch: [2/5][0/10]	Loss: 0.000865
[INFO][10:24:07]: [Client #473] Epoch: [3/5][0/10]	Loss: 0.000359
[INFO][10:24:07]: [Client #149] Epoch: [3/5][0/10]	Loss: 0.000188
[INFO][10:24:07]: [Client #457] Epoch: [3/5][0/10]	Loss: 0.000053
[INFO][10:24:07]: [Client #473] Epoch: [4/5][0/10]	Loss: 0.000107
[INFO][10:24:07]: [Client #149] Epoch: [4/5][0/10]	Loss: 0.000540
[INFO][10:24:07]: [Client #457] Epoch: [4/5][0/10]	Loss: 0.000081
[INFO][10:24:08]: [Client #149] Epoch: [5/5][0/10]	Loss: 0.000319
[INFO][10:24:08]: [Client #473] Epoch: [5/5][0/10]	Loss: 0.000002
[INFO][10:24:08]: [Client #457] Epoch: [5/5][0/10]	Loss: 0.000314
[INFO][10:24:08]: [Client #149] Model saved to /data/ykang/plato/results/test/model/lenet5_149_1127977.pth.
[INFO][10:24:08]: [Client #473] Model saved to /data/ykang/plato/results/test/model/lenet5_473_1127979.pth.
[INFO][10:24:08]: [Client #457] Model saved to /data/ykang/plato/results/test/model/lenet5_457_1127978.pth.
[INFO][10:24:08]: [Client #149] Loading a model from /data/ykang/plato/results/test/model/lenet5_149_1127977.pth.
[INFO][10:24:08]: [Client #149] Model trained.
[INFO][10:24:08]: [Client #149] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:24:08]: [Server #1127936] Received 0.24 MB of payload data from client #149 (simulated).
[INFO][10:24:08]: [Client #473] Loading a model from /data/ykang/plato/results/test/model/lenet5_473_1127979.pth.
[INFO][10:24:09]: [Client #473] Model trained.
[INFO][10:24:09]: [Client #473] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:24:09]: [Server #1127936] Received 0.24 MB of payload data from client #473 (simulated).
[INFO][10:24:09]: [Client #457] Loading a model from /data/ykang/plato/results/test/model/lenet5_457_1127978.pth.
[INFO][10:24:09]: [Client #457] Model trained.
[INFO][10:24:09]: [Client #457] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:24:09]: [Server #1127936] Received 0.24 MB of payload data from client #457 (simulated).
[INFO][10:24:09]: [Server #1127936] Selecting client #310 for training.
[INFO][10:24:09]: [Server #1127936] Sending the current model to client #310 (simulated).
[INFO][10:24:09]: [Server #1127936] Sending 0.24 MB of payload data to client #310 (simulated).
[INFO][10:24:09]: [Client #310] Selected by the server.
[INFO][10:24:09]: [Client #310] Loading its data source...
[INFO][10:24:09]: [Client #310] Dataset size: 60000
[INFO][10:24:09]: [Client #310] Sampler: noniid
[INFO][10:24:09]: [Client #310] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:24:09]: [93m[1m[Client #310] Started training in communication round #65.[0m
[INFO][10:24:11]: [Client #310] Loading the dataset.
[INFO][10:24:16]: [Client #310] Epoch: [1/5][0/10]	Loss: 0.001355
[INFO][10:24:16]: [Client #310] Epoch: [2/5][0/10]	Loss: 0.000545
[INFO][10:24:16]: [Client #310] Epoch: [3/5][0/10]	Loss: 0.002441
[INFO][10:24:16]: [Client #310] Epoch: [4/5][0/10]	Loss: 0.105785
[INFO][10:24:16]: [Client #310] Epoch: [5/5][0/10]	Loss: 0.031968
[INFO][10:24:16]: [Client #310] Model saved to /data/ykang/plato/results/test/model/lenet5_310_1127977.pth.
[INFO][10:24:17]: [Client #310] Loading a model from /data/ykang/plato/results/test/model/lenet5_310_1127977.pth.
[INFO][10:24:17]: [Client #310] Model trained.
[INFO][10:24:17]: [Client #310] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:24:17]: [Server #1127936] Received 0.24 MB of payload data from client #310 (simulated).
[INFO][10:24:17]: [Server #1127936] Adding client #352 to the list of clients for aggregation.
[INFO][10:24:17]: [Server #1127936] Adding client #110 to the list of clients for aggregation.
[INFO][10:24:17]: [Server #1127936] Adding client #365 to the list of clients for aggregation.
[INFO][10:24:17]: [Server #1127936] Adding client #265 to the list of clients for aggregation.
[INFO][10:24:17]: [Server #1127936] Adding client #200 to the list of clients for aggregation.
[INFO][10:24:17]: [Server #1127936] Adding client #103 to the list of clients for aggregation.
[INFO][10:24:17]: [Server #1127936] Adding client #448 to the list of clients for aggregation.
[INFO][10:24:17]: [Server #1127936] Adding client #310 to the list of clients for aggregation.
[INFO][10:24:17]: [Server #1127936] Adding client #473 to the list of clients for aggregation.
[INFO][10:24:17]: [Server #1127936] Adding client #316 to the list of clients for aggregation.
[INFO][10:24:17]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01476522 0.         0.         0.         0.         0.
 0.         0.00586512 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00354303 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00467174 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01642635 0.         0.
 0.         0.         0.         0.0036371  0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00239099 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01018059 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.02455228 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00638843 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 0. 1. 0. 0. 0. 1. 0. 1. 3. 1. 0. 0. 0. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 2. 0. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 2. 1. 0. 1. 0.
 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 2. 1. 6. 0. 1. 2. 1. 0. 1. 1. 2. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 0. 6. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 2. 0. 0.
 1. 0. 1. 2. 0. 0. 0. 2. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1.
 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 4. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0.
 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 4. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 3. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 0. 0. 0. 6. 1. 1.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 6. 1. 0. 0. 1. 0. 0. 0. 2. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 2. 2. 1. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01476522 0.         0.         0.         0.         0.
 0.         0.00586512 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00354303 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00467174 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01642635 0.         0.
 0.         0.         0.         0.0036371  0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00239099 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01018059 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.02455228 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00638843 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:24:19]: [Server #1127936] Global model accuracy: 95.71%

[INFO][10:24:19]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_65.pth.
[INFO][10:24:19]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_65.pth.
[INFO][10:24:19]: [93m[1m
[Server #1127936] Starting round 66/100.[0m
[0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.002 0.1
 0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  4e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  4e-05  8e-10  8e-10
 6:  6.8876e+00  6.8875e+00  4e-05  4e-10  4e-10
 7:  6.8875e+00  6.8875e+00  4e-05  1e-09  4e-11
 8:  6.8875e+00  6.8875e+00  3e-05  1e-09  4e-11
 9:  6.8875e+00  6.8875e+00  1e-05  4e-09  2e-10
10:  6.8875e+00  6.8875e+00  1e-06  4e-09  2e-10
Optimal solution found.
The calculated probability is:  [4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 8.71187779e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 1.17886365e-03
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 6.17641498e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 5.65069498e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84591943e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84629997e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 5.22806573e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 9.74983109e-01 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 1.72638349e-04
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84625906e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05 4.84631959e-05 4.84631959e-05
 4.84631959e-05 4.84631959e-05]
current clients pool:  [INFO][10:24:20]: [Server #1127936] Selected clients: [365 183 377 448 286  48  68  56  92  67]
[INFO][10:24:20]: [Server #1127936] Selecting client #365 for training.
[INFO][10:24:20]: [Server #1127936] Sending the current model to client #365 (simulated).
[INFO][10:24:20]: [Server #1127936] Sending 0.24 MB of payload data to client #365 (simulated).
[INFO][10:24:20]: [Server #1127936] Selecting client #183 for training.
[INFO][10:24:20]: [Server #1127936] Sending the current model to client #183 (simulated).
[INFO][10:24:20]: [Server #1127936] Sending 0.24 MB of payload data to client #183 (simulated).
[INFO][10:24:20]: [Server #1127936] Selecting client #377 for training.
[INFO][10:24:20]: [Server #1127936] Sending the current model to client #377 (simulated).
[INFO][10:24:20]: [Client #365] Selected by the server.
[INFO][10:24:20]: [Client #365] Loading its data source...
[INFO][10:24:20]: [Client #365] Dataset size: 60000
[INFO][10:24:20]: [Client #365] Sampler: noniid
[INFO][10:24:20]: [Server #1127936] Sending 0.24 MB of payload data to client #377 (simulated).
[INFO][10:24:20]: [Client #183] Selected by the server.
[INFO][10:24:20]: [Client #183] Loading its data source...
[INFO][10:24:20]: [Client #183] Dataset size: 60000
[INFO][10:24:20]: [Client #183] Sampler: noniid
[INFO][10:24:20]: [Client #377] Selected by the server.
[INFO][10:24:20]: [Client #365] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:24:20]: [Client #377] Loading its data source...
[INFO][10:24:20]: [Client #377] Dataset size: 60000
[INFO][10:24:20]: [Client #377] Sampler: noniid
[INFO][10:24:20]: [Client #183] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:24:20]: [Client #377] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:24:20]: [93m[1m[Client #183] Started training in communication round #66.[0m
[INFO][10:24:20]: [93m[1m[Client #365] Started training in communication round #66.[0m
[INFO][10:24:20]: [93m[1m[Client #377] Started training in communication round #66.[0m
[INFO][10:24:22]: [Client #365] Loading the dataset.
[INFO][10:24:22]: [Client #377] Loading the dataset.
[INFO][10:24:22]: [Client #183] Loading the dataset.
[INFO][10:24:28]: [Client #365] Epoch: [1/5][0/10]	Loss: 0.003299
[INFO][10:24:28]: [Client #365] Epoch: [2/5][0/10]	Loss: 0.002084
[INFO][10:24:28]: [Client #183] Epoch: [1/5][0/10]	Loss: 0.015753
[INFO][10:24:28]: [Client #377] Epoch: [1/5][0/10]	Loss: 0.007129
[INFO][10:24:28]: [Client #377] Epoch: [2/5][0/10]	Loss: 0.002139
[INFO][10:24:28]: [Client #365] Epoch: [3/5][0/10]	Loss: 0.000085
[INFO][10:24:28]: [Client #183] Epoch: [2/5][0/10]	Loss: 0.000339
[INFO][10:24:29]: [Client #377] Epoch: [3/5][0/10]	Loss: 0.000074
[INFO][10:24:29]: [Client #365] Epoch: [4/5][0/10]	Loss: 0.001128
[INFO][10:24:29]: [Client #183] Epoch: [3/5][0/10]	Loss: 0.000173
[INFO][10:24:29]: [Client #377] Epoch: [4/5][0/10]	Loss: 0.000961
[INFO][10:24:29]: [Client #365] Epoch: [5/5][0/10]	Loss: 0.007019
[INFO][10:24:29]: [Client #183] Epoch: [4/5][0/10]	Loss: 0.024170
[INFO][10:24:29]: [Client #377] Epoch: [5/5][0/10]	Loss: 0.005275
[INFO][10:24:29]: [Client #365] Model saved to /data/ykang/plato/results/test/model/lenet5_365_1127977.pth.
[INFO][10:24:29]: [Client #377] Model saved to /data/ykang/plato/results/test/model/lenet5_377_1127979.pth.
[INFO][10:24:29]: [Client #183] Epoch: [5/5][0/10]	Loss: 0.016662
[INFO][10:24:29]: [Client #183] Model saved to /data/ykang/plato/results/test/model/lenet5_183_1127978.pth.
[INFO][10:24:29]: [Client #365] Loading a model from /data/ykang/plato/results/test/model/lenet5_365_1127977.pth.
[INFO][10:24:30]: [Client #365] Model trained.
[INFO][10:24:30]: [Client #365] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:24:30]: [Server #1127936] Received 0.24 MB of payload data from client #365 (simulated).
[INFO][10:24:30]: [Client #377] Loading a model from /data/ykang/plato/results/test/model/lenet5_377_1127979.pth.
[INFO][10:24:30]: [Client #377] Model trained.
[INFO][10:24:30]: [Client #377] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:24:30]: [Server #1127936] Received 0.24 MB of payload data from client #377 (simulated).
[INFO][10:24:30]: [Client #183] Loading a model from /data/ykang/plato/results/test/model/lenet5_183_1127978.pth.
[INFO][10:24:30]: [Client #183] Model trained.
[INFO][10:24:30]: [Client #183] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:24:30]: [Server #1127936] Received 0.24 MB of payload data from client #183 (simulated).
[INFO][10:24:30]: [Server #1127936] Selecting client #448 for training.
[INFO][10:24:30]: [Server #1127936] Sending the current model to client #448 (simulated).
[INFO][10:24:30]: [Server #1127936] Sending 0.24 MB of payload data to client #448 (simulated).
[INFO][10:24:30]: [Server #1127936] Selecting client #286 for training.
[INFO][10:24:30]: [Server #1127936] Sending the current model to client #286 (simulated).
[INFO][10:24:30]: [Server #1127936] Sending 0.24 MB of payload data to client #286 (simulated).
[INFO][10:24:30]: [Server #1127936] Selecting client #48 for training.
[INFO][10:24:30]: [Server #1127936] Sending the current model to client #48 (simulated).
[INFO][10:24:30]: [Client #448] Selected by the server.
[INFO][10:24:30]: [Client #448] Loading its data source...
[INFO][10:24:30]: [Client #448] Dataset size: 60000
[INFO][10:24:30]: [Client #448] Sampler: noniid
[INFO][10:24:30]: [Server #1127936] Sending 0.24 MB of payload data to client #48 (simulated).
[INFO][10:24:30]: [Client #286] Selected by the server.
[INFO][10:24:30]: [Client #48] Selected by the server.
[INFO][10:24:30]: [Client #48] Loading its data source...
[INFO][10:24:30]: [Client #286] Loading its data source...
[INFO][10:24:30]: [Client #48] Dataset size: 60000
[INFO][10:24:30]: [Client #286] Dataset size: 60000
[INFO][10:24:30]: [Client #48] Sampler: noniid
[INFO][10:24:30]: [Client #286] Sampler: noniid
[INFO][10:24:30]: [Client #448] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:24:30]: [Client #286] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:24:30]: [93m[1m[Client #448] Started training in communication round #66.[0m
[INFO][10:24:30]: [Client #48] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:24:30]: [93m[1m[Client #48] Started training in communication round #66.[0m
[INFO][10:24:30]: [93m[1m[Client #286] Started training in communication round #66.[0m
[INFO][10:24:32]: [Client #286] Loading the dataset.
[INFO][10:24:32]: [Client #448] Loading the dataset.
[INFO][10:24:32]: [Client #48] Loading the dataset.
[INFO][10:24:38]: [Client #48] Epoch: [1/5][0/10]	Loss: 0.020835
[INFO][10:24:38]: [Client #286] Epoch: [1/5][0/10]	Loss: 0.005203
[INFO][10:24:38]: [Client #448] Epoch: [1/5][0/10]	Loss: 0.003674
[INFO][10:24:38]: [Client #48] Epoch: [2/5][0/10]	Loss: 0.002974
[INFO][10:24:38]: [Client #286] Epoch: [2/5][0/10]	Loss: 0.001251
[INFO][10:24:38]: [Client #448] Epoch: [2/5][0/10]	Loss: 0.003179
[INFO][10:24:38]: [Client #286] Epoch: [3/5][0/10]	Loss: 0.000015
[INFO][10:24:38]: [Client #48] Epoch: [3/5][0/10]	Loss: 0.000022
[INFO][10:24:38]: [Client #448] Epoch: [3/5][0/10]	Loss: 0.000152
[INFO][10:24:38]: [Client #48] Epoch: [4/5][0/10]	Loss: 0.000440
[INFO][10:24:38]: [Client #286] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:24:38]: [Client #448] Epoch: [4/5][0/10]	Loss: 0.000328
[INFO][10:24:38]: [Client #48] Epoch: [5/5][0/10]	Loss: 0.003548
[INFO][10:24:38]: [Client #286] Epoch: [5/5][0/10]	Loss: 0.000274
[INFO][10:24:38]: [Client #48] Model saved to /data/ykang/plato/results/test/model/lenet5_48_1127979.pth.
[INFO][10:24:38]: [Client #448] Epoch: [5/5][0/10]	Loss: 0.000204
[INFO][10:24:39]: [Client #286] Model saved to /data/ykang/plato/results/test/model/lenet5_286_1127978.pth.
[INFO][10:24:39]: [Client #448] Model saved to /data/ykang/plato/results/test/model/lenet5_448_1127977.pth.
[INFO][10:24:39]: [Client #48] Loading a model from /data/ykang/plato/results/test/model/lenet5_48_1127979.pth.
[INFO][10:24:39]: [Client #48] Model trained.
[INFO][10:24:39]: [Client #48] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:24:39]: [Server #1127936] Received 0.24 MB of payload data from client #48 (simulated).
[INFO][10:24:39]: [Client #448] Loading a model from /data/ykang/plato/results/test/model/lenet5_448_1127977.pth.
[INFO][10:24:39]: [Client #448] Model trained.
[INFO][10:24:39]: [Client #448] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:24:39]: [Server #1127936] Received 0.24 MB of payload data from client #448 (simulated).
[INFO][10:24:39]: [Client #286] Loading a model from /data/ykang/plato/results/test/model/lenet5_286_1127978.pth.
[INFO][10:24:39]: [Client #286] Model trained.
[INFO][10:24:39]: [Client #286] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:24:39]: [Server #1127936] Received 0.24 MB of payload data from client #286 (simulated).
[INFO][10:24:39]: [Server #1127936] Selecting client #68 for training.
[INFO][10:24:39]: [Server #1127936] Sending the current model to client #68 (simulated).
[INFO][10:24:39]: [Server #1127936] Sending 0.24 MB of payload data to client #68 (simulated).
[INFO][10:24:39]: [Server #1127936] Selecting client #56 for training.
[INFO][10:24:39]: [Server #1127936] Sending the current model to client #56 (simulated).
[INFO][10:24:39]: [Server #1127936] Sending 0.24 MB of payload data to client #56 (simulated).
[INFO][10:24:39]: [Server #1127936] Selecting client #92 for training.
[INFO][10:24:39]: [Server #1127936] Sending the current model to client #92 (simulated).
[INFO][10:24:39]: [Client #68] Selected by the server.
[INFO][10:24:39]: [Client #68] Loading its data source...
[INFO][10:24:39]: [Client #68] Dataset size: 60000
[INFO][10:24:39]: [Client #68] Sampler: noniid
[INFO][10:24:39]: [Server #1127936] Sending 0.24 MB of payload data to client #92 (simulated).
[INFO][10:24:39]: [Client #56] Selected by the server.
[INFO][10:24:39]: [Client #68] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:24:39]: [Client #56] Loading its data source...
[INFO][10:24:39]: [Client #56] Dataset size: 60000
[INFO][10:24:39]: [Client #56] Sampler: noniid
[INFO][10:24:39]: [Client #92] Selected by the server.
[INFO][10:24:39]: [Client #92] Loading its data source...
[INFO][10:24:39]: [Client #92] Dataset size: 60000
[INFO][10:24:39]: [Client #92] Sampler: noniid
[INFO][10:24:39]: [Client #92] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:24:39]: [Client #56] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:24:39]: [93m[1m[Client #68] Started training in communication round #66.[0m
[INFO][10:24:39]: [93m[1m[Client #92] Started training in communication round #66.[0m
[INFO][10:24:39]: [93m[1m[Client #56] Started training in communication round #66.[0m
[INFO][10:24:42]: [Client #68] Loading the dataset.
[INFO][10:24:42]: [Client #56] Loading the dataset.
[INFO][10:24:42]: [Client #92] Loading the dataset.
[INFO][10:24:48]: [Client #92] Epoch: [1/5][0/10]	Loss: 0.000675
[INFO][10:24:48]: [Client #68] Epoch: [1/5][0/10]	Loss: 0.005694
[INFO][10:24:48]: [Client #92] Epoch: [2/5][0/10]	Loss: 0.005816
[INFO][10:24:48]: [Client #56] Epoch: [1/5][0/10]	Loss: 0.001166
[INFO][10:24:48]: [Client #68] Epoch: [2/5][0/10]	Loss: 0.000209
[INFO][10:24:48]: [Client #92] Epoch: [3/5][0/10]	Loss: 0.000125
[INFO][10:24:48]: [Client #56] Epoch: [2/5][0/10]	Loss: 0.000653
[INFO][10:24:48]: [Client #68] Epoch: [3/5][0/10]	Loss: 0.000869
[INFO][10:24:48]: [Client #92] Epoch: [4/5][0/10]	Loss: 0.000401
[INFO][10:24:48]: [Client #56] Epoch: [3/5][0/10]	Loss: 0.000268
[INFO][10:24:48]: [Client #68] Epoch: [4/5][0/10]	Loss: 0.000013
[INFO][10:24:48]: [Client #56] Epoch: [4/5][0/10]	Loss: 0.000079
[INFO][10:24:48]: [Client #92] Epoch: [5/5][0/10]	Loss: 0.002291
[INFO][10:24:48]: [Client #68] Epoch: [5/5][0/10]	Loss: 0.000085
[INFO][10:24:48]: [Client #92] Model saved to /data/ykang/plato/results/test/model/lenet5_92_1127979.pth.
[INFO][10:24:48]: [Client #68] Model saved to /data/ykang/plato/results/test/model/lenet5_68_1127977.pth.
[INFO][10:24:48]: [Client #56] Epoch: [5/5][0/10]	Loss: 0.000257
[INFO][10:24:49]: [Client #56] Model saved to /data/ykang/plato/results/test/model/lenet5_56_1127978.pth.
[INFO][10:24:49]: [Client #92] Loading a model from /data/ykang/plato/results/test/model/lenet5_92_1127979.pth.
[INFO][10:24:49]: [Client #92] Model trained.
[INFO][10:24:49]: [Client #92] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:24:49]: [Server #1127936] Received 0.24 MB of payload data from client #92 (simulated).
[INFO][10:24:49]: [Client #68] Loading a model from /data/ykang/plato/results/test/model/lenet5_68_1127977.pth.
[INFO][10:24:49]: [Client #68] Model trained.
[INFO][10:24:49]: [Client #68] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:24:49]: [Server #1127936] Received 0.24 MB of payload data from client #68 (simulated).
[INFO][10:24:49]: [Client #56] Loading a model from /data/ykang/plato/results/test/model/lenet5_56_1127978.pth.
[INFO][10:24:49]: [Client #56] Model trained.
[INFO][10:24:49]: [Client #56] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:24:49]: [Server #1127936] Received 0.24 MB of payload data from client #56 (simulated).
[INFO][10:24:49]: [Server #1127936] Selecting client #67 for training.
[INFO][10:24:49]: [Server #1127936] Sending the current model to client #67 (simulated).
[INFO][10:24:49]: [Server #1127936] Sending 0.24 MB of payload data to client #67 (simulated).
[INFO][10:24:49]: [Client #67] Selected by the server.
[INFO][10:24:49]: [Client #67] Loading its data source...
[INFO][10:24:49]: [Client #67] Dataset size: 60000
[INFO][10:24:49]: [Client #67] Sampler: noniid
[INFO][10:24:49]: [Client #67] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:24:49]: [93m[1m[Client #67] Started training in communication round #66.[0m
[INFO][10:24:51]: [Client #67] Loading the dataset.
[INFO][10:24:57]: [Client #67] Epoch: [1/5][0/10]	Loss: 0.005092
[INFO][10:24:57]: [Client #67] Epoch: [2/5][0/10]	Loss: 0.000478
[INFO][10:24:57]: [Client #67] Epoch: [3/5][0/10]	Loss: 0.000271
[INFO][10:24:57]: [Client #67] Epoch: [4/5][0/10]	Loss: 0.000166
[INFO][10:24:57]: [Client #67] Epoch: [5/5][0/10]	Loss: 0.000658
[INFO][10:24:57]: [Client #67] Model saved to /data/ykang/plato/results/test/model/lenet5_67_1127977.pth.
[INFO][10:24:58]: [Client #67] Loading a model from /data/ykang/plato/results/test/model/lenet5_67_1127977.pth.
[INFO][10:24:58]: [Client #67] Model trained.
[INFO][10:24:58]: [Client #67] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:24:58]: [Server #1127936] Received 0.24 MB of payload data from client #67 (simulated).
[INFO][10:24:58]: [Server #1127936] Adding client #329 to the list of clients for aggregation.
[INFO][10:24:58]: [Server #1127936] Adding client #149 to the list of clients for aggregation.
[INFO][10:24:58]: [Server #1127936] Adding client #131 to the list of clients for aggregation.
[INFO][10:24:58]: [Server #1127936] Adding client #238 to the list of clients for aggregation.
[INFO][10:24:58]: [Server #1127936] Adding client #188 to the list of clients for aggregation.
[INFO][10:24:58]: [Server #1127936] Adding client #79 to the list of clients for aggregation.
[INFO][10:24:58]: [Server #1127936] Adding client #457 to the list of clients for aggregation.
[INFO][10:24:58]: [Server #1127936] Adding client #56 to the list of clients for aggregation.
[INFO][10:24:58]: [Server #1127936] Adding client #48 to the list of clients for aggregation.
[INFO][10:24:58]: [Server #1127936] Adding client #68 to the list of clients for aggregation.
[INFO][10:24:58]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 458, 459, 460, 461, 462, 463, 464, 465, 466, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01912701
 0.         0.         0.         0.         0.         0.
 0.         0.00127183 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00688998 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.02595611 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00486542 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.0054645  0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00607653 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00347009 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00568061 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00716195 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 0. 1. 0. 0. 0. 1. 0. 1. 3. 1. 0. 0. 0. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 2. 0. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 2. 1. 0. 1. 0.
 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 2. 1. 6. 0. 1. 2. 1. 0. 1. 1. 2. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 0. 6. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 2. 0. 0.
 1. 0. 1. 2. 0. 0. 0. 2. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1.
 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 4. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0.
 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 4. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 3. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 0. 0. 0. 6. 1. 1.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 6. 1. 0. 0. 1. 0. 0. 0. 2. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 2. 2. 1. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01912701
 0.         0.         0.         0.         0.         0.
 0.         0.00127183 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00688998 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.02595611 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00486542 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.0054645  0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00607653 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00347009 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00568061 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00716195 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:25:00]: [Server #1127936] Global model accuracy: 95.62%

[INFO][10:25:00]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_66.pth.
[INFO][10:25:00]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_66.pth.
[INFO][10:25:00]: [93m[1m
[Server #1127936] Starting round 67/100.[0m
[0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  3e-05  5e-10  5e-10
 6:  6.8876e+00  6.8875e+00  3e-05  3e-10  3e-10
 7:  6.8875e+00  6.8875e+00  3e-05  7e-10  2e-11
 8:  6.8875e+00  6.8875e+00  2e-05  6e-10  2e-11
 9:  6.8875e+00  6.8875e+00  1e-05  1e-09  3e-11
10:  6.8875e+00  6.8875e+00  8e-07  2e-09  5e-11
Optimal solution found.
The calculated probability is:  [5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92122982e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92218626e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92206583e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 9.70925719e-01 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 7.54547109e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 7.80786951e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 8.09510727e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 6.99681204e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 7.90697701e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 8.65895473e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05 5.92219050e-05 5.92219050e-05
 5.92219050e-05 5.92219050e-05]
current clients pool:  [INFO][10:25:01]: [Server #1127936] Selected clients: [ 79  94 400 406 430 231 466 132 468 311]
[INFO][10:25:01]: [Server #1127936] Selecting client #79 for training.
[INFO][10:25:01]: [Server #1127936] Sending the current model to client #79 (simulated).
[INFO][10:25:01]: [Server #1127936] Sending 0.24 MB of payload data to client #79 (simulated).
[INFO][10:25:01]: [Server #1127936] Selecting client #94 for training.
[INFO][10:25:01]: [Server #1127936] Sending the current model to client #94 (simulated).
[INFO][10:25:01]: [Server #1127936] Sending 0.24 MB of payload data to client #94 (simulated).
[INFO][10:25:01]: [Server #1127936] Selecting client #400 for training.
[INFO][10:25:01]: [Server #1127936] Sending the current model to client #400 (simulated).
[INFO][10:25:01]: [Client #79] Selected by the server.
[INFO][10:25:01]: [Client #79] Loading its data source...
[INFO][10:25:01]: [Client #79] Dataset size: 60000
[INFO][10:25:01]: [Client #79] Sampler: noniid
[INFO][10:25:01]: [Server #1127936] Sending 0.24 MB of payload data to client #400 (simulated).
[INFO][10:25:01]: [Client #94] Selected by the server.
[INFO][10:25:01]: [Client #94] Loading its data source...
[INFO][10:25:01]: [Client #94] Dataset size: 60000
[INFO][10:25:01]: [Client #94] Sampler: noniid
[INFO][10:25:01]: [Client #79] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:25:01]: [Client #94] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:25:01]: [93m[1m[Client #79] Started training in communication round #67.[0m
[INFO][10:25:01]: [93m[1m[Client #94] Started training in communication round #67.[0m
[INFO][10:25:01]: [Client #400] Selected by the server.
[INFO][10:25:01]: [Client #400] Loading its data source...
[INFO][10:25:01]: [Client #400] Dataset size: 60000
[INFO][10:25:01]: [Client #400] Sampler: noniid
[INFO][10:25:01]: [Client #400] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:25:01]: [93m[1m[Client #400] Started training in communication round #67.[0m
[INFO][10:25:03]: [Client #400] Loading the dataset.
[INFO][10:25:03]: [Client #94] Loading the dataset.
[INFO][10:25:03]: [Client #79] Loading the dataset.
[INFO][10:25:09]: [Client #400] Epoch: [1/5][0/10]	Loss: 0.001128
[INFO][10:25:09]: [Client #79] Epoch: [1/5][0/10]	Loss: 0.002838
[INFO][10:25:09]: [Client #94] Epoch: [1/5][0/10]	Loss: 0.001035
[INFO][10:25:09]: [Client #400] Epoch: [2/5][0/10]	Loss: 0.002378
[INFO][10:25:09]: [Client #94] Epoch: [2/5][0/10]	Loss: 0.001659
[INFO][10:25:09]: [Client #79] Epoch: [2/5][0/10]	Loss: 0.005115
[INFO][10:25:09]: [Client #400] Epoch: [3/5][0/10]	Loss: 0.000485
[INFO][10:25:09]: [Client #94] Epoch: [3/5][0/10]	Loss: 0.000139
[INFO][10:25:09]: [Client #79] Epoch: [3/5][0/10]	Loss: 0.000039
[INFO][10:25:10]: [Client #400] Epoch: [4/5][0/10]	Loss: 0.000317
[INFO][10:25:10]: [Client #94] Epoch: [4/5][0/10]	Loss: 0.000332
[INFO][10:25:10]: [Client #79] Epoch: [4/5][0/10]	Loss: 0.000007
[INFO][10:25:10]: [Client #400] Epoch: [5/5][0/10]	Loss: 0.000007
[INFO][10:25:10]: [Client #94] Epoch: [5/5][0/10]	Loss: 0.001182
[INFO][10:25:10]: [Client #94] Model saved to /data/ykang/plato/results/test/model/lenet5_94_1127978.pth.
[INFO][10:25:10]: [Client #400] Model saved to /data/ykang/plato/results/test/model/lenet5_400_1127979.pth.
[INFO][10:25:10]: [Client #79] Epoch: [5/5][0/10]	Loss: 0.006410
[INFO][10:25:10]: [Client #79] Model saved to /data/ykang/plato/results/test/model/lenet5_79_1127977.pth.
[INFO][10:25:11]: [Client #94] Loading a model from /data/ykang/plato/results/test/model/lenet5_94_1127978.pth.
[INFO][10:25:11]: [Client #94] Model trained.
[INFO][10:25:11]: [Client #94] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:25:11]: [Server #1127936] Received 0.24 MB of payload data from client #94 (simulated).
[INFO][10:25:11]: [Client #400] Loading a model from /data/ykang/plato/results/test/model/lenet5_400_1127979.pth.
[INFO][10:25:11]: [Client #400] Model trained.
[INFO][10:25:11]: [Client #400] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:25:11]: [Server #1127936] Received 0.24 MB of payload data from client #400 (simulated).
[INFO][10:25:11]: [Client #79] Loading a model from /data/ykang/plato/results/test/model/lenet5_79_1127977.pth.
[INFO][10:25:11]: [Client #79] Model trained.
[INFO][10:25:11]: [Client #79] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:25:11]: [Server #1127936] Received 0.24 MB of payload data from client #79 (simulated).
[INFO][10:25:11]: [Server #1127936] Selecting client #406 for training.
[INFO][10:25:11]: [Server #1127936] Sending the current model to client #406 (simulated).
[INFO][10:25:11]: [Server #1127936] Sending 0.24 MB of payload data to client #406 (simulated).
[INFO][10:25:11]: [Server #1127936] Selecting client #430 for training.
[INFO][10:25:11]: [Server #1127936] Sending the current model to client #430 (simulated).
[INFO][10:25:11]: [Server #1127936] Sending 0.24 MB of payload data to client #430 (simulated).
[INFO][10:25:11]: [Server #1127936] Selecting client #231 for training.
[INFO][10:25:11]: [Server #1127936] Sending the current model to client #231 (simulated).
[INFO][10:25:11]: [Client #406] Selected by the server.
[INFO][10:25:11]: [Client #406] Loading its data source...
[INFO][10:25:11]: [Client #406] Dataset size: 60000
[INFO][10:25:11]: [Client #406] Sampler: noniid
[INFO][10:25:11]: [Server #1127936] Sending 0.24 MB of payload data to client #231 (simulated).
[INFO][10:25:11]: [Client #430] Selected by the server.
[INFO][10:25:11]: [Client #430] Loading its data source...
[INFO][10:25:11]: [Client #430] Dataset size: 60000
[INFO][10:25:11]: [Client #430] Sampler: noniid
[INFO][10:25:11]: [Client #231] Selected by the server.
[INFO][10:25:11]: [Client #231] Loading its data source...
[INFO][10:25:11]: [Client #231] Dataset size: 60000
[INFO][10:25:11]: [Client #231] Sampler: noniid
[INFO][10:25:11]: [Client #430] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:25:11]: [Client #231] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:25:11]: [Client #406] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:25:11]: [93m[1m[Client #430] Started training in communication round #67.[0m
[INFO][10:25:11]: [93m[1m[Client #231] Started training in communication round #67.[0m
[INFO][10:25:11]: [93m[1m[Client #406] Started training in communication round #67.[0m
[INFO][10:25:13]: [Client #406] Loading the dataset.
[INFO][10:25:13]: [Client #430] Loading the dataset.
[INFO][10:25:13]: [Client #231] Loading the dataset.
[INFO][10:25:19]: [Client #406] Epoch: [1/5][0/10]	Loss: 0.001018
[INFO][10:25:19]: [Client #430] Epoch: [1/5][0/10]	Loss: 0.002322
[INFO][10:25:19]: [Client #231] Epoch: [1/5][0/10]	Loss: 0.008029
[INFO][10:25:19]: [Client #406] Epoch: [2/5][0/10]	Loss: 0.003525
[INFO][10:25:19]: [Client #430] Epoch: [2/5][0/10]	Loss: 0.000780
[INFO][10:25:19]: [Client #231] Epoch: [2/5][0/10]	Loss: 0.000389
[INFO][10:25:19]: [Client #406] Epoch: [3/5][0/10]	Loss: 0.000815
[INFO][10:25:19]: [Client #430] Epoch: [3/5][0/10]	Loss: 0.000344
[INFO][10:25:19]: [Client #231] Epoch: [3/5][0/10]	Loss: 0.000045
[INFO][10:25:19]: [Client #430] Epoch: [4/5][0/10]	Loss: 0.000140
[INFO][10:25:19]: [Client #406] Epoch: [4/5][0/10]	Loss: 0.000621
[INFO][10:25:19]: [Client #231] Epoch: [4/5][0/10]	Loss: 0.000218
[INFO][10:25:19]: [Client #406] Epoch: [5/5][0/10]	Loss: 0.000236
[INFO][10:25:19]: [Client #430] Epoch: [5/5][0/10]	Loss: 0.000559
[INFO][10:25:19]: [Client #406] Model saved to /data/ykang/plato/results/test/model/lenet5_406_1127977.pth.
[INFO][10:25:19]: [Client #430] Model saved to /data/ykang/plato/results/test/model/lenet5_430_1127978.pth.
[INFO][10:25:19]: [Client #231] Epoch: [5/5][0/10]	Loss: 0.001047
[INFO][10:25:19]: [Client #231] Model saved to /data/ykang/plato/results/test/model/lenet5_231_1127979.pth.
[INFO][10:25:20]: [Client #406] Loading a model from /data/ykang/plato/results/test/model/lenet5_406_1127977.pth.
[INFO][10:25:20]: [Client #406] Model trained.
[INFO][10:25:20]: [Client #406] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:25:20]: [Server #1127936] Received 0.24 MB of payload data from client #406 (simulated).
[INFO][10:25:20]: [Client #430] Loading a model from /data/ykang/plato/results/test/model/lenet5_430_1127978.pth.
[INFO][10:25:20]: [Client #430] Model trained.
[INFO][10:25:20]: [Client #430] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:25:20]: [Server #1127936] Received 0.24 MB of payload data from client #430 (simulated).
[INFO][10:25:20]: [Client #231] Loading a model from /data/ykang/plato/results/test/model/lenet5_231_1127979.pth.
[INFO][10:25:20]: [Client #231] Model trained.
[INFO][10:25:20]: [Client #231] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:25:20]: [Server #1127936] Received 0.24 MB of payload data from client #231 (simulated).
[INFO][10:25:20]: [Server #1127936] Selecting client #466 for training.
[INFO][10:25:20]: [Server #1127936] Sending the current model to client #466 (simulated).
[INFO][10:25:20]: [Server #1127936] Sending 0.24 MB of payload data to client #466 (simulated).
[INFO][10:25:20]: [Server #1127936] Selecting client #132 for training.
[INFO][10:25:20]: [Server #1127936] Sending the current model to client #132 (simulated).
[INFO][10:25:20]: [Server #1127936] Sending 0.24 MB of payload data to client #132 (simulated).
[INFO][10:25:20]: [Server #1127936] Selecting client #468 for training.
[INFO][10:25:20]: [Server #1127936] Sending the current model to client #468 (simulated).
[INFO][10:25:20]: [Client #466] Selected by the server.
[INFO][10:25:20]: [Client #466] Loading its data source...
[INFO][10:25:20]: [Client #466] Dataset size: 60000
[INFO][10:25:20]: [Client #466] Sampler: noniid
[INFO][10:25:20]: [Server #1127936] Sending 0.24 MB of payload data to client #468 (simulated).
[INFO][10:25:20]: [Client #132] Selected by the server.
[INFO][10:25:20]: [Client #132] Loading its data source...
[INFO][10:25:20]: [Client #468] Selected by the server.
[INFO][10:25:20]: [Client #132] Dataset size: 60000
[INFO][10:25:20]: [Client #468] Loading its data source...
[INFO][10:25:20]: [Client #132] Sampler: noniid
[INFO][10:25:20]: [Client #468] Dataset size: 60000
[INFO][10:25:20]: [Client #468] Sampler: noniid
[INFO][10:25:20]: [Client #466] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:25:20]: [Client #132] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:25:20]: [Client #468] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:25:20]: [93m[1m[Client #466] Started training in communication round #67.[0m
[INFO][10:25:20]: [93m[1m[Client #468] Started training in communication round #67.[0m
[INFO][10:25:20]: [93m[1m[Client #132] Started training in communication round #67.[0m
[INFO][10:25:22]: [Client #466] Loading the dataset.
[INFO][10:25:23]: [Client #468] Loading the dataset.
[INFO][10:25:23]: [Client #132] Loading the dataset.
[INFO][10:25:29]: [Client #466] Epoch: [1/5][0/10]	Loss: 0.000866
[INFO][10:25:29]: [Client #466] Epoch: [2/5][0/10]	Loss: 0.000918
[INFO][10:25:29]: [Client #468] Epoch: [1/5][0/10]	Loss: 0.000229
[INFO][10:25:29]: [Client #132] Epoch: [1/5][0/10]	Loss: 0.032952
[INFO][10:25:29]: [Client #466] Epoch: [3/5][0/10]	Loss: 0.000161
[INFO][10:25:29]: [Client #468] Epoch: [2/5][0/10]	Loss: 0.001502
[INFO][10:25:29]: [Client #466] Epoch: [4/5][0/10]	Loss: 0.000004
[INFO][10:25:29]: [Client #132] Epoch: [2/5][0/10]	Loss: 0.000510
[INFO][10:25:29]: [Client #468] Epoch: [3/5][0/10]	Loss: 0.000187
[INFO][10:25:29]: [Client #466] Epoch: [5/5][0/10]	Loss: 0.000283
[INFO][10:25:29]: [Client #466] Model saved to /data/ykang/plato/results/test/model/lenet5_466_1127977.pth.
[INFO][10:25:29]: [Client #132] Epoch: [3/5][0/10]	Loss: 0.000314
[INFO][10:25:29]: [Client #468] Epoch: [4/5][0/10]	Loss: 0.000108
[INFO][10:25:29]: [Client #132] Epoch: [4/5][0/10]	Loss: 0.002024
[INFO][10:25:29]: [Client #468] Epoch: [5/5][0/10]	Loss: 0.000001
[INFO][10:25:29]: [Client #468] Model saved to /data/ykang/plato/results/test/model/lenet5_468_1127979.pth.
[INFO][10:25:29]: [Client #132] Epoch: [5/5][0/10]	Loss: 0.099438
[INFO][10:25:29]: [Client #132] Model saved to /data/ykang/plato/results/test/model/lenet5_132_1127978.pth.
[INFO][10:25:30]: [Client #466] Loading a model from /data/ykang/plato/results/test/model/lenet5_466_1127977.pth.
[INFO][10:25:30]: [Client #466] Model trained.
[INFO][10:25:30]: [Client #466] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:25:30]: [Server #1127936] Received 0.24 MB of payload data from client #466 (simulated).
[INFO][10:25:30]: [Client #468] Loading a model from /data/ykang/plato/results/test/model/lenet5_468_1127979.pth.
[INFO][10:25:30]: [Client #468] Model trained.
[INFO][10:25:30]: [Client #468] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:25:30]: [Server #1127936] Received 0.24 MB of payload data from client #468 (simulated).
[INFO][10:25:30]: [Client #132] Loading a model from /data/ykang/plato/results/test/model/lenet5_132_1127978.pth.
[INFO][10:25:30]: [Client #132] Model trained.
[INFO][10:25:30]: [Client #132] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:25:30]: [Server #1127936] Received 0.24 MB of payload data from client #132 (simulated).
[INFO][10:25:30]: [Server #1127936] Selecting client #311 for training.
[INFO][10:25:30]: [Server #1127936] Sending the current model to client #311 (simulated).
[INFO][10:25:30]: [Server #1127936] Sending 0.24 MB of payload data to client #311 (simulated).
[INFO][10:25:30]: [Client #311] Selected by the server.
[INFO][10:25:30]: [Client #311] Loading its data source...
[INFO][10:25:30]: [Client #311] Dataset size: 60000
[INFO][10:25:30]: [Client #311] Sampler: noniid
[INFO][10:25:30]: [Client #311] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:25:30]: [93m[1m[Client #311] Started training in communication round #67.[0m
[INFO][10:25:32]: [Client #311] Loading the dataset.
[INFO][10:25:38]: [Client #311] Epoch: [1/5][0/10]	Loss: 0.000907
[INFO][10:25:38]: [Client #311] Epoch: [2/5][0/10]	Loss: 0.002207
[INFO][10:25:38]: [Client #311] Epoch: [3/5][0/10]	Loss: 0.001415
[INFO][10:25:38]: [Client #311] Epoch: [4/5][0/10]	Loss: 0.000034
[INFO][10:25:38]: [Client #311] Epoch: [5/5][0/10]	Loss: 0.000089
[INFO][10:25:38]: [Client #311] Model saved to /data/ykang/plato/results/test/model/lenet5_311_1127977.pth.
[INFO][10:25:39]: [Client #311] Loading a model from /data/ykang/plato/results/test/model/lenet5_311_1127977.pth.
[INFO][10:25:39]: [Client #311] Model trained.
[INFO][10:25:39]: [Client #311] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:25:39]: [Server #1127936] Received 0.24 MB of payload data from client #311 (simulated).
[INFO][10:25:39]: [Server #1127936] Adding client #67 to the list of clients for aggregation.
[INFO][10:25:39]: [Server #1127936] Adding client #183 to the list of clients for aggregation.
[INFO][10:25:39]: [Server #1127936] Adding client #172 to the list of clients for aggregation.
[INFO][10:25:39]: [Server #1127936] Adding client #286 to the list of clients for aggregation.
[INFO][10:25:39]: [Server #1127936] Adding client #377 to the list of clients for aggregation.
[INFO][10:25:39]: [Server #1127936] Adding client #448 to the list of clients for aggregation.
[INFO][10:25:39]: [Server #1127936] Adding client #467 to the list of clients for aggregation.
[INFO][10:25:39]: [Server #1127936] Adding client #468 to the list of clients for aggregation.
[INFO][10:25:39]: [Server #1127936] Adding client #466 to the list of clients for aggregation.
[INFO][10:25:39]: [Server #1127936] Adding client #231 to the list of clients for aggregation.
[INFO][10:25:39]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00377705 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00663264 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00687013 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00408753 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00696581 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00585059 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00509295 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00418288 0.00905343 0.00174374
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 0. 1. 0. 0. 0. 1. 0. 1. 3. 1. 0. 0. 0. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 2. 0. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 2. 1. 0. 1. 0.
 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 2. 1. 6. 0. 1. 2. 1. 0. 1. 1. 2. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 0. 6. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 2. 0. 0.
 1. 0. 1. 2. 0. 0. 0. 2. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1.
 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 4. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0.
 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 4. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 3. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 0. 0. 0. 6. 1. 1.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 5. 0. 0. 0. 1. 0. 0. 0. 2. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 2. 2. 1. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00377705 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00663264 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00687013 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00408753 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00696581 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00585059 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00509295 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00418288 0.00905343 0.00174374
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:25:41]: [Server #1127936] Global model accuracy: 95.98%

[INFO][10:25:41]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_67.pth.
[INFO][10:25:41]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_67.pth.
[INFO][10:25:41]: [93m[1m
[Server #1127936] Starting round 68/100.[0m
[0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  4e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  5e-05  8e-10  8e-10
 6:  6.8876e+00  6.8875e+00  5e-05  5e-10  5e-10
 7:  6.8875e+00  6.8875e+00  4e-05  1e-09  5e-11
 8:  6.8875e+00  6.8875e+00  3e-05  1e-09  6e-11
 9:  6.8875e+00  6.8875e+00  1e-05  8e-09  4e-10
10:  6.8875e+00  6.8875e+00  6e-07  4e-09  2e-10
Optimal solution found.
The calculated probability is:  [2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.97397132e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 1.25157150e-03 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 3.31097903e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64155822e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 3.32255014e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 3.19223710e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 3.10907566e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64155760e-05
 9.85830551e-01 2.64156897e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05 2.64157136e-05 2.64157136e-05
 2.64157136e-05 2.64157136e-05]
current clients pool:  [INFO][10:25:42]: [Server #1127936] Selected clients: [467 228  46 377 245 356 235 174 493  16]
[INFO][10:25:42]: [Server #1127936] Selecting client #467 for training.
[INFO][10:25:42]: [Server #1127936] Sending the current model to client #467 (simulated).
[INFO][10:25:42]: [Server #1127936] Sending 0.24 MB of payload data to client #467 (simulated).
[INFO][10:25:42]: [Server #1127936] Selecting client #228 for training.
[INFO][10:25:42]: [Server #1127936] Sending the current model to client #228 (simulated).
[INFO][10:25:42]: [Server #1127936] Sending 0.24 MB of payload data to client #228 (simulated).
[INFO][10:25:42]: [Server #1127936] Selecting client #46 for training.
[INFO][10:25:42]: [Server #1127936] Sending the current model to client #46 (simulated).
[INFO][10:25:42]: [Client #467] Selected by the server.
[INFO][10:25:42]: [Client #467] Loading its data source...
[INFO][10:25:42]: [Client #467] Dataset size: 60000
[INFO][10:25:42]: [Client #467] Sampler: noniid
[INFO][10:25:42]: [Server #1127936] Sending 0.24 MB of payload data to client #46 (simulated).
[INFO][10:25:42]: [Client #228] Selected by the server.
[INFO][10:25:42]: [Client #228] Loading its data source...
[INFO][10:25:42]: [Client #228] Dataset size: 60000
[INFO][10:25:42]: [Client #228] Sampler: noniid
[INFO][10:25:42]: [Client #467] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:25:42]: [Client #228] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:25:42]: [93m[1m[Client #467] Started training in communication round #68.[0m
[INFO][10:25:42]: [93m[1m[Client #228] Started training in communication round #68.[0m
[INFO][10:25:42]: [Client #46] Selected by the server.
[INFO][10:25:42]: [Client #46] Loading its data source...
[INFO][10:25:42]: [Client #46] Dataset size: 60000
[INFO][10:25:42]: [Client #46] Sampler: noniid
[INFO][10:25:42]: [Client #46] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:25:42]: [93m[1m[Client #46] Started training in communication round #68.[0m
[INFO][10:25:44]: [Client #46] Loading the dataset.
[INFO][10:25:44]: [Client #228] Loading the dataset.
[INFO][10:25:44]: [Client #467] Loading the dataset.
[INFO][10:25:50]: [Client #228] Epoch: [1/5][0/10]	Loss: 0.000595
[INFO][10:25:50]: [Client #467] Epoch: [1/5][0/10]	Loss: 0.018938
[INFO][10:25:50]: [Client #46] Epoch: [1/5][0/10]	Loss: 0.000171
[INFO][10:25:50]: [Client #228] Epoch: [2/5][0/10]	Loss: 0.000650
[INFO][10:25:50]: [Client #46] Epoch: [2/5][0/10]	Loss: 0.001439
[INFO][10:25:50]: [Client #467] Epoch: [2/5][0/10]	Loss: 0.000135
[INFO][10:25:50]: [Client #228] Epoch: [3/5][0/10]	Loss: 0.000135
[INFO][10:25:50]: [Client #46] Epoch: [3/5][0/10]	Loss: 0.000150
[INFO][10:25:50]: [Client #467] Epoch: [3/5][0/10]	Loss: 0.000127
[INFO][10:25:50]: [Client #228] Epoch: [4/5][0/10]	Loss: 0.000023
[INFO][10:25:50]: [Client #46] Epoch: [4/5][0/10]	Loss: 0.000108
[INFO][10:25:50]: [Client #467] Epoch: [4/5][0/10]	Loss: 0.000011
[INFO][10:25:50]: [Client #228] Epoch: [5/5][0/10]	Loss: 0.000239
[INFO][10:25:50]: [Client #228] Model saved to /data/ykang/plato/results/test/model/lenet5_228_1127978.pth.
[INFO][10:25:50]: [Client #46] Epoch: [5/5][0/10]	Loss: 0.000007
[INFO][10:25:50]: [Client #467] Epoch: [5/5][0/10]	Loss: 0.000199
[INFO][10:25:50]: [Client #46] Model saved to /data/ykang/plato/results/test/model/lenet5_46_1127979.pth.
[INFO][10:25:50]: [Client #467] Model saved to /data/ykang/plato/results/test/model/lenet5_467_1127977.pth.
[INFO][10:25:51]: [Client #228] Loading a model from /data/ykang/plato/results/test/model/lenet5_228_1127978.pth.
[INFO][10:25:51]: [Client #228] Model trained.
[INFO][10:25:51]: [Client #228] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:25:51]: [Server #1127936] Received 0.24 MB of payload data from client #228 (simulated).
[INFO][10:25:51]: [Client #467] Loading a model from /data/ykang/plato/results/test/model/lenet5_467_1127977.pth.
[INFO][10:25:51]: [Client #467] Model trained.
[INFO][10:25:51]: [Client #467] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:25:51]: [Server #1127936] Received 0.24 MB of payload data from client #467 (simulated).
[INFO][10:25:51]: [Client #46] Loading a model from /data/ykang/plato/results/test/model/lenet5_46_1127979.pth.
[INFO][10:25:51]: [Client #46] Model trained.
[INFO][10:25:51]: [Client #46] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:25:51]: [Server #1127936] Received 0.24 MB of payload data from client #46 (simulated).
[INFO][10:25:51]: [Server #1127936] Selecting client #377 for training.
[INFO][10:25:51]: [Server #1127936] Sending the current model to client #377 (simulated).
[INFO][10:25:51]: [Server #1127936] Sending 0.24 MB of payload data to client #377 (simulated).
[INFO][10:25:51]: [Server #1127936] Selecting client #245 for training.
[INFO][10:25:51]: [Server #1127936] Sending the current model to client #245 (simulated).
[INFO][10:25:51]: [Server #1127936] Sending 0.24 MB of payload data to client #245 (simulated).
[INFO][10:25:51]: [Server #1127936] Selecting client #356 for training.
[INFO][10:25:51]: [Server #1127936] Sending the current model to client #356 (simulated).
[INFO][10:25:51]: [Client #377] Selected by the server.
[INFO][10:25:51]: [Client #377] Loading its data source...
[INFO][10:25:51]: [Client #377] Dataset size: 60000
[INFO][10:25:51]: [Client #377] Sampler: noniid
[INFO][10:25:51]: [Server #1127936] Sending 0.24 MB of payload data to client #356 (simulated).
[INFO][10:25:51]: [Client #245] Selected by the server.
[INFO][10:25:51]: [Client #245] Loading its data source...
[INFO][10:25:51]: [Client #245] Dataset size: 60000
[INFO][10:25:51]: [Client #245] Sampler: noniid
[INFO][10:25:51]: [Client #356] Selected by the server.
[INFO][10:25:51]: [Client #356] Loading its data source...
[INFO][10:25:51]: [Client #356] Dataset size: 60000
[INFO][10:25:51]: [Client #356] Sampler: noniid
[INFO][10:25:51]: [Client #377] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:25:51]: [Client #245] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:25:51]: [93m[1m[Client #377] Started training in communication round #68.[0m
[INFO][10:25:51]: [93m[1m[Client #245] Started training in communication round #68.[0m
[INFO][10:25:51]: [Client #356] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:25:51]: [93m[1m[Client #356] Started training in communication round #68.[0m
[INFO][10:25:54]: [Client #377] Loading the dataset.
[INFO][10:25:54]: [Client #356] Loading the dataset.
[INFO][10:25:54]: [Client #245] Loading the dataset.
[INFO][10:26:00]: [Client #377] Epoch: [1/5][0/10]	Loss: 0.012026
[INFO][10:26:00]: [Client #356] Epoch: [1/5][0/10]	Loss: 0.000969
[INFO][10:26:00]: [Client #377] Epoch: [2/5][0/10]	Loss: 0.001098
[INFO][10:26:00]: [Client #356] Epoch: [2/5][0/10]	Loss: 0.000351
[INFO][10:26:00]: [Client #245] Epoch: [1/5][0/10]	Loss: 0.000592
[INFO][10:26:00]: [Client #377] Epoch: [3/5][0/10]	Loss: 0.000007
[INFO][10:26:00]: [Client #245] Epoch: [2/5][0/10]	Loss: 0.000836
[INFO][10:26:00]: [Client #356] Epoch: [3/5][0/10]	Loss: 0.000008
[INFO][10:26:00]: [Client #377] Epoch: [4/5][0/10]	Loss: 0.000175
[INFO][10:26:00]: [Client #245] Epoch: [3/5][0/10]	Loss: 0.000074
[INFO][10:26:00]: [Client #356] Epoch: [4/5][0/10]	Loss: 0.000318
[INFO][10:26:00]: [Client #245] Epoch: [4/5][0/10]	Loss: 0.000079
[INFO][10:26:00]: [Client #377] Epoch: [5/5][0/10]	Loss: 0.005590
[INFO][10:26:00]: [Client #356] Epoch: [5/5][0/10]	Loss: 0.000007
[INFO][10:26:00]: [Client #377] Model saved to /data/ykang/plato/results/test/model/lenet5_377_1127977.pth.
[INFO][10:26:00]: [Client #245] Epoch: [5/5][0/10]	Loss: 0.000180
[INFO][10:26:00]: [Client #356] Model saved to /data/ykang/plato/results/test/model/lenet5_356_1127979.pth.
[INFO][10:26:00]: [Client #245] Model saved to /data/ykang/plato/results/test/model/lenet5_245_1127978.pth.
[INFO][10:26:01]: [Client #377] Loading a model from /data/ykang/plato/results/test/model/lenet5_377_1127977.pth.
[INFO][10:26:01]: [Client #377] Model trained.
[INFO][10:26:01]: [Client #377] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:26:01]: [Server #1127936] Received 0.24 MB of payload data from client #377 (simulated).
[INFO][10:26:01]: [Client #245] Loading a model from /data/ykang/plato/results/test/model/lenet5_245_1127978.pth.
[INFO][10:26:01]: [Client #245] Model trained.
[INFO][10:26:01]: [Client #245] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:26:01]: [Server #1127936] Received 0.24 MB of payload data from client #245 (simulated).
[INFO][10:26:01]: [Client #356] Loading a model from /data/ykang/plato/results/test/model/lenet5_356_1127979.pth.
[INFO][10:26:01]: [Client #356] Model trained.
[INFO][10:26:01]: [Client #356] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:26:01]: [Server #1127936] Received 0.24 MB of payload data from client #356 (simulated).
[INFO][10:26:01]: [Server #1127936] Selecting client #235 for training.
[INFO][10:26:01]: [Server #1127936] Sending the current model to client #235 (simulated).
[INFO][10:26:01]: [Server #1127936] Sending 0.24 MB of payload data to client #235 (simulated).
[INFO][10:26:01]: [Server #1127936] Selecting client #174 for training.
[INFO][10:26:01]: [Server #1127936] Sending the current model to client #174 (simulated).
[INFO][10:26:01]: [Server #1127936] Sending 0.24 MB of payload data to client #174 (simulated).
[INFO][10:26:01]: [Server #1127936] Selecting client #493 for training.
[INFO][10:26:01]: [Server #1127936] Sending the current model to client #493 (simulated).
[INFO][10:26:01]: [Client #235] Selected by the server.
[INFO][10:26:01]: [Client #235] Loading its data source...
[INFO][10:26:01]: [Client #235] Dataset size: 60000
[INFO][10:26:01]: [Client #235] Sampler: noniid
[INFO][10:26:01]: [Server #1127936] Sending 0.24 MB of payload data to client #493 (simulated).
[INFO][10:26:01]: [Client #174] Selected by the server.
[INFO][10:26:01]: [Client #493] Selected by the server.
[INFO][10:26:01]: [Client #174] Loading its data source...
[INFO][10:26:01]: [Client #493] Loading its data source...
[INFO][10:26:01]: [Client #493] Dataset size: 60000
[INFO][10:26:01]: [Client #174] Dataset size: 60000
[INFO][10:26:01]: [Client #493] Sampler: noniid
[INFO][10:26:01]: [Client #174] Sampler: noniid
[INFO][10:26:01]: [Client #235] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:26:01]: [Client #493] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:26:01]: [Client #174] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:26:01]: [93m[1m[Client #235] Started training in communication round #68.[0m
[INFO][10:26:01]: [93m[1m[Client #493] Started training in communication round #68.[0m
[INFO][10:26:01]: [93m[1m[Client #174] Started training in communication round #68.[0m
[INFO][10:26:03]: [Client #174] Loading the dataset.
[INFO][10:26:03]: [Client #493] Loading the dataset.
[INFO][10:26:03]: [Client #235] Loading the dataset.
[INFO][10:26:10]: [Client #174] Epoch: [1/5][0/10]	Loss: 0.018938
[INFO][10:26:10]: [Client #235] Epoch: [1/5][0/10]	Loss: 0.000433
[INFO][10:26:10]: [Client #493] Epoch: [1/5][0/10]	Loss: 0.000140
[INFO][10:26:10]: [Client #174] Epoch: [2/5][0/10]	Loss: 0.000012
[INFO][10:26:10]: [Client #493] Epoch: [2/5][0/10]	Loss: 0.001411
[INFO][10:26:10]: [Client #174] Epoch: [3/5][0/10]	Loss: 0.006544
[INFO][10:26:10]: [Client #235] Epoch: [2/5][0/10]	Loss: 0.000151
[INFO][10:26:10]: [Client #493] Epoch: [3/5][0/10]	Loss: 0.000213
[INFO][10:26:10]: [Client #174] Epoch: [4/5][0/10]	Loss: 0.000345
[INFO][10:26:10]: [Client #235] Epoch: [3/5][0/10]	Loss: 0.063806
[INFO][10:26:10]: [Client #493] Epoch: [4/5][0/10]	Loss: 0.000300
[INFO][10:26:10]: [Client #174] Epoch: [5/5][0/10]	Loss: 0.053836
[INFO][10:26:10]: [Client #235] Epoch: [4/5][0/10]	Loss: 0.000076
[INFO][10:26:10]: [Client #174] Model saved to /data/ykang/plato/results/test/model/lenet5_174_1127978.pth.
[INFO][10:26:10]: [Client #493] Epoch: [5/5][0/10]	Loss: 0.000095
[INFO][10:26:10]: [Client #235] Epoch: [5/5][0/10]	Loss: 0.000493
[INFO][10:26:10]: [Client #493] Model saved to /data/ykang/plato/results/test/model/lenet5_493_1127979.pth.
[INFO][10:26:10]: [Client #235] Model saved to /data/ykang/plato/results/test/model/lenet5_235_1127977.pth.
[INFO][10:26:11]: [Client #174] Loading a model from /data/ykang/plato/results/test/model/lenet5_174_1127978.pth.
[INFO][10:26:11]: [Client #174] Model trained.
[INFO][10:26:11]: [Client #174] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:26:11]: [Server #1127936] Received 0.24 MB of payload data from client #174 (simulated).
[INFO][10:26:11]: [Client #235] Loading a model from /data/ykang/plato/results/test/model/lenet5_235_1127977.pth.
[INFO][10:26:11]: [Client #235] Model trained.
[INFO][10:26:11]: [Client #235] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:26:11]: [Server #1127936] Received 0.24 MB of payload data from client #235 (simulated).
[INFO][10:26:11]: [Client #493] Loading a model from /data/ykang/plato/results/test/model/lenet5_493_1127979.pth.
[INFO][10:26:11]: [Client #493] Model trained.
[INFO][10:26:11]: [Client #493] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:26:11]: [Server #1127936] Received 0.24 MB of payload data from client #493 (simulated).
[INFO][10:26:11]: [Server #1127936] Selecting client #16 for training.
[INFO][10:26:11]: [Server #1127936] Sending the current model to client #16 (simulated).
[INFO][10:26:11]: [Server #1127936] Sending 0.24 MB of payload data to client #16 (simulated).
[INFO][10:26:11]: [Client #16] Selected by the server.
[INFO][10:26:11]: [Client #16] Loading its data source...
[INFO][10:26:11]: [Client #16] Dataset size: 60000
[INFO][10:26:11]: [Client #16] Sampler: noniid
[INFO][10:26:11]: [Client #16] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:26:11]: [93m[1m[Client #16] Started training in communication round #68.[0m
[INFO][10:26:13]: [Client #16] Loading the dataset.
[INFO][10:26:19]: [Client #16] Epoch: [1/5][0/10]	Loss: 0.006600
[INFO][10:26:19]: [Client #16] Epoch: [2/5][0/10]	Loss: 0.002294
[INFO][10:26:19]: [Client #16] Epoch: [3/5][0/10]	Loss: 0.001569
[INFO][10:26:19]: [Client #16] Epoch: [4/5][0/10]	Loss: 0.000963
[INFO][10:26:19]: [Client #16] Epoch: [5/5][0/10]	Loss: 0.005916
[INFO][10:26:19]: [Client #16] Model saved to /data/ykang/plato/results/test/model/lenet5_16_1127977.pth.
[INFO][10:26:20]: [Client #16] Loading a model from /data/ykang/plato/results/test/model/lenet5_16_1127977.pth.
[INFO][10:26:20]: [Client #16] Model trained.
[INFO][10:26:20]: [Client #16] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:26:20]: [Server #1127936] Received 0.24 MB of payload data from client #16 (simulated).
[INFO][10:26:20]: [Server #1127936] Adding client #400 to the list of clients for aggregation.
[INFO][10:26:20]: [Server #1127936] Adding client #94 to the list of clients for aggregation.
[INFO][10:26:20]: [Server #1127936] Adding client #430 to the list of clients for aggregation.
[INFO][10:26:20]: [Server #1127936] Adding client #79 to the list of clients for aggregation.
[INFO][10:26:20]: [Server #1127936] Adding client #92 to the list of clients for aggregation.
[INFO][10:26:20]: [Server #1127936] Adding client #311 to the list of clients for aggregation.
[INFO][10:26:20]: [Server #1127936] Adding client #356 to the list of clients for aggregation.
[INFO][10:26:20]: [Server #1127936] Adding client #174 to the list of clients for aggregation.
[INFO][10:26:20]: [Server #1127936] Adding client #493 to the list of clients for aggregation.
[INFO][10:26:20]: [Server #1127936] Adding client #46 to the list of clients for aggregation.
[INFO][10:26:20]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 401, 402, 403, 404, 405, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00268697 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00269387 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00210852 0.         0.0088862  0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00969997
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00482907 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00499672 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00299522 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01142947 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00285293 0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 0. 1. 0. 0. 0. 1. 0. 1. 3. 1. 0. 0. 0. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 2. 0. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 2. 1. 1. 1. 0.
 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 2. 1. 6. 0. 1. 2. 1. 0. 1. 1. 2. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 0. 6. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 2. 0. 0.
 1. 0. 1. 2. 0. 0. 0. 2. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1.
 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 4. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0.
 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0.
 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0.
 1. 1. 1. 0. 4. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 3. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 0. 0. 0. 6. 1. 1.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 5. 0. 0. 0. 1. 0. 0. 0. 2. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 2. 2. 1. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00268697 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00269387 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00210852 0.         0.0088862  0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00969997
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00482907 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00499672 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00299522 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01142947 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00285293 0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:26:22]: [Server #1127936] Global model accuracy: 95.97%

[INFO][10:26:22]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_68.pth.
[INFO][10:26:22]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_68.pth.
[INFO][10:26:22]: [93m[1m
[Server #1127936] Starting round 69/100.[0m
[INFO][10:26:22]: [Server #1127936] Selected clients: [430 298 248 352 405 452 266 432 172 499]
[INFO][10:26:22]: [Server #1127936] Selecting client #430 for training.
[INFO][10:26:22]: [Server #1127936] Sending the current model to client #430 (simulated).
[INFO][10:26:22]: [Server #1127936] Sending 0.24 MB of payload data to client #430 (simulated).
[INFO][10:26:22]: [Server #1127936] Selecting client #298 for training.
[INFO][10:26:22]: [Server #1127936] Sending the current model to client #298 (simulated).
[INFO][10:26:22]: [Server #1127936] Sending 0.24 MB of payload data to client #298 (simulated).
[INFO][10:26:22]: [Server #1127936] Selecting client #248 for training.
[INFO][10:26:22]: [Server #1127936] Sending the current model to client #248 (simulated).
[INFO][10:26:22]: [Client #430] Selected by the server.
[INFO][10:26:22]: [Client #430] Loading its data source...
[INFO][10:26:22]: [Client #430] Dataset size: 60000
[INFO][10:26:22]: [Client #430] Sampler: noniid
[INFO][10:26:22]: [Server #1127936] Sending 0.24 MB of payload data to client #248 (simulated).
[INFO][10:26:22]: [Client #298] Selected by the server.
[INFO][10:26:22]: [Client #298] Loading its data source...
[INFO][10:26:22]: [Client #298] Dataset size: 60000
[INFO][10:26:22]: [Client #298] Sampler: noniid
[INFO][10:26:22]: [Client #430] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:26:22]: [Client #298] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:26:22]: [93m[1m[Client #430] Started training in communication round #69.[0m
[INFO][10:26:22]: [93m[1m[Client #298] Started training in communication round #69.[0m
[INFO][10:26:22]: [Client #248] Selected by the server.
[INFO][10:26:22]: [Client #248] Loading its data source...
[INFO][10:26:22]: [Client #248] Dataset size: 60000
[INFO][10:26:22]: [Client #248] Sampler: noniid
[INFO][10:26:22]: [Client #248] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:26:22]: [93m[1m[Client #248] Started training in communication round #69.[0m
[INFO][10:26:25]: [Client #298] Loading the dataset.
[INFO][10:26:25]: [Client #430] Loading the dataset.
[INFO][10:26:25]: [Client #248] Loading the dataset.
[INFO][10:26:31]: [Client #248] Epoch: [1/5][0/10]	Loss: 0.005369
[INFO][10:26:31]: [Client #298] Epoch: [1/5][0/10]	Loss: 0.005996
[INFO][10:26:31]: [Client #248] Epoch: [2/5][0/10]	Loss: 0.006165
[INFO][10:26:31]: [Client #430] Epoch: [1/5][0/10]	Loss: 0.000269
[INFO][10:26:31]: [Client #298] Epoch: [2/5][0/10]	Loss: 0.000495
[INFO][10:26:31]: [Client #248] Epoch: [3/5][0/10]	Loss: 0.000024
[INFO][10:26:31]: [Client #298] Epoch: [3/5][0/10]	Loss: 0.000112
[INFO][10:26:31]: [Client #430] Epoch: [2/5][0/10]	Loss: 0.000266
[INFO][10:26:31]: [Client #248] Epoch: [4/5][0/10]	Loss: 0.000001
[INFO][10:26:31]: [Client #298] Epoch: [4/5][0/10]	Loss: 0.000777
[INFO][10:26:31]: [Client #430] Epoch: [3/5][0/10]	Loss: 0.000412
[INFO][10:26:31]: [Client #248] Epoch: [5/5][0/10]	Loss: 0.000388
[INFO][10:26:31]: [Client #298] Epoch: [5/5][0/10]	Loss: 0.000372
[INFO][10:26:31]: [Client #430] Epoch: [4/5][0/10]	Loss: 0.000095
[INFO][10:26:31]: [Client #248] Model saved to /data/ykang/plato/results/test/model/lenet5_248_1127979.pth.
[INFO][10:26:31]: [Client #298] Model saved to /data/ykang/plato/results/test/model/lenet5_298_1127978.pth.
[INFO][10:26:31]: [Client #430] Epoch: [5/5][0/10]	Loss: 0.000646
[INFO][10:26:31]: [Client #430] Model saved to /data/ykang/plato/results/test/model/lenet5_430_1127977.pth.
[INFO][10:26:32]: [Client #248] Loading a model from /data/ykang/plato/results/test/model/lenet5_248_1127979.pth.
[INFO][10:26:32]: [Client #248] Model trained.
[INFO][10:26:32]: [Client #248] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:26:32]: [Server #1127936] Received 0.24 MB of payload data from client #248 (simulated).
[INFO][10:26:32]: [Client #298] Loading a model from /data/ykang/plato/results/test/model/lenet5_298_1127978.pth.
[INFO][10:26:32]: [Client #298] Model trained.
[INFO][10:26:32]: [Client #298] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:26:32]: [Server #1127936] Received 0.24 MB of payload data from client #298 (simulated).
[INFO][10:26:32]: [Client #430] Loading a model from /data/ykang/plato/results/test/model/lenet5_430_1127977.pth.
[INFO][10:26:32]: [Client #430] Model trained.
[INFO][10:26:32]: [Client #430] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:26:32]: [Server #1127936] Received 0.24 MB of payload data from client #430 (simulated).
[INFO][10:26:32]: [Server #1127936] Selecting client #352 for training.
[INFO][10:26:32]: [Server #1127936] Sending the current model to client #352 (simulated).
[INFO][10:26:32]: [Server #1127936] Sending 0.24 MB of payload data to client #352 (simulated).
[INFO][10:26:32]: [Server #1127936] Selecting client #405 for training.
[INFO][10:26:32]: [Server #1127936] Sending the current model to client #405 (simulated).
[INFO][10:26:32]: [Server #1127936] Sending 0.24 MB of payload data to client #405 (simulated).
[INFO][10:26:32]: [Server #1127936] Selecting client #452 for training.
[INFO][10:26:32]: [Server #1127936] Sending the current model to client #452 (simulated).
[INFO][10:26:32]: [Client #352] Selected by the server.
[INFO][10:26:32]: [Client #352] Loading its data source...
[INFO][10:26:32]: [Client #352] Dataset size: 60000
[INFO][10:26:32]: [Client #352] Sampler: noniid
[INFO][10:26:32]: [Server #1127936] Sending 0.24 MB of payload data to client #452 (simulated).
[INFO][10:26:32]: [Client #452] Selected by the server.
[INFO][10:26:32]: [Client #452] Loading its data source...
[INFO][10:26:32]: [Client #405] Selected by the server.
[INFO][10:26:32]: [Client #452] Dataset size: 60000
[INFO][10:26:32]: [Client #452] Sampler: noniid
[INFO][10:26:32]: [Client #405] Loading its data source...
[INFO][10:26:32]: [Client #405] Dataset size: 60000
[INFO][10:26:32]: [Client #405] Sampler: noniid
[INFO][10:26:32]: [Client #352] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:26:32]: [Client #452] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:26:32]: [Client #405] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:26:32]: [93m[1m[Client #452] Started training in communication round #69.[0m
[INFO][10:26:32]: [93m[1m[Client #352] Started training in communication round #69.[0m
[INFO][10:26:32]: [93m[1m[Client #405] Started training in communication round #69.[0m
[INFO][10:26:34]: [Client #452] Loading the dataset.
[INFO][10:26:34]: [Client #352] Loading the dataset.
[INFO][10:26:34]: [Client #405] Loading the dataset.
[INFO][10:26:40]: [Client #405] Epoch: [1/5][0/10]	Loss: 0.008381
[INFO][10:26:40]: [Client #352] Epoch: [1/5][0/10]	Loss: 0.000439
[INFO][10:26:40]: [Client #452] Epoch: [1/5][0/10]	Loss: 0.000262
[INFO][10:26:40]: [Client #352] Epoch: [2/5][0/10]	Loss: 0.001111
[INFO][10:26:41]: [Client #452] Epoch: [2/5][0/10]	Loss: 0.000325
[INFO][10:26:41]: [Client #405] Epoch: [2/5][0/10]	Loss: 0.002150
[INFO][10:26:41]: [Client #352] Epoch: [3/5][0/10]	Loss: 0.000099
[INFO][10:26:41]: [Client #452] Epoch: [3/5][0/10]	Loss: 0.000072
[INFO][10:26:41]: [Client #405] Epoch: [3/5][0/10]	Loss: 0.000069
[INFO][10:26:41]: [Client #352] Epoch: [4/5][0/10]	Loss: 0.000037
[INFO][10:26:41]: [Client #452] Epoch: [4/5][0/10]	Loss: 0.000314
[INFO][10:26:41]: [Client #405] Epoch: [4/5][0/10]	Loss: 0.000039
[INFO][10:26:41]: [Client #352] Epoch: [5/5][0/10]	Loss: 0.000037
[INFO][10:26:41]: [Client #405] Epoch: [5/5][0/10]	Loss: 0.000192
[INFO][10:26:41]: [Client #452] Epoch: [5/5][0/10]	Loss: 0.023231
[INFO][10:26:41]: [Client #352] Model saved to /data/ykang/plato/results/test/model/lenet5_352_1127977.pth.
[INFO][10:26:41]: [Client #405] Model saved to /data/ykang/plato/results/test/model/lenet5_405_1127978.pth.
[INFO][10:26:41]: [Client #452] Model saved to /data/ykang/plato/results/test/model/lenet5_452_1127979.pth.
[INFO][10:26:42]: [Client #352] Loading a model from /data/ykang/plato/results/test/model/lenet5_352_1127977.pth.
[INFO][10:26:42]: [Client #352] Model trained.
[INFO][10:26:42]: [Client #352] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:26:42]: [Server #1127936] Received 0.24 MB of payload data from client #352 (simulated).
[INFO][10:26:42]: [Client #452] Loading a model from /data/ykang/plato/results/test/model/lenet5_452_1127979.pth.
[INFO][10:26:42]: [Client #452] Model trained.
[INFO][10:26:42]: [Client #452] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:26:42]: [Server #1127936] Received 0.24 MB of payload data from client #452 (simulated).
[INFO][10:26:42]: [Client #405] Loading a model from /data/ykang/plato/results/test/model/lenet5_405_1127978.pth.
[INFO][10:26:42]: [Client #405] Model trained.
[INFO][10:26:42]: [Client #405] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:26:42]: [Server #1127936] Received 0.24 MB of payload data from client #405 (simulated).
[INFO][10:26:42]: [Server #1127936] Selecting client #266 for training.
[INFO][10:26:42]: [Server #1127936] Sending the current model to client #266 (simulated).
[INFO][10:26:42]: [Server #1127936] Sending 0.24 MB of payload data to client #266 (simulated).
[INFO][10:26:42]: [Server #1127936] Selecting client #432 for training.
[INFO][10:26:42]: [Server #1127936] Sending the current model to client #432 (simulated).
[INFO][10:26:42]: [Server #1127936] Sending 0.24 MB of payload data to client #432 (simulated).
[INFO][10:26:42]: [Server #1127936] Selecting client #172 for training.
[INFO][10:26:42]: [Server #1127936] Sending the current model to client #172 (simulated).
[INFO][10:26:42]: [Client #266] Selected by the server.
[INFO][10:26:42]: [Client #266] Loading its data source...
[INFO][10:26:42]: [Client #266] Dataset size: 60000
[INFO][10:26:42]: [Client #266] Sampler: noniid
[INFO][10:26:42]: [Server #1127936] Sending 0.24 MB of payload data to client #172 (simulated).
[INFO][10:26:42]: [Client #432] Selected by the server.
[INFO][10:26:42]: [Client #432] Loading its data source...
[INFO][10:26:42]: [Client #432] Dataset size: 60000
[INFO][10:26:42]: [Client #432] Sampler: noniid
[INFO][10:26:42]: [Client #172] Selected by the server.
[INFO][10:26:42]: [Client #172] Loading its data source...
[INFO][10:26:42]: [Client #172] Dataset size: 60000
[INFO][10:26:42]: [Client #172] Sampler: noniid
[INFO][10:26:42]: [Client #266] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:26:42]: [Client #172] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:26:42]: [93m[1m[Client #266] Started training in communication round #69.[0m
[INFO][10:26:42]: [Client #432] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:26:42]: [93m[1m[Client #172] Started training in communication round #69.[0m
[INFO][10:26:42]: [93m[1m[Client #432] Started training in communication round #69.[0m
[INFO][10:26:44]: [Client #432] Loading the dataset.
[INFO][10:26:44]: [Client #172] Loading the dataset.
[INFO][10:26:44]: [Client #266] Loading the dataset.
[INFO][10:26:50]: [Client #266] Epoch: [1/5][0/10]	Loss: 0.006189
[INFO][10:26:50]: [Client #432] Epoch: [1/5][0/10]	Loss: 0.001018
[INFO][10:26:50]: [Client #172] Epoch: [1/5][0/10]	Loss: 0.000229
[INFO][10:26:50]: [Client #266] Epoch: [2/5][0/10]	Loss: 0.008064
[INFO][10:26:50]: [Client #172] Epoch: [2/5][0/10]	Loss: 0.000261
[INFO][10:26:50]: [Client #432] Epoch: [2/5][0/10]	Loss: 0.000009
[INFO][10:26:50]: [Client #266] Epoch: [3/5][0/10]	Loss: 0.000001
[INFO][10:26:50]: [Client #432] Epoch: [3/5][0/10]	Loss: 0.000108
[INFO][10:26:50]: [Client #172] Epoch: [3/5][0/10]	Loss: 0.000110
[INFO][10:26:51]: [Client #432] Epoch: [4/5][0/10]	Loss: 0.000014
[INFO][10:26:51]: [Client #172] Epoch: [4/5][0/10]	Loss: 0.000001
[INFO][10:26:51]: [Client #266] Epoch: [4/5][0/10]	Loss: 0.000002
[INFO][10:26:51]: [Client #432] Epoch: [5/5][0/10]	Loss: 0.000426
[INFO][10:26:51]: [Client #266] Epoch: [5/5][0/10]	Loss: 0.040805
[INFO][10:26:51]: [Client #172] Epoch: [5/5][0/10]	Loss: 0.000001
[INFO][10:26:51]: [Client #432] Model saved to /data/ykang/plato/results/test/model/lenet5_432_1127978.pth.
[INFO][10:26:51]: [Client #266] Model saved to /data/ykang/plato/results/test/model/lenet5_266_1127977.pth.
[INFO][10:26:51]: [Client #172] Model saved to /data/ykang/plato/results/test/model/lenet5_172_1127979.pth.
[INFO][10:26:52]: [Client #432] Loading a model from /data/ykang/plato/results/test/model/lenet5_432_1127978.pth.
[INFO][10:26:52]: [Client #432] Model trained.
[INFO][10:26:52]: [Client #432] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:26:52]: [Server #1127936] Received 0.24 MB of payload data from client #432 (simulated).
[INFO][10:26:52]: [Client #266] Loading a model from /data/ykang/plato/results/test/model/lenet5_266_1127977.pth.
[INFO][10:26:52]: [Client #266] Model trained.
[INFO][10:26:52]: [Client #266] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:26:52]: [Server #1127936] Received 0.24 MB of payload data from client #266 (simulated).
[INFO][10:26:52]: [Client #172] Loading a model from /data/ykang/plato/results/test/model/lenet5_172_1127979.pth.
[INFO][10:26:52]: [Client #172] Model trained.
[INFO][10:26:52]: [Client #172] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:26:52]: [Server #1127936] Received 0.24 MB of payload data from client #172 (simulated).
[INFO][10:26:52]: [Server #1127936] Selecting client #499 for training.
[INFO][10:26:52]: [Server #1127936] Sending the current model to client #499 (simulated).
[INFO][10:26:52]: [Server #1127936] Sending 0.24 MB of payload data to client #499 (simulated).
[INFO][10:26:52]: [Client #499] Selected by the server.
[INFO][10:26:52]: [Client #499] Loading its data source...
[INFO][10:26:52]: [Client #499] Dataset size: 60000
[INFO][10:26:52]: [Client #499] Sampler: noniid
[INFO][10:26:52]: [Client #499] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:26:52]: [93m[1m[Client #499] Started training in communication round #69.[0m
[INFO][10:26:54]: [Client #499] Loading the dataset.
[INFO][10:26:59]: [Client #499] Epoch: [1/5][0/10]	Loss: 0.004710
[INFO][10:26:59]: [Client #499] Epoch: [2/5][0/10]	Loss: 0.000005
[INFO][10:26:59]: [Client #499] Epoch: [3/5][0/10]	Loss: 0.000005
[INFO][10:26:59]: [Client #499] Epoch: [4/5][0/10]	Loss: 0.000002
[INFO][10:26:59]: [Client #499] Epoch: [5/5][0/10]	Loss: 0.000004
[INFO][10:26:59]: [Client #499] Model saved to /data/ykang/plato/results/test/model/lenet5_499_1127977.pth.
[INFO][10:27:00]: [Client #499] Loading a model from /data/ykang/plato/results/test/model/lenet5_499_1127977.pth.
[INFO][10:27:00]: [Client #499] Model trained.
[INFO][10:27:00]: [Client #499] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:27:00]: [Server #1127936] Received 0.24 MB of payload data from client #499 (simulated).
[INFO][10:27:00]: [Server #1127936] Adding client #235 to the list of clients for aggregation.
[INFO][10:27:00]: [Server #1127936] Adding client #16 to the list of clients for aggregation.
[INFO][10:27:00]: [Server #1127936] Adding client #245 to the list of clients for aggregation.
[INFO][10:27:00]: [Server #1127936] Adding client #377 to the list of clients for aggregation.
[INFO][10:27:00]: [Server #1127936] Adding client #406 to the list of clients for aggregation.
[INFO][10:27:00]: [Server #1127936] Adding client #228 to the list of clients for aggregation.
[INFO][10:27:00]: [Server #1127936] Adding client #298 to the list of clients for aggregation.
[INFO][10:27:00]: [Server #1127936] Adding client #266 to the list of clients for aggregation.
[INFO][10:27:00]: [Server #1127936] Adding client #248 to the list of clients for aggregation.
[INFO][10:27:00]: [Server #1127936] Adding client #432 to the list of clients for aggregation.
[INFO][10:27:00]: [Server #1127936] Aggregating 10 clients in total.
[0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  1e-05  3e-10  3e-10
 6:  6.8876e+00  6.8875e+00  1e-05  1e-10  1e-10
 7:  6.8875e+00  6.8875e+00  1e-05  2e-10  2e-12
 8:  6.8875e+00  6.8875e+00  7e-06  1e-10  2e-12
 9:  6.8875e+00  6.8875e+00  4e-06  2e-10  3e-12
Optimal solution found.
The calculated probability is:  [0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072316 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.0009625  0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00118011
 0.00072317 0.00351968 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072311 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00129665 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072315 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00099914 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.64202787 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072316 0.00072317 0.00072317 0.00072317
 0.00072317 0.00072317 0.00072317 0.00072317]
current clients pool:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 229, 230, 231, 232, 233, 234, 236, 237, 238, 239, 240, 241, 242, 243, 244, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00758044 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00999354
 0.         0.         0.         0.         0.         0.
 0.01035818 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00260261 0.
 0.         0.01094844 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01413678 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00481637 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.0059226  0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00131586 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00739748
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 0. 1. 0. 0. 0. 1. 0. 1. 3. 1. 0. 0. 0. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 2. 0. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 2. 1. 1. 1. 0.
 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 2. 1. 6. 0. 1. 2. 1. 0. 1. 1. 2. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 0. 6. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 2. 0. 0.
 1. 0. 1. 2. 0. 0. 0. 2. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 4. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0.
 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0.
 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0.
 1. 1. 1. 0. 4. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 3. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 0. 0. 0. 6. 1. 1.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 5. 0. 0. 0. 1. 0. 0. 0. 2. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 2. 2. 1. 1. 0.]
local_gradient_bounds:  [INFO][10:27:02]: [Server #1127936] Global model accuracy: 96.34%

[INFO][10:27:02]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_69.pth.
[INFO][10:27:02]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_69.pth.
[INFO][10:27:02]: [93m[1m
[Server #1127936] Starting round 70/100.[0m
[0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00758044 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00999354
 0.         0.         0.         0.         0.         0.
 0.01035818 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00260261 0.
 0.         0.01094844 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01413678 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00481637 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.0059226  0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00131586 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00739748
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  1e-05  2e-10  2e-10
 6:  6.8876e+00  6.8875e+00  1e-05  1e-10  1e-10
 7:  6.8875e+00  6.8875e+00  1e-05  1e-10  2e-12
 8:  6.8875e+00  6.8875e+00  6e-06  1e-10  1e-12
Optimal solution found.
The calculated probability is:  [0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00474884 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.03202504 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.37854507 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00162727 0.00119522 0.00119522 0.00119507 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119497 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119519 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00293605
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00163381 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119515 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522 0.00119522
 0.00119522 0.00119522 0.00119522 0.00119522]
current clients pool:  [INFO][10:27:02]: [Server #1127936] Selected clients: [235 228  32 491 273 264  40 431 462 275]
[INFO][10:27:02]: [Server #1127936] Selecting client #235 for training.
[INFO][10:27:02]: [Server #1127936] Sending the current model to client #235 (simulated).
[INFO][10:27:02]: [Server #1127936] Sending 0.24 MB of payload data to client #235 (simulated).
[INFO][10:27:02]: [Server #1127936] Selecting client #228 for training.
[INFO][10:27:02]: [Server #1127936] Sending the current model to client #228 (simulated).
[INFO][10:27:02]: [Server #1127936] Sending 0.24 MB of payload data to client #228 (simulated).
[INFO][10:27:02]: [Server #1127936] Selecting client #32 for training.
[INFO][10:27:02]: [Server #1127936] Sending the current model to client #32 (simulated).
[INFO][10:27:02]: [Client #235] Selected by the server.
[INFO][10:27:02]: [Client #235] Loading its data source...
[INFO][10:27:02]: [Client #235] Dataset size: 60000
[INFO][10:27:02]: [Client #235] Sampler: noniid
[INFO][10:27:02]: [Server #1127936] Sending 0.24 MB of payload data to client #32 (simulated).
[INFO][10:27:02]: [Client #228] Selected by the server.
[INFO][10:27:02]: [Client #228] Loading its data source...
[INFO][10:27:02]: [Client #32] Selected by the server.
[INFO][10:27:02]: [Client #228] Dataset size: 60000
[INFO][10:27:02]: [Client #32] Loading its data source...
[INFO][10:27:02]: [Client #228] Sampler: noniid
[INFO][10:27:02]: [Client #32] Dataset size: 60000
[INFO][10:27:02]: [Client #235] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:27:02]: [Client #32] Sampler: noniid
[INFO][10:27:02]: [Client #32] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:27:02]: [93m[1m[Client #235] Started training in communication round #70.[0m
[INFO][10:27:02]: [93m[1m[Client #32] Started training in communication round #70.[0m
[INFO][10:27:02]: [Client #228] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:27:02]: [93m[1m[Client #228] Started training in communication round #70.[0m
[INFO][10:27:05]: [Client #32] Loading the dataset.
[INFO][10:27:05]: [Client #235] Loading the dataset.
[INFO][10:27:05]: [Client #228] Loading the dataset.
[INFO][10:27:10]: [Client #32] Epoch: [1/5][0/10]	Loss: 0.003986
[INFO][10:27:10]: [Client #228] Epoch: [1/5][0/10]	Loss: 0.000456
[INFO][10:27:11]: [Client #32] Epoch: [2/5][0/10]	Loss: 0.000983
[INFO][10:27:11]: [Client #235] Epoch: [1/5][0/10]	Loss: 0.000799
[INFO][10:27:11]: [Client #228] Epoch: [2/5][0/10]	Loss: 0.000427
[INFO][10:27:11]: [Client #32] Epoch: [3/5][0/10]	Loss: 0.000041
[INFO][10:27:11]: [Client #235] Epoch: [2/5][0/10]	Loss: 0.000503
[INFO][10:27:11]: [Client #228] Epoch: [3/5][0/10]	Loss: 0.000327
[INFO][10:27:11]: [Client #32] Epoch: [4/5][0/10]	Loss: 0.000006
[INFO][10:27:11]: [Client #228] Epoch: [4/5][0/10]	Loss: 0.000035
[INFO][10:27:11]: [Client #235] Epoch: [3/5][0/10]	Loss: 0.000035
[INFO][10:27:11]: [Client #32] Epoch: [5/5][0/10]	Loss: 0.002457
[INFO][10:27:11]: [Client #235] Epoch: [4/5][0/10]	Loss: 0.000078
[INFO][10:27:11]: [Client #228] Epoch: [5/5][0/10]	Loss: 0.000591
[INFO][10:27:11]: [Client #32] Model saved to /data/ykang/plato/results/test/model/lenet5_32_1127979.pth.
[INFO][10:27:11]: [Client #228] Model saved to /data/ykang/plato/results/test/model/lenet5_228_1127978.pth.
[INFO][10:27:11]: [Client #235] Epoch: [5/5][0/10]	Loss: 0.000666
[INFO][10:27:11]: [Client #235] Model saved to /data/ykang/plato/results/test/model/lenet5_235_1127977.pth.
[INFO][10:27:12]: [Client #32] Loading a model from /data/ykang/plato/results/test/model/lenet5_32_1127979.pth.
[INFO][10:27:12]: [Client #32] Model trained.
[INFO][10:27:12]: [Client #32] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:27:12]: [Server #1127936] Received 0.24 MB of payload data from client #32 (simulated).
[INFO][10:27:12]: [Client #228] Loading a model from /data/ykang/plato/results/test/model/lenet5_228_1127978.pth.
[INFO][10:27:12]: [Client #228] Model trained.
[INFO][10:27:12]: [Client #228] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:27:12]: [Server #1127936] Received 0.24 MB of payload data from client #228 (simulated).
[INFO][10:27:12]: [Client #235] Loading a model from /data/ykang/plato/results/test/model/lenet5_235_1127977.pth.
[INFO][10:27:12]: [Client #235] Model trained.
[INFO][10:27:12]: [Client #235] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:27:12]: [Server #1127936] Received 0.24 MB of payload data from client #235 (simulated).
[INFO][10:27:12]: [Server #1127936] Selecting client #491 for training.
[INFO][10:27:12]: [Server #1127936] Sending the current model to client #491 (simulated).
[INFO][10:27:12]: [Server #1127936] Sending 0.24 MB of payload data to client #491 (simulated).
[INFO][10:27:12]: [Server #1127936] Selecting client #273 for training.
[INFO][10:27:12]: [Server #1127936] Sending the current model to client #273 (simulated).
[INFO][10:27:12]: [Server #1127936] Sending 0.24 MB of payload data to client #273 (simulated).
[INFO][10:27:12]: [Server #1127936] Selecting client #264 for training.
[INFO][10:27:12]: [Server #1127936] Sending the current model to client #264 (simulated).
[INFO][10:27:12]: [Client #491] Selected by the server.
[INFO][10:27:12]: [Client #491] Loading its data source...
[INFO][10:27:12]: [Client #491] Dataset size: 60000
[INFO][10:27:12]: [Client #491] Sampler: noniid
[INFO][10:27:12]: [Server #1127936] Sending 0.24 MB of payload data to client #264 (simulated).
[INFO][10:27:12]: [Client #273] Selected by the server.
[INFO][10:27:12]: [Client #273] Loading its data source...
[INFO][10:27:12]: [Client #273] Dataset size: 60000
[INFO][10:27:12]: [Client #273] Sampler: noniid
[INFO][10:27:12]: [Client #264] Selected by the server.
[INFO][10:27:12]: [Client #264] Loading its data source...
[INFO][10:27:12]: [Client #264] Dataset size: 60000
[INFO][10:27:12]: [Client #264] Sampler: noniid
[INFO][10:27:12]: [Client #491] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:27:12]: [Client #273] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:27:12]: [Client #264] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:27:12]: [93m[1m[Client #264] Started training in communication round #70.[0m
[INFO][10:27:12]: [93m[1m[Client #491] Started training in communication round #70.[0m
[INFO][10:27:12]: [93m[1m[Client #273] Started training in communication round #70.[0m
[INFO][10:27:14]: [Client #264] Loading the dataset.
[INFO][10:27:14]: [Client #273] Loading the dataset.
[INFO][10:27:14]: [Client #491] Loading the dataset.
[INFO][10:27:22]: [Client #273] Epoch: [1/5][0/10]	Loss: 0.004735
[INFO][10:27:22]: [Client #264] Epoch: [1/5][0/10]	Loss: 0.006860
[INFO][10:27:22]: [Client #491] Epoch: [1/5][0/10]	Loss: 0.000080
[INFO][10:27:22]: [Client #273] Epoch: [2/5][0/10]	Loss: 0.003715
[INFO][10:27:22]: [Client #264] Epoch: [2/5][0/10]	Loss: 0.001383
[INFO][10:27:22]: [Client #273] Epoch: [3/5][0/10]	Loss: 0.000188
[INFO][10:27:22]: [Client #491] Epoch: [2/5][0/10]	Loss: 0.001211
[INFO][10:27:22]: [Client #273] Epoch: [4/5][0/10]	Loss: 0.045234
[INFO][10:27:22]: [Client #264] Epoch: [3/5][0/10]	Loss: 0.000089
[INFO][10:27:22]: [Client #491] Epoch: [3/5][0/10]	Loss: 0.000002
[INFO][10:27:22]: [Client #273] Epoch: [5/5][0/10]	Loss: 0.044625
[INFO][10:27:22]: [Client #264] Epoch: [4/5][0/10]	Loss: 0.000009
[INFO][10:27:22]: [Client #491] Epoch: [4/5][0/10]	Loss: 0.000002
[INFO][10:27:22]: [Client #273] Model saved to /data/ykang/plato/results/test/model/lenet5_273_1127978.pth.
[INFO][10:27:22]: [Client #264] Epoch: [5/5][0/10]	Loss: 0.000012
[INFO][10:27:22]: [Client #491] Epoch: [5/5][0/10]	Loss: 0.000015
[INFO][10:27:22]: [Client #264] Model saved to /data/ykang/plato/results/test/model/lenet5_264_1127979.pth.
[INFO][10:27:22]: [Client #491] Model saved to /data/ykang/plato/results/test/model/lenet5_491_1127977.pth.
[INFO][10:27:23]: [Client #273] Loading a model from /data/ykang/plato/results/test/model/lenet5_273_1127978.pth.
[INFO][10:27:23]: [Client #273] Model trained.
[INFO][10:27:23]: [Client #273] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:27:23]: [Server #1127936] Received 0.24 MB of payload data from client #273 (simulated).
[INFO][10:27:23]: [Client #264] Loading a model from /data/ykang/plato/results/test/model/lenet5_264_1127979.pth.
[INFO][10:27:23]: [Client #264] Model trained.
[INFO][10:27:23]: [Client #264] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:27:23]: [Server #1127936] Received 0.24 MB of payload data from client #264 (simulated).
[INFO][10:27:23]: [Client #491] Loading a model from /data/ykang/plato/results/test/model/lenet5_491_1127977.pth.
[INFO][10:27:23]: [Client #491] Model trained.
[INFO][10:27:23]: [Client #491] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:27:23]: [Server #1127936] Received 0.24 MB of payload data from client #491 (simulated).
[INFO][10:27:23]: [Server #1127936] Selecting client #40 for training.
[INFO][10:27:23]: [Server #1127936] Sending the current model to client #40 (simulated).
[INFO][10:27:23]: [Server #1127936] Sending 0.24 MB of payload data to client #40 (simulated).
[INFO][10:27:23]: [Server #1127936] Selecting client #431 for training.
[INFO][10:27:23]: [Server #1127936] Sending the current model to client #431 (simulated).
[INFO][10:27:23]: [Server #1127936] Sending 0.24 MB of payload data to client #431 (simulated).
[INFO][10:27:23]: [Server #1127936] Selecting client #462 for training.
[INFO][10:27:23]: [Server #1127936] Sending the current model to client #462 (simulated).
[INFO][10:27:23]: [Client #40] Selected by the server.
[INFO][10:27:23]: [Client #40] Loading its data source...
[INFO][10:27:23]: [Client #40] Dataset size: 60000
[INFO][10:27:23]: [Client #40] Sampler: noniid
[INFO][10:27:23]: [Server #1127936] Sending 0.24 MB of payload data to client #462 (simulated).
[INFO][10:27:23]: [Client #431] Selected by the server.
[INFO][10:27:23]: [Client #462] Selected by the server.
[INFO][10:27:23]: [Client #431] Loading its data source...
[INFO][10:27:23]: [Client #431] Dataset size: 60000
[INFO][10:27:23]: [Client #462] Loading its data source...
[INFO][10:27:23]: [Client #431] Sampler: noniid
[INFO][10:27:23]: [Client #462] Dataset size: 60000
[INFO][10:27:23]: [Client #462] Sampler: noniid
[INFO][10:27:23]: [Client #40] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:27:23]: [Client #462] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:27:23]: [Client #431] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:27:23]: [93m[1m[Client #40] Started training in communication round #70.[0m
[INFO][10:27:23]: [93m[1m[Client #431] Started training in communication round #70.[0m
[INFO][10:27:23]: [93m[1m[Client #462] Started training in communication round #70.[0m
[INFO][10:27:25]: [Client #431] Loading the dataset.
[INFO][10:27:25]: [Client #462] Loading the dataset.
[INFO][10:27:25]: [Client #40] Loading the dataset.
[INFO][10:27:32]: [Client #462] Epoch: [1/5][0/10]	Loss: 0.005949
[INFO][10:27:32]: [Client #40] Epoch: [1/5][0/10]	Loss: 0.001346
[INFO][10:27:32]: [Client #431] Epoch: [1/5][0/10]	Loss: 0.000999
[INFO][10:27:32]: [Client #462] Epoch: [2/5][0/10]	Loss: 0.000001
[INFO][10:27:32]: [Client #431] Epoch: [2/5][0/10]	Loss: 0.000297
[INFO][10:27:32]: [Client #40] Epoch: [2/5][0/10]	Loss: 0.000355
[INFO][10:27:32]: [Client #431] Epoch: [3/5][0/10]	Loss: 0.000001
[INFO][10:27:32]: [Client #462] Epoch: [3/5][0/10]	Loss: 0.000003
[INFO][10:27:32]: [Client #40] Epoch: [3/5][0/10]	Loss: 0.017690
[INFO][10:27:32]: [Client #462] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:27:32]: [Client #431] Epoch: [4/5][0/10]	Loss: 0.000004
[INFO][10:27:32]: [Client #40] Epoch: [4/5][0/10]	Loss: 0.010144
[INFO][10:27:32]: [Client #462] Epoch: [5/5][0/10]	Loss: 0.000013
[INFO][10:27:32]: [Client #462] Model saved to /data/ykang/plato/results/test/model/lenet5_462_1127979.pth.
[INFO][10:27:32]: [Client #40] Epoch: [5/5][0/10]	Loss: 0.032209
[INFO][10:27:32]: [Client #431] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:27:32]: [Client #40] Model saved to /data/ykang/plato/results/test/model/lenet5_40_1127977.pth.
[INFO][10:27:32]: [Client #431] Model saved to /data/ykang/plato/results/test/model/lenet5_431_1127978.pth.
[INFO][10:27:33]: [Client #462] Loading a model from /data/ykang/plato/results/test/model/lenet5_462_1127979.pth.
[INFO][10:27:33]: [Client #462] Model trained.
[INFO][10:27:33]: [Client #462] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:27:33]: [Server #1127936] Received 0.24 MB of payload data from client #462 (simulated).
[INFO][10:27:33]: [Client #40] Loading a model from /data/ykang/plato/results/test/model/lenet5_40_1127977.pth.
[INFO][10:27:33]: [Client #40] Model trained.
[INFO][10:27:33]: [Client #40] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:27:33]: [Server #1127936] Received 0.24 MB of payload data from client #40 (simulated).
[INFO][10:27:33]: [Client #431] Loading a model from /data/ykang/plato/results/test/model/lenet5_431_1127978.pth.
[INFO][10:27:33]: [Client #431] Model trained.
[INFO][10:27:33]: [Client #431] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:27:33]: [Server #1127936] Received 0.24 MB of payload data from client #431 (simulated).
[INFO][10:27:33]: [Server #1127936] Selecting client #275 for training.
[INFO][10:27:33]: [Server #1127936] Sending the current model to client #275 (simulated).
[INFO][10:27:33]: [Server #1127936] Sending 0.24 MB of payload data to client #275 (simulated).
[INFO][10:27:33]: [Client #275] Selected by the server.
[INFO][10:27:33]: [Client #275] Loading its data source...
[INFO][10:27:33]: [Client #275] Dataset size: 60000
[INFO][10:27:33]: [Client #275] Sampler: noniid
[INFO][10:27:33]: [Client #275] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:27:33]: [93m[1m[Client #275] Started training in communication round #70.[0m
[INFO][10:27:35]: [Client #275] Loading the dataset.
[INFO][10:27:41]: [Client #275] Epoch: [1/5][0/10]	Loss: 0.000870
[INFO][10:27:41]: [Client #275] Epoch: [2/5][0/10]	Loss: 0.000344
[INFO][10:27:41]: [Client #275] Epoch: [3/5][0/10]	Loss: 0.000047
[INFO][10:27:41]: [Client #275] Epoch: [4/5][0/10]	Loss: 0.007414
[INFO][10:27:41]: [Client #275] Epoch: [5/5][0/10]	Loss: 0.256165
[INFO][10:27:41]: [Client #275] Model saved to /data/ykang/plato/results/test/model/lenet5_275_1127977.pth.
[INFO][10:27:42]: [Client #275] Loading a model from /data/ykang/plato/results/test/model/lenet5_275_1127977.pth.
[INFO][10:27:42]: [Client #275] Model trained.
[INFO][10:27:42]: [Client #275] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:27:42]: [Server #1127936] Received 0.24 MB of payload data from client #275 (simulated).
[INFO][10:27:42]: [Server #1127936] Adding client #405 to the list of clients for aggregation.
[INFO][10:27:42]: [Server #1127936] Adding client #452 to the list of clients for aggregation.
[INFO][10:27:42]: [Server #1127936] Adding client #352 to the list of clients for aggregation.
[INFO][10:27:42]: [Server #1127936] Adding client #430 to the list of clients for aggregation.
[INFO][10:27:42]: [Server #1127936] Adding client #41 to the list of clients for aggregation.
[INFO][10:27:42]: [Server #1127936] Adding client #365 to the list of clients for aggregation.
[INFO][10:27:42]: [Server #1127936] Adding client #132 to the list of clients for aggregation.
[INFO][10:27:42]: [Server #1127936] Adding client #499 to the list of clients for aggregation.
[INFO][10:27:42]: [Server #1127936] Adding client #32 to the list of clients for aggregation.
[INFO][10:27:42]: [Server #1127936] Adding client #431 to the list of clients for aggregation.
[INFO][10:27:42]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00126202 0.         0.         0.         0.
 0.         0.         0.         0.         0.01395164 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00456211
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00288527 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00198558 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00331805 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00234828 0.01592836 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00128018 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00476052 0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 0. 1. 0. 0. 0. 1. 0. 0. 3. 1. 0. 0. 0. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 2. 0. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 2. 1. 1. 1. 0.
 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 2. 1. 6. 0. 1. 2. 1. 0. 1. 1. 2. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 3. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 0. 6. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 2. 0. 0.
 1. 0. 1. 2. 0. 0. 0. 2. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 4. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0.
 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0.
 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0.
 1. 1. 1. 0. 4. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 3. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 0. 1. 0. 6. 1. 1.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 5. 0. 0. 0. 1. 0. 0. 0. 2. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 2. 2. 1. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00126202 0.         0.         0.         0.
 0.         0.         0.         0.         0.01395164 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00456211
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00288527 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00198558 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00331805 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00234828 0.01592836 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00128018 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00476052 0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:27:44]: [Server #1127936] Global model accuracy: 95.90%

[INFO][10:27:44]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_70.pth.
[INFO][10:27:44]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_70.pth.
[INFO][10:27:44]: [93m[1m
[Server #1127936] Starting round 71/100.[0m
[0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  4e-04  7e-09  7e-09
 5:  6.8876e+00  6.8875e+00  9e-05  1e-09  1e-09
 6:  6.8875e+00  6.8875e+00  8e-05  6e-10  5e-10
 7:  6.8875e+00  6.8875e+00  7e-05  3e-09  3e-10
 8:  6.8875e+00  6.8875e+00  6e-05  4e-09  3e-10
 9:  6.8875e+00  6.8875e+00  2e-05  3e-08  2e-09
10:  6.8875e+00  6.8875e+00  6e-06  1e-08  1e-09
Optimal solution found.
The calculated probability is:  [1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630734e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 9.21358492e-01
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.98862919e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.67429886e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.80827759e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.68499090e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.66121743e-04 1.60625032e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.63578889e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.60630770e-04 1.60630770e-04 1.60630770e-04 1.60630770e-04
 1.72162255e-04 1.60630770e-04]
current clients pool:  [INFO][10:27:45]: [Server #1127936] Selected clients: [271  41  76 196 478  57 326 263 176  99]
[INFO][10:27:45]: [Server #1127936] Selecting client #271 for training.
[INFO][10:27:45]: [Server #1127936] Sending the current model to client #271 (simulated).
[INFO][10:27:45]: [Server #1127936] Sending 0.24 MB of payload data to client #271 (simulated).
[INFO][10:27:45]: [Server #1127936] Selecting client #41 for training.
[INFO][10:27:45]: [Server #1127936] Sending the current model to client #41 (simulated).
[INFO][10:27:45]: [Server #1127936] Sending 0.24 MB of payload data to client #41 (simulated).
[INFO][10:27:45]: [Client #271] Selected by the server.
[INFO][10:27:45]: [Client #271] Loading its data source...
[INFO][10:27:45]: [Client #271] Dataset size: 60000
[INFO][10:27:45]: [Client #271] Sampler: noniid
[INFO][10:27:45]: [Client #271] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:27:45]: [Server #1127936] Selecting client #76 for training.
[INFO][10:27:45]: [Server #1127936] Sending the current model to client #76 (simulated).
[INFO][10:27:45]: [Server #1127936] Sending 0.24 MB of payload data to client #76 (simulated).
[INFO][10:27:45]: [Client #41] Selected by the server.
[INFO][10:27:45]: [Client #41] Loading its data source...
[INFO][10:27:45]: [Client #41] Dataset size: 60000
[INFO][10:27:45]: [Client #41] Sampler: noniid
[INFO][10:27:45]: [Client #76] Selected by the server.
[INFO][10:27:45]: [Client #76] Loading its data source...
[INFO][10:27:45]: [Client #76] Dataset size: 60000
[INFO][10:27:45]: [Client #76] Sampler: noniid
[INFO][10:27:45]: [Client #41] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:27:45]: [Client #76] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:27:45]: [93m[1m[Client #271] Started training in communication round #71.[0m
[INFO][10:27:45]: [93m[1m[Client #41] Started training in communication round #71.[0m
[INFO][10:27:45]: [93m[1m[Client #76] Started training in communication round #71.[0m
[INFO][10:27:47]: [Client #76] Loading the dataset.
[INFO][10:27:47]: [Client #271] Loading the dataset.
[INFO][10:27:47]: [Client #41] Loading the dataset.
[INFO][10:27:53]: [Client #271] Epoch: [1/5][0/10]	Loss: 0.004120
[INFO][10:27:53]: [Client #76] Epoch: [1/5][0/10]	Loss: 0.009675
[INFO][10:27:53]: [Client #41] Epoch: [1/5][0/10]	Loss: 0.001141
[INFO][10:27:53]: [Client #271] Epoch: [2/5][0/10]	Loss: 0.000012
[INFO][10:27:53]: [Client #76] Epoch: [2/5][0/10]	Loss: 0.001762
[INFO][10:27:53]: [Client #41] Epoch: [2/5][0/10]	Loss: 0.000190
[INFO][10:27:53]: [Client #76] Epoch: [3/5][0/10]	Loss: 0.000049
[INFO][10:27:53]: [Client #271] Epoch: [3/5][0/10]	Loss: 0.000012
[INFO][10:27:53]: [Client #41] Epoch: [3/5][0/10]	Loss: 0.002116
[INFO][10:27:53]: [Client #76] Epoch: [4/5][0/10]	Loss: 0.000152
[INFO][10:27:53]: [Client #271] Epoch: [4/5][0/10]	Loss: 0.000001
[INFO][10:27:53]: [Client #41] Epoch: [4/5][0/10]	Loss: 0.132221
[INFO][10:27:54]: [Client #76] Epoch: [5/5][0/10]	Loss: 0.003380
[INFO][10:27:54]: [Client #41] Epoch: [5/5][0/10]	Loss: 0.034967
[INFO][10:27:54]: [Client #271] Epoch: [5/5][0/10]	Loss: 0.000002
[INFO][10:27:54]: [Client #76] Model saved to /data/ykang/plato/results/test/model/lenet5_76_1127979.pth.
[INFO][10:27:54]: [Client #41] Model saved to /data/ykang/plato/results/test/model/lenet5_41_1127978.pth.
[INFO][10:27:54]: [Client #271] Model saved to /data/ykang/plato/results/test/model/lenet5_271_1127977.pth.
[INFO][10:27:55]: [Client #41] Loading a model from /data/ykang/plato/results/test/model/lenet5_41_1127978.pth.
[INFO][10:27:55]: [Client #41] Model trained.
[INFO][10:27:55]: [Client #41] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:27:55]: [Server #1127936] Received 0.24 MB of payload data from client #41 (simulated).
[INFO][10:27:55]: [Client #76] Loading a model from /data/ykang/plato/results/test/model/lenet5_76_1127979.pth.
[INFO][10:27:55]: [Client #76] Model trained.
[INFO][10:27:55]: [Client #76] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:27:55]: [Server #1127936] Received 0.24 MB of payload data from client #76 (simulated).
[INFO][10:27:55]: [Client #271] Loading a model from /data/ykang/plato/results/test/model/lenet5_271_1127977.pth.
[INFO][10:27:55]: [Client #271] Model trained.
[INFO][10:27:55]: [Client #271] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:27:55]: [Server #1127936] Received 0.24 MB of payload data from client #271 (simulated).
[INFO][10:27:55]: [Server #1127936] Selecting client #196 for training.
[INFO][10:27:55]: [Server #1127936] Sending the current model to client #196 (simulated).
[INFO][10:27:55]: [Server #1127936] Sending 0.24 MB of payload data to client #196 (simulated).
[INFO][10:27:55]: [Server #1127936] Selecting client #478 for training.
[INFO][10:27:55]: [Server #1127936] Sending the current model to client #478 (simulated).
[INFO][10:27:55]: [Server #1127936] Sending 0.24 MB of payload data to client #478 (simulated).
[INFO][10:27:55]: [Server #1127936] Selecting client #57 for training.
[INFO][10:27:55]: [Client #196] Selected by the server.
[INFO][10:27:55]: [Client #196] Loading its data source...
[INFO][10:27:55]: [Server #1127936] Sending the current model to client #57 (simulated).
[INFO][10:27:55]: [Client #196] Dataset size: 60000
[INFO][10:27:55]: [Client #196] Sampler: noniid
[INFO][10:27:55]: [Server #1127936] Sending 0.24 MB of payload data to client #57 (simulated).
[INFO][10:27:55]: [Client #478] Selected by the server.
[INFO][10:27:55]: [Client #478] Loading its data source...
[INFO][10:27:55]: [Client #478] Dataset size: 60000
[INFO][10:27:55]: [Client #57] Selected by the server.
[INFO][10:27:55]: [Client #478] Sampler: noniid
[INFO][10:27:55]: [Client #57] Loading its data source...
[INFO][10:27:55]: [Client #57] Dataset size: 60000
[INFO][10:27:55]: [Client #57] Sampler: noniid
[INFO][10:27:55]: [Client #196] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:27:55]: [Client #478] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:27:55]: [Client #57] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:27:55]: [93m[1m[Client #57] Started training in communication round #71.[0m
[INFO][10:27:55]: [93m[1m[Client #196] Started training in communication round #71.[0m
[INFO][10:27:55]: [93m[1m[Client #478] Started training in communication round #71.[0m
[INFO][10:27:57]: [Client #478] Loading the dataset.
[INFO][10:27:57]: [Client #57] Loading the dataset.
[INFO][10:27:57]: [Client #196] Loading the dataset.
[INFO][10:28:03]: [Client #57] Epoch: [1/5][0/10]	Loss: 0.003814
[INFO][10:28:03]: [Client #478] Epoch: [1/5][0/10]	Loss: 0.000395
[INFO][10:28:03]: [Client #196] Epoch: [1/5][0/10]	Loss: 0.001104
[INFO][10:28:03]: [Client #57] Epoch: [2/5][0/10]	Loss: 0.000019
[INFO][10:28:03]: [Client #478] Epoch: [2/5][0/10]	Loss: 0.003207
[INFO][10:28:03]: [Client #196] Epoch: [2/5][0/10]	Loss: 0.000037
[INFO][10:28:03]: [Client #57] Epoch: [3/5][0/10]	Loss: 0.000008
[INFO][10:28:03]: [Client #478] Epoch: [3/5][0/10]	Loss: 0.000024
[INFO][10:28:03]: [Client #196] Epoch: [3/5][0/10]	Loss: 0.000008
[INFO][10:28:03]: [Client #57] Epoch: [4/5][0/10]	Loss: 0.002019
[INFO][10:28:03]: [Client #478] Epoch: [4/5][0/10]	Loss: 0.000001
[INFO][10:28:04]: [Client #196] Epoch: [4/5][0/10]	Loss: 0.000003
[INFO][10:28:04]: [Client #57] Epoch: [5/5][0/10]	Loss: 0.092322
[INFO][10:28:04]: [Client #478] Epoch: [5/5][0/10]	Loss: 0.000019
[INFO][10:28:04]: [Client #57] Model saved to /data/ykang/plato/results/test/model/lenet5_57_1127979.pth.
[INFO][10:28:04]: [Client #478] Model saved to /data/ykang/plato/results/test/model/lenet5_478_1127978.pth.
[INFO][10:28:04]: [Client #196] Epoch: [5/5][0/10]	Loss: 0.000124
[INFO][10:28:04]: [Client #196] Model saved to /data/ykang/plato/results/test/model/lenet5_196_1127977.pth.
[INFO][10:28:04]: [Client #57] Loading a model from /data/ykang/plato/results/test/model/lenet5_57_1127979.pth.
[INFO][10:28:04]: [Client #57] Model trained.
[INFO][10:28:04]: [Client #57] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:28:04]: [Server #1127936] Received 0.24 MB of payload data from client #57 (simulated).
[INFO][10:28:04]: [Client #478] Loading a model from /data/ykang/plato/results/test/model/lenet5_478_1127978.pth.
[INFO][10:28:04]: [Client #478] Model trained.
[INFO][10:28:04]: [Client #478] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:28:04]: [Server #1127936] Received 0.24 MB of payload data from client #478 (simulated).
[INFO][10:28:05]: [Client #196] Loading a model from /data/ykang/plato/results/test/model/lenet5_196_1127977.pth.
[INFO][10:28:05]: [Client #196] Model trained.
[INFO][10:28:05]: [Client #196] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:28:05]: [Server #1127936] Received 0.24 MB of payload data from client #196 (simulated).
[INFO][10:28:05]: [Server #1127936] Selecting client #326 for training.
[INFO][10:28:05]: [Server #1127936] Sending the current model to client #326 (simulated).
[INFO][10:28:05]: [Server #1127936] Sending 0.24 MB of payload data to client #326 (simulated).
[INFO][10:28:05]: [Server #1127936] Selecting client #263 for training.
[INFO][10:28:05]: [Server #1127936] Sending the current model to client #263 (simulated).
[INFO][10:28:05]: [Server #1127936] Sending 0.24 MB of payload data to client #263 (simulated).
[INFO][10:28:05]: [Server #1127936] Selecting client #176 for training.
[INFO][10:28:05]: [Server #1127936] Sending the current model to client #176 (simulated).
[INFO][10:28:05]: [Client #326] Selected by the server.
[INFO][10:28:05]: [Client #326] Loading its data source...
[INFO][10:28:05]: [Client #326] Dataset size: 60000
[INFO][10:28:05]: [Client #326] Sampler: noniid
[INFO][10:28:05]: [Server #1127936] Sending 0.24 MB of payload data to client #176 (simulated).
[INFO][10:28:05]: [Client #176] Selected by the server.
[INFO][10:28:05]: [Client #263] Selected by the server.
[INFO][10:28:05]: [Client #176] Loading its data source...
[INFO][10:28:05]: [Client #263] Loading its data source...
[INFO][10:28:05]: [Client #176] Dataset size: 60000
[INFO][10:28:05]: [Client #176] Sampler: noniid
[INFO][10:28:05]: [Client #263] Dataset size: 60000
[INFO][10:28:05]: [Client #263] Sampler: noniid
[INFO][10:28:05]: [Client #326] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:28:05]: [Client #263] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:28:05]: [Client #176] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:28:05]: [93m[1m[Client #263] Started training in communication round #71.[0m
[INFO][10:28:05]: [93m[1m[Client #176] Started training in communication round #71.[0m
[INFO][10:28:05]: [93m[1m[Client #326] Started training in communication round #71.[0m
[INFO][10:28:07]: [Client #263] Loading the dataset.
[INFO][10:28:07]: [Client #176] Loading the dataset.
[INFO][10:28:07]: [Client #326] Loading the dataset.
[INFO][10:28:13]: [Client #263] Epoch: [1/5][0/10]	Loss: 0.026185
[INFO][10:28:13]: [Client #326] Epoch: [1/5][0/10]	Loss: 0.037618
[INFO][10:28:13]: [Client #176] Epoch: [1/5][0/10]	Loss: 0.002884
[INFO][10:28:13]: [Client #263] Epoch: [2/5][0/10]	Loss: 0.000256
[INFO][10:28:13]: [Client #326] Epoch: [2/5][0/10]	Loss: 0.000019
[INFO][10:28:13]: [Client #176] Epoch: [2/5][0/10]	Loss: 0.000270
[INFO][10:28:13]: [Client #263] Epoch: [3/5][0/10]	Loss: 0.000131
[INFO][10:28:13]: [Client #326] Epoch: [3/5][0/10]	Loss: 0.000000
[INFO][10:28:13]: [Client #176] Epoch: [3/5][0/10]	Loss: 0.000154
[INFO][10:28:13]: [Client #263] Epoch: [4/5][0/10]	Loss: 0.000117
[INFO][10:28:13]: [Client #326] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:28:13]: [Client #176] Epoch: [4/5][0/10]	Loss: 0.024960
[INFO][10:28:13]: [Client #263] Epoch: [5/5][0/10]	Loss: 0.000085
[INFO][10:28:14]: [Client #326] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:28:14]: [Client #176] Epoch: [5/5][0/10]	Loss: 0.307687
[INFO][10:28:14]: [Client #263] Model saved to /data/ykang/plato/results/test/model/lenet5_263_1127978.pth.
[INFO][10:28:14]: [Client #326] Model saved to /data/ykang/plato/results/test/model/lenet5_326_1127977.pth.
[INFO][10:28:14]: [Client #176] Model saved to /data/ykang/plato/results/test/model/lenet5_176_1127979.pth.
[INFO][10:28:14]: [Client #263] Loading a model from /data/ykang/plato/results/test/model/lenet5_263_1127978.pth.
[INFO][10:28:14]: [Client #263] Model trained.
[INFO][10:28:14]: [Client #263] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:28:14]: [Server #1127936] Received 0.24 MB of payload data from client #263 (simulated).
[INFO][10:28:14]: [Client #326] Loading a model from /data/ykang/plato/results/test/model/lenet5_326_1127977.pth.
[INFO][10:28:14]: [Client #326] Model trained.
[INFO][10:28:14]: [Client #326] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:28:14]: [Server #1127936] Received 0.24 MB of payload data from client #326 (simulated).
[INFO][10:28:14]: [Client #176] Loading a model from /data/ykang/plato/results/test/model/lenet5_176_1127979.pth.
[INFO][10:28:14]: [Client #176] Model trained.
[INFO][10:28:14]: [Client #176] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:28:14]: [Server #1127936] Received 0.24 MB of payload data from client #176 (simulated).
[INFO][10:28:14]: [Server #1127936] Selecting client #99 for training.
[INFO][10:28:14]: [Server #1127936] Sending the current model to client #99 (simulated).
[INFO][10:28:14]: [Server #1127936] Sending 0.24 MB of payload data to client #99 (simulated).
[INFO][10:28:14]: [Client #99] Selected by the server.
[INFO][10:28:14]: [Client #99] Loading its data source...
[INFO][10:28:14]: [Client #99] Dataset size: 60000
[INFO][10:28:14]: [Client #99] Sampler: noniid
[INFO][10:28:14]: [Client #99] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:28:14]: [93m[1m[Client #99] Started training in communication round #71.[0m
[INFO][10:28:16]: [Client #99] Loading the dataset.
[INFO][10:28:22]: [Client #99] Epoch: [1/5][0/10]	Loss: 0.000571
[INFO][10:28:22]: [Client #99] Epoch: [2/5][0/10]	Loss: 0.002176
[INFO][10:28:22]: [Client #99] Epoch: [3/5][0/10]	Loss: 0.000033
[INFO][10:28:22]: [Client #99] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:28:22]: [Client #99] Epoch: [5/5][0/10]	Loss: 0.164849
[INFO][10:28:22]: [Client #99] Model saved to /data/ykang/plato/results/test/model/lenet5_99_1127977.pth.
[INFO][10:28:23]: [Client #99] Loading a model from /data/ykang/plato/results/test/model/lenet5_99_1127977.pth.
[INFO][10:28:23]: [Client #99] Model trained.
[INFO][10:28:23]: [Client #99] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:28:23]: [Server #1127936] Received 0.24 MB of payload data from client #99 (simulated).
[INFO][10:28:23]: [Server #1127936] Adding client #235 to the list of clients for aggregation.
[INFO][10:28:23]: [Server #1127936] Adding client #491 to the list of clients for aggregation.
[INFO][10:28:23]: [Server #1127936] Adding client #273 to the list of clients for aggregation.
[INFO][10:28:23]: [Server #1127936] Adding client #264 to the list of clients for aggregation.
[INFO][10:28:23]: [Server #1127936] Adding client #40 to the list of clients for aggregation.
[INFO][10:28:23]: [Server #1127936] Adding client #462 to the list of clients for aggregation.
[INFO][10:28:23]: [Server #1127936] Adding client #228 to the list of clients for aggregation.
[INFO][10:28:23]: [Server #1127936] Adding client #99 to the list of clients for aggregation.
[INFO][10:28:23]: [Server #1127936] Adding client #76 to the list of clients for aggregation.
[INFO][10:28:23]: [Server #1127936] Adding client #271 to the list of clients for aggregation.
[INFO][10:28:23]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 229, 230, 231, 232, 233, 234, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 265, 266, 267, 268, 269, 270, 271, 272, 274, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 463, 464, 465, 466, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01135633 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.0049787  0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.0074039  0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00943616
 0.         0.         0.         0.         0.         0.
 0.00160802 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00178846
 0.         0.         0.         0.         0.         0.
 0.01339554 0.         0.00295692 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00405026
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00460392 0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 0. 1. 0. 0. 0. 1. 0. 0. 3. 1. 0. 0. 0. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 2. 0. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 2. 1. 1. 1. 0.
 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 2. 1. 6. 0. 1. 2. 1. 0. 1. 1. 2. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 3. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 0. 6. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 2. 0. 0.
 1. 0. 1. 2. 0. 0. 0. 2. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 4. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1.
 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0.
 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0.
 1. 1. 1. 0. 4. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 3. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 0. 1. 0. 6. 1. 1.
 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 5. 0. 0. 0. 1. 0. 0. 0. 2. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 2. 2. 1. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01135633 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.0049787  0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.0074039  0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00943616
 0.         0.         0.         0.         0.         0.
 0.00160802 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00178846
 0.         0.         0.         0.         0.         0.
 0.01339554 0.         0.00295692 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00405026
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00460392 0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:28:25]: [Server #1127936] Global model accuracy: 96.32%

[INFO][10:28:25]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_71.pth.
[INFO][10:28:25]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_71.pth.
[INFO][10:28:25]: [93m[1m
[Server #1127936] Starting round 72/100.[0m
[INFO][10:28:26]: [Server #1127936] Selected clients: [ 40 104 112 415 288 194 214 240 468 433]
[INFO][10:28:26]: [Server #1127936] Selecting client #40 for training.
[INFO][10:28:26]: [Server #1127936] Sending the current model to client #40 (simulated).
[INFO][10:28:26]: [Server #1127936] Sending 0.24 MB of payload data to client #40 (simulated).
[INFO][10:28:26]: [Server #1127936] Selecting client #104 for training.
[INFO][10:28:26]: [Server #1127936] Sending the current model to client #104 (simulated).
[INFO][10:28:26]: [Server #1127936] Sending 0.24 MB of payload data to client #104 (simulated).
[INFO][10:28:26]: [Server #1127936] Selecting client #112 for training.
[INFO][10:28:26]: [Server #1127936] Sending the current model to client #112 (simulated).
[INFO][10:28:26]: [Client #40] Selected by the server.
[INFO][10:28:26]: [Client #40] Loading its data source...
[INFO][10:28:26]: [Client #40] Dataset size: 60000
[INFO][10:28:26]: [Client #40] Sampler: noniid
[INFO][10:28:26]: [Server #1127936] Sending 0.24 MB of payload data to client #112 (simulated).
[INFO][10:28:26]: [Client #104] Selected by the server.
[INFO][10:28:26]: [Client #104] Loading its data source...
[INFO][10:28:26]: [Client #104] Dataset size: 60000
[INFO][10:28:26]: [Client #104] Sampler: noniid
[INFO][10:28:26]: [Client #40] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:28:26]: [Client #104] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:28:26]: [93m[1m[Client #40] Started training in communication round #72.[0m
[INFO][10:28:26]: [Client #112] Selected by the server.
[INFO][10:28:26]: [Client #112] Loading its data source...
[INFO][10:28:26]: [Client #112] Dataset size: 60000
[INFO][10:28:26]: [Client #112] Sampler: noniid
[INFO][10:28:26]: [Client #112] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:28:26]: [93m[1m[Client #112] Started training in communication round #72.[0m
[INFO][10:28:26]: [93m[1m[Client #104] Started training in communication round #72.[0m
[INFO][10:28:28]: [Client #40] Loading the dataset.
[INFO][10:28:28]: [Client #104] Loading the dataset.
[INFO][10:28:28]: [Client #112] Loading the dataset.
[INFO][10:28:34]: [Client #40] Epoch: [1/5][0/10]	Loss: 0.000831
[INFO][10:28:34]: [Client #104] Epoch: [1/5][0/10]	Loss: 0.000468
[INFO][10:28:34]: [Client #112] Epoch: [1/5][0/10]	Loss: 0.000484
[INFO][10:28:34]: [Client #40] Epoch: [2/5][0/10]	Loss: 0.000969
[INFO][10:28:34]: [Client #104] Epoch: [2/5][0/10]	Loss: 0.000064
[INFO][10:28:34]: [Client #40] Epoch: [3/5][0/10]	Loss: 0.008373
[INFO][10:28:34]: [Client #112] Epoch: [2/5][0/10]	Loss: 0.000312
[INFO][10:28:34]: [Client #104] Epoch: [3/5][0/10]	Loss: 0.000001
[INFO][10:28:35]: [Client #40] Epoch: [4/5][0/10]	Loss: 0.005445
[INFO][10:28:35]: [Client #112] Epoch: [3/5][0/10]	Loss: 0.000099
[INFO][10:28:35]: [Client #104] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:28:35]: [Client #40] Epoch: [5/5][0/10]	Loss: 0.034741
[INFO][10:28:35]: [Client #40] Model saved to /data/ykang/plato/results/test/model/lenet5_40_1127977.pth.
[INFO][10:28:35]: [Client #112] Epoch: [4/5][0/10]	Loss: 0.025389
[INFO][10:28:35]: [Client #104] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:28:35]: [Client #104] Model saved to /data/ykang/plato/results/test/model/lenet5_104_1127978.pth.
[INFO][10:28:35]: [Client #112] Epoch: [5/5][0/10]	Loss: 0.073280
[INFO][10:28:35]: [Client #112] Model saved to /data/ykang/plato/results/test/model/lenet5_112_1127979.pth.
[INFO][10:28:35]: [Client #40] Loading a model from /data/ykang/plato/results/test/model/lenet5_40_1127977.pth.
[INFO][10:28:35]: [Client #40] Model trained.
[INFO][10:28:35]: [Client #40] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:28:35]: [Server #1127936] Received 0.24 MB of payload data from client #40 (simulated).
[INFO][10:28:36]: [Client #104] Loading a model from /data/ykang/plato/results/test/model/lenet5_104_1127978.pth.
[INFO][10:28:36]: [Client #104] Model trained.
[INFO][10:28:36]: [Client #104] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:28:36]: [Server #1127936] Received 0.24 MB of payload data from client #104 (simulated).
[INFO][10:28:36]: [Client #112] Loading a model from /data/ykang/plato/results/test/model/lenet5_112_1127979.pth.
[INFO][10:28:36]: [Client #112] Model trained.
[INFO][10:28:36]: [Client #112] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:28:36]: [Server #1127936] Received 0.24 MB of payload data from client #112 (simulated).
[INFO][10:28:36]: [Server #1127936] Selecting client #415 for training.
[INFO][10:28:36]: [Server #1127936] Sending the current model to client #415 (simulated).
[INFO][10:28:36]: [Server #1127936] Sending 0.24 MB of payload data to client #415 (simulated).
[INFO][10:28:36]: [Server #1127936] Selecting client #288 for training.
[INFO][10:28:36]: [Server #1127936] Sending the current model to client #288 (simulated).
[INFO][10:28:36]: [Server #1127936] Sending 0.24 MB of payload data to client #288 (simulated).
[INFO][10:28:36]: [Server #1127936] Selecting client #194 for training.
[INFO][10:28:36]: [Server #1127936] Sending the current model to client #194 (simulated).
[INFO][10:28:36]: [Client #415] Selected by the server.
[INFO][10:28:36]: [Client #415] Loading its data source...
[INFO][10:28:36]: [Client #415] Dataset size: 60000
[INFO][10:28:36]: [Client #415] Sampler: noniid
[INFO][10:28:36]: [Server #1127936] Sending 0.24 MB of payload data to client #194 (simulated).
[INFO][10:28:36]: [Client #415] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:28:36]: [Client #288] Selected by the server.
[INFO][10:28:36]: [Client #288] Loading its data source...
[INFO][10:28:36]: [Client #288] Dataset size: 60000
[INFO][10:28:36]: [Client #288] Sampler: noniid
[INFO][10:28:36]: [Client #194] Selected by the server.
[INFO][10:28:36]: [Client #194] Loading its data source...
[INFO][10:28:36]: [Client #194] Dataset size: 60000
[INFO][10:28:36]: [Client #194] Sampler: noniid
[INFO][10:28:36]: [Client #288] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:28:36]: [Client #194] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:28:36]: [93m[1m[Client #415] Started training in communication round #72.[0m
[INFO][10:28:36]: [93m[1m[Client #288] Started training in communication round #72.[0m
[INFO][10:28:36]: [93m[1m[Client #194] Started training in communication round #72.[0m
[INFO][10:28:38]: [Client #288] Loading the dataset.
[INFO][10:28:38]: [Client #194] Loading the dataset.
[INFO][10:28:38]: [Client #415] Loading the dataset.
[INFO][10:28:44]: [Client #288] Epoch: [1/5][0/10]	Loss: 0.006836
[INFO][10:28:44]: [Client #194] Epoch: [1/5][0/10]	Loss: 0.000991
[INFO][10:28:44]: [Client #288] Epoch: [2/5][0/10]	Loss: 0.000330
[INFO][10:28:44]: [Client #415] Epoch: [1/5][0/10]	Loss: 0.000744
[INFO][10:28:44]: [Client #415] Epoch: [2/5][0/10]	Loss: 0.000427
[INFO][10:28:44]: [Client #194] Epoch: [2/5][0/10]	Loss: 0.001133
[INFO][10:28:44]: [Client #288] Epoch: [3/5][0/10]	Loss: 0.000001
[INFO][10:28:44]: [Client #194] Epoch: [3/5][0/10]	Loss: 0.000028
[INFO][10:28:44]: [Client #415] Epoch: [3/5][0/10]	Loss: 0.000013
[INFO][10:28:44]: [Client #288] Epoch: [4/5][0/10]	Loss: 0.023682
[INFO][10:28:44]: [Client #415] Epoch: [4/5][0/10]	Loss: 0.000064
[INFO][10:28:44]: [Client #194] Epoch: [4/5][0/10]	Loss: 0.000136
[INFO][10:28:44]: [Client #288] Epoch: [5/5][0/10]	Loss: 0.000658
[INFO][10:28:44]: [Client #288] Model saved to /data/ykang/plato/results/test/model/lenet5_288_1127978.pth.
[INFO][10:28:44]: [Client #415] Epoch: [5/5][0/10]	Loss: 0.030428
[INFO][10:28:44]: [Client #194] Epoch: [5/5][0/10]	Loss: 0.000332
[INFO][10:28:44]: [Client #415] Model saved to /data/ykang/plato/results/test/model/lenet5_415_1127977.pth.
[INFO][10:28:44]: [Client #194] Model saved to /data/ykang/plato/results/test/model/lenet5_194_1127979.pth.
[INFO][10:28:45]: [Client #288] Loading a model from /data/ykang/plato/results/test/model/lenet5_288_1127978.pth.
[INFO][10:28:45]: [Client #288] Model trained.
[INFO][10:28:45]: [Client #288] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:28:45]: [Server #1127936] Received 0.24 MB of payload data from client #288 (simulated).
[INFO][10:28:45]: [Client #415] Loading a model from /data/ykang/plato/results/test/model/lenet5_415_1127977.pth.
[INFO][10:28:45]: [Client #415] Model trained.
[INFO][10:28:45]: [Client #415] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:28:45]: [Server #1127936] Received 0.24 MB of payload data from client #415 (simulated).
[INFO][10:28:45]: [Client #194] Loading a model from /data/ykang/plato/results/test/model/lenet5_194_1127979.pth.
[INFO][10:28:45]: [Client #194] Model trained.
[INFO][10:28:45]: [Client #194] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:28:45]: [Server #1127936] Received 0.24 MB of payload data from client #194 (simulated).
[INFO][10:28:45]: [Server #1127936] Selecting client #214 for training.
[INFO][10:28:45]: [Server #1127936] Sending the current model to client #214 (simulated).
[INFO][10:28:45]: [Server #1127936] Sending 0.24 MB of payload data to client #214 (simulated).
[INFO][10:28:45]: [Server #1127936] Selecting client #240 for training.
[INFO][10:28:45]: [Server #1127936] Sending the current model to client #240 (simulated).
[INFO][10:28:45]: [Server #1127936] Sending 0.24 MB of payload data to client #240 (simulated).
[INFO][10:28:45]: [Server #1127936] Selecting client #468 for training.
[INFO][10:28:45]: [Server #1127936] Sending the current model to client #468 (simulated).
[INFO][10:28:45]: [Client #214] Selected by the server.
[INFO][10:28:45]: [Client #214] Loading its data source...
[INFO][10:28:45]: [Client #214] Dataset size: 60000
[INFO][10:28:45]: [Client #214] Sampler: noniid
[INFO][10:28:45]: [Server #1127936] Sending 0.24 MB of payload data to client #468 (simulated).
[INFO][10:28:45]: [Client #240] Selected by the server.
[INFO][10:28:45]: [Client #468] Selected by the server.
[INFO][10:28:45]: [Client #240] Loading its data source...
[INFO][10:28:45]: [Client #240] Dataset size: 60000
[INFO][10:28:45]: [Client #468] Loading its data source...
[INFO][10:28:45]: [Client #240] Sampler: noniid
[INFO][10:28:45]: [Client #468] Dataset size: 60000
[INFO][10:28:45]: [Client #468] Sampler: noniid
[INFO][10:28:45]: [Client #214] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:28:45]: [Client #240] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:28:45]: [Client #468] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:28:45]: [93m[1m[Client #240] Started training in communication round #72.[0m
[INFO][10:28:45]: [93m[1m[Client #214] Started training in communication round #72.[0m
[INFO][10:28:45]: [93m[1m[Client #468] Started training in communication round #72.[0m
[INFO][10:28:48]: [Client #214] Loading the dataset.
[INFO][10:28:48]: [Client #240] Loading the dataset.
[INFO][10:28:48]: [Client #468] Loading the dataset.
[INFO][10:28:54]: [Client #214] Epoch: [1/5][0/10]	Loss: 0.000468
[INFO][10:28:54]: [Client #468] Epoch: [1/5][0/10]	Loss: 0.005263
[INFO][10:28:54]: [Client #214] Epoch: [2/5][0/10]	Loss: 0.000111
[INFO][10:28:54]: [Client #240] Epoch: [1/5][0/10]	Loss: 0.000636
[INFO][10:28:54]: [Client #214] Epoch: [3/5][0/10]	Loss: 0.000044
[INFO][10:28:54]: [Client #240] Epoch: [2/5][0/10]	Loss: 0.001484
[INFO][10:28:54]: [Client #468] Epoch: [2/5][0/10]	Loss: 0.002301
[INFO][10:28:54]: [Client #214] Epoch: [4/5][0/10]	Loss: 0.004014
[INFO][10:28:54]: [Client #468] Epoch: [3/5][0/10]	Loss: 0.000049
[INFO][10:28:54]: [Client #240] Epoch: [3/5][0/10]	Loss: 0.000683
[INFO][10:28:54]: [Client #214] Epoch: [5/5][0/10]	Loss: 0.000056
[INFO][10:28:54]: [Client #214] Model saved to /data/ykang/plato/results/test/model/lenet5_214_1127977.pth.
[INFO][10:28:54]: [Client #468] Epoch: [4/5][0/10]	Loss: 0.000034
[INFO][10:28:54]: [Client #240] Epoch: [4/5][0/10]	Loss: 0.001380
[INFO][10:28:54]: [Client #468] Epoch: [5/5][0/10]	Loss: 0.000001
[INFO][10:28:54]: [Client #240] Epoch: [5/5][0/10]	Loss: 0.001718
[INFO][10:28:54]: [Client #468] Model saved to /data/ykang/plato/results/test/model/lenet5_468_1127979.pth.
[INFO][10:28:54]: [Client #240] Model saved to /data/ykang/plato/results/test/model/lenet5_240_1127978.pth.
[INFO][10:28:55]: [Client #214] Loading a model from /data/ykang/plato/results/test/model/lenet5_214_1127977.pth.
[INFO][10:28:55]: [Client #214] Model trained.
[INFO][10:28:55]: [Client #214] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:28:55]: [Server #1127936] Received 0.24 MB of payload data from client #214 (simulated).
[INFO][10:28:55]: [Client #468] Loading a model from /data/ykang/plato/results/test/model/lenet5_468_1127979.pth.
[INFO][10:28:55]: [Client #468] Model trained.
[INFO][10:28:55]: [Client #468] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:28:55]: [Server #1127936] Received 0.24 MB of payload data from client #468 (simulated).
[INFO][10:28:55]: [Client #240] Loading a model from /data/ykang/plato/results/test/model/lenet5_240_1127978.pth.
[INFO][10:28:55]: [Client #240] Model trained.
[INFO][10:28:55]: [Client #240] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:28:55]: [Server #1127936] Received 0.24 MB of payload data from client #240 (simulated).
[INFO][10:28:55]: [Server #1127936] Selecting client #433 for training.
[INFO][10:28:55]: [Server #1127936] Sending the current model to client #433 (simulated).
[INFO][10:28:55]: [Server #1127936] Sending 0.24 MB of payload data to client #433 (simulated).
[INFO][10:28:55]: [Client #433] Selected by the server.
[INFO][10:28:55]: [Client #433] Loading its data source...
[INFO][10:28:55]: [Client #433] Dataset size: 60000
[INFO][10:28:55]: [Client #433] Sampler: noniid
[INFO][10:28:55]: [Client #433] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:28:55]: [93m[1m[Client #433] Started training in communication round #72.[0m
[INFO][10:28:57]: [Client #433] Loading the dataset.
[INFO][10:29:03]: [Client #433] Epoch: [1/5][0/10]	Loss: 0.000744
[INFO][10:29:03]: [Client #433] Epoch: [2/5][0/10]	Loss: 0.000451
[INFO][10:29:03]: [Client #433] Epoch: [3/5][0/10]	Loss: 0.000006
[INFO][10:29:03]: [Client #433] Epoch: [4/5][0/10]	Loss: 0.000066
[INFO][10:29:03]: [Client #433] Epoch: [5/5][0/10]	Loss: 0.087162
[INFO][10:29:03]: [Client #433] Model saved to /data/ykang/plato/results/test/model/lenet5_433_1127977.pth.
[INFO][10:29:04]: [Client #433] Loading a model from /data/ykang/plato/results/test/model/lenet5_433_1127977.pth.
[INFO][10:29:04]: [Client #433] Model trained.
[INFO][10:29:04]: [Client #433] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:29:04]: [Server #1127936] Received 0.24 MB of payload data from client #433 (simulated).
[INFO][10:29:04]: [Server #1127936] Adding client #326 to the list of clients for aggregation.
[INFO][10:29:04]: [Server #1127936] Adding client #57 to the list of clients for aggregation.
[INFO][10:29:04]: [Server #1127936] Adding client #176 to the list of clients for aggregation.
[INFO][10:29:04]: [Server #1127936] Adding client #263 to the list of clients for aggregation.
[INFO][10:29:04]: [Server #1127936] Adding client #478 to the list of clients for aggregation.
[INFO][10:29:04]: [Server #1127936] Adding client #214 to the list of clients for aggregation.
[INFO][10:29:04]: [Server #1127936] Adding client #112 to the list of clients for aggregation.
[INFO][10:29:04]: [Server #1127936] Adding client #468 to the list of clients for aggregation.
[INFO][10:29:04]: [Server #1127936] Adding client #104 to the list of clients for aggregation.
[INFO][10:29:04]: [Server #1127936] Adding client #433 to the list of clients for aggregation.
[INFO][10:29:04]: [Server #1127936] Aggregating 10 clients in total.
[0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  1e-05  3e-10  3e-10
 6:  6.8876e+00  6.8875e+00  1e-05  1e-10  1e-10
 7:  6.8875e+00  6.8875e+00  1e-05  2e-10  2e-12
 8:  6.8875e+00  6.8875e+00  7e-06  1e-10  2e-12
 9:  6.8875e+00  6.8875e+00  4e-06  2e-10  3e-12
Optimal solution found.
The calculated probability is:  [0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.64076197 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072346 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072344 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00466741 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.000851   0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00086808
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072336 0.00072348 0.00099713 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00115657 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00125759 0.00072348 0.00072348 0.00072348 0.00072348 0.00072348
 0.00072348 0.00072348 0.00072348 0.00072348]
current clients pool:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.02025074 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01112588 0.         0.         0.         0.
 0.         0.         0.         0.0056016  0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01076484 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00854362 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00468273 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01387329 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00449805 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00906526
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00181266 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 0. 1. 0. 0. 0. 1. 0. 0. 3. 1. 0. 0. 0. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 2. 0. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 2. 1. 1. 1. 0.
 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 2. 1. 6. 0. 0. 2. 1. 0. 1. 1. 2. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 3. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 0. 6. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 2. 0. 0.
 1. 0. 1. 2. 0. 0. 0. 2. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 4. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1.
 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0.
 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0.
 1. 1. 1. 0. 4. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 3. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 0. 1. 0. 6. 1. 1.
 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 5. 0. 0. 0. 1. 0. 0. 0. 2. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 2. 2. 1. 1. 0.]
local_gradient_bounds:  [INFO][10:29:06]: [Server #1127936] Global model accuracy: 96.14%

[INFO][10:29:06]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_72.pth.
[INFO][10:29:06]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_72.pth.
[INFO][10:29:06]: [93m[1m
[Server #1127936] Starting round 73/100.[0m
[0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.02025074 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01112588 0.         0.         0.         0.
 0.         0.         0.         0.0056016  0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01076484 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00854362 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00468273 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01387329 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00449805 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00906526
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00181266 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  2e-05  4e-10  4e-10
 6:  6.8876e+00  6.8875e+00  2e-05  2e-10  2e-10
 7:  6.8875e+00  6.8875e+00  2e-05  5e-10  1e-11
 8:  6.8875e+00  6.8875e+00  1e-05  4e-10  9e-12
 9:  6.8875e+00  6.8875e+00  8e-06  7e-10  2e-11
10:  6.8875e+00  6.8875e+00  5e-07  1e-09  2e-11
Optimal solution found.
The calculated probability is:  [4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 9.75568932e-01 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94462487e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94487988e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 1.21801744e-04 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94476504e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 6.68766871e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 2.04302189e-04 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94491064e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94473969e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 5.50095581e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05 4.94496648e-05 4.94496648e-05
 4.94496648e-05 4.94496648e-05]
current clients pool:  [INFO][10:29:07]: [Server #1127936] Selected clients: [ 57 434 425 404  68 271 451 385 213 490]
[INFO][10:29:07]: [Server #1127936] Selecting client #57 for training.
[INFO][10:29:07]: [Server #1127936] Sending the current model to client #57 (simulated).
[INFO][10:29:07]: [Server #1127936] Sending 0.24 MB of payload data to client #57 (simulated).
[INFO][10:29:07]: [Server #1127936] Selecting client #434 for training.
[INFO][10:29:07]: [Server #1127936] Sending the current model to client #434 (simulated).
[INFO][10:29:07]: [Server #1127936] Sending 0.24 MB of payload data to client #434 (simulated).
[INFO][10:29:07]: [Client #57] Selected by the server.
[INFO][10:29:07]: [Client #57] Loading its data source...
[INFO][10:29:07]: [Client #57] Dataset size: 60000
[INFO][10:29:07]: [Client #57] Sampler: noniid
[INFO][10:29:07]: [Client #57] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:29:07]: [93m[1m[Client #57] Started training in communication round #73.[0m
[INFO][10:29:07]: [Server #1127936] Selecting client #425 for training.
[INFO][10:29:07]: [Server #1127936] Sending the current model to client #425 (simulated).
[INFO][10:29:07]: [Server #1127936] Sending 0.24 MB of payload data to client #425 (simulated).
[INFO][10:29:07]: [Client #434] Selected by the server.
[INFO][10:29:07]: [Client #434] Loading its data source...
[INFO][10:29:07]: [Client #434] Dataset size: 60000
[INFO][10:29:07]: [Client #434] Sampler: noniid
[INFO][10:29:07]: [Client #434] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:29:07]: [93m[1m[Client #434] Started training in communication round #73.[0m
[INFO][10:29:07]: [Client #425] Selected by the server.
[INFO][10:29:07]: [Client #425] Loading its data source...
[INFO][10:29:07]: [Client #425] Dataset size: 60000
[INFO][10:29:07]: [Client #425] Sampler: noniid
[INFO][10:29:07]: [Client #425] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:29:07]: [93m[1m[Client #425] Started training in communication round #73.[0m
[INFO][10:29:09]: [Client #434] Loading the dataset.
[INFO][10:29:09]: [Client #57] Loading the dataset.
[INFO][10:29:09]: [Client #425] Loading the dataset.
[INFO][10:29:15]: [Client #57] Epoch: [1/5][0/10]	Loss: 0.001987
[INFO][10:29:15]: [Client #425] Epoch: [1/5][0/10]	Loss: 0.002809
[INFO][10:29:15]: [Client #434] Epoch: [1/5][0/10]	Loss: 0.019572
[INFO][10:29:15]: [Client #57] Epoch: [2/5][0/10]	Loss: 0.000069
[INFO][10:29:15]: [Client #425] Epoch: [2/5][0/10]	Loss: 0.000561
[INFO][10:29:15]: [Client #434] Epoch: [2/5][0/10]	Loss: 0.007664
[INFO][10:29:15]: [Client #57] Epoch: [3/5][0/10]	Loss: 0.000005
[INFO][10:29:15]: [Client #425] Epoch: [3/5][0/10]	Loss: 0.000059
[INFO][10:29:15]: [Client #434] Epoch: [3/5][0/10]	Loss: 0.000044
[INFO][10:29:15]: [Client #57] Epoch: [4/5][0/10]	Loss: 0.270954
[INFO][10:29:16]: [Client #425] Epoch: [4/5][0/10]	Loss: 0.001264
[INFO][10:29:16]: [Client #434] Epoch: [4/5][0/10]	Loss: 0.000023
[INFO][10:29:16]: [Client #57] Epoch: [5/5][0/10]	Loss: 0.263483
[INFO][10:29:16]: [Client #425] Epoch: [5/5][0/10]	Loss: 0.126483
[INFO][10:29:16]: [Client #57] Model saved to /data/ykang/plato/results/test/model/lenet5_57_1127977.pth.
[INFO][10:29:16]: [Client #434] Epoch: [5/5][0/10]	Loss: 0.000007
[INFO][10:29:16]: [Client #425] Model saved to /data/ykang/plato/results/test/model/lenet5_425_1127979.pth.
[INFO][10:29:16]: [Client #434] Model saved to /data/ykang/plato/results/test/model/lenet5_434_1127978.pth.
[INFO][10:29:16]: [Client #57] Loading a model from /data/ykang/plato/results/test/model/lenet5_57_1127977.pth.
[INFO][10:29:16]: [Client #57] Model trained.
[INFO][10:29:16]: [Client #57] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:29:16]: [Server #1127936] Received 0.24 MB of payload data from client #57 (simulated).
[INFO][10:29:17]: [Client #434] Loading a model from /data/ykang/plato/results/test/model/lenet5_434_1127978.pth.
[INFO][10:29:17]: [Client #434] Model trained.
[INFO][10:29:17]: [Client #434] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:29:17]: [Server #1127936] Received 0.24 MB of payload data from client #434 (simulated).
[INFO][10:29:17]: [Client #425] Loading a model from /data/ykang/plato/results/test/model/lenet5_425_1127979.pth.
[INFO][10:29:17]: [Client #425] Model trained.
[INFO][10:29:17]: [Client #425] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:29:17]: [Server #1127936] Received 0.24 MB of payload data from client #425 (simulated).
[INFO][10:29:17]: [Server #1127936] Selecting client #404 for training.
[INFO][10:29:17]: [Server #1127936] Sending the current model to client #404 (simulated).
[INFO][10:29:17]: [Server #1127936] Sending 0.24 MB of payload data to client #404 (simulated).
[INFO][10:29:17]: [Server #1127936] Selecting client #68 for training.
[INFO][10:29:17]: [Server #1127936] Sending the current model to client #68 (simulated).
[INFO][10:29:17]: [Server #1127936] Sending 0.24 MB of payload data to client #68 (simulated).
[INFO][10:29:17]: [Server #1127936] Selecting client #271 for training.
[INFO][10:29:17]: [Server #1127936] Sending the current model to client #271 (simulated).
[INFO][10:29:17]: [Client #404] Selected by the server.
[INFO][10:29:17]: [Client #404] Loading its data source...
[INFO][10:29:17]: [Client #404] Dataset size: 60000
[INFO][10:29:17]: [Client #404] Sampler: noniid
[INFO][10:29:17]: [Server #1127936] Sending 0.24 MB of payload data to client #271 (simulated).
[INFO][10:29:17]: [Client #68] Selected by the server.
[INFO][10:29:17]: [Client #271] Selected by the server.
[INFO][10:29:17]: [Client #68] Loading its data source...
[INFO][10:29:17]: [Client #271] Loading its data source...
[INFO][10:29:17]: [Client #68] Dataset size: 60000
[INFO][10:29:17]: [Client #271] Dataset size: 60000
[INFO][10:29:17]: [Client #68] Sampler: noniid
[INFO][10:29:17]: [Client #271] Sampler: noniid
[INFO][10:29:17]: [Client #404] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:29:17]: [93m[1m[Client #404] Started training in communication round #73.[0m
[INFO][10:29:17]: [Client #271] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:29:17]: [Client #68] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:29:17]: [93m[1m[Client #68] Started training in communication round #73.[0m
[INFO][10:29:17]: [93m[1m[Client #271] Started training in communication round #73.[0m
[INFO][10:29:19]: [Client #68] Loading the dataset.
[INFO][10:29:19]: [Client #404] Loading the dataset.
[INFO][10:29:19]: [Client #271] Loading the dataset.
[INFO][10:29:25]: [Client #68] Epoch: [1/5][0/10]	Loss: 0.001288
[INFO][10:29:25]: [Client #404] Epoch: [1/5][0/10]	Loss: 0.000206
[INFO][10:29:25]: [Client #271] Epoch: [1/5][0/10]	Loss: 0.002919
[INFO][10:29:25]: [Client #68] Epoch: [2/5][0/10]	Loss: 0.000202
[INFO][10:29:25]: [Client #404] Epoch: [2/5][0/10]	Loss: 0.002468
[INFO][10:29:25]: [Client #271] Epoch: [2/5][0/10]	Loss: 0.000028
[INFO][10:29:25]: [Client #68] Epoch: [3/5][0/10]	Loss: 0.000400
[INFO][10:29:25]: [Client #404] Epoch: [3/5][0/10]	Loss: 0.000125
[INFO][10:29:25]: [Client #271] Epoch: [3/5][0/10]	Loss: 0.000024
[INFO][10:29:25]: [Client #404] Epoch: [4/5][0/10]	Loss: 0.000010
[INFO][10:29:25]: [Client #68] Epoch: [4/5][0/10]	Loss: 0.000020
[INFO][10:29:25]: [Client #271] Epoch: [4/5][0/10]	Loss: 0.000001
[INFO][10:29:25]: [Client #404] Epoch: [5/5][0/10]	Loss: 0.001070
[INFO][10:29:25]: [Client #68] Epoch: [5/5][0/10]	Loss: 0.000025
[INFO][10:29:25]: [Client #68] Model saved to /data/ykang/plato/results/test/model/lenet5_68_1127978.pth.
[INFO][10:29:25]: [Client #404] Model saved to /data/ykang/plato/results/test/model/lenet5_404_1127977.pth.
[INFO][10:29:25]: [Client #271] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:29:26]: [Client #271] Model saved to /data/ykang/plato/results/test/model/lenet5_271_1127979.pth.
[INFO][10:29:26]: [Client #68] Loading a model from /data/ykang/plato/results/test/model/lenet5_68_1127978.pth.
[INFO][10:29:26]: [Client #68] Model trained.
[INFO][10:29:26]: [Client #68] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:29:26]: [Server #1127936] Received 0.24 MB of payload data from client #68 (simulated).
[INFO][10:29:26]: [Client #404] Loading a model from /data/ykang/plato/results/test/model/lenet5_404_1127977.pth.
[INFO][10:29:26]: [Client #404] Model trained.
[INFO][10:29:26]: [Client #404] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:29:26]: [Server #1127936] Received 0.24 MB of payload data from client #404 (simulated).
[INFO][10:29:26]: [Client #271] Loading a model from /data/ykang/plato/results/test/model/lenet5_271_1127979.pth.
[INFO][10:29:26]: [Client #271] Model trained.
[INFO][10:29:26]: [Client #271] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:29:26]: [Server #1127936] Received 0.24 MB of payload data from client #271 (simulated).
[INFO][10:29:26]: [Server #1127936] Selecting client #451 for training.
[INFO][10:29:26]: [Server #1127936] Sending the current model to client #451 (simulated).
[INFO][10:29:26]: [Server #1127936] Sending 0.24 MB of payload data to client #451 (simulated).
[INFO][10:29:26]: [Server #1127936] Selecting client #385 for training.
[INFO][10:29:26]: [Server #1127936] Sending the current model to client #385 (simulated).
[INFO][10:29:26]: [Server #1127936] Sending 0.24 MB of payload data to client #385 (simulated).
[INFO][10:29:26]: [Server #1127936] Selecting client #213 for training.
[INFO][10:29:26]: [Server #1127936] Sending the current model to client #213 (simulated).
[INFO][10:29:26]: [Client #451] Selected by the server.
[INFO][10:29:26]: [Client #451] Loading its data source...
[INFO][10:29:26]: [Client #451] Dataset size: 60000
[INFO][10:29:26]: [Client #451] Sampler: noniid
[INFO][10:29:26]: [Server #1127936] Sending 0.24 MB of payload data to client #213 (simulated).
[INFO][10:29:26]: [Client #213] Selected by the server.
[INFO][10:29:26]: [Client #213] Loading its data source...
[INFO][10:29:26]: [Client #213] Dataset size: 60000
[INFO][10:29:26]: [Client #213] Sampler: noniid
[INFO][10:29:26]: [Client #451] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:29:26]: [Client #385] Selected by the server.
[INFO][10:29:26]: [Client #385] Loading its data source...
[INFO][10:29:26]: [Client #385] Dataset size: 60000
[INFO][10:29:26]: [Client #385] Sampler: noniid
[INFO][10:29:26]: [Client #385] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:29:26]: [93m[1m[Client #451] Started training in communication round #73.[0m
[INFO][10:29:26]: [93m[1m[Client #385] Started training in communication round #73.[0m
[INFO][10:29:26]: [Client #213] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:29:26]: [93m[1m[Client #213] Started training in communication round #73.[0m
[INFO][10:29:29]: [Client #385] Loading the dataset.
[INFO][10:29:29]: [Client #213] Loading the dataset.
[INFO][10:29:29]: [Client #451] Loading the dataset.
[INFO][10:29:35]: [Client #451] Epoch: [1/5][0/10]	Loss: 0.019681
[INFO][10:29:35]: [Client #213] Epoch: [1/5][0/10]	Loss: 0.000032
[INFO][10:29:35]: [Client #385] Epoch: [1/5][0/10]	Loss: 0.022790
[INFO][10:29:35]: [Client #213] Epoch: [2/5][0/10]	Loss: 0.000085
[INFO][10:29:35]: [Client #385] Epoch: [2/5][0/10]	Loss: 0.015078
[INFO][10:29:35]: [Client #451] Epoch: [2/5][0/10]	Loss: 0.001841
[INFO][10:29:35]: [Client #213] Epoch: [3/5][0/10]	Loss: 0.000017
[INFO][10:29:35]: [Client #385] Epoch: [3/5][0/10]	Loss: 0.000003
[INFO][10:29:35]: [Client #451] Epoch: [3/5][0/10]	Loss: 0.000012
[INFO][10:29:35]: [Client #213] Epoch: [4/5][0/10]	Loss: 0.000137
[INFO][10:29:35]: [Client #385] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:29:35]: [Client #451] Epoch: [4/5][0/10]	Loss: 0.000004
[INFO][10:29:35]: [Client #213] Epoch: [5/5][0/10]	Loss: 0.000009
[INFO][10:29:35]: [Client #451] Epoch: [5/5][0/10]	Loss: 0.000036
[INFO][10:29:35]: [Client #385] Epoch: [5/5][0/10]	Loss: 0.000006
[INFO][10:29:35]: [Client #213] Model saved to /data/ykang/plato/results/test/model/lenet5_213_1127979.pth.
[INFO][10:29:36]: [Client #451] Model saved to /data/ykang/plato/results/test/model/lenet5_451_1127977.pth.
[INFO][10:29:36]: [Client #385] Model saved to /data/ykang/plato/results/test/model/lenet5_385_1127978.pth.
[INFO][10:29:36]: [Client #451] Loading a model from /data/ykang/plato/results/test/model/lenet5_451_1127977.pth.
[INFO][10:29:36]: [Client #451] Model trained.
[INFO][10:29:36]: [Client #451] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:29:36]: [Server #1127936] Received 0.24 MB of payload data from client #451 (simulated).
[INFO][10:29:36]: [Client #385] Loading a model from /data/ykang/plato/results/test/model/lenet5_385_1127978.pth.
[INFO][10:29:36]: [Client #385] Model trained.
[INFO][10:29:36]: [Client #385] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:29:36]: [Server #1127936] Received 0.24 MB of payload data from client #385 (simulated).
[INFO][10:29:37]: [Client #213] Loading a model from /data/ykang/plato/results/test/model/lenet5_213_1127979.pth.
[INFO][10:29:37]: [Client #213] Model trained.
[INFO][10:29:37]: [Client #213] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:29:37]: [Server #1127936] Received 0.24 MB of payload data from client #213 (simulated).
[INFO][10:29:37]: [Server #1127936] Selecting client #490 for training.
[INFO][10:29:37]: [Server #1127936] Sending the current model to client #490 (simulated).
[INFO][10:29:37]: [Server #1127936] Sending 0.24 MB of payload data to client #490 (simulated).
[INFO][10:29:37]: [Client #490] Selected by the server.
[INFO][10:29:37]: [Client #490] Loading its data source...
[INFO][10:29:37]: [Client #490] Dataset size: 60000
[INFO][10:29:37]: [Client #490] Sampler: noniid
[INFO][10:29:37]: [Client #490] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:29:37]: [93m[1m[Client #490] Started training in communication round #73.[0m
[INFO][10:29:39]: [Client #490] Loading the dataset.
[INFO][10:29:44]: [Client #490] Epoch: [1/5][0/10]	Loss: 0.005729
[INFO][10:29:44]: [Client #490] Epoch: [2/5][0/10]	Loss: 0.013797
[INFO][10:29:44]: [Client #490] Epoch: [3/5][0/10]	Loss: 0.000233
[INFO][10:29:44]: [Client #490] Epoch: [4/5][0/10]	Loss: 0.000733
[INFO][10:29:44]: [Client #490] Epoch: [5/5][0/10]	Loss: 0.001670
[INFO][10:29:44]: [Client #490] Model saved to /data/ykang/plato/results/test/model/lenet5_490_1127977.pth.
[INFO][10:29:45]: [Client #490] Loading a model from /data/ykang/plato/results/test/model/lenet5_490_1127977.pth.
[INFO][10:29:45]: [Client #490] Model trained.
[INFO][10:29:45]: [Client #490] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:29:45]: [Server #1127936] Received 0.24 MB of payload data from client #490 (simulated).
[INFO][10:29:45]: [Server #1127936] Adding client #40 to the list of clients for aggregation.
[INFO][10:29:45]: [Server #1127936] Adding client #196 to the list of clients for aggregation.
[INFO][10:29:45]: [Server #1127936] Adding client #415 to the list of clients for aggregation.
[INFO][10:29:45]: [Server #1127936] Adding client #194 to the list of clients for aggregation.
[INFO][10:29:45]: [Server #1127936] Adding client #288 to the list of clients for aggregation.
[INFO][10:29:45]: [Server #1127936] Adding client #240 to the list of clients for aggregation.
[INFO][10:29:45]: [Server #1127936] Adding client #213 to the list of clients for aggregation.
[INFO][10:29:45]: [Server #1127936] Adding client #451 to the list of clients for aggregation.
[INFO][10:29:45]: [Server #1127936] Adding client #385 to the list of clients for aggregation.
[INFO][10:29:45]: [Server #1127936] Adding client #490 to the list of clients for aggregation.
[INFO][10:29:45]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00809111 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00333129 0.         0.01449017 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00490966 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00790382
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00654036
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00597621 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00362649 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00256609 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00875412 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 0. 1. 0. 0. 0. 1. 0. 0. 3. 1. 0. 0. 0. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 2. 0. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 2. 1. 1. 1. 0.
 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 2. 1. 6. 0. 0. 2. 1. 0. 1. 1. 2. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 3. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 0. 6. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 2. 0. 0.
 1. 1. 1. 2. 0. 0. 0. 2. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 4. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1.
 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0.
 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0.
 1. 1. 1. 0. 4. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 3. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 0. 1. 0. 6. 1. 1.
 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 5. 0. 0. 0. 1. 0. 0. 0. 2. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 2. 2. 1. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00809111 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00333129 0.         0.01449017 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00490966 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00790382
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00654036
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00597621 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00362649 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00256609 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00875412 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:29:47]: [Server #1127936] Global model accuracy: 96.10%

[INFO][10:29:47]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_73.pth.
[INFO][10:29:47]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_73.pth.
[INFO][10:29:47]: [93m[1m
[Server #1127936] Starting round 74/100.[0m
[0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  3e-05  6e-10  6e-10
 6:  6.8876e+00  6.8875e+00  3e-05  3e-10  3e-10
 7:  6.8875e+00  6.8875e+00  3e-05  8e-10  2e-11
 8:  6.8875e+00  6.8875e+00  2e-05  7e-10  2e-11
 9:  6.8875e+00  6.8875e+00  1e-05  2e-09  5e-11
10:  6.8875e+00  6.8875e+00  9e-07  2e-09  7e-11
Optimal solution found.
The calculated probability is:  [6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 9.32081622e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 7.28516246e-05 6.31383462e-05 9.69023474e-01
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31377359e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 9.21994921e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 8.54543505e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31374420e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 7.38560749e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31381795e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31364060e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05 6.31383462e-05 6.31383462e-05
 6.31383462e-05 6.31383462e-05]
current clients pool:  [INFO][10:29:48]: [Server #1127936] Selected clients: [196 188 362 232 183 293 267 277 308 200]
[INFO][10:29:48]: [Server #1127936] Selecting client #196 for training.
[INFO][10:29:48]: [Server #1127936] Sending the current model to client #196 (simulated).
[INFO][10:29:48]: [Server #1127936] Sending 0.24 MB of payload data to client #196 (simulated).
[INFO][10:29:48]: [Server #1127936] Selecting client #188 for training.
[INFO][10:29:48]: [Server #1127936] Sending the current model to client #188 (simulated).
[INFO][10:29:48]: [Server #1127936] Sending 0.24 MB of payload data to client #188 (simulated).
[INFO][10:29:48]: [Server #1127936] Selecting client #362 for training.
[INFO][10:29:48]: [Server #1127936] Sending the current model to client #362 (simulated).
[INFO][10:29:48]: [Client #196] Selected by the server.
[INFO][10:29:48]: [Client #196] Loading its data source...
[INFO][10:29:48]: [Client #196] Dataset size: 60000
[INFO][10:29:48]: [Client #196] Sampler: noniid
[INFO][10:29:48]: [Server #1127936] Sending 0.24 MB of payload data to client #362 (simulated).
[INFO][10:29:48]: [Client #362] Selected by the server.
[INFO][10:29:48]: [Client #196] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:29:48]: [Client #362] Loading its data source...
[INFO][10:29:48]: [Client #362] Dataset size: 60000
[INFO][10:29:48]: [Client #362] Sampler: noniid
[INFO][10:29:48]: [Client #362] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:29:48]: [Client #188] Selected by the server.
[INFO][10:29:48]: [Client #188] Loading its data source...
[INFO][10:29:48]: [Client #188] Dataset size: 60000
[INFO][10:29:48]: [Client #188] Sampler: noniid
[INFO][10:29:48]: [Client #188] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:29:48]: [93m[1m[Client #188] Started training in communication round #74.[0m
[INFO][10:29:48]: [93m[1m[Client #196] Started training in communication round #74.[0m
[INFO][10:29:48]: [93m[1m[Client #362] Started training in communication round #74.[0m
[INFO][10:29:50]: [Client #196] Loading the dataset.
[INFO][10:29:50]: [Client #188] Loading the dataset.
[INFO][10:29:50]: [Client #362] Loading the dataset.
[INFO][10:29:56]: [Client #362] Epoch: [1/5][0/10]	Loss: 0.003406
[INFO][10:29:56]: [Client #196] Epoch: [1/5][0/10]	Loss: 0.000543
[INFO][10:29:56]: [Client #188] Epoch: [1/5][0/10]	Loss: 0.013675
[INFO][10:29:56]: [Client #362] Epoch: [2/5][0/10]	Loss: 0.000449
[INFO][10:29:57]: [Client #196] Epoch: [2/5][0/10]	Loss: 0.000813
[INFO][10:29:57]: [Client #188] Epoch: [2/5][0/10]	Loss: 0.002345
[INFO][10:29:57]: [Client #362] Epoch: [3/5][0/10]	Loss: 0.000022
[INFO][10:29:57]: [Client #196] Epoch: [3/5][0/10]	Loss: 0.000074
[INFO][10:29:57]: [Client #188] Epoch: [3/5][0/10]	Loss: 0.000091
[INFO][10:29:57]: [Client #362] Epoch: [4/5][0/10]	Loss: 0.000544
[INFO][10:29:57]: [Client #196] Epoch: [4/5][0/10]	Loss: 0.000072
[INFO][10:29:57]: [Client #188] Epoch: [4/5][0/10]	Loss: 0.000026
[INFO][10:29:57]: [Client #362] Epoch: [5/5][0/10]	Loss: 0.012629
[INFO][10:29:57]: [Client #362] Model saved to /data/ykang/plato/results/test/model/lenet5_362_1127979.pth.
[INFO][10:29:57]: [Client #196] Epoch: [5/5][0/10]	Loss: 0.001599
[INFO][10:29:57]: [Client #188] Epoch: [5/5][0/10]	Loss: 0.000390
[INFO][10:29:57]: [Client #196] Model saved to /data/ykang/plato/results/test/model/lenet5_196_1127977.pth.
[INFO][10:29:57]: [Client #188] Model saved to /data/ykang/plato/results/test/model/lenet5_188_1127978.pth.
[INFO][10:29:58]: [Client #362] Loading a model from /data/ykang/plato/results/test/model/lenet5_362_1127979.pth.
[INFO][10:29:58]: [Client #362] Model trained.
[INFO][10:29:58]: [Client #362] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:29:58]: [Server #1127936] Received 0.24 MB of payload data from client #362 (simulated).
[INFO][10:29:58]: [Client #196] Loading a model from /data/ykang/plato/results/test/model/lenet5_196_1127977.pth.
[INFO][10:29:58]: [Client #196] Model trained.
[INFO][10:29:58]: [Client #196] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:29:58]: [Server #1127936] Received 0.24 MB of payload data from client #196 (simulated).
[INFO][10:29:58]: [Client #188] Loading a model from /data/ykang/plato/results/test/model/lenet5_188_1127978.pth.
[INFO][10:29:58]: [Client #188] Model trained.
[INFO][10:29:58]: [Client #188] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:29:58]: [Server #1127936] Received 0.24 MB of payload data from client #188 (simulated).
[INFO][10:29:58]: [Server #1127936] Selecting client #232 for training.
[INFO][10:29:58]: [Server #1127936] Sending the current model to client #232 (simulated).
[INFO][10:29:58]: [Server #1127936] Sending 0.24 MB of payload data to client #232 (simulated).
[INFO][10:29:58]: [Server #1127936] Selecting client #183 for training.
[INFO][10:29:58]: [Server #1127936] Sending the current model to client #183 (simulated).
[INFO][10:29:58]: [Server #1127936] Sending 0.24 MB of payload data to client #183 (simulated).
[INFO][10:29:58]: [Server #1127936] Selecting client #293 for training.
[INFO][10:29:58]: [Server #1127936] Sending the current model to client #293 (simulated).
[INFO][10:29:58]: [Client #232] Selected by the server.
[INFO][10:29:58]: [Client #232] Loading its data source...
[INFO][10:29:58]: [Client #232] Dataset size: 60000
[INFO][10:29:58]: [Client #232] Sampler: noniid
[INFO][10:29:58]: [Server #1127936] Sending 0.24 MB of payload data to client #293 (simulated).
[INFO][10:29:58]: [Client #183] Selected by the server.
[INFO][10:29:58]: [Client #293] Selected by the server.
[INFO][10:29:58]: [Client #183] Loading its data source...
[INFO][10:29:58]: [Client #293] Loading its data source...
[INFO][10:29:58]: [Client #183] Dataset size: 60000
[INFO][10:29:58]: [Client #293] Dataset size: 60000
[INFO][10:29:58]: [Client #293] Sampler: noniid
[INFO][10:29:58]: [Client #183] Sampler: noniid
[INFO][10:29:58]: [Client #232] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:29:58]: [Client #293] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:29:58]: [Client #183] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:29:58]: [93m[1m[Client #232] Started training in communication round #74.[0m
[INFO][10:29:58]: [93m[1m[Client #183] Started training in communication round #74.[0m
[INFO][10:29:58]: [93m[1m[Client #293] Started training in communication round #74.[0m
[INFO][10:30:00]: [Client #183] Loading the dataset.
[INFO][10:30:00]: [Client #232] Loading the dataset.
[INFO][10:30:00]: [Client #293] Loading the dataset.
[INFO][10:30:06]: [Client #183] Epoch: [1/5][0/10]	Loss: 0.005609
[INFO][10:30:06]: [Client #183] Epoch: [2/5][0/10]	Loss: 0.000032
[INFO][10:30:06]: [Client #232] Epoch: [1/5][0/10]	Loss: 0.003495
[INFO][10:30:06]: [Client #293] Epoch: [1/5][0/10]	Loss: 0.008230
[INFO][10:30:06]: [Client #183] Epoch: [3/5][0/10]	Loss: 0.000109
[INFO][10:30:06]: [Client #232] Epoch: [2/5][0/10]	Loss: 0.000033
[INFO][10:30:06]: [Client #293] Epoch: [2/5][0/10]	Loss: 0.003453
[INFO][10:30:06]: [Client #183] Epoch: [4/5][0/10]	Loss: 0.008720
[INFO][10:30:06]: [Client #232] Epoch: [3/5][0/10]	Loss: 0.000016
[INFO][10:30:06]: [Client #293] Epoch: [3/5][0/10]	Loss: 0.000160
[INFO][10:30:06]: [Client #183] Epoch: [5/5][0/10]	Loss: 0.006137
[INFO][10:30:06]: [Client #293] Epoch: [4/5][0/10]	Loss: 0.000683
[INFO][10:30:06]: [Client #232] Epoch: [4/5][0/10]	Loss: 0.320282
[INFO][10:30:06]: [Client #183] Model saved to /data/ykang/plato/results/test/model/lenet5_183_1127978.pth.
[INFO][10:30:07]: [Client #232] Epoch: [5/5][0/10]	Loss: 0.266718
[INFO][10:30:07]: [Client #293] Epoch: [5/5][0/10]	Loss: 0.002758
[INFO][10:30:07]: [Client #232] Model saved to /data/ykang/plato/results/test/model/lenet5_232_1127977.pth.
[INFO][10:30:07]: [Client #293] Model saved to /data/ykang/plato/results/test/model/lenet5_293_1127979.pth.
[INFO][10:30:07]: [Client #183] Loading a model from /data/ykang/plato/results/test/model/lenet5_183_1127978.pth.
[INFO][10:30:07]: [Client #183] Model trained.
[INFO][10:30:07]: [Client #183] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:30:07]: [Server #1127936] Received 0.24 MB of payload data from client #183 (simulated).
[INFO][10:30:07]: [Client #232] Loading a model from /data/ykang/plato/results/test/model/lenet5_232_1127977.pth.
[INFO][10:30:07]: [Client #232] Model trained.
[INFO][10:30:07]: [Client #232] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:30:07]: [Server #1127936] Received 0.24 MB of payload data from client #232 (simulated).
[INFO][10:30:07]: [Client #293] Loading a model from /data/ykang/plato/results/test/model/lenet5_293_1127979.pth.
[INFO][10:30:07]: [Client #293] Model trained.
[INFO][10:30:07]: [Client #293] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:30:07]: [Server #1127936] Received 0.24 MB of payload data from client #293 (simulated).
[INFO][10:30:07]: [Server #1127936] Selecting client #267 for training.
[INFO][10:30:07]: [Server #1127936] Sending the current model to client #267 (simulated).
[INFO][10:30:08]: [Server #1127936] Sending 0.24 MB of payload data to client #267 (simulated).
[INFO][10:30:08]: [Server #1127936] Selecting client #277 for training.
[INFO][10:30:08]: [Server #1127936] Sending the current model to client #277 (simulated).
[INFO][10:30:08]: [Server #1127936] Sending 0.24 MB of payload data to client #277 (simulated).
[INFO][10:30:08]: [Server #1127936] Selecting client #308 for training.
[INFO][10:30:08]: [Server #1127936] Sending the current model to client #308 (simulated).
[INFO][10:30:08]: [Client #267] Selected by the server.
[INFO][10:30:08]: [Client #267] Loading its data source...
[INFO][10:30:08]: [Client #267] Dataset size: 60000
[INFO][10:30:08]: [Client #267] Sampler: noniid
[INFO][10:30:08]: [Server #1127936] Sending 0.24 MB of payload data to client #308 (simulated).
[INFO][10:30:08]: [Client #277] Selected by the server.
[INFO][10:30:08]: [Client #308] Selected by the server.
[INFO][10:30:08]: [Client #277] Loading its data source...
[INFO][10:30:08]: [Client #277] Dataset size: 60000
[INFO][10:30:08]: [Client #308] Loading its data source...
[INFO][10:30:08]: [Client #277] Sampler: noniid
[INFO][10:30:08]: [Client #308] Dataset size: 60000
[INFO][10:30:08]: [Client #308] Sampler: noniid
[INFO][10:30:08]: [Client #267] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:30:08]: [Client #308] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:30:08]: [Client #277] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:30:08]: [93m[1m[Client #277] Started training in communication round #74.[0m
[INFO][10:30:08]: [93m[1m[Client #267] Started training in communication round #74.[0m
[INFO][10:30:08]: [93m[1m[Client #308] Started training in communication round #74.[0m
[INFO][10:30:10]: [Client #277] Loading the dataset.
[INFO][10:30:10]: [Client #267] Loading the dataset.
[INFO][10:30:10]: [Client #308] Loading the dataset.
[INFO][10:30:16]: [Client #277] Epoch: [1/5][0/10]	Loss: 0.003326
[INFO][10:30:16]: [Client #267] Epoch: [1/5][0/10]	Loss: 0.017671
[INFO][10:30:16]: [Client #277] Epoch: [2/5][0/10]	Loss: 0.002255
[INFO][10:30:16]: [Client #308] Epoch: [1/5][0/10]	Loss: 0.000400
[INFO][10:30:16]: [Client #267] Epoch: [2/5][0/10]	Loss: 0.000057
[INFO][10:30:16]: [Client #277] Epoch: [3/5][0/10]	Loss: 0.000112
[INFO][10:30:16]: [Client #308] Epoch: [2/5][0/10]	Loss: 0.036741
[INFO][10:30:16]: [Client #267] Epoch: [3/5][0/10]	Loss: 0.000001
[INFO][10:30:16]: [Client #308] Epoch: [3/5][0/10]	Loss: 0.000134
[INFO][10:30:16]: [Client #277] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:30:16]: [Client #308] Epoch: [4/5][0/10]	Loss: 0.000112
[INFO][10:30:16]: [Client #267] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:30:16]: [Client #277] Epoch: [5/5][0/10]	Loss: 0.021856
[INFO][10:30:16]: [Client #308] Epoch: [5/5][0/10]	Loss: 0.000034
[INFO][10:30:16]: [Client #277] Model saved to /data/ykang/plato/results/test/model/lenet5_277_1127978.pth.
[INFO][10:30:16]: [Client #267] Epoch: [5/5][0/10]	Loss: 0.006570
[INFO][10:30:16]: [Client #308] Model saved to /data/ykang/plato/results/test/model/lenet5_308_1127979.pth.
[INFO][10:30:16]: [Client #267] Model saved to /data/ykang/plato/results/test/model/lenet5_267_1127977.pth.
[INFO][10:30:17]: [Client #277] Loading a model from /data/ykang/plato/results/test/model/lenet5_277_1127978.pth.
[INFO][10:30:17]: [Client #277] Model trained.
[INFO][10:30:17]: [Client #277] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:30:17]: [Server #1127936] Received 0.24 MB of payload data from client #277 (simulated).
[INFO][10:30:17]: [Client #267] Loading a model from /data/ykang/plato/results/test/model/lenet5_267_1127977.pth.
[INFO][10:30:17]: [Client #267] Model trained.
[INFO][10:30:17]: [Client #267] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:30:17]: [Server #1127936] Received 0.24 MB of payload data from client #267 (simulated).
[INFO][10:30:17]: [Client #308] Loading a model from /data/ykang/plato/results/test/model/lenet5_308_1127979.pth.
[INFO][10:30:17]: [Client #308] Model trained.
[INFO][10:30:17]: [Client #308] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:30:17]: [Server #1127936] Received 0.24 MB of payload data from client #308 (simulated).
[INFO][10:30:17]: [Server #1127936] Selecting client #200 for training.
[INFO][10:30:17]: [Server #1127936] Sending the current model to client #200 (simulated).
[INFO][10:30:17]: [Server #1127936] Sending 0.24 MB of payload data to client #200 (simulated).
[INFO][10:30:17]: [Client #200] Selected by the server.
[INFO][10:30:17]: [Client #200] Loading its data source...
[INFO][10:30:17]: [Client #200] Dataset size: 60000
[INFO][10:30:17]: [Client #200] Sampler: noniid
[INFO][10:30:17]: [Client #200] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:30:17]: [93m[1m[Client #200] Started training in communication round #74.[0m
[INFO][10:30:19]: [Client #200] Loading the dataset.
[INFO][10:30:25]: [Client #200] Epoch: [1/5][0/10]	Loss: 0.000336
[INFO][10:30:25]: [Client #200] Epoch: [2/5][0/10]	Loss: 0.001697
[INFO][10:30:25]: [Client #200] Epoch: [3/5][0/10]	Loss: 0.000603
[INFO][10:30:25]: [Client #200] Epoch: [4/5][0/10]	Loss: 0.000842
[INFO][10:30:25]: [Client #200] Epoch: [5/5][0/10]	Loss: 0.001351
[INFO][10:30:25]: [Client #200] Model saved to /data/ykang/plato/results/test/model/lenet5_200_1127977.pth.
[INFO][10:30:26]: [Client #200] Loading a model from /data/ykang/plato/results/test/model/lenet5_200_1127977.pth.
[INFO][10:30:26]: [Client #200] Model trained.
[INFO][10:30:26]: [Client #200] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:30:26]: [Server #1127936] Received 0.24 MB of payload data from client #200 (simulated).
[INFO][10:30:26]: [Server #1127936] Adding client #271 to the list of clients for aggregation.
[INFO][10:30:26]: [Server #1127936] Adding client #425 to the list of clients for aggregation.
[INFO][10:30:26]: [Server #1127936] Adding client #434 to the list of clients for aggregation.
[INFO][10:30:26]: [Server #1127936] Adding client #68 to the list of clients for aggregation.
[INFO][10:30:26]: [Server #1127936] Adding client #57 to the list of clients for aggregation.
[INFO][10:30:26]: [Server #1127936] Adding client #467 to the list of clients for aggregation.
[INFO][10:30:26]: [Server #1127936] Adding client #404 to the list of clients for aggregation.
[INFO][10:30:26]: [Server #1127936] Adding client #275 to the list of clients for aggregation.
[INFO][10:30:26]: [Server #1127936] Adding client #362 to the list of clients for aggregation.
[INFO][10:30:26]: [Server #1127936] Adding client #277 to the list of clients for aggregation.
[INFO][10:30:26]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 272, 273, 274, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 426, 427, 428, 429, 430, 431, 432, 433, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.02276866 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00397105 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.0068407  0.         0.         0.         0.06364622 0.
 0.00418349 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.02347888 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.0119441  0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00521121 0.
 0.         0.         0.         0.         0.         0.
 0.         0.00310518 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00657342 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 0. 1. 0. 0. 0. 1. 0. 0. 3. 1. 0. 0. 0. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 2. 0. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 2. 1. 1. 1. 0.
 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 2. 1. 6. 0. 0. 2. 1. 0. 1. 1. 2. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 3. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 0. 6. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 2. 0. 0.
 1. 1. 1. 2. 0. 0. 0. 2. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 4. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1.
 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0.
 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0.
 1. 0. 1. 0. 4. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 3. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 0. 1. 0. 6. 1. 1.
 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 6. 0. 0. 0. 1. 0. 0. 0. 2. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 2. 2. 1. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.02276866 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00397105 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.0068407  0.         0.         0.         0.06364622 0.
 0.00418349 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.02347888 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.0119441  0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00521121 0.
 0.         0.         0.         0.         0.         0.
 0.         0.00310518 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00657342 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:30:28]: [Server #1127936] Global model accuracy: 95.76%

[INFO][10:30:28]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_74.pth.
[INFO][10:30:28]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_74.pth.
[INFO][10:30:28]: [93m[1m
[Server #1127936] Starting round 75/100.[0m
[0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.002 0.002 0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8870e+00  6e-04  1e-08  1e-08
 5:  6.8876e+00  6.8873e+00  3e-04  4e-09  4e-09
 6:  6.8875e+00  6.8873e+00  3e-04  2e-08  6e-09
 7:  6.8875e+00  6.8873e+00  2e-04  2e-08  6e-09
 8:  6.8874e+00  6.8873e+00  9e-05  2e-07  5e-08
 9:  6.8874e+00  6.8873e+00  4e-05  1e-07  3e-08
10:  6.8873e+00  6.8873e+00  2e-06  4e-08  1e-08
Optimal solution found.
The calculated probability is:  [2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.62410247e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.17808172e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.23803669e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 9.89712578e-01 2.09923633e-05
 2.09923295e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09913005e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.35135446e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.20367462e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.16049204e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 3.15468965e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05 2.09923633e-05 2.09923633e-05
 2.09923633e-05 2.09923633e-05]
current clients pool:  [INFO][10:30:29]: [Server #1127936] Selected clients: [275 171 459 316 115 389  53 282  48 295]
[INFO][10:30:29]: [Server #1127936] Selecting client #275 for training.
[INFO][10:30:29]: [Server #1127936] Sending the current model to client #275 (simulated).
[INFO][10:30:29]: [Server #1127936] Sending 0.24 MB of payload data to client #275 (simulated).
[INFO][10:30:29]: [Server #1127936] Selecting client #171 for training.
[INFO][10:30:29]: [Server #1127936] Sending the current model to client #171 (simulated).
[INFO][10:30:29]: [Server #1127936] Sending 0.24 MB of payload data to client #171 (simulated).
[INFO][10:30:29]: [Server #1127936] Selecting client #459 for training.
[INFO][10:30:29]: [Server #1127936] Sending the current model to client #459 (simulated).
[INFO][10:30:29]: [Client #275] Selected by the server.
[INFO][10:30:29]: [Client #275] Loading its data source...
[INFO][10:30:29]: [Client #275] Dataset size: 60000
[INFO][10:30:29]: [Client #275] Sampler: noniid
[INFO][10:30:29]: [Server #1127936] Sending 0.24 MB of payload data to client #459 (simulated).
[INFO][10:30:29]: [Client #171] Selected by the server.
[INFO][10:30:29]: [Client #171] Loading its data source...
[INFO][10:30:29]: [Client #171] Dataset size: 60000
[INFO][10:30:29]: [Client #275] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:30:29]: [Client #171] Sampler: noniid
[INFO][10:30:29]: [Client #459] Selected by the server.
[INFO][10:30:29]: [Client #459] Loading its data source...
[INFO][10:30:29]: [Client #459] Dataset size: 60000
[INFO][10:30:29]: [Client #459] Sampler: noniid
[INFO][10:30:29]: [Client #171] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:30:29]: [Client #459] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:30:29]: [93m[1m[Client #459] Started training in communication round #75.[0m
[INFO][10:30:29]: [93m[1m[Client #171] Started training in communication round #75.[0m
[INFO][10:30:29]: [93m[1m[Client #275] Started training in communication round #75.[0m
[INFO][10:30:31]: [Client #171] Loading the dataset.
[INFO][10:30:31]: [Client #275] Loading the dataset.
[INFO][10:30:31]: [Client #459] Loading the dataset.
[INFO][10:30:37]: [Client #459] Epoch: [1/5][0/10]	Loss: 0.009839
[INFO][10:30:37]: [Client #171] Epoch: [1/5][0/10]	Loss: 0.003249
[INFO][10:30:37]: [Client #275] Epoch: [1/5][0/10]	Loss: 0.000683
[INFO][10:30:37]: [Client #459] Epoch: [2/5][0/10]	Loss: 0.001601
[INFO][10:30:37]: [Client #171] Epoch: [2/5][0/10]	Loss: 0.000699
[INFO][10:30:38]: [Client #275] Epoch: [2/5][0/10]	Loss: 0.000316
[INFO][10:30:38]: [Client #171] Epoch: [3/5][0/10]	Loss: 0.000150
[INFO][10:30:38]: [Client #459] Epoch: [3/5][0/10]	Loss: 0.000020
[INFO][10:30:38]: [Client #275] Epoch: [3/5][0/10]	Loss: 0.000101
[INFO][10:30:38]: [Client #459] Epoch: [4/5][0/10]	Loss: 0.000002
[INFO][10:30:38]: [Client #171] Epoch: [4/5][0/10]	Loss: 0.000592
[INFO][10:30:38]: [Client #459] Epoch: [5/5][0/10]	Loss: 0.000150
[INFO][10:30:38]: [Client #275] Epoch: [4/5][0/10]	Loss: 0.029417
[INFO][10:30:38]: [Client #171] Epoch: [5/5][0/10]	Loss: 0.000081
[INFO][10:30:38]: [Client #171] Model saved to /data/ykang/plato/results/test/model/lenet5_171_1127978.pth.
[INFO][10:30:38]: [Client #459] Model saved to /data/ykang/plato/results/test/model/lenet5_459_1127979.pth.
[INFO][10:30:38]: [Client #275] Epoch: [5/5][0/10]	Loss: 0.000072
[INFO][10:30:38]: [Client #275] Model saved to /data/ykang/plato/results/test/model/lenet5_275_1127977.pth.
[INFO][10:30:39]: [Client #171] Loading a model from /data/ykang/plato/results/test/model/lenet5_171_1127978.pth.
[INFO][10:30:39]: [Client #171] Model trained.
[INFO][10:30:39]: [Client #171] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:30:39]: [Server #1127936] Received 0.24 MB of payload data from client #171 (simulated).
[INFO][10:30:39]: [Client #459] Loading a model from /data/ykang/plato/results/test/model/lenet5_459_1127979.pth.
[INFO][10:30:39]: [Client #459] Model trained.
[INFO][10:30:39]: [Client #459] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:30:39]: [Server #1127936] Received 0.24 MB of payload data from client #459 (simulated).
[INFO][10:30:39]: [Client #275] Loading a model from /data/ykang/plato/results/test/model/lenet5_275_1127977.pth.
[INFO][10:30:39]: [Client #275] Model trained.
[INFO][10:30:39]: [Client #275] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:30:39]: [Server #1127936] Received 0.24 MB of payload data from client #275 (simulated).
[INFO][10:30:39]: [Server #1127936] Selecting client #316 for training.
[INFO][10:30:39]: [Server #1127936] Sending the current model to client #316 (simulated).
[INFO][10:30:39]: [Server #1127936] Sending 0.24 MB of payload data to client #316 (simulated).
[INFO][10:30:39]: [Server #1127936] Selecting client #115 for training.
[INFO][10:30:39]: [Server #1127936] Sending the current model to client #115 (simulated).
[INFO][10:30:39]: [Server #1127936] Sending 0.24 MB of payload data to client #115 (simulated).
[INFO][10:30:39]: [Server #1127936] Selecting client #389 for training.
[INFO][10:30:39]: [Server #1127936] Sending the current model to client #389 (simulated).
[INFO][10:30:39]: [Client #316] Selected by the server.
[INFO][10:30:39]: [Client #316] Loading its data source...
[INFO][10:30:39]: [Client #316] Dataset size: 60000
[INFO][10:30:39]: [Client #316] Sampler: noniid
[INFO][10:30:39]: [Server #1127936] Sending 0.24 MB of payload data to client #389 (simulated).
[INFO][10:30:39]: [Client #115] Selected by the server.
[INFO][10:30:39]: [Client #115] Loading its data source...
[INFO][10:30:39]: [Client #115] Dataset size: 60000
[INFO][10:30:39]: [Client #115] Sampler: noniid
[INFO][10:30:39]: [Client #389] Selected by the server.
[INFO][10:30:39]: [Client #389] Loading its data source...
[INFO][10:30:39]: [Client #389] Dataset size: 60000
[INFO][10:30:39]: [Client #389] Sampler: noniid
[INFO][10:30:39]: [Client #316] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:30:39]: [Client #115] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:30:39]: [Client #389] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:30:39]: [93m[1m[Client #389] Started training in communication round #75.[0m
[INFO][10:30:39]: [93m[1m[Client #115] Started training in communication round #75.[0m
[INFO][10:30:39]: [93m[1m[Client #316] Started training in communication round #75.[0m
[INFO][10:30:41]: [Client #389] Loading the dataset.
[INFO][10:30:41]: [Client #115] Loading the dataset.
[INFO][10:30:41]: [Client #316] Loading the dataset.
[INFO][10:30:47]: [Client #389] Epoch: [1/5][0/10]	Loss: 0.004197
[INFO][10:30:47]: [Client #316] Epoch: [1/5][0/10]	Loss: 0.001296
[INFO][10:30:47]: [Client #115] Epoch: [1/5][0/10]	Loss: 0.003297
[INFO][10:30:47]: [Client #389] Epoch: [2/5][0/10]	Loss: 0.007208
[INFO][10:30:47]: [Client #316] Epoch: [2/5][0/10]	Loss: 0.003347
[INFO][10:30:47]: [Client #389] Epoch: [3/5][0/10]	Loss: 0.000274
[INFO][10:30:47]: [Client #115] Epoch: [2/5][0/10]	Loss: 0.000345
[INFO][10:30:47]: [Client #316] Epoch: [3/5][0/10]	Loss: 0.000107
[INFO][10:30:47]: [Client #389] Epoch: [4/5][0/10]	Loss: 0.000900
[INFO][10:30:47]: [Client #115] Epoch: [3/5][0/10]	Loss: 0.000140
[INFO][10:30:47]: [Client #316] Epoch: [4/5][0/10]	Loss: 0.000043
[INFO][10:30:47]: [Client #115] Epoch: [4/5][0/10]	Loss: 0.009874
[INFO][10:30:47]: [Client #389] Epoch: [5/5][0/10]	Loss: 0.000365
[INFO][10:30:47]: [Client #316] Epoch: [5/5][0/10]	Loss: 0.000027
[INFO][10:30:47]: [Client #115] Epoch: [5/5][0/10]	Loss: 0.001266
[INFO][10:30:47]: [Client #389] Model saved to /data/ykang/plato/results/test/model/lenet5_389_1127979.pth.
[INFO][10:30:48]: [Client #316] Model saved to /data/ykang/plato/results/test/model/lenet5_316_1127977.pth.
[INFO][10:30:48]: [Client #115] Model saved to /data/ykang/plato/results/test/model/lenet5_115_1127978.pth.
[INFO][10:30:48]: [Client #316] Loading a model from /data/ykang/plato/results/test/model/lenet5_316_1127977.pth.
[INFO][10:30:48]: [Client #316] Model trained.
[INFO][10:30:48]: [Client #316] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:30:48]: [Server #1127936] Received 0.24 MB of payload data from client #316 (simulated).
[INFO][10:30:48]: [Client #389] Loading a model from /data/ykang/plato/results/test/model/lenet5_389_1127979.pth.
[INFO][10:30:48]: [Client #389] Model trained.
[INFO][10:30:48]: [Client #389] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:30:48]: [Server #1127936] Received 0.24 MB of payload data from client #389 (simulated).
[INFO][10:30:48]: [Client #115] Loading a model from /data/ykang/plato/results/test/model/lenet5_115_1127978.pth.
[INFO][10:30:48]: [Client #115] Model trained.
[INFO][10:30:48]: [Client #115] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:30:48]: [Server #1127936] Received 0.24 MB of payload data from client #115 (simulated).
[INFO][10:30:48]: [Server #1127936] Selecting client #53 for training.
[INFO][10:30:48]: [Server #1127936] Sending the current model to client #53 (simulated).
[INFO][10:30:48]: [Server #1127936] Sending 0.24 MB of payload data to client #53 (simulated).
[INFO][10:30:48]: [Server #1127936] Selecting client #282 for training.
[INFO][10:30:48]: [Server #1127936] Sending the current model to client #282 (simulated).
[INFO][10:30:48]: [Server #1127936] Sending 0.24 MB of payload data to client #282 (simulated).
[INFO][10:30:48]: [Server #1127936] Selecting client #48 for training.
[INFO][10:30:48]: [Server #1127936] Sending the current model to client #48 (simulated).
[INFO][10:30:48]: [Client #53] Selected by the server.
[INFO][10:30:48]: [Client #53] Loading its data source...
[INFO][10:30:48]: [Client #53] Dataset size: 60000
[INFO][10:30:48]: [Client #53] Sampler: noniid
[INFO][10:30:48]: [Server #1127936] Sending 0.24 MB of payload data to client #48 (simulated).
[INFO][10:30:48]: [Client #282] Selected by the server.
[INFO][10:30:48]: [Client #282] Loading its data source...
[INFO][10:30:48]: [Client #282] Dataset size: 60000
[INFO][10:30:48]: [Client #48] Selected by the server.
[INFO][10:30:48]: [Client #282] Sampler: noniid
[INFO][10:30:48]: [Client #48] Loading its data source...
[INFO][10:30:48]: [Client #48] Dataset size: 60000
[INFO][10:30:48]: [Client #48] Sampler: noniid
[INFO][10:30:48]: [Client #53] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:30:48]: [Client #282] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:30:48]: [Client #48] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:30:49]: [93m[1m[Client #53] Started training in communication round #75.[0m
[INFO][10:30:49]: [93m[1m[Client #48] Started training in communication round #75.[0m
[INFO][10:30:49]: [93m[1m[Client #282] Started training in communication round #75.[0m
[INFO][10:30:51]: [Client #53] Loading the dataset.
[INFO][10:30:51]: [Client #282] Loading the dataset.
[INFO][10:30:51]: [Client #48] Loading the dataset.
[INFO][10:30:57]: [Client #48] Epoch: [1/5][0/10]	Loss: 0.009761
[INFO][10:30:57]: [Client #53] Epoch: [1/5][0/10]	Loss: 0.005499
[INFO][10:30:57]: [Client #48] Epoch: [2/5][0/10]	Loss: 0.003004
[INFO][10:30:57]: [Client #53] Epoch: [2/5][0/10]	Loss: 0.001096
[INFO][10:30:57]: [Client #282] Epoch: [1/5][0/10]	Loss: 0.090049
[INFO][10:30:57]: [Client #282] Epoch: [2/5][0/10]	Loss: 0.000337
[INFO][10:30:57]: [Client #48] Epoch: [3/5][0/10]	Loss: 0.000017
[INFO][10:30:57]: [Client #53] Epoch: [3/5][0/10]	Loss: 0.000035
[INFO][10:30:57]: [Client #282] Epoch: [3/5][0/10]	Loss: 0.000000
[INFO][10:30:57]: [Client #48] Epoch: [4/5][0/10]	Loss: 0.000301
[INFO][10:30:57]: [Client #53] Epoch: [4/5][0/10]	Loss: 0.000004
[INFO][10:30:57]: [Client #53] Epoch: [5/5][0/10]	Loss: 0.000430
[INFO][10:30:57]: [Client #282] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:30:57]: [Client #48] Epoch: [5/5][0/10]	Loss: 0.001102
[INFO][10:30:57]: [Client #53] Model saved to /data/ykang/plato/results/test/model/lenet5_53_1127977.pth.
[INFO][10:30:57]: [Client #48] Model saved to /data/ykang/plato/results/test/model/lenet5_48_1127979.pth.
[INFO][10:30:57]: [Client #282] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:30:57]: [Client #282] Model saved to /data/ykang/plato/results/test/model/lenet5_282_1127978.pth.
[INFO][10:30:58]: [Client #53] Loading a model from /data/ykang/plato/results/test/model/lenet5_53_1127977.pth.
[INFO][10:30:58]: [Client #53] Model trained.
[INFO][10:30:58]: [Client #53] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:30:58]: [Server #1127936] Received 0.24 MB of payload data from client #53 (simulated).
[INFO][10:30:58]: [Client #48] Loading a model from /data/ykang/plato/results/test/model/lenet5_48_1127979.pth.
[INFO][10:30:58]: [Client #48] Model trained.
[INFO][10:30:58]: [Client #48] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:30:58]: [Server #1127936] Received 0.24 MB of payload data from client #48 (simulated).
[INFO][10:30:58]: [Client #282] Loading a model from /data/ykang/plato/results/test/model/lenet5_282_1127978.pth.
[INFO][10:30:58]: [Client #282] Model trained.
[INFO][10:30:58]: [Client #282] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:30:58]: [Server #1127936] Received 0.24 MB of payload data from client #282 (simulated).
[INFO][10:30:58]: [Server #1127936] Selecting client #295 for training.
[INFO][10:30:58]: [Server #1127936] Sending the current model to client #295 (simulated).
[INFO][10:30:58]: [Server #1127936] Sending 0.24 MB of payload data to client #295 (simulated).
[INFO][10:30:58]: [Client #295] Selected by the server.
[INFO][10:30:58]: [Client #295] Loading its data source...
[INFO][10:30:58]: [Client #295] Dataset size: 60000
[INFO][10:30:58]: [Client #295] Sampler: noniid
[INFO][10:30:58]: [Client #295] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:30:58]: [93m[1m[Client #295] Started training in communication round #75.[0m
[INFO][10:31:00]: [Client #295] Loading the dataset.
[INFO][10:31:05]: [Client #295] Epoch: [1/5][0/10]	Loss: 0.041914
[INFO][10:31:05]: [Client #295] Epoch: [2/5][0/10]	Loss: 0.000047
[INFO][10:31:05]: [Client #295] Epoch: [3/5][0/10]	Loss: 0.000015
[INFO][10:31:05]: [Client #295] Epoch: [4/5][0/10]	Loss: 0.000013
[INFO][10:31:05]: [Client #295] Epoch: [5/5][0/10]	Loss: 0.000087
[INFO][10:31:05]: [Client #295] Model saved to /data/ykang/plato/results/test/model/lenet5_295_1127977.pth.
[INFO][10:31:06]: [Client #295] Loading a model from /data/ykang/plato/results/test/model/lenet5_295_1127977.pth.
[INFO][10:31:06]: [Client #295] Model trained.
[INFO][10:31:06]: [Client #295] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:31:06]: [Server #1127936] Received 0.24 MB of payload data from client #295 (simulated).
[INFO][10:31:06]: [Server #1127936] Adding client #308 to the list of clients for aggregation.
[INFO][10:31:06]: [Server #1127936] Adding client #232 to the list of clients for aggregation.
[INFO][10:31:06]: [Server #1127936] Adding client #293 to the list of clients for aggregation.
[INFO][10:31:06]: [Server #1127936] Adding client #188 to the list of clients for aggregation.
[INFO][10:31:06]: [Server #1127936] Adding client #267 to the list of clients for aggregation.
[INFO][10:31:06]: [Server #1127936] Adding client #183 to the list of clients for aggregation.
[INFO][10:31:06]: [Server #1127936] Adding client #172 to the list of clients for aggregation.
[INFO][10:31:06]: [Server #1127936] Adding client #389 to the list of clients for aggregation.
[INFO][10:31:06]: [Server #1127936] Adding client #115 to the list of clients for aggregation.
[INFO][10:31:06]: [Server #1127936] Adding client #53 to the list of clients for aggregation.
[INFO][10:31:06]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 184, 185, 186, 187, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00217131 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00396284 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00602475 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00508021 0.         0.         0.
 0.         0.00692576 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01971088 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01127619 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00345555 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00454909 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00400145 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 0. 1. 0. 0. 0. 1. 0. 0. 3. 1. 0. 0. 0. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 2. 0. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 2. 1. 1. 1. 0.
 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 2. 1. 6. 0. 0. 2. 1. 0. 1. 1. 2. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 3. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 0. 6. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 2. 0. 0.
 1. 1. 1. 2. 0. 0. 0. 2. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 4. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1.
 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 4. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0.
 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0.
 1. 0. 1. 0. 4. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 3. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 0. 1. 0. 6. 1. 1.
 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 6. 0. 0. 0. 1. 0. 0. 0. 2. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 2. 2. 1. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00217131 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00396284 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00602475 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00508021 0.         0.         0.
 0.         0.00692576 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01971088 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01127619 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00345555 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00454909 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00400145 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:31:08]: [Server #1127936] Global model accuracy: 96.28%

[INFO][10:31:08]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_75.pth.
[INFO][10:31:08]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_75.pth.
[INFO][10:31:08]: [93m[1m
[Server #1127936] Starting round 76/100.[0m
[0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  4e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  4e-05  7e-10  7e-10
 6:  6.8876e+00  6.8875e+00  4e-05  4e-10  4e-10
 7:  6.8875e+00  6.8875e+00  4e-05  1e-09  4e-11
 8:  6.8875e+00  6.8875e+00  3e-05  9e-10  4e-11
 9:  6.8875e+00  6.8875e+00  1e-05  3e-09  1e-10
10:  6.8875e+00  6.8875e+00  1e-06  4e-09  1e-10
Optimal solution found.
The calculated probability is:  [6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25924859e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25922577e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 9.69194819e-01 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 7.52220495e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 8.11335477e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 1.72850407e-04 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 9.94100932e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 7.06739558e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 7.36736312e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25922513e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05 6.25925837e-05 6.25925837e-05
 6.25925837e-05 6.25925837e-05]
current clients pool:  [INFO][10:31:09]: [Server #1127936] Selected clients: [172 438 353  94 284 455  77 423 411 250]
[INFO][10:31:09]: [Server #1127936] Selecting client #172 for training.
[INFO][10:31:09]: [Server #1127936] Sending the current model to client #172 (simulated).
[INFO][10:31:09]: [Server #1127936] Sending 0.24 MB of payload data to client #172 (simulated).
[INFO][10:31:09]: [Server #1127936] Selecting client #438 for training.
[INFO][10:31:09]: [Server #1127936] Sending the current model to client #438 (simulated).
[INFO][10:31:09]: [Server #1127936] Sending 0.24 MB of payload data to client #438 (simulated).
[INFO][10:31:09]: [Server #1127936] Selecting client #353 for training.
[INFO][10:31:09]: [Server #1127936] Sending the current model to client #353 (simulated).
[INFO][10:31:09]: [Client #172] Selected by the server.
[INFO][10:31:09]: [Client #172] Loading its data source...
[INFO][10:31:09]: [Client #172] Dataset size: 60000
[INFO][10:31:09]: [Client #172] Sampler: noniid
[INFO][10:31:09]: [Server #1127936] Sending 0.24 MB of payload data to client #353 (simulated).
[INFO][10:31:09]: [Client #438] Selected by the server.
[INFO][10:31:09]: [Client #438] Loading its data source...
[INFO][10:31:09]: [Client #438] Dataset size: 60000
[INFO][10:31:09]: [Client #438] Sampler: noniid
[INFO][10:31:09]: [Client #172] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:31:09]: [Client #438] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:31:09]: [93m[1m[Client #172] Started training in communication round #76.[0m
[INFO][10:31:09]: [93m[1m[Client #438] Started training in communication round #76.[0m
[INFO][10:31:09]: [Client #353] Selected by the server.
[INFO][10:31:09]: [Client #353] Loading its data source...
[INFO][10:31:09]: [Client #353] Dataset size: 60000
[INFO][10:31:09]: [Client #353] Sampler: noniid
[INFO][10:31:09]: [Client #353] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:31:09]: [93m[1m[Client #353] Started training in communication round #76.[0m
[INFO][10:31:11]: [Client #172] Loading the dataset.
[INFO][10:31:11]: [Client #438] Loading the dataset.
[INFO][10:31:11]: [Client #353] Loading the dataset.
[INFO][10:31:17]: [Client #172] Epoch: [1/5][0/10]	Loss: 0.001081
[INFO][10:31:17]: [Client #438] Epoch: [1/5][0/10]	Loss: 0.002629
[INFO][10:31:17]: [Client #353] Epoch: [1/5][0/10]	Loss: 0.000850
[INFO][10:31:17]: [Client #172] Epoch: [2/5][0/10]	Loss: 0.000561
[INFO][10:31:17]: [Client #438] Epoch: [2/5][0/10]	Loss: 0.000315
[INFO][10:31:17]: [Client #353] Epoch: [2/5][0/10]	Loss: 0.000280
[INFO][10:31:17]: [Client #172] Epoch: [3/5][0/10]	Loss: 0.000018
[INFO][10:31:17]: [Client #438] Epoch: [3/5][0/10]	Loss: 0.136194
[INFO][10:31:17]: [Client #353] Epoch: [3/5][0/10]	Loss: 0.000251
[INFO][10:31:17]: [Client #172] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:31:17]: [Client #438] Epoch: [4/5][0/10]	Loss: 0.003519
[INFO][10:31:17]: [Client #172] Epoch: [5/5][0/10]	Loss: 0.000001
[INFO][10:31:17]: [Client #353] Epoch: [4/5][0/10]	Loss: 0.000203
[INFO][10:31:17]: [Client #172] Model saved to /data/ykang/plato/results/test/model/lenet5_172_1127977.pth.
[INFO][10:31:17]: [Client #438] Epoch: [5/5][0/10]	Loss: 0.133597
[INFO][10:31:17]: [Client #353] Epoch: [5/5][0/10]	Loss: 0.000058
[INFO][10:31:17]: [Client #438] Model saved to /data/ykang/plato/results/test/model/lenet5_438_1127978.pth.
[INFO][10:31:17]: [Client #353] Model saved to /data/ykang/plato/results/test/model/lenet5_353_1127979.pth.
[INFO][10:31:18]: [Client #172] Loading a model from /data/ykang/plato/results/test/model/lenet5_172_1127977.pth.
[INFO][10:31:18]: [Client #172] Model trained.
[INFO][10:31:18]: [Client #172] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:31:18]: [Server #1127936] Received 0.24 MB of payload data from client #172 (simulated).
[INFO][10:31:18]: [Client #438] Loading a model from /data/ykang/plato/results/test/model/lenet5_438_1127978.pth.
[INFO][10:31:18]: [Client #438] Model trained.
[INFO][10:31:18]: [Client #438] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:31:18]: [Server #1127936] Received 0.24 MB of payload data from client #438 (simulated).
[INFO][10:31:18]: [Client #353] Loading a model from /data/ykang/plato/results/test/model/lenet5_353_1127979.pth.
[INFO][10:31:18]: [Client #353] Model trained.
[INFO][10:31:18]: [Client #353] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:31:18]: [Server #1127936] Received 0.24 MB of payload data from client #353 (simulated).
[INFO][10:31:18]: [Server #1127936] Selecting client #94 for training.
[INFO][10:31:18]: [Server #1127936] Sending the current model to client #94 (simulated).
[INFO][10:31:18]: [Server #1127936] Sending 0.24 MB of payload data to client #94 (simulated).
[INFO][10:31:18]: [Server #1127936] Selecting client #284 for training.
[INFO][10:31:18]: [Server #1127936] Sending the current model to client #284 (simulated).
[INFO][10:31:18]: [Server #1127936] Sending 0.24 MB of payload data to client #284 (simulated).
[INFO][10:31:18]: [Server #1127936] Selecting client #455 for training.
[INFO][10:31:18]: [Server #1127936] Sending the current model to client #455 (simulated).
[INFO][10:31:18]: [Client #94] Selected by the server.
[INFO][10:31:18]: [Client #94] Loading its data source...
[INFO][10:31:18]: [Client #94] Dataset size: 60000
[INFO][10:31:18]: [Client #94] Sampler: noniid
[INFO][10:31:18]: [Server #1127936] Sending 0.24 MB of payload data to client #455 (simulated).
[INFO][10:31:18]: [Client #284] Selected by the server.
[INFO][10:31:18]: [Client #284] Loading its data source...
[INFO][10:31:18]: [Client #284] Dataset size: 60000
[INFO][10:31:18]: [Client #284] Sampler: noniid
[INFO][10:31:18]: [Client #455] Selected by the server.
[INFO][10:31:18]: [Client #455] Loading its data source...
[INFO][10:31:18]: [Client #455] Dataset size: 60000
[INFO][10:31:18]: [Client #455] Sampler: noniid
[INFO][10:31:18]: [Client #94] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:31:18]: [Client #284] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:31:18]: [93m[1m[Client #94] Started training in communication round #76.[0m
[INFO][10:31:18]: [Client #455] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:31:18]: [93m[1m[Client #455] Started training in communication round #76.[0m
[INFO][10:31:18]: [93m[1m[Client #284] Started training in communication round #76.[0m
[INFO][10:31:20]: [Client #455] Loading the dataset.
[INFO][10:31:20]: [Client #284] Loading the dataset.
[INFO][10:31:20]: [Client #94] Loading the dataset.
[INFO][10:31:26]: [Client #94] Epoch: [1/5][0/10]	Loss: 0.002491
[INFO][10:31:26]: [Client #455] Epoch: [1/5][0/10]	Loss: 0.001069
[INFO][10:31:26]: [Client #284] Epoch: [1/5][0/10]	Loss: 0.001169
[INFO][10:31:26]: [Client #94] Epoch: [2/5][0/10]	Loss: 0.005588
[INFO][10:31:26]: [Client #455] Epoch: [2/5][0/10]	Loss: 0.000203
[INFO][10:31:26]: [Client #284] Epoch: [2/5][0/10]	Loss: 0.000409
[INFO][10:31:26]: [Client #284] Epoch: [3/5][0/10]	Loss: 0.000000
[INFO][10:31:26]: [Client #94] Epoch: [3/5][0/10]	Loss: 0.000037
[INFO][10:31:27]: [Client #455] Epoch: [3/5][0/10]	Loss: 0.000000
[INFO][10:31:27]: [Client #284] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:31:27]: [Client #94] Epoch: [4/5][0/10]	Loss: 0.000653
[INFO][10:31:27]: [Client #455] Epoch: [4/5][0/10]	Loss: 0.000022
[INFO][10:31:27]: [Client #94] Epoch: [5/5][0/10]	Loss: 0.000517
[INFO][10:31:27]: [Client #284] Epoch: [5/5][0/10]	Loss: 0.000001
[INFO][10:31:27]: [Client #94] Model saved to /data/ykang/plato/results/test/model/lenet5_94_1127977.pth.
[INFO][10:31:27]: [Client #455] Epoch: [5/5][0/10]	Loss: 0.000178
[INFO][10:31:27]: [Client #284] Model saved to /data/ykang/plato/results/test/model/lenet5_284_1127978.pth.
[INFO][10:31:27]: [Client #455] Model saved to /data/ykang/plato/results/test/model/lenet5_455_1127979.pth.
[INFO][10:31:28]: [Client #94] Loading a model from /data/ykang/plato/results/test/model/lenet5_94_1127977.pth.
[INFO][10:31:28]: [Client #94] Model trained.
[INFO][10:31:28]: [Client #94] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:31:28]: [Server #1127936] Received 0.24 MB of payload data from client #94 (simulated).
[INFO][10:31:28]: [Client #284] Loading a model from /data/ykang/plato/results/test/model/lenet5_284_1127978.pth.
[INFO][10:31:28]: [Client #284] Model trained.
[INFO][10:31:28]: [Client #284] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:31:28]: [Server #1127936] Received 0.24 MB of payload data from client #284 (simulated).
[INFO][10:31:28]: [Client #455] Loading a model from /data/ykang/plato/results/test/model/lenet5_455_1127979.pth.
[INFO][10:31:28]: [Client #455] Model trained.
[INFO][10:31:28]: [Client #455] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:31:28]: [Server #1127936] Received 0.24 MB of payload data from client #455 (simulated).
[INFO][10:31:28]: [Server #1127936] Selecting client #77 for training.
[INFO][10:31:28]: [Server #1127936] Sending the current model to client #77 (simulated).
[INFO][10:31:28]: [Server #1127936] Sending 0.24 MB of payload data to client #77 (simulated).
[INFO][10:31:28]: [Server #1127936] Selecting client #423 for training.
[INFO][10:31:28]: [Server #1127936] Sending the current model to client #423 (simulated).
[INFO][10:31:28]: [Server #1127936] Sending 0.24 MB of payload data to client #423 (simulated).
[INFO][10:31:28]: [Server #1127936] Selecting client #411 for training.
[INFO][10:31:28]: [Server #1127936] Sending the current model to client #411 (simulated).
[INFO][10:31:28]: [Client #77] Selected by the server.
[INFO][10:31:28]: [Client #77] Loading its data source...
[INFO][10:31:28]: [Client #77] Dataset size: 60000
[INFO][10:31:28]: [Client #77] Sampler: noniid
[INFO][10:31:28]: [Server #1127936] Sending 0.24 MB of payload data to client #411 (simulated).
[INFO][10:31:28]: [Client #77] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:31:28]: [Client #423] Selected by the server.
[INFO][10:31:28]: [Client #423] Loading its data source...
[INFO][10:31:28]: [Client #411] Selected by the server.
[INFO][10:31:28]: [Client #423] Dataset size: 60000
[INFO][10:31:28]: [Client #411] Loading its data source...
[INFO][10:31:28]: [Client #423] Sampler: noniid
[INFO][10:31:28]: [Client #411] Dataset size: 60000
[INFO][10:31:28]: [Client #411] Sampler: noniid
[INFO][10:31:28]: [Client #411] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:31:28]: [Client #423] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:31:28]: [93m[1m[Client #411] Started training in communication round #76.[0m
[INFO][10:31:28]: [93m[1m[Client #77] Started training in communication round #76.[0m
[INFO][10:31:28]: [93m[1m[Client #423] Started training in communication round #76.[0m
[INFO][10:31:30]: [Client #411] Loading the dataset.
[INFO][10:31:30]: [Client #77] Loading the dataset.
[INFO][10:31:30]: [Client #423] Loading the dataset.
[INFO][10:31:36]: [Client #411] Epoch: [1/5][0/10]	Loss: 0.002205
[INFO][10:31:36]: [Client #423] Epoch: [1/5][0/10]	Loss: 0.013768
[INFO][10:31:36]: [Client #77] Epoch: [1/5][0/10]	Loss: 0.000596
[INFO][10:31:36]: [Client #411] Epoch: [2/5][0/10]	Loss: 0.001955
[INFO][10:31:36]: [Client #77] Epoch: [2/5][0/10]	Loss: 0.001188
[INFO][10:31:36]: [Client #423] Epoch: [2/5][0/10]	Loss: 0.000042
[INFO][10:31:36]: [Client #411] Epoch: [3/5][0/10]	Loss: 0.000201
[INFO][10:31:36]: [Client #77] Epoch: [3/5][0/10]	Loss: 0.000092
[INFO][10:31:36]: [Client #423] Epoch: [3/5][0/10]	Loss: 0.000004
[INFO][10:31:36]: [Client #411] Epoch: [4/5][0/10]	Loss: 0.000117
[INFO][10:31:36]: [Client #77] Epoch: [4/5][0/10]	Loss: 0.000007
[INFO][10:31:36]: [Client #423] Epoch: [4/5][0/10]	Loss: 0.000024
[INFO][10:31:36]: [Client #411] Epoch: [5/5][0/10]	Loss: 0.000015
[INFO][10:31:36]: [Client #77] Epoch: [5/5][0/10]	Loss: 0.001575
[INFO][10:31:36]: [Client #411] Model saved to /data/ykang/plato/results/test/model/lenet5_411_1127979.pth.
[INFO][10:31:36]: [Client #423] Epoch: [5/5][0/10]	Loss: 0.000076
[INFO][10:31:36]: [Client #77] Model saved to /data/ykang/plato/results/test/model/lenet5_77_1127977.pth.
[INFO][10:31:36]: [Client #423] Model saved to /data/ykang/plato/results/test/model/lenet5_423_1127978.pth.
[INFO][10:31:37]: [Client #411] Loading a model from /data/ykang/plato/results/test/model/lenet5_411_1127979.pth.
[INFO][10:31:37]: [Client #411] Model trained.
[INFO][10:31:37]: [Client #411] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:31:37]: [Server #1127936] Received 0.24 MB of payload data from client #411 (simulated).
[INFO][10:31:37]: [Client #423] Loading a model from /data/ykang/plato/results/test/model/lenet5_423_1127978.pth.
[INFO][10:31:37]: [Client #423] Model trained.
[INFO][10:31:37]: [Client #423] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:31:37]: [Server #1127936] Received 0.24 MB of payload data from client #423 (simulated).
[INFO][10:31:37]: [Client #77] Loading a model from /data/ykang/plato/results/test/model/lenet5_77_1127977.pth.
[INFO][10:31:37]: [Client #77] Model trained.
[INFO][10:31:37]: [Client #77] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:31:37]: [Server #1127936] Received 0.24 MB of payload data from client #77 (simulated).
[INFO][10:31:37]: [Server #1127936] Selecting client #250 for training.
[INFO][10:31:37]: [Server #1127936] Sending the current model to client #250 (simulated).
[INFO][10:31:37]: [Server #1127936] Sending 0.24 MB of payload data to client #250 (simulated).
[INFO][10:31:37]: [Client #250] Selected by the server.
[INFO][10:31:37]: [Client #250] Loading its data source...
[INFO][10:31:37]: [Client #250] Dataset size: 60000
[INFO][10:31:37]: [Client #250] Sampler: noniid
[INFO][10:31:37]: [Client #250] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:31:37]: [93m[1m[Client #250] Started training in communication round #76.[0m
[INFO][10:31:39]: [Client #250] Loading the dataset.
[INFO][10:31:44]: [Client #250] Epoch: [1/5][0/10]	Loss: 0.005442
[INFO][10:31:45]: [Client #250] Epoch: [2/5][0/10]	Loss: 0.000170
[INFO][10:31:45]: [Client #250] Epoch: [3/5][0/10]	Loss: 0.000019
[INFO][10:31:45]: [Client #250] Epoch: [4/5][0/10]	Loss: 0.000016
[INFO][10:31:45]: [Client #250] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:31:45]: [Client #250] Model saved to /data/ykang/plato/results/test/model/lenet5_250_1127977.pth.
[INFO][10:31:45]: [Client #250] Loading a model from /data/ykang/plato/results/test/model/lenet5_250_1127977.pth.
[INFO][10:31:45]: [Client #250] Model trained.
[INFO][10:31:45]: [Client #250] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:31:45]: [Server #1127936] Received 0.24 MB of payload data from client #250 (simulated).
[INFO][10:31:45]: [Server #1127936] Adding client #459 to the list of clients for aggregation.
[INFO][10:31:45]: [Server #1127936] Adding client #48 to the list of clients for aggregation.
[INFO][10:31:45]: [Server #1127936] Adding client #171 to the list of clients for aggregation.
[INFO][10:31:45]: [Server #1127936] Adding client #295 to the list of clients for aggregation.
[INFO][10:31:45]: [Server #1127936] Adding client #316 to the list of clients for aggregation.
[INFO][10:31:45]: [Server #1127936] Adding client #196 to the list of clients for aggregation.
[INFO][10:31:45]: [Server #1127936] Adding client #200 to the list of clients for aggregation.
[INFO][10:31:45]: [Server #1127936] Adding client #282 to the list of clients for aggregation.
[INFO][10:31:45]: [Server #1127936] Adding client #423 to the list of clients for aggregation.
[INFO][10:31:45]: [Server #1127936] Adding client #438 to the list of clients for aggregation.
[INFO][10:31:45]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 276, 277, 278, 279, 280, 281, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.03406856
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00283007 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00327966 0.         0.
 0.         0.00188836 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01154844
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00541804 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00341484 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00332174 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.02306357
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.0048108  0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 0. 1. 0. 0. 0. 1. 0. 0. 3. 1. 0. 0. 0. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 2. 0. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 2. 1. 1. 1. 0.
 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 2. 1. 6. 0. 0. 2. 1. 0. 1. 1. 2. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 3. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 1. 6. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 2. 0. 0.
 1. 1. 1. 2. 0. 0. 0. 2. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 4. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1.
 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 4. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0.
 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0.
 1. 0. 1. 0. 4. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 3. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 0. 1. 0. 6. 1. 1.
 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 6. 0. 0. 0. 1. 0. 0. 0. 2. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 2. 2. 1. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.03406856
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00283007 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00327966 0.         0.
 0.         0.00188836 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01154844
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00541804 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00341484 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00332174 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.02306357
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.0048108  0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:31:47]: [Server #1127936] Global model accuracy: 96.18%

[INFO][10:31:47]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_76.pth.
[INFO][10:31:47]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_76.pth.
[INFO][10:31:47]: [93m[1m
[Server #1127936] Starting round 77/100.[0m
[0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  4e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  4e-05  6e-10  6e-10
 6:  6.8876e+00  6.8875e+00  3e-05  4e-10  4e-10
 7:  6.8875e+00  6.8875e+00  3e-05  9e-10  3e-11
 8:  6.8875e+00  6.8875e+00  2e-05  9e-10  3e-11
 9:  6.8875e+00  6.8875e+00  1e-05  2e-09  8e-11
10:  6.8875e+00  6.8875e+00  1e-06  3e-09  1e-10
Optimal solution found.
The calculated probability is:  [6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 9.68337386e-01 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 7.15550065e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 8.35044203e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 7.42589366e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 1.07221212e-04
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 7.94525571e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 7.32017775e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45142376e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45025357e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 7.74501574e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05 6.45144856e-05 6.45144856e-05
 6.45144856e-05 6.45144856e-05]
current clients pool:  [INFO][10:31:48]: [Server #1127936] Selected clients: [ 48 202 311 118 234 246 385 319 235 335]
[INFO][10:31:48]: [Server #1127936] Selecting client #48 for training.
[INFO][10:31:48]: [Server #1127936] Sending the current model to client #48 (simulated).
[INFO][10:31:48]: [Server #1127936] Sending 0.24 MB of payload data to client #48 (simulated).
[INFO][10:31:48]: [Server #1127936] Selecting client #202 for training.
[INFO][10:31:48]: [Server #1127936] Sending the current model to client #202 (simulated).
[INFO][10:31:48]: [Server #1127936] Sending 0.24 MB of payload data to client #202 (simulated).
[INFO][10:31:48]: [Server #1127936] Selecting client #311 for training.
[INFO][10:31:48]: [Server #1127936] Sending the current model to client #311 (simulated).
[INFO][10:31:48]: [Client #48] Selected by the server.
[INFO][10:31:48]: [Client #48] Loading its data source...
[INFO][10:31:48]: [Client #48] Dataset size: 60000
[INFO][10:31:48]: [Client #48] Sampler: noniid
[INFO][10:31:48]: [Server #1127936] Sending 0.24 MB of payload data to client #311 (simulated).
[INFO][10:31:48]: [Client #202] Selected by the server.
[INFO][10:31:48]: [Client #202] Loading its data source...
[INFO][10:31:48]: [Client #202] Dataset size: 60000
[INFO][10:31:48]: [Client #202] Sampler: noniid
[INFO][10:31:48]: [Client #311] Selected by the server.
[INFO][10:31:48]: [Client #48] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:31:48]: [Client #311] Loading its data source...
[INFO][10:31:48]: [Client #311] Dataset size: 60000
[INFO][10:31:48]: [Client #311] Sampler: noniid
[INFO][10:31:48]: [Client #202] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:31:48]: [Client #311] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:31:48]: [93m[1m[Client #311] Started training in communication round #77.[0m
[INFO][10:31:48]: [93m[1m[Client #202] Started training in communication round #77.[0m
[INFO][10:31:48]: [93m[1m[Client #48] Started training in communication round #77.[0m
[INFO][10:31:50]: [Client #311] Loading the dataset.
[INFO][10:31:50]: [Client #48] Loading the dataset.
[INFO][10:31:50]: [Client #202] Loading the dataset.
[INFO][10:31:56]: [Client #311] Epoch: [1/5][0/10]	Loss: 0.000407
[INFO][10:31:56]: [Client #202] Epoch: [1/5][0/10]	Loss: 0.000431
[INFO][10:31:56]: [Client #311] Epoch: [2/5][0/10]	Loss: 0.001247
[INFO][10:31:56]: [Client #48] Epoch: [1/5][0/10]	Loss: 0.010084
[INFO][10:31:56]: [Client #311] Epoch: [3/5][0/10]	Loss: 0.000681
[INFO][10:31:56]: [Client #202] Epoch: [2/5][0/10]	Loss: 0.000080
[INFO][10:31:56]: [Client #48] Epoch: [2/5][0/10]	Loss: 0.001991
[INFO][10:31:56]: [Client #311] Epoch: [4/5][0/10]	Loss: 0.000040
[INFO][10:31:56]: [Client #202] Epoch: [3/5][0/10]	Loss: 0.000007
[INFO][10:31:56]: [Client #48] Epoch: [3/5][0/10]	Loss: 0.000021
[INFO][10:31:56]: [Client #311] Epoch: [5/5][0/10]	Loss: 0.000002
[INFO][10:31:56]: [Client #202] Epoch: [4/5][0/10]	Loss: 0.002819
[INFO][10:31:56]: [Client #311] Model saved to /data/ykang/plato/results/test/model/lenet5_311_1127979.pth.
[INFO][10:31:56]: [Client #48] Epoch: [4/5][0/10]	Loss: 0.000046
[INFO][10:31:56]: [Client #202] Epoch: [5/5][0/10]	Loss: 0.000017
[INFO][10:31:56]: [Client #48] Epoch: [5/5][0/10]	Loss: 0.000362
[INFO][10:31:57]: [Client #202] Model saved to /data/ykang/plato/results/test/model/lenet5_202_1127978.pth.
[INFO][10:31:57]: [Client #48] Model saved to /data/ykang/plato/results/test/model/lenet5_48_1127977.pth.
[INFO][10:31:57]: [Client #311] Loading a model from /data/ykang/plato/results/test/model/lenet5_311_1127979.pth.
[INFO][10:31:57]: [Client #311] Model trained.
[INFO][10:31:57]: [Client #311] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:31:57]: [Server #1127936] Received 0.24 MB of payload data from client #311 (simulated).
[INFO][10:31:57]: [Client #202] Loading a model from /data/ykang/plato/results/test/model/lenet5_202_1127978.pth.
[INFO][10:31:57]: [Client #202] Model trained.
[INFO][10:31:57]: [Client #202] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:31:57]: [Server #1127936] Received 0.24 MB of payload data from client #202 (simulated).
[INFO][10:31:57]: [Client #48] Loading a model from /data/ykang/plato/results/test/model/lenet5_48_1127977.pth.
[INFO][10:31:57]: [Client #48] Model trained.
[INFO][10:31:57]: [Client #48] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:31:57]: [Server #1127936] Received 0.24 MB of payload data from client #48 (simulated).
[INFO][10:31:57]: [Server #1127936] Selecting client #118 for training.
[INFO][10:31:57]: [Server #1127936] Sending the current model to client #118 (simulated).
[INFO][10:31:57]: [Server #1127936] Sending 0.24 MB of payload data to client #118 (simulated).
[INFO][10:31:57]: [Server #1127936] Selecting client #234 for training.
[INFO][10:31:57]: [Server #1127936] Sending the current model to client #234 (simulated).
[INFO][10:31:57]: [Server #1127936] Sending 0.24 MB of payload data to client #234 (simulated).
[INFO][10:31:57]: [Server #1127936] Selecting client #246 for training.
[INFO][10:31:57]: [Server #1127936] Sending the current model to client #246 (simulated).
[INFO][10:31:57]: [Client #118] Selected by the server.
[INFO][10:31:57]: [Client #118] Loading its data source...
[INFO][10:31:57]: [Client #118] Dataset size: 60000
[INFO][10:31:57]: [Client #118] Sampler: noniid
[INFO][10:31:57]: [Server #1127936] Sending 0.24 MB of payload data to client #246 (simulated).
[INFO][10:31:57]: [Client #234] Selected by the server.
[INFO][10:31:57]: [Client #234] Loading its data source...
[INFO][10:31:57]: [Client #246] Selected by the server.
[INFO][10:31:57]: [Client #234] Dataset size: 60000
[INFO][10:31:57]: [Client #246] Loading its data source...
[INFO][10:31:57]: [Client #234] Sampler: noniid
[INFO][10:31:57]: [Client #246] Dataset size: 60000
[INFO][10:31:57]: [Client #246] Sampler: noniid
[INFO][10:31:57]: [Client #118] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:31:57]: [Client #246] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:31:57]: [Client #234] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:31:57]: [93m[1m[Client #118] Started training in communication round #77.[0m
[INFO][10:31:57]: [93m[1m[Client #246] Started training in communication round #77.[0m
[INFO][10:31:57]: [93m[1m[Client #234] Started training in communication round #77.[0m
[INFO][10:31:59]: [Client #246] Loading the dataset.
[INFO][10:32:00]: [Client #118] Loading the dataset.
[INFO][10:32:00]: [Client #234] Loading the dataset.
[INFO][10:32:05]: [Client #246] Epoch: [1/5][0/10]	Loss: 0.000431
[INFO][10:32:05]: [Client #118] Epoch: [1/5][0/10]	Loss: 0.002881
[INFO][10:32:05]: [Client #246] Epoch: [2/5][0/10]	Loss: 0.000206
[INFO][10:32:05]: [Client #234] Epoch: [1/5][0/10]	Loss: 0.000405
[INFO][10:32:05]: [Client #118] Epoch: [2/5][0/10]	Loss: 0.002223
[INFO][10:32:05]: [Client #234] Epoch: [2/5][0/10]	Loss: 0.000260
[INFO][10:32:05]: [Client #246] Epoch: [3/5][0/10]	Loss: 0.000002
[INFO][10:32:05]: [Client #118] Epoch: [3/5][0/10]	Loss: 0.000219
[INFO][10:32:05]: [Client #234] Epoch: [3/5][0/10]	Loss: 0.000021
[INFO][10:32:06]: [Client #118] Epoch: [4/5][0/10]	Loss: 0.000183
[INFO][10:32:06]: [Client #246] Epoch: [4/5][0/10]	Loss: 0.000074
[INFO][10:32:06]: [Client #234] Epoch: [4/5][0/10]	Loss: 0.000034
[INFO][10:32:06]: [Client #118] Epoch: [5/5][0/10]	Loss: 0.001712
[INFO][10:32:06]: [Client #246] Epoch: [5/5][0/10]	Loss: 0.000010
[INFO][10:32:06]: [Client #234] Epoch: [5/5][0/10]	Loss: 0.001558
[INFO][10:32:06]: [Client #246] Model saved to /data/ykang/plato/results/test/model/lenet5_246_1127979.pth.
[INFO][10:32:06]: [Client #118] Model saved to /data/ykang/plato/results/test/model/lenet5_118_1127977.pth.
[INFO][10:32:06]: [Client #234] Model saved to /data/ykang/plato/results/test/model/lenet5_234_1127978.pth.
[INFO][10:32:07]: [Client #246] Loading a model from /data/ykang/plato/results/test/model/lenet5_246_1127979.pth.
[INFO][10:32:07]: [Client #246] Model trained.
[INFO][10:32:07]: [Client #246] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:32:07]: [Server #1127936] Received 0.24 MB of payload data from client #246 (simulated).
[INFO][10:32:07]: [Client #118] Loading a model from /data/ykang/plato/results/test/model/lenet5_118_1127977.pth.
[INFO][10:32:07]: [Client #118] Model trained.
[INFO][10:32:07]: [Client #118] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:32:07]: [Server #1127936] Received 0.24 MB of payload data from client #118 (simulated).
[INFO][10:32:07]: [Client #234] Loading a model from /data/ykang/plato/results/test/model/lenet5_234_1127978.pth.
[INFO][10:32:07]: [Client #234] Model trained.
[INFO][10:32:07]: [Client #234] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:32:07]: [Server #1127936] Received 0.24 MB of payload data from client #234 (simulated).
[INFO][10:32:07]: [Server #1127936] Selecting client #385 for training.
[INFO][10:32:07]: [Server #1127936] Sending the current model to client #385 (simulated).
[INFO][10:32:07]: [Server #1127936] Sending 0.24 MB of payload data to client #385 (simulated).
[INFO][10:32:07]: [Server #1127936] Selecting client #319 for training.
[INFO][10:32:07]: [Server #1127936] Sending the current model to client #319 (simulated).
[INFO][10:32:07]: [Server #1127936] Sending 0.24 MB of payload data to client #319 (simulated).
[INFO][10:32:07]: [Server #1127936] Selecting client #235 for training.
[INFO][10:32:07]: [Server #1127936] Sending the current model to client #235 (simulated).
[INFO][10:32:07]: [Client #385] Selected by the server.
[INFO][10:32:07]: [Client #385] Loading its data source...
[INFO][10:32:07]: [Client #385] Dataset size: 60000
[INFO][10:32:07]: [Client #385] Sampler: noniid
[INFO][10:32:07]: [Server #1127936] Sending 0.24 MB of payload data to client #235 (simulated).
[INFO][10:32:07]: [Client #319] Selected by the server.
[INFO][10:32:07]: [Client #319] Loading its data source...
[INFO][10:32:07]: [Client #319] Dataset size: 60000
[INFO][10:32:07]: [Client #319] Sampler: noniid
[INFO][10:32:07]: [Client #235] Selected by the server.
[INFO][10:32:07]: [Client #235] Loading its data source...
[INFO][10:32:07]: [Client #235] Dataset size: 60000
[INFO][10:32:07]: [Client #235] Sampler: noniid
[INFO][10:32:07]: [Client #385] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:32:07]: [Client #319] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:32:07]: [93m[1m[Client #385] Started training in communication round #77.[0m
[INFO][10:32:07]: [Client #235] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:32:07]: [93m[1m[Client #319] Started training in communication round #77.[0m
[INFO][10:32:07]: [93m[1m[Client #235] Started training in communication round #77.[0m
[INFO][10:32:09]: [Client #385] Loading the dataset.
[INFO][10:32:09]: [Client #235] Loading the dataset.
[INFO][10:32:09]: [Client #319] Loading the dataset.
[INFO][10:32:15]: [Client #385] Epoch: [1/5][0/10]	Loss: 0.010084
[INFO][10:32:15]: [Client #235] Epoch: [1/5][0/10]	Loss: 0.001118
[INFO][10:32:15]: [Client #319] Epoch: [1/5][0/10]	Loss: 0.009870
[INFO][10:32:15]: [Client #385] Epoch: [2/5][0/10]	Loss: 0.004419
[INFO][10:32:15]: [Client #235] Epoch: [2/5][0/10]	Loss: 0.000545
[INFO][10:32:15]: [Client #319] Epoch: [2/5][0/10]	Loss: 0.002277
[INFO][10:32:15]: [Client #235] Epoch: [3/5][0/10]	Loss: 0.000051
[INFO][10:32:15]: [Client #385] Epoch: [3/5][0/10]	Loss: 0.000018
[INFO][10:32:15]: [Client #319] Epoch: [3/5][0/10]	Loss: 0.000101
[INFO][10:32:15]: [Client #235] Epoch: [4/5][0/10]	Loss: 0.000506
[INFO][10:32:15]: [Client #385] Epoch: [4/5][0/10]	Loss: 0.000002
[INFO][10:32:15]: [Client #319] Epoch: [4/5][0/10]	Loss: 0.000026
[INFO][10:32:15]: [Client #385] Epoch: [5/5][0/10]	Loss: 0.000023
[INFO][10:32:15]: [Client #235] Epoch: [5/5][0/10]	Loss: 0.000239
[INFO][10:32:15]: [Client #385] Model saved to /data/ykang/plato/results/test/model/lenet5_385_1127977.pth.
[INFO][10:32:15]: [Client #235] Model saved to /data/ykang/plato/results/test/model/lenet5_235_1127979.pth.
[INFO][10:32:15]: [Client #319] Epoch: [5/5][0/10]	Loss: 0.021632
[INFO][10:32:15]: [Client #319] Model saved to /data/ykang/plato/results/test/model/lenet5_319_1127978.pth.
[INFO][10:32:16]: [Client #385] Loading a model from /data/ykang/plato/results/test/model/lenet5_385_1127977.pth.
[INFO][10:32:16]: [Client #385] Model trained.
[INFO][10:32:16]: [Client #385] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:32:16]: [Server #1127936] Received 0.24 MB of payload data from client #385 (simulated).
[INFO][10:32:16]: [Client #235] Loading a model from /data/ykang/plato/results/test/model/lenet5_235_1127979.pth.
[INFO][10:32:16]: [Client #235] Model trained.
[INFO][10:32:16]: [Client #235] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:32:16]: [Server #1127936] Received 0.24 MB of payload data from client #235 (simulated).
[INFO][10:32:16]: [Client #319] Loading a model from /data/ykang/plato/results/test/model/lenet5_319_1127978.pth.
[INFO][10:32:16]: [Client #319] Model trained.
[INFO][10:32:16]: [Client #319] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:32:16]: [Server #1127936] Received 0.24 MB of payload data from client #319 (simulated).
[INFO][10:32:16]: [Server #1127936] Selecting client #335 for training.
[INFO][10:32:16]: [Server #1127936] Sending the current model to client #335 (simulated).
[INFO][10:32:16]: [Server #1127936] Sending 0.24 MB of payload data to client #335 (simulated).
[INFO][10:32:16]: [Client #335] Selected by the server.
[INFO][10:32:16]: [Client #335] Loading its data source...
[INFO][10:32:16]: [Client #335] Dataset size: 60000
[INFO][10:32:16]: [Client #335] Sampler: noniid
[INFO][10:32:16]: [Client #335] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:32:16]: [93m[1m[Client #335] Started training in communication round #77.[0m
[INFO][10:32:18]: [Client #335] Loading the dataset.
[INFO][10:32:23]: [Client #335] Epoch: [1/5][0/10]	Loss: 0.001437
[INFO][10:32:24]: [Client #335] Epoch: [2/5][0/10]	Loss: 0.001009
[INFO][10:32:24]: [Client #335] Epoch: [3/5][0/10]	Loss: 0.000257
[INFO][10:32:24]: [Client #335] Epoch: [4/5][0/10]	Loss: 0.000038
[INFO][10:32:24]: [Client #335] Epoch: [5/5][0/10]	Loss: 0.000106
[INFO][10:32:24]: [Client #335] Model saved to /data/ykang/plato/results/test/model/lenet5_335_1127977.pth.
[INFO][10:32:24]: [Client #335] Loading a model from /data/ykang/plato/results/test/model/lenet5_335_1127977.pth.
[INFO][10:32:24]: [Client #335] Model trained.
[INFO][10:32:24]: [Client #335] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:32:24]: [Server #1127936] Received 0.24 MB of payload data from client #335 (simulated).
[INFO][10:32:24]: [Server #1127936] Adding client #411 to the list of clients for aggregation.
[INFO][10:32:24]: [Server #1127936] Adding client #353 to the list of clients for aggregation.
[INFO][10:32:24]: [Server #1127936] Adding client #284 to the list of clients for aggregation.
[INFO][10:32:24]: [Server #1127936] Adding client #94 to the list of clients for aggregation.
[INFO][10:32:24]: [Server #1127936] Adding client #455 to the list of clients for aggregation.
[INFO][10:32:24]: [Server #1127936] Adding client #41 to the list of clients for aggregation.
[INFO][10:32:24]: [Server #1127936] Adding client #385 to the list of clients for aggregation.
[INFO][10:32:24]: [Server #1127936] Adding client #319 to the list of clients for aggregation.
[INFO][10:32:24]: [Server #1127936] Adding client #234 to the list of clients for aggregation.
[INFO][10:32:24]: [Server #1127936] Adding client #48 to the list of clients for aggregation.
[INFO][10:32:24]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 276, 277, 278, 279, 280, 281, 282, 283, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.02220377 0.
 0.         0.         0.         0.         0.         0.01136173
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01116597 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00856066
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00646777 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00534527 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00888358 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00374181 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00294206 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00783993 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 0. 1. 0. 0. 0. 1. 0. 0. 3. 1. 0. 0. 0. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 2. 0. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 2. 1. 1. 1. 0.
 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 2. 1. 6. 0. 0. 2. 1. 0. 1. 1. 2. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 3. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 1. 6. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 2. 0. 0.
 1. 1. 1. 2. 0. 0. 0. 2. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 4. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1.
 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 4. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0.
 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0.
 1. 0. 1. 0. 4. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 3. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 0. 1. 0. 6. 1. 1.
 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 6. 0. 0. 0. 1. 0. 0. 0. 2. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 2. 2. 1. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.02220377 0.
 0.         0.         0.         0.         0.         0.01136173
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01116597 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00856066
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00646777 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00534527 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00888358 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00374181 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00294206 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00783993 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:32:26]: [Server #1127936] Global model accuracy: 96.31%

[INFO][10:32:26]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_77.pth.
[INFO][10:32:26]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_77.pth.
[INFO][10:32:26]: [93m[1m
[Server #1127936] Starting round 78/100.[0m
[0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8871e+00  5e-04  8e-09  8e-09
 5:  6.8876e+00  6.8874e+00  1e-04  2e-09  2e-09
 6:  6.8875e+00  6.8874e+00  1e-04  2e-09  3e-10
 7:  6.8875e+00  6.8874e+00  1e-04  3e-09  4e-10
 8:  6.8875e+00  6.8874e+00  7e-05  4e-08  5e-09
 9:  6.8875e+00  6.8874e+00  4e-05  3e-08  5e-09
10:  6.8874e+00  6.8874e+00  3e-06  3e-08  4e-09
Optimal solution found.
The calculated probability is:  [4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 9.76218856e-01 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85799078e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 5.63702920e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85802490e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 5.28412438e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85805223e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 5.46056345e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806114e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 5.04404229e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 5.38308434e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05 4.85806970e-05 4.85806970e-05
 4.85806970e-05 4.85806970e-05]
current clients pool:  [INFO][10:32:27]: [Server #1127936] Selected clients: [ 41  87 304 367 208 347 474 101 407 486]
[INFO][10:32:27]: [Server #1127936] Selecting client #41 for training.
[INFO][10:32:27]: [Server #1127936] Sending the current model to client #41 (simulated).
[INFO][10:32:27]: [Server #1127936] Sending 0.24 MB of payload data to client #41 (simulated).
[INFO][10:32:27]: [Server #1127936] Selecting client #87 for training.
[INFO][10:32:27]: [Server #1127936] Sending the current model to client #87 (simulated).
[INFO][10:32:27]: [Server #1127936] Sending 0.24 MB of payload data to client #87 (simulated).
[INFO][10:32:27]: [Server #1127936] Selecting client #304 for training.
[INFO][10:32:27]: [Server #1127936] Sending the current model to client #304 (simulated).
[INFO][10:32:27]: [Client #41] Selected by the server.
[INFO][10:32:27]: [Client #41] Loading its data source...
[INFO][10:32:27]: [Client #41] Dataset size: 60000
[INFO][10:32:27]: [Client #41] Sampler: noniid
[INFO][10:32:27]: [Server #1127936] Sending 0.24 MB of payload data to client #304 (simulated).
[INFO][10:32:27]: [Client #87] Selected by the server.
[INFO][10:32:27]: [Client #87] Loading its data source...
[INFO][10:32:27]: [Client #87] Dataset size: 60000
[INFO][10:32:27]: [Client #41] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:32:27]: [Client #87] Sampler: noniid
[INFO][10:32:27]: [Client #87] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:32:27]: [93m[1m[Client #41] Started training in communication round #78.[0m
[INFO][10:32:27]: [93m[1m[Client #87] Started training in communication round #78.[0m
[INFO][10:32:27]: [Client #304] Selected by the server.
[INFO][10:32:27]: [Client #304] Loading its data source...
[INFO][10:32:27]: [Client #304] Dataset size: 60000
[INFO][10:32:27]: [Client #304] Sampler: noniid
[INFO][10:32:27]: [Client #304] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:32:27]: [93m[1m[Client #304] Started training in communication round #78.[0m
[INFO][10:32:29]: [Client #304] Loading the dataset.
[INFO][10:32:29]: [Client #87] Loading the dataset.
[INFO][10:32:29]: [Client #41] Loading the dataset.
[INFO][10:32:35]: [Client #304] Epoch: [1/5][0/10]	Loss: 0.000685
[INFO][10:32:35]: [Client #87] Epoch: [1/5][0/10]	Loss: 0.000455
[INFO][10:32:35]: [Client #304] Epoch: [2/5][0/10]	Loss: 0.000262
[INFO][10:32:35]: [Client #41] Epoch: [1/5][0/10]	Loss: 0.001098
[INFO][10:32:35]: [Client #41] Epoch: [2/5][0/10]	Loss: 0.000637
[INFO][10:32:35]: [Client #87] Epoch: [2/5][0/10]	Loss: 0.000585
[INFO][10:32:35]: [Client #304] Epoch: [3/5][0/10]	Loss: 0.000022
[INFO][10:32:35]: [Client #41] Epoch: [3/5][0/10]	Loss: 0.010041
[INFO][10:32:35]: [Client #87] Epoch: [3/5][0/10]	Loss: 0.000113
[INFO][10:32:35]: [Client #41] Epoch: [4/5][0/10]	Loss: 0.000902
[INFO][10:32:35]: [Client #304] Epoch: [4/5][0/10]	Loss: 0.001319
[INFO][10:32:35]: [Client #87] Epoch: [4/5][0/10]	Loss: 0.000074
[INFO][10:32:35]: [Client #41] Epoch: [5/5][0/10]	Loss: 0.017996
[INFO][10:32:35]: [Client #304] Epoch: [5/5][0/10]	Loss: 0.001336
[INFO][10:32:35]: [Client #41] Model saved to /data/ykang/plato/results/test/model/lenet5_41_1127977.pth.
[INFO][10:32:35]: [Client #87] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:32:35]: [Client #304] Model saved to /data/ykang/plato/results/test/model/lenet5_304_1127979.pth.
[INFO][10:32:35]: [Client #87] Model saved to /data/ykang/plato/results/test/model/lenet5_87_1127978.pth.
[INFO][10:32:36]: [Client #41] Loading a model from /data/ykang/plato/results/test/model/lenet5_41_1127977.pth.
[INFO][10:32:36]: [Client #41] Model trained.
[INFO][10:32:36]: [Client #41] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:32:36]: [Server #1127936] Received 0.24 MB of payload data from client #41 (simulated).
[INFO][10:32:36]: [Client #304] Loading a model from /data/ykang/plato/results/test/model/lenet5_304_1127979.pth.
[INFO][10:32:36]: [Client #304] Model trained.
[INFO][10:32:36]: [Client #304] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:32:36]: [Server #1127936] Received 0.24 MB of payload data from client #304 (simulated).
[INFO][10:32:36]: [Client #87] Loading a model from /data/ykang/plato/results/test/model/lenet5_87_1127978.pth.
[INFO][10:32:36]: [Client #87] Model trained.
[INFO][10:32:36]: [Client #87] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:32:36]: [Server #1127936] Received 0.24 MB of payload data from client #87 (simulated).
[INFO][10:32:36]: [Server #1127936] Selecting client #367 for training.
[INFO][10:32:36]: [Server #1127936] Sending the current model to client #367 (simulated).
[INFO][10:32:36]: [Server #1127936] Sending 0.24 MB of payload data to client #367 (simulated).
[INFO][10:32:36]: [Server #1127936] Selecting client #208 for training.
[INFO][10:32:36]: [Server #1127936] Sending the current model to client #208 (simulated).
[INFO][10:32:36]: [Server #1127936] Sending 0.24 MB of payload data to client #208 (simulated).
[INFO][10:32:36]: [Server #1127936] Selecting client #347 for training.
[INFO][10:32:36]: [Server #1127936] Sending the current model to client #347 (simulated).
[INFO][10:32:36]: [Client #367] Selected by the server.
[INFO][10:32:36]: [Client #367] Loading its data source...
[INFO][10:32:36]: [Client #367] Dataset size: 60000
[INFO][10:32:36]: [Client #367] Sampler: noniid
[INFO][10:32:36]: [Server #1127936] Sending 0.24 MB of payload data to client #347 (simulated).
[INFO][10:32:36]: [Client #208] Selected by the server.
[INFO][10:32:36]: [Client #208] Loading its data source...
[INFO][10:32:36]: [Client #208] Dataset size: 60000
[INFO][10:32:36]: [Client #208] Sampler: noniid
[INFO][10:32:36]: [Client #347] Selected by the server.
[INFO][10:32:36]: [Client #347] Loading its data source...
[INFO][10:32:36]: [Client #347] Dataset size: 60000
[INFO][10:32:36]: [Client #347] Sampler: noniid
[INFO][10:32:36]: [Client #367] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:32:36]: [Client #208] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:32:36]: [Client #347] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:32:36]: [93m[1m[Client #367] Started training in communication round #78.[0m
[INFO][10:32:36]: [93m[1m[Client #347] Started training in communication round #78.[0m
[INFO][10:32:36]: [93m[1m[Client #208] Started training in communication round #78.[0m
[INFO][10:32:38]: [Client #208] Loading the dataset.
[INFO][10:32:38]: [Client #367] Loading the dataset.
[INFO][10:32:38]: [Client #347] Loading the dataset.
[INFO][10:32:44]: [Client #208] Epoch: [1/5][0/10]	Loss: 0.006772
[INFO][10:32:44]: [Client #347] Epoch: [1/5][0/10]	Loss: 0.000673
[INFO][10:32:44]: [Client #208] Epoch: [2/5][0/10]	Loss: 0.001937
[INFO][10:32:44]: [Client #367] Epoch: [1/5][0/10]	Loss: 0.005957
[INFO][10:32:44]: [Client #347] Epoch: [2/5][0/10]	Loss: 0.000613
[INFO][10:32:44]: [Client #208] Epoch: [3/5][0/10]	Loss: 0.000006
[INFO][10:32:44]: [Client #367] Epoch: [2/5][0/10]	Loss: 0.001288
[INFO][10:32:44]: [Client #347] Epoch: [3/5][0/10]	Loss: 0.000141
[INFO][10:32:45]: [Client #208] Epoch: [4/5][0/10]	Loss: 0.000055
[INFO][10:32:45]: [Client #367] Epoch: [3/5][0/10]	Loss: 0.000001
[INFO][10:32:45]: [Client #347] Epoch: [4/5][0/10]	Loss: 0.000348
[INFO][10:32:45]: [Client #208] Epoch: [5/5][0/10]	Loss: 0.327475
[INFO][10:32:45]: [Client #208] Model saved to /data/ykang/plato/results/test/model/lenet5_208_1127978.pth.
[INFO][10:32:45]: [Client #347] Epoch: [5/5][0/10]	Loss: 0.004466
[INFO][10:32:45]: [Client #367] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:32:45]: [Client #347] Model saved to /data/ykang/plato/results/test/model/lenet5_347_1127979.pth.
[INFO][10:32:45]: [Client #367] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:32:45]: [Client #367] Model saved to /data/ykang/plato/results/test/model/lenet5_367_1127977.pth.
[INFO][10:32:45]: [Client #208] Loading a model from /data/ykang/plato/results/test/model/lenet5_208_1127978.pth.
[INFO][10:32:45]: [Client #208] Model trained.
[INFO][10:32:45]: [Client #208] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:32:45]: [Server #1127936] Received 0.24 MB of payload data from client #208 (simulated).
[INFO][10:32:46]: [Client #347] Loading a model from /data/ykang/plato/results/test/model/lenet5_347_1127979.pth.
[INFO][10:32:46]: [Client #347] Model trained.
[INFO][10:32:46]: [Client #347] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:32:46]: [Server #1127936] Received 0.24 MB of payload data from client #347 (simulated).
[INFO][10:32:46]: [Client #367] Loading a model from /data/ykang/plato/results/test/model/lenet5_367_1127977.pth.
[INFO][10:32:46]: [Client #367] Model trained.
[INFO][10:32:46]: [Client #367] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:32:46]: [Server #1127936] Received 0.24 MB of payload data from client #367 (simulated).
[INFO][10:32:46]: [Server #1127936] Selecting client #474 for training.
[INFO][10:32:46]: [Server #1127936] Sending the current model to client #474 (simulated).
[INFO][10:32:46]: [Server #1127936] Sending 0.24 MB of payload data to client #474 (simulated).
[INFO][10:32:46]: [Server #1127936] Selecting client #101 for training.
[INFO][10:32:46]: [Server #1127936] Sending the current model to client #101 (simulated).
[INFO][10:32:46]: [Server #1127936] Sending 0.24 MB of payload data to client #101 (simulated).
[INFO][10:32:46]: [Server #1127936] Selecting client #407 for training.
[INFO][10:32:46]: [Server #1127936] Sending the current model to client #407 (simulated).
[INFO][10:32:46]: [Client #474] Selected by the server.
[INFO][10:32:46]: [Client #474] Loading its data source...
[INFO][10:32:46]: [Client #474] Dataset size: 60000
[INFO][10:32:46]: [Client #474] Sampler: noniid
[INFO][10:32:46]: [Server #1127936] Sending 0.24 MB of payload data to client #407 (simulated).
[INFO][10:32:46]: [Client #101] Selected by the server.
[INFO][10:32:46]: [Client #407] Selected by the server.
[INFO][10:32:46]: [Client #101] Loading its data source...
[INFO][10:32:46]: [Client #101] Dataset size: 60000
[INFO][10:32:46]: [Client #407] Loading its data source...
[INFO][10:32:46]: [Client #101] Sampler: noniid
[INFO][10:32:46]: [Client #407] Dataset size: 60000
[INFO][10:32:46]: [Client #407] Sampler: noniid
[INFO][10:32:46]: [Client #474] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:32:46]: [Client #101] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:32:46]: [Client #407] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:32:46]: [93m[1m[Client #474] Started training in communication round #78.[0m
[INFO][10:32:46]: [93m[1m[Client #407] Started training in communication round #78.[0m
[INFO][10:32:46]: [93m[1m[Client #101] Started training in communication round #78.[0m
[INFO][10:32:48]: [Client #474] Loading the dataset.
[INFO][10:32:48]: [Client #407] Loading the dataset.
[INFO][10:32:48]: [Client #101] Loading the dataset.
[INFO][10:32:54]: [Client #474] Epoch: [1/5][0/10]	Loss: 0.008265
[INFO][10:32:54]: [Client #407] Epoch: [1/5][0/10]	Loss: 0.000604
[INFO][10:32:54]: [Client #101] Epoch: [1/5][0/10]	Loss: 0.004512
[INFO][10:32:54]: [Client #474] Epoch: [2/5][0/10]	Loss: 0.000521
[INFO][10:32:54]: [Client #407] Epoch: [2/5][0/10]	Loss: 0.013405
[INFO][10:32:54]: [Client #101] Epoch: [2/5][0/10]	Loss: 0.000002
[INFO][10:32:54]: [Client #474] Epoch: [3/5][0/10]	Loss: 0.000000
[INFO][10:32:54]: [Client #407] Epoch: [3/5][0/10]	Loss: 0.000380
[INFO][10:32:54]: [Client #101] Epoch: [3/5][0/10]	Loss: 0.000204
[INFO][10:32:54]: [Client #474] Epoch: [4/5][0/10]	Loss: 0.000095
[INFO][10:32:54]: [Client #407] Epoch: [4/5][0/10]	Loss: 0.000008
[INFO][10:32:54]: [Client #101] Epoch: [4/5][0/10]	Loss: 0.137974
[INFO][10:32:54]: [Client #407] Epoch: [5/5][0/10]	Loss: 0.001005
[INFO][10:32:54]: [Client #474] Epoch: [5/5][0/10]	Loss: 0.000250
[INFO][10:32:54]: [Client #407] Model saved to /data/ykang/plato/results/test/model/lenet5_407_1127979.pth.
[INFO][10:32:54]: [Client #101] Epoch: [5/5][0/10]	Loss: 0.000006
[INFO][10:32:54]: [Client #474] Model saved to /data/ykang/plato/results/test/model/lenet5_474_1127977.pth.
[INFO][10:32:54]: [Client #101] Model saved to /data/ykang/plato/results/test/model/lenet5_101_1127978.pth.
[INFO][10:32:55]: [Client #407] Loading a model from /data/ykang/plato/results/test/model/lenet5_407_1127979.pth.
[INFO][10:32:55]: [Client #407] Model trained.
[INFO][10:32:55]: [Client #407] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:32:55]: [Server #1127936] Received 0.24 MB of payload data from client #407 (simulated).
[INFO][10:32:55]: [Client #474] Loading a model from /data/ykang/plato/results/test/model/lenet5_474_1127977.pth.
[INFO][10:32:55]: [Client #474] Model trained.
[INFO][10:32:55]: [Client #474] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:32:55]: [Server #1127936] Received 0.24 MB of payload data from client #474 (simulated).
[INFO][10:32:55]: [Client #101] Loading a model from /data/ykang/plato/results/test/model/lenet5_101_1127978.pth.
[INFO][10:32:55]: [Client #101] Model trained.
[INFO][10:32:55]: [Client #101] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:32:55]: [Server #1127936] Received 0.24 MB of payload data from client #101 (simulated).
[INFO][10:32:55]: [Server #1127936] Selecting client #486 for training.
[INFO][10:32:55]: [Server #1127936] Sending the current model to client #486 (simulated).
[INFO][10:32:55]: [Server #1127936] Sending 0.24 MB of payload data to client #486 (simulated).
[INFO][10:32:55]: [Client #486] Selected by the server.
[INFO][10:32:55]: [Client #486] Loading its data source...
[INFO][10:32:55]: [Client #486] Dataset size: 60000
[INFO][10:32:55]: [Client #486] Sampler: noniid
[INFO][10:32:55]: [Client #486] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:32:55]: [93m[1m[Client #486] Started training in communication round #78.[0m
[INFO][10:32:57]: [Client #486] Loading the dataset.
[INFO][10:33:02]: [Client #486] Epoch: [1/5][0/10]	Loss: 0.009477
[INFO][10:33:03]: [Client #486] Epoch: [2/5][0/10]	Loss: 0.000024
[INFO][10:33:03]: [Client #486] Epoch: [3/5][0/10]	Loss: 0.019102
[INFO][10:33:03]: [Client #486] Epoch: [4/5][0/10]	Loss: 0.000006
[INFO][10:33:03]: [Client #486] Epoch: [5/5][0/10]	Loss: 0.001502
[INFO][10:33:03]: [Client #486] Model saved to /data/ykang/plato/results/test/model/lenet5_486_1127977.pth.
[INFO][10:33:03]: [Client #486] Loading a model from /data/ykang/plato/results/test/model/lenet5_486_1127977.pth.
[INFO][10:33:03]: [Client #486] Model trained.
[INFO][10:33:03]: [Client #486] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:33:03]: [Server #1127936] Received 0.24 MB of payload data from client #486 (simulated).
[INFO][10:33:03]: [Server #1127936] Adding client #235 to the list of clients for aggregation.
[INFO][10:33:03]: [Server #1127936] Adding client #250 to the list of clients for aggregation.
[INFO][10:33:03]: [Server #1127936] Adding client #202 to the list of clients for aggregation.
[INFO][10:33:03]: [Server #1127936] Adding client #246 to the list of clients for aggregation.
[INFO][10:33:03]: [Server #1127936] Adding client #77 to the list of clients for aggregation.
[INFO][10:33:03]: [Server #1127936] Adding client #335 to the list of clients for aggregation.
[INFO][10:33:03]: [Server #1127936] Adding client #407 to the list of clients for aggregation.
[INFO][10:33:03]: [Server #1127936] Adding client #101 to the list of clients for aggregation.
[INFO][10:33:03]: [Server #1127936] Adding client #311 to the list of clients for aggregation.
[INFO][10:33:03]: [Server #1127936] Adding client #87 to the list of clients for aggregation.
[INFO][10:33:03]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00137884 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00550907 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00761947 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00443902 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00237243 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00387817
 0.         0.         0.         0.00312734 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00772589 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00327093 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00142998 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 0. 1. 0. 0. 0. 1. 0. 0. 3. 1. 0. 0. 0. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 2. 0. 1. 0.
 0. 0. 1. 0. 2. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 2. 1. 1. 1. 0.
 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 2. 1. 6. 0. 0. 2. 1. 0. 1. 1. 2. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 3. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 1. 6. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 2. 0. 0.
 1. 1. 1. 2. 0. 0. 0. 2. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 1. 2. 0. 4. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1.
 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 4. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0.
 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0.
 1. 0. 1. 0. 4. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 3. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 0. 1. 0. 6. 1. 1.
 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 6. 0. 0. 0. 1. 0. 0. 0. 2. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 2. 2. 1. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00137884 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00550907 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00761947 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00443902 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00237243 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00387817
 0.         0.         0.         0.00312734 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00772589 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00327093 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00142998 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:33:05]: [Server #1127936] Global model accuracy: 96.22%

[INFO][10:33:05]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_78.pth.
[INFO][10:33:05]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_78.pth.
[INFO][10:33:05]: [93m[1m
[Server #1127936] Starting round 79/100.[0m
[INFO][10:33:06]: [Server #1127936] Selected clients: [ 12 175  95 311  66 499 310 423 153 420]
[INFO][10:33:06]: [Server #1127936] Selecting client #12 for training.
[INFO][10:33:06]: [Server #1127936] Sending the current model to client #12 (simulated).
[INFO][10:33:06]: [Server #1127936] Sending 0.24 MB of payload data to client #12 (simulated).
[INFO][10:33:06]: [Server #1127936] Selecting client #175 for training.
[INFO][10:33:06]: [Server #1127936] Sending the current model to client #175 (simulated).
[INFO][10:33:06]: [Server #1127936] Sending 0.24 MB of payload data to client #175 (simulated).
[INFO][10:33:06]: [Server #1127936] Selecting client #95 for training.
[INFO][10:33:06]: [Server #1127936] Sending the current model to client #95 (simulated).
[INFO][10:33:06]: [Client #12] Selected by the server.
[INFO][10:33:06]: [Client #12] Loading its data source...
[INFO][10:33:06]: [Client #12] Dataset size: 60000
[INFO][10:33:06]: [Client #12] Sampler: noniid
[INFO][10:33:06]: [Server #1127936] Sending 0.24 MB of payload data to client #95 (simulated).
[INFO][10:33:06]: [Client #175] Selected by the server.
[INFO][10:33:06]: [Client #175] Loading its data source...
[INFO][10:33:06]: [Client #175] Dataset size: 60000
[INFO][10:33:06]: [Client #175] Sampler: noniid
[INFO][10:33:06]: [Client #12] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:33:06]: [Client #95] Selected by the server.
[INFO][10:33:06]: [Client #95] Loading its data source...
[INFO][10:33:06]: [Client #95] Dataset size: 60000
[INFO][10:33:06]: [Client #95] Sampler: noniid
[INFO][10:33:06]: [Client #175] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:33:06]: [Client #95] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:33:06]: [93m[1m[Client #95] Started training in communication round #79.[0m
[INFO][10:33:06]: [93m[1m[Client #175] Started training in communication round #79.[0m
[INFO][10:33:06]: [93m[1m[Client #12] Started training in communication round #79.[0m
[INFO][10:33:08]: [Client #12] Loading the dataset.
[INFO][10:33:08]: [Client #175] Loading the dataset.
[INFO][10:33:08]: [Client #95] Loading the dataset.
[INFO][10:33:14]: [Client #175] Epoch: [1/5][0/10]	Loss: 0.000334
[INFO][10:33:14]: [Client #12] Epoch: [1/5][0/10]	Loss: 0.005505
[INFO][10:33:14]: [Client #95] Epoch: [1/5][0/10]	Loss: 0.000364
[INFO][10:33:14]: [Client #175] Epoch: [2/5][0/10]	Loss: 0.001610
[INFO][10:33:14]: [Client #95] Epoch: [2/5][0/10]	Loss: 0.000109
[INFO][10:33:14]: [Client #12] Epoch: [2/5][0/10]	Loss: 0.000646
[INFO][10:33:14]: [Client #175] Epoch: [3/5][0/10]	Loss: 0.000009
[INFO][10:33:14]: [Client #175] Epoch: [4/5][0/10]	Loss: 0.000031
[INFO][10:33:14]: [Client #95] Epoch: [3/5][0/10]	Loss: 0.000177
[INFO][10:33:14]: [Client #12] Epoch: [3/5][0/10]	Loss: 0.003744
[INFO][10:33:14]: [Client #175] Epoch: [5/5][0/10]	Loss: 0.000143
[INFO][10:33:14]: [Client #12] Epoch: [4/5][0/10]	Loss: 0.000169
[INFO][10:33:14]: [Client #95] Epoch: [4/5][0/10]	Loss: 0.000408
[INFO][10:33:14]: [Client #175] Model saved to /data/ykang/plato/results/test/model/lenet5_175_1127978.pth.
[INFO][10:33:14]: [Client #12] Epoch: [5/5][0/10]	Loss: 0.044395
[INFO][10:33:14]: [Client #95] Epoch: [5/5][0/10]	Loss: 0.000119
[INFO][10:33:14]: [Client #12] Model saved to /data/ykang/plato/results/test/model/lenet5_12_1127977.pth.
[INFO][10:33:14]: [Client #95] Model saved to /data/ykang/plato/results/test/model/lenet5_95_1127979.pth.
[INFO][10:33:15]: [Client #175] Loading a model from /data/ykang/plato/results/test/model/lenet5_175_1127978.pth.
[INFO][10:33:15]: [Client #175] Model trained.
[INFO][10:33:15]: [Client #175] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:33:15]: [Server #1127936] Received 0.24 MB of payload data from client #175 (simulated).
[INFO][10:33:15]: [Client #12] Loading a model from /data/ykang/plato/results/test/model/lenet5_12_1127977.pth.
[INFO][10:33:15]: [Client #12] Model trained.
[INFO][10:33:15]: [Client #12] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:33:15]: [Server #1127936] Received 0.24 MB of payload data from client #12 (simulated).
[INFO][10:33:15]: [Client #95] Loading a model from /data/ykang/plato/results/test/model/lenet5_95_1127979.pth.
[INFO][10:33:15]: [Client #95] Model trained.
[INFO][10:33:15]: [Client #95] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:33:15]: [Server #1127936] Received 0.24 MB of payload data from client #95 (simulated).
[INFO][10:33:15]: [Server #1127936] Selecting client #311 for training.
[INFO][10:33:15]: [Server #1127936] Sending the current model to client #311 (simulated).
[INFO][10:33:15]: [Server #1127936] Sending 0.24 MB of payload data to client #311 (simulated).
[INFO][10:33:15]: [Server #1127936] Selecting client #66 for training.
[INFO][10:33:15]: [Server #1127936] Sending the current model to client #66 (simulated).
[INFO][10:33:15]: [Server #1127936] Sending 0.24 MB of payload data to client #66 (simulated).
[INFO][10:33:15]: [Server #1127936] Selecting client #499 for training.
[INFO][10:33:15]: [Server #1127936] Sending the current model to client #499 (simulated).
[INFO][10:33:15]: [Client #311] Selected by the server.
[INFO][10:33:15]: [Client #311] Loading its data source...
[INFO][10:33:15]: [Client #311] Dataset size: 60000
[INFO][10:33:15]: [Client #311] Sampler: noniid
[INFO][10:33:15]: [Server #1127936] Sending 0.24 MB of payload data to client #499 (simulated).
[INFO][10:33:15]: [Client #311] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:33:15]: [Client #66] Selected by the server.
[INFO][10:33:15]: [Client #66] Loading its data source...
[INFO][10:33:15]: [Client #66] Dataset size: 60000
[INFO][10:33:15]: [Client #66] Sampler: noniid
[INFO][10:33:15]: [Client #499] Selected by the server.
[INFO][10:33:15]: [Client #499] Loading its data source...
[INFO][10:33:15]: [Client #499] Dataset size: 60000
[INFO][10:33:15]: [Client #499] Sampler: noniid
[INFO][10:33:15]: [Client #66] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:33:15]: [Client #499] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:33:15]: [93m[1m[Client #311] Started training in communication round #79.[0m
[INFO][10:33:15]: [93m[1m[Client #66] Started training in communication round #79.[0m
[INFO][10:33:15]: [93m[1m[Client #499] Started training in communication round #79.[0m
[INFO][10:33:17]: [Client #499] Loading the dataset.
[INFO][10:33:17]: [Client #311] Loading the dataset.
[INFO][10:33:17]: [Client #66] Loading the dataset.
[INFO][10:33:23]: [Client #499] Epoch: [1/5][0/10]	Loss: 0.005918
[INFO][10:33:23]: [Client #499] Epoch: [2/5][0/10]	Loss: 0.000004
[INFO][10:33:23]: [Client #311] Epoch: [1/5][0/10]	Loss: 0.000123
[INFO][10:33:23]: [Client #66] Epoch: [1/5][0/10]	Loss: 0.005634
[INFO][10:33:23]: [Client #499] Epoch: [3/5][0/10]	Loss: 0.000005
[INFO][10:33:23]: [Client #66] Epoch: [2/5][0/10]	Loss: 0.000712
[INFO][10:33:23]: [Client #311] Epoch: [2/5][0/10]	Loss: 0.000650
[INFO][10:33:23]: [Client #499] Epoch: [4/5][0/10]	Loss: 0.000012
[INFO][10:33:23]: [Client #311] Epoch: [3/5][0/10]	Loss: 0.000450
[INFO][10:33:23]: [Client #66] Epoch: [3/5][0/10]	Loss: 0.000042
[INFO][10:33:24]: [Client #499] Epoch: [5/5][0/10]	Loss: 0.000019
[INFO][10:33:24]: [Client #311] Epoch: [4/5][0/10]	Loss: 0.000026
[INFO][10:33:24]: [Client #66] Epoch: [4/5][0/10]	Loss: 0.268999
[INFO][10:33:24]: [Client #499] Model saved to /data/ykang/plato/results/test/model/lenet5_499_1127979.pth.
[INFO][10:33:24]: [Client #311] Epoch: [5/5][0/10]	Loss: 0.000038
[INFO][10:33:24]: [Client #66] Epoch: [5/5][0/10]	Loss: 0.061916
[INFO][10:33:24]: [Client #66] Model saved to /data/ykang/plato/results/test/model/lenet5_66_1127978.pth.
[INFO][10:33:24]: [Client #311] Model saved to /data/ykang/plato/results/test/model/lenet5_311_1127977.pth.
[INFO][10:33:24]: [Client #499] Loading a model from /data/ykang/plato/results/test/model/lenet5_499_1127979.pth.
[INFO][10:33:24]: [Client #499] Model trained.
[INFO][10:33:24]: [Client #499] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:33:24]: [Server #1127936] Received 0.24 MB of payload data from client #499 (simulated).
[INFO][10:33:25]: [Client #311] Loading a model from /data/ykang/plato/results/test/model/lenet5_311_1127977.pth.
[INFO][10:33:25]: [Client #311] Model trained.
[INFO][10:33:25]: [Client #311] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:33:25]: [Server #1127936] Received 0.24 MB of payload data from client #311 (simulated).
[INFO][10:33:25]: [Client #66] Loading a model from /data/ykang/plato/results/test/model/lenet5_66_1127978.pth.
[INFO][10:33:25]: [Client #66] Model trained.
[INFO][10:33:25]: [Client #66] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:33:25]: [Server #1127936] Received 0.24 MB of payload data from client #66 (simulated).
[INFO][10:33:25]: [Server #1127936] Selecting client #310 for training.
[INFO][10:33:25]: [Server #1127936] Sending the current model to client #310 (simulated).
[INFO][10:33:25]: [Server #1127936] Sending 0.24 MB of payload data to client #310 (simulated).
[INFO][10:33:25]: [Server #1127936] Selecting client #423 for training.
[INFO][10:33:25]: [Server #1127936] Sending the current model to client #423 (simulated).
[INFO][10:33:25]: [Server #1127936] Sending 0.24 MB of payload data to client #423 (simulated).
[INFO][10:33:25]: [Server #1127936] Selecting client #153 for training.
[INFO][10:33:25]: [Server #1127936] Sending the current model to client #153 (simulated).
[INFO][10:33:25]: [Client #310] Selected by the server.
[INFO][10:33:25]: [Client #310] Loading its data source...
[INFO][10:33:25]: [Client #310] Dataset size: 60000
[INFO][10:33:25]: [Client #310] Sampler: noniid
[INFO][10:33:25]: [Server #1127936] Sending 0.24 MB of payload data to client #153 (simulated).
[INFO][10:33:25]: [Client #423] Selected by the server.
[INFO][10:33:25]: [Client #423] Loading its data source...
[INFO][10:33:25]: [Client #423] Dataset size: 60000
[INFO][10:33:25]: [Client #423] Sampler: noniid
[INFO][10:33:25]: [Client #153] Selected by the server.
[INFO][10:33:25]: [Client #153] Loading its data source...
[INFO][10:33:25]: [Client #153] Dataset size: 60000
[INFO][10:33:25]: [Client #153] Sampler: noniid
[INFO][10:33:25]: [Client #310] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:33:25]: [Client #423] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:33:25]: [Client #153] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:33:25]: [93m[1m[Client #153] Started training in communication round #79.[0m
[INFO][10:33:25]: [93m[1m[Client #310] Started training in communication round #79.[0m
[INFO][10:33:25]: [93m[1m[Client #423] Started training in communication round #79.[0m
[INFO][10:33:27]: [Client #153] Loading the dataset.
[INFO][10:33:27]: [Client #423] Loading the dataset.
[INFO][10:33:27]: [Client #310] Loading the dataset.
[INFO][10:33:33]: [Client #310] Epoch: [1/5][0/10]	Loss: 0.000144
[INFO][10:33:33]: [Client #423] Epoch: [1/5][0/10]	Loss: 0.005854
[INFO][10:33:33]: [Client #153] Epoch: [1/5][0/10]	Loss: 0.000385
[INFO][10:33:33]: [Client #310] Epoch: [2/5][0/10]	Loss: 0.000088
[INFO][10:33:33]: [Client #423] Epoch: [2/5][0/10]	Loss: 0.000020
[INFO][10:33:33]: [Client #153] Epoch: [2/5][0/10]	Loss: 0.013979
[INFO][10:33:33]: [Client #310] Epoch: [3/5][0/10]	Loss: 0.000077
[INFO][10:33:33]: [Client #423] Epoch: [3/5][0/10]	Loss: 0.000003
[INFO][10:33:33]: [Client #153] Epoch: [3/5][0/10]	Loss: 0.000025
[INFO][10:33:33]: [Client #310] Epoch: [4/5][0/10]	Loss: 0.001895
[INFO][10:33:33]: [Client #153] Epoch: [4/5][0/10]	Loss: 0.000147
[INFO][10:33:33]: [Client #423] Epoch: [4/5][0/10]	Loss: 0.000023
[INFO][10:33:33]: [Client #310] Epoch: [5/5][0/10]	Loss: 0.000013
[INFO][10:33:33]: [Client #153] Epoch: [5/5][0/10]	Loss: 0.000010
[INFO][10:33:33]: [Client #423] Epoch: [5/5][0/10]	Loss: 0.000045
[INFO][10:33:33]: [Client #310] Model saved to /data/ykang/plato/results/test/model/lenet5_310_1127977.pth.
[INFO][10:33:33]: [Client #153] Model saved to /data/ykang/plato/results/test/model/lenet5_153_1127979.pth.
[INFO][10:33:33]: [Client #423] Model saved to /data/ykang/plato/results/test/model/lenet5_423_1127978.pth.
[INFO][10:33:34]: [Client #423] Loading a model from /data/ykang/plato/results/test/model/lenet5_423_1127978.pth.
[INFO][10:33:34]: [Client #423] Model trained.
[INFO][10:33:34]: [Client #423] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:33:34]: [Server #1127936] Received 0.24 MB of payload data from client #423 (simulated).
[INFO][10:33:34]: [Client #153] Loading a model from /data/ykang/plato/results/test/model/lenet5_153_1127979.pth.
[INFO][10:33:34]: [Client #153] Model trained.
[INFO][10:33:34]: [Client #153] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:33:34]: [Server #1127936] Received 0.24 MB of payload data from client #153 (simulated).
[INFO][10:33:34]: [Client #310] Loading a model from /data/ykang/plato/results/test/model/lenet5_310_1127977.pth.
[INFO][10:33:34]: [Client #310] Model trained.
[INFO][10:33:34]: [Client #310] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:33:34]: [Server #1127936] Received 0.24 MB of payload data from client #310 (simulated).
[INFO][10:33:34]: [Server #1127936] Selecting client #420 for training.
[INFO][10:33:34]: [Server #1127936] Sending the current model to client #420 (simulated).
[INFO][10:33:34]: [Server #1127936] Sending 0.24 MB of payload data to client #420 (simulated).
[INFO][10:33:34]: [Client #420] Selected by the server.
[INFO][10:33:34]: [Client #420] Loading its data source...
[INFO][10:33:34]: [Client #420] Dataset size: 60000
[INFO][10:33:34]: [Client #420] Sampler: noniid
[INFO][10:33:34]: [Client #420] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:33:34]: [93m[1m[Client #420] Started training in communication round #79.[0m
[INFO][10:33:36]: [Client #420] Loading the dataset.
[INFO][10:33:41]: [Client #420] Epoch: [1/5][0/10]	Loss: 0.006505
[INFO][10:33:41]: [Client #420] Epoch: [2/5][0/10]	Loss: 0.002290
[INFO][10:33:42]: [Client #420] Epoch: [3/5][0/10]	Loss: 0.000058
[INFO][10:33:42]: [Client #420] Epoch: [4/5][0/10]	Loss: 0.000343
[INFO][10:33:42]: [Client #420] Epoch: [5/5][0/10]	Loss: 0.000112
[INFO][10:33:42]: [Client #420] Model saved to /data/ykang/plato/results/test/model/lenet5_420_1127977.pth.
[INFO][10:33:42]: [Client #420] Loading a model from /data/ykang/plato/results/test/model/lenet5_420_1127977.pth.
[INFO][10:33:42]: [Client #420] Model trained.
[INFO][10:33:42]: [Client #420] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:33:42]: [Server #1127936] Received 0.24 MB of payload data from client #420 (simulated).
[INFO][10:33:42]: [Server #1127936] Adding client #474 to the list of clients for aggregation.
[INFO][10:33:42]: [Server #1127936] Adding client #208 to the list of clients for aggregation.
[INFO][10:33:42]: [Server #1127936] Adding client #304 to the list of clients for aggregation.
[INFO][10:33:42]: [Server #1127936] Adding client #118 to the list of clients for aggregation.
[INFO][10:33:42]: [Server #1127936] Adding client #486 to the list of clients for aggregation.
[INFO][10:33:42]: [Server #1127936] Adding client #347 to the list of clients for aggregation.
[INFO][10:33:42]: [Server #1127936] Adding client #275 to the list of clients for aggregation.
[INFO][10:33:42]: [Server #1127936] Adding client #423 to the list of clients for aggregation.
[INFO][10:33:42]: [Server #1127936] Adding client #310 to the list of clients for aggregation.
[INFO][10:33:42]: [Server #1127936] Adding client #95 to the list of clients for aggregation.
[INFO][10:33:42]: [Server #1127936] Aggregating 10 clients in total.
[0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002
 0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  1e-05  2e-10  2e-10
 6:  6.8876e+00  6.8875e+00  8e-06  1e-10  1e-10
 7:  6.8876e+00  6.8875e+00  8e-06  8e-11  6e-13
 8:  6.8875e+00  6.8875e+00  5e-06  6e-11  5e-13
Optimal solution found.
The calculated probability is:  [0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00200524 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124978 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124974 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00312272 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00185083 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.0026378  0.00124984 0.00124984 0.00124984 0.00730928
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.37714913 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00225412 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124983 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984 0.00124984
 0.00124984 0.00124984 0.00124984 0.00124984]
current clients pool:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00149293 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00140204 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01107713 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00720147 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00229615 0.         0.
 0.         0.         0.         0.00129966 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00522158 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00260596 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00721414
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01538757
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 0. 1. 0. 0. 0. 1. 0. 0. 3. 1. 0. 0. 0. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 2. 0. 1. 0.
 0. 0. 1. 0. 2. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 2. 1. 1. 0. 0.
 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 2. 1. 6. 0. 0. 2. 1. 0. 1. 1. 2. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 3. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 1. 6. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 2. 0. 0.
 1. 1. 1. 2. 0. 0. 0. 2. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 1. 2. 0. 4. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1.
 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 4. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0.
 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0.
 1. 0. 1. 0. 4. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 3. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 0. 1. 0. 6. 1. 1.
 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 6. 0. 0. 0. 1. 0. 0. 1. 2. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 2. 2. 1. 1. 0.]
local_gradient_bounds:  [INFO][10:33:44]: [Server #1127936] Global model accuracy: 96.49%

[INFO][10:33:44]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_79.pth.
[INFO][10:33:44]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_79.pth.
[INFO][10:33:44]: [93m[1m
[Server #1127936] Starting round 80/100.[0m
[0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00149293 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00140204 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01107713 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00720147 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00229615 0.         0.
 0.         0.         0.         0.00129966 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00522158 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00260596 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00721414
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01538757
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  3e-05  6e-10  6e-10
 6:  6.8876e+00  6.8875e+00  3e-05  3e-10  3e-10
 7:  6.8875e+00  6.8875e+00  3e-05  8e-10  2e-11
 8:  6.8875e+00  6.8875e+00  2e-05  7e-10  2e-11
 9:  6.8875e+00  6.8875e+00  1e-05  2e-09  5e-11
10:  6.8875e+00  6.8875e+00  9e-07  2e-09  7e-11
Optimal solution found.
The calculated probability is:  [6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159330e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 7.12552639e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 1.13364402e-04 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 9.68881359e-01 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.96528168e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159468e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 7.99941274e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32158168e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 8.89496988e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 1.62101627e-04 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05 6.32159898e-05 6.32159898e-05
 6.32159898e-05 6.32159898e-05]
current clients pool:  [INFO][10:33:45]: [Server #1127936] Selected clients: [275 427 183  39 215 303 407 473 437 352]
[INFO][10:33:45]: [Server #1127936] Selecting client #275 for training.
[INFO][10:33:45]: [Server #1127936] Sending the current model to client #275 (simulated).
[INFO][10:33:45]: [Server #1127936] Sending 0.24 MB of payload data to client #275 (simulated).
[INFO][10:33:45]: [Server #1127936] Selecting client #427 for training.
[INFO][10:33:45]: [Server #1127936] Sending the current model to client #427 (simulated).
[INFO][10:33:45]: [Server #1127936] Sending 0.24 MB of payload data to client #427 (simulated).
[INFO][10:33:45]: [Server #1127936] Selecting client #183 for training.
[INFO][10:33:45]: [Server #1127936] Sending the current model to client #183 (simulated).
[INFO][10:33:45]: [Client #275] Selected by the server.
[INFO][10:33:45]: [Client #275] Loading its data source...
[INFO][10:33:45]: [Client #275] Dataset size: 60000
[INFO][10:33:45]: [Client #275] Sampler: noniid
[INFO][10:33:45]: [Server #1127936] Sending 0.24 MB of payload data to client #183 (simulated).
[INFO][10:33:45]: [Client #427] Selected by the server.
[INFO][10:33:45]: [Client #427] Loading its data source...
[INFO][10:33:45]: [Client #427] Dataset size: 60000
[INFO][10:33:45]: [Client #427] Sampler: noniid
[INFO][10:33:45]: [Client #275] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:33:45]: [Client #183] Selected by the server.
[INFO][10:33:45]: [Client #183] Loading its data source...
[INFO][10:33:45]: [Client #183] Dataset size: 60000
[INFO][10:33:45]: [Client #183] Sampler: noniid
[INFO][10:33:45]: [Client #427] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:33:45]: [Client #183] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:33:45]: [93m[1m[Client #427] Started training in communication round #80.[0m
[INFO][10:33:45]: [93m[1m[Client #275] Started training in communication round #80.[0m
[INFO][10:33:45]: [93m[1m[Client #183] Started training in communication round #80.[0m
[INFO][10:33:47]: [Client #275] Loading the dataset.
[INFO][10:33:47]: [Client #183] Loading the dataset.
[INFO][10:33:47]: [Client #427] Loading the dataset.
[INFO][10:33:53]: [Client #275] Epoch: [1/5][0/10]	Loss: 0.000134
[INFO][10:33:53]: [Client #183] Epoch: [1/5][0/10]	Loss: 0.002379
[INFO][10:33:53]: [Client #427] Epoch: [1/5][0/10]	Loss: 0.002881
[INFO][10:33:53]: [Client #275] Epoch: [2/5][0/10]	Loss: 0.000116
[INFO][10:33:53]: [Client #183] Epoch: [2/5][0/10]	Loss: 0.000036
[INFO][10:33:53]: [Client #183] Epoch: [3/5][0/10]	Loss: 0.000139
[INFO][10:33:53]: [Client #275] Epoch: [3/5][0/10]	Loss: 0.000031
[INFO][10:33:53]: [Client #427] Epoch: [2/5][0/10]	Loss: 0.001422
[INFO][10:33:53]: [Client #183] Epoch: [4/5][0/10]	Loss: 0.017233
[INFO][10:33:53]: [Client #275] Epoch: [4/5][0/10]	Loss: 0.001527
[INFO][10:33:53]: [Client #427] Epoch: [3/5][0/10]	Loss: 0.000004
[INFO][10:33:53]: [Client #275] Epoch: [5/5][0/10]	Loss: 0.000010
[INFO][10:33:53]: [Client #183] Epoch: [5/5][0/10]	Loss: 0.032220
[INFO][10:33:53]: [Client #427] Epoch: [4/5][0/10]	Loss: 0.000026
[INFO][10:33:53]: [Client #275] Model saved to /data/ykang/plato/results/test/model/lenet5_275_1127977.pth.
[INFO][10:33:53]: [Client #183] Model saved to /data/ykang/plato/results/test/model/lenet5_183_1127979.pth.
[INFO][10:33:53]: [Client #427] Epoch: [5/5][0/10]	Loss: 0.003591
[INFO][10:33:53]: [Client #427] Model saved to /data/ykang/plato/results/test/model/lenet5_427_1127978.pth.
[INFO][10:33:54]: [Client #275] Loading a model from /data/ykang/plato/results/test/model/lenet5_275_1127977.pth.
[INFO][10:33:54]: [Client #275] Model trained.
[INFO][10:33:54]: [Client #275] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:33:54]: [Server #1127936] Received 0.24 MB of payload data from client #275 (simulated).
[INFO][10:33:54]: [Client #183] Loading a model from /data/ykang/plato/results/test/model/lenet5_183_1127979.pth.
[INFO][10:33:54]: [Client #183] Model trained.
[INFO][10:33:54]: [Client #183] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:33:54]: [Server #1127936] Received 0.24 MB of payload data from client #183 (simulated).
[INFO][10:33:54]: [Client #427] Loading a model from /data/ykang/plato/results/test/model/lenet5_427_1127978.pth.
[INFO][10:33:54]: [Client #427] Model trained.
[INFO][10:33:54]: [Client #427] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:33:54]: [Server #1127936] Received 0.24 MB of payload data from client #427 (simulated).
[INFO][10:33:54]: [Server #1127936] Selecting client #39 for training.
[INFO][10:33:54]: [Server #1127936] Sending the current model to client #39 (simulated).
[INFO][10:33:54]: [Server #1127936] Sending 0.24 MB of payload data to client #39 (simulated).
[INFO][10:33:54]: [Server #1127936] Selecting client #215 for training.
[INFO][10:33:54]: [Server #1127936] Sending the current model to client #215 (simulated).
[INFO][10:33:54]: [Server #1127936] Sending 0.24 MB of payload data to client #215 (simulated).
[INFO][10:33:54]: [Server #1127936] Selecting client #303 for training.
[INFO][10:33:54]: [Server #1127936] Sending the current model to client #303 (simulated).
[INFO][10:33:54]: [Client #39] Selected by the server.
[INFO][10:33:54]: [Client #39] Loading its data source...
[INFO][10:33:54]: [Client #39] Dataset size: 60000
[INFO][10:33:54]: [Client #39] Sampler: noniid
[INFO][10:33:54]: [Server #1127936] Sending 0.24 MB of payload data to client #303 (simulated).
[INFO][10:33:54]: [Client #215] Selected by the server.
[INFO][10:33:54]: [Client #303] Selected by the server.
[INFO][10:33:54]: [Client #215] Loading its data source...
[INFO][10:33:54]: [Client #303] Loading its data source...
[INFO][10:33:54]: [Client #215] Dataset size: 60000
[INFO][10:33:54]: [Client #303] Dataset size: 60000
[INFO][10:33:54]: [Client #215] Sampler: noniid
[INFO][10:33:54]: [Client #303] Sampler: noniid
[INFO][10:33:54]: [Client #39] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:33:54]: [93m[1m[Client #39] Started training in communication round #80.[0m
[INFO][10:33:54]: [Client #215] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:33:54]: [Client #303] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:33:54]: [93m[1m[Client #215] Started training in communication round #80.[0m
[INFO][10:33:54]: [93m[1m[Client #303] Started training in communication round #80.[0m
[INFO][10:33:56]: [Client #215] Loading the dataset.
[INFO][10:33:56]: [Client #303] Loading the dataset.
[INFO][10:33:56]: [Client #39] Loading the dataset.
[INFO][10:34:02]: [Client #215] Epoch: [1/5][0/10]	Loss: 0.012032
[INFO][10:34:02]: [Client #39] Epoch: [1/5][0/10]	Loss: 0.001361
[INFO][10:34:02]: [Client #303] Epoch: [1/5][0/10]	Loss: 0.001177
[INFO][10:34:02]: [Client #215] Epoch: [2/5][0/10]	Loss: 0.002739
[INFO][10:34:02]: [Client #39] Epoch: [2/5][0/10]	Loss: 0.002630
[INFO][10:34:02]: [Client #215] Epoch: [3/5][0/10]	Loss: 0.000652
[INFO][10:34:02]: [Client #303] Epoch: [2/5][0/10]	Loss: 0.005068
[INFO][10:34:02]: [Client #39] Epoch: [3/5][0/10]	Loss: 0.000105
[INFO][10:34:02]: [Client #215] Epoch: [4/5][0/10]	Loss: 0.000010
[INFO][10:34:02]: [Client #303] Epoch: [3/5][0/10]	Loss: 0.000046
[INFO][10:34:02]: [Client #39] Epoch: [4/5][0/10]	Loss: 0.000073
[INFO][10:34:03]: [Client #215] Epoch: [5/5][0/10]	Loss: 0.000012
[INFO][10:34:03]: [Client #303] Epoch: [4/5][0/10]	Loss: 0.000589
[INFO][10:34:03]: [Client #39] Epoch: [5/5][0/10]	Loss: 0.001672
[INFO][10:34:03]: [Client #39] Model saved to /data/ykang/plato/results/test/model/lenet5_39_1127977.pth.
[INFO][10:34:03]: [Client #215] Model saved to /data/ykang/plato/results/test/model/lenet5_215_1127978.pth.
[INFO][10:34:03]: [Client #303] Epoch: [5/5][0/10]	Loss: 0.003561
[INFO][10:34:03]: [Client #303] Model saved to /data/ykang/plato/results/test/model/lenet5_303_1127979.pth.
[INFO][10:34:03]: [Client #39] Loading a model from /data/ykang/plato/results/test/model/lenet5_39_1127977.pth.
[INFO][10:34:03]: [Client #39] Model trained.
[INFO][10:34:03]: [Client #39] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:34:03]: [Server #1127936] Received 0.24 MB of payload data from client #39 (simulated).
[INFO][10:34:03]: [Client #215] Loading a model from /data/ykang/plato/results/test/model/lenet5_215_1127978.pth.
[INFO][10:34:04]: [Client #215] Model trained.
[INFO][10:34:04]: [Client #215] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:34:04]: [Server #1127936] Received 0.24 MB of payload data from client #215 (simulated).
[INFO][10:34:04]: [Client #303] Loading a model from /data/ykang/plato/results/test/model/lenet5_303_1127979.pth.
[INFO][10:34:04]: [Client #303] Model trained.
[INFO][10:34:04]: [Client #303] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:34:04]: [Server #1127936] Received 0.24 MB of payload data from client #303 (simulated).
[INFO][10:34:04]: [Server #1127936] Selecting client #407 for training.
[INFO][10:34:04]: [Server #1127936] Sending the current model to client #407 (simulated).
[INFO][10:34:04]: [Server #1127936] Sending 0.24 MB of payload data to client #407 (simulated).
[INFO][10:34:04]: [Server #1127936] Selecting client #473 for training.
[INFO][10:34:04]: [Server #1127936] Sending the current model to client #473 (simulated).
[INFO][10:34:04]: [Server #1127936] Sending 0.24 MB of payload data to client #473 (simulated).
[INFO][10:34:04]: [Server #1127936] Selecting client #437 for training.
[INFO][10:34:04]: [Server #1127936] Sending the current model to client #437 (simulated).
[INFO][10:34:04]: [Client #407] Selected by the server.
[INFO][10:34:04]: [Client #407] Loading its data source...
[INFO][10:34:04]: [Client #407] Dataset size: 60000
[INFO][10:34:04]: [Client #407] Sampler: noniid
[INFO][10:34:04]: [Server #1127936] Sending 0.24 MB of payload data to client #437 (simulated).
[INFO][10:34:04]: [Client #473] Selected by the server.
[INFO][10:34:04]: [Client #437] Selected by the server.
[INFO][10:34:04]: [Client #473] Loading its data source...
[INFO][10:34:04]: [Client #437] Loading its data source...
[INFO][10:34:04]: [Client #473] Dataset size: 60000
[INFO][10:34:04]: [Client #437] Dataset size: 60000
[INFO][10:34:04]: [Client #473] Sampler: noniid
[INFO][10:34:04]: [Client #437] Sampler: noniid
[INFO][10:34:04]: [Client #407] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:34:04]: [Client #437] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:34:04]: [Client #473] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:34:04]: [93m[1m[Client #473] Started training in communication round #80.[0m
[INFO][10:34:04]: [93m[1m[Client #407] Started training in communication round #80.[0m
[INFO][10:34:04]: [93m[1m[Client #437] Started training in communication round #80.[0m
[INFO][10:34:06]: [Client #437] Loading the dataset.
[INFO][10:34:06]: [Client #473] Loading the dataset.
[INFO][10:34:06]: [Client #407] Loading the dataset.
[INFO][10:34:12]: [Client #407] Epoch: [1/5][0/10]	Loss: 0.001421
[INFO][10:34:12]: [Client #473] Epoch: [1/5][0/10]	Loss: 0.002235
[INFO][10:34:12]: [Client #437] Epoch: [1/5][0/10]	Loss: 0.000118
[INFO][10:34:12]: [Client #407] Epoch: [2/5][0/10]	Loss: 0.002764
[INFO][10:34:12]: [Client #437] Epoch: [2/5][0/10]	Loss: 0.000066
[INFO][10:34:12]: [Client #473] Epoch: [2/5][0/10]	Loss: 0.003058
[INFO][10:34:12]: [Client #437] Epoch: [3/5][0/10]	Loss: 0.000020
[INFO][10:34:12]: [Client #407] Epoch: [3/5][0/10]	Loss: 0.000250
[INFO][10:34:12]: [Client #473] Epoch: [3/5][0/10]	Loss: 0.000282
[INFO][10:34:12]: [Client #437] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:34:12]: [Client #407] Epoch: [4/5][0/10]	Loss: 0.000009
[INFO][10:34:12]: [Client #473] Epoch: [4/5][0/10]	Loss: 0.000294
[INFO][10:34:12]: [Client #437] Epoch: [5/5][0/10]	Loss: 0.000580
[INFO][10:34:12]: [Client #437] Model saved to /data/ykang/plato/results/test/model/lenet5_437_1127979.pth.
[INFO][10:34:12]: [Client #407] Epoch: [5/5][0/10]	Loss: 0.000569
[INFO][10:34:12]: [Client #473] Epoch: [5/5][0/10]	Loss: 0.000005
[INFO][10:34:12]: [Client #407] Model saved to /data/ykang/plato/results/test/model/lenet5_407_1127977.pth.
[INFO][10:34:12]: [Client #473] Model saved to /data/ykang/plato/results/test/model/lenet5_473_1127978.pth.
[INFO][10:34:13]: [Client #437] Loading a model from /data/ykang/plato/results/test/model/lenet5_437_1127979.pth.
[INFO][10:34:13]: [Client #437] Model trained.
[INFO][10:34:13]: [Client #437] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:34:13]: [Server #1127936] Received 0.24 MB of payload data from client #437 (simulated).
[INFO][10:34:13]: [Client #407] Loading a model from /data/ykang/plato/results/test/model/lenet5_407_1127977.pth.
[INFO][10:34:13]: [Client #407] Model trained.
[INFO][10:34:13]: [Client #407] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:34:13]: [Server #1127936] Received 0.24 MB of payload data from client #407 (simulated).
[INFO][10:34:13]: [Client #473] Loading a model from /data/ykang/plato/results/test/model/lenet5_473_1127978.pth.
[INFO][10:34:13]: [Client #473] Model trained.
[INFO][10:34:13]: [Client #473] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:34:13]: [Server #1127936] Received 0.24 MB of payload data from client #473 (simulated).
[INFO][10:34:13]: [Server #1127936] Selecting client #352 for training.
[INFO][10:34:13]: [Server #1127936] Sending the current model to client #352 (simulated).
[INFO][10:34:13]: [Server #1127936] Sending 0.24 MB of payload data to client #352 (simulated).
[INFO][10:34:13]: [Client #352] Selected by the server.
[INFO][10:34:13]: [Client #352] Loading its data source...
[INFO][10:34:13]: [Client #352] Dataset size: 60000
[INFO][10:34:13]: [Client #352] Sampler: noniid
[INFO][10:34:13]: [Client #352] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:34:13]: [93m[1m[Client #352] Started training in communication round #80.[0m
[INFO][10:34:15]: [Client #352] Loading the dataset.
[INFO][10:34:20]: [Client #352] Epoch: [1/5][0/10]	Loss: 0.001004
[INFO][10:34:20]: [Client #352] Epoch: [2/5][0/10]	Loss: 0.003494
[INFO][10:34:20]: [Client #352] Epoch: [3/5][0/10]	Loss: 0.000091
[INFO][10:34:21]: [Client #352] Epoch: [4/5][0/10]	Loss: 0.000055
[INFO][10:34:21]: [Client #352] Epoch: [5/5][0/10]	Loss: 0.000116
[INFO][10:34:21]: [Client #352] Model saved to /data/ykang/plato/results/test/model/lenet5_352_1127977.pth.
[INFO][10:34:21]: [Client #352] Loading a model from /data/ykang/plato/results/test/model/lenet5_352_1127977.pth.
[INFO][10:34:21]: [Client #352] Model trained.
[INFO][10:34:21]: [Client #352] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:34:21]: [Server #1127936] Received 0.24 MB of payload data from client #352 (simulated).
[INFO][10:34:21]: [Server #1127936] Adding client #66 to the list of clients for aggregation.
[INFO][10:34:21]: [Server #1127936] Adding client #175 to the list of clients for aggregation.
[INFO][10:34:21]: [Server #1127936] Adding client #153 to the list of clients for aggregation.
[INFO][10:34:21]: [Server #1127936] Adding client #420 to the list of clients for aggregation.
[INFO][10:34:21]: [Server #1127936] Adding client #12 to the list of clients for aggregation.
[INFO][10:34:21]: [Server #1127936] Adding client #39 to the list of clients for aggregation.
[INFO][10:34:21]: [Server #1127936] Adding client #427 to the list of clients for aggregation.
[INFO][10:34:21]: [Server #1127936] Adding client #215 to the list of clients for aggregation.
[INFO][10:34:21]: [Server #1127936] Adding client #311 to the list of clients for aggregation.
[INFO][10:34:21]: [Server #1127936] Adding client #407 to the list of clients for aggregation.
[INFO][10:34:21]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00459871
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00642429 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00917372
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00255988 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00123703 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00453214 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00089438 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.0015152  0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00223545
 0.         0.         0.         0.         0.         0.
 0.00104132 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 0. 1. 0. 0. 0. 1. 0. 0. 3. 1. 0. 0. 0. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 2. 0. 1. 0.
 0. 0. 1. 0. 2. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 2. 1. 1. 0. 0.
 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 2. 1. 6. 0. 0. 2. 1. 0. 1. 1. 2. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 3. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 1. 6. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 2. 0. 0.
 1. 1. 1. 2. 0. 0. 0. 2. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 1. 2. 0. 4. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1.
 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 4. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0.
 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0.
 1. 0. 1. 0. 4. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 3. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 0. 1. 0. 6. 1. 1.
 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 6. 0. 0. 0. 1. 0. 0. 1. 2. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 2. 2. 1. 1. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00459871
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00642429 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00917372
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00255988 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00123703 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00453214 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00089438 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.0015152  0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00223545
 0.         0.         0.         0.         0.         0.
 0.00104132 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:34:23]: [Server #1127936] Global model accuracy: 96.31%

[INFO][10:34:23]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_80.pth.
[INFO][10:34:23]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_80.pth.
[INFO][10:34:23]: [93m[1m
[Server #1127936] Starting round 81/100.[0m
[INFO][10:34:24]: [Server #1127936] Selected clients: [ 66 237  11 485 422 340 334 212 423 222]
[INFO][10:34:24]: [Server #1127936] Selecting client #66 for training.
[INFO][10:34:24]: [Server #1127936] Sending the current model to client #66 (simulated).
[INFO][10:34:24]: [Server #1127936] Sending 0.24 MB of payload data to client #66 (simulated).
[INFO][10:34:24]: [Server #1127936] Selecting client #237 for training.
[INFO][10:34:24]: [Server #1127936] Sending the current model to client #237 (simulated).
[INFO][10:34:24]: [Server #1127936] Sending 0.24 MB of payload data to client #237 (simulated).
[INFO][10:34:24]: [Server #1127936] Selecting client #11 for training.
[INFO][10:34:24]: [Server #1127936] Sending the current model to client #11 (simulated).
[INFO][10:34:24]: [Client #66] Selected by the server.
[INFO][10:34:24]: [Client #66] Loading its data source...
[INFO][10:34:24]: [Client #66] Dataset size: 60000
[INFO][10:34:24]: [Client #66] Sampler: noniid
[INFO][10:34:24]: [Server #1127936] Sending 0.24 MB of payload data to client #11 (simulated).
[INFO][10:34:24]: [Client #11] Selected by the server.
[INFO][10:34:24]: [Client #11] Loading its data source...
[INFO][10:34:24]: [Client #66] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:34:24]: [Client #11] Dataset size: 60000
[INFO][10:34:24]: [Client #11] Sampler: noniid
[INFO][10:34:24]: [Client #11] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:34:24]: [Client #237] Selected by the server.
[INFO][10:34:24]: [Client #237] Loading its data source...
[INFO][10:34:24]: [Client #237] Dataset size: 60000
[INFO][10:34:24]: [Client #237] Sampler: noniid
[INFO][10:34:24]: [Client #237] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:34:24]: [93m[1m[Client #237] Started training in communication round #81.[0m
[INFO][10:34:24]: [93m[1m[Client #11] Started training in communication round #81.[0m
[INFO][10:34:24]: [93m[1m[Client #66] Started training in communication round #81.[0m
[INFO][10:34:26]: [Client #237] Loading the dataset.
[INFO][10:34:26]: [Client #11] Loading the dataset.
[INFO][10:34:26]: [Client #66] Loading the dataset.
[INFO][10:34:34]: [Client #66] Epoch: [1/5][0/10]	Loss: 0.003524
[INFO][10:34:34]: [Client #237] Epoch: [1/5][0/10]	Loss: 0.002147
[INFO][10:34:34]: [Client #66] Epoch: [2/5][0/10]	Loss: 0.000473
[INFO][10:34:34]: [Client #11] Epoch: [1/5][0/10]	Loss: 0.000541
[INFO][10:34:34]: [Client #237] Epoch: [2/5][0/10]	Loss: 0.003413
[INFO][10:34:34]: [Client #66] Epoch: [3/5][0/10]	Loss: 0.000028
[INFO][10:34:34]: [Client #237] Epoch: [3/5][0/10]	Loss: 0.000076
[INFO][10:34:34]: [Client #11] Epoch: [2/5][0/10]	Loss: 0.002562
[INFO][10:34:34]: [Client #66] Epoch: [4/5][0/10]	Loss: 0.316505
[INFO][10:34:34]: [Client #11] Epoch: [3/5][0/10]	Loss: 0.000205
[INFO][10:34:34]: [Client #237] Epoch: [4/5][0/10]	Loss: 0.000462
[INFO][10:34:34]: [Client #66] Epoch: [5/5][0/10]	Loss: 0.041382
[INFO][10:34:34]: [Client #11] Epoch: [4/5][0/10]	Loss: 0.000041
[INFO][10:34:34]: [Client #237] Epoch: [5/5][0/10]	Loss: 0.000006
[INFO][10:34:34]: [Client #66] Model saved to /data/ykang/plato/results/test/model/lenet5_66_1127977.pth.
[INFO][10:34:34]: [Client #237] Model saved to /data/ykang/plato/results/test/model/lenet5_237_1127978.pth.
[INFO][10:34:34]: [Client #11] Epoch: [5/5][0/10]	Loss: 0.000042
[INFO][10:34:34]: [Client #11] Model saved to /data/ykang/plato/results/test/model/lenet5_11_1127979.pth.
[INFO][10:34:35]: [Client #66] Loading a model from /data/ykang/plato/results/test/model/lenet5_66_1127977.pth.
[INFO][10:34:35]: [Client #66] Model trained.
[INFO][10:34:35]: [Client #66] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:34:35]: [Server #1127936] Received 0.24 MB of payload data from client #66 (simulated).
[INFO][10:34:35]: [Client #237] Loading a model from /data/ykang/plato/results/test/model/lenet5_237_1127978.pth.
[INFO][10:34:35]: [Client #237] Model trained.
[INFO][10:34:35]: [Client #237] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:34:35]: [Server #1127936] Received 0.24 MB of payload data from client #237 (simulated).
[INFO][10:34:35]: [Client #11] Loading a model from /data/ykang/plato/results/test/model/lenet5_11_1127979.pth.
[INFO][10:34:35]: [Client #11] Model trained.
[INFO][10:34:35]: [Client #11] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:34:35]: [Server #1127936] Received 0.24 MB of payload data from client #11 (simulated).
[INFO][10:34:35]: [Server #1127936] Selecting client #485 for training.
[INFO][10:34:35]: [Server #1127936] Sending the current model to client #485 (simulated).
[INFO][10:34:35]: [Server #1127936] Sending 0.24 MB of payload data to client #485 (simulated).
[INFO][10:34:35]: [Server #1127936] Selecting client #422 for training.
[INFO][10:34:35]: [Server #1127936] Sending the current model to client #422 (simulated).
[INFO][10:34:35]: [Server #1127936] Sending 0.24 MB of payload data to client #422 (simulated).
[INFO][10:34:35]: [Server #1127936] Selecting client #340 for training.
[INFO][10:34:35]: [Server #1127936] Sending the current model to client #340 (simulated).
[INFO][10:34:35]: [Client #485] Selected by the server.
[INFO][10:34:35]: [Client #485] Loading its data source...
[INFO][10:34:35]: [Client #485] Dataset size: 60000
[INFO][10:34:35]: [Client #485] Sampler: noniid
[INFO][10:34:35]: [Server #1127936] Sending 0.24 MB of payload data to client #340 (simulated).
[INFO][10:34:35]: [Client #422] Selected by the server.
[INFO][10:34:35]: [Client #422] Loading its data source...
[INFO][10:34:35]: [Client #340] Selected by the server.
[INFO][10:34:35]: [Client #422] Dataset size: 60000
[INFO][10:34:35]: [Client #422] Sampler: noniid
[INFO][10:34:35]: [Client #340] Loading its data source...
[INFO][10:34:35]: [Client #340] Dataset size: 60000
[INFO][10:34:35]: [Client #340] Sampler: noniid
[INFO][10:34:35]: [Client #485] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:34:35]: [Client #340] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:34:35]: [Client #422] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:34:35]: [93m[1m[Client #485] Started training in communication round #81.[0m
[INFO][10:34:35]: [93m[1m[Client #422] Started training in communication round #81.[0m
[INFO][10:34:35]: [93m[1m[Client #340] Started training in communication round #81.[0m
[INFO][10:34:37]: [Client #340] Loading the dataset.
[INFO][10:34:37]: [Client #422] Loading the dataset.
[INFO][10:34:37]: [Client #485] Loading the dataset.
[INFO][10:34:43]: [Client #485] Epoch: [1/5][0/10]	Loss: 0.000197
[INFO][10:34:43]: [Client #340] Epoch: [1/5][0/10]	Loss: 0.000146
[INFO][10:34:43]: [Client #422] Epoch: [1/5][0/10]	Loss: 0.003249
[INFO][10:34:44]: [Client #422] Epoch: [2/5][0/10]	Loss: 0.000743
[INFO][10:34:44]: [Client #485] Epoch: [2/5][0/10]	Loss: 0.002258
[INFO][10:34:44]: [Client #340] Epoch: [2/5][0/10]	Loss: 0.000084
[INFO][10:34:44]: [Client #340] Epoch: [3/5][0/10]	Loss: 0.000116
[INFO][10:34:44]: [Client #422] Epoch: [3/5][0/10]	Loss: 0.000090
[INFO][10:34:44]: [Client #485] Epoch: [3/5][0/10]	Loss: 0.000037
[INFO][10:34:44]: [Client #340] Epoch: [4/5][0/10]	Loss: 0.000127
[INFO][10:34:44]: [Client #422] Epoch: [4/5][0/10]	Loss: 0.000059
[INFO][10:34:44]: [Client #485] Epoch: [4/5][0/10]	Loss: 0.000003
[INFO][10:34:44]: [Client #422] Epoch: [5/5][0/10]	Loss: 0.000003
[INFO][10:34:44]: [Client #485] Epoch: [5/5][0/10]	Loss: 0.000002
[INFO][10:34:44]: [Client #340] Epoch: [5/5][0/10]	Loss: 0.000009
[INFO][10:34:44]: [Client #422] Model saved to /data/ykang/plato/results/test/model/lenet5_422_1127978.pth.
[INFO][10:34:44]: [Client #485] Model saved to /data/ykang/plato/results/test/model/lenet5_485_1127977.pth.
[INFO][10:34:44]: [Client #340] Model saved to /data/ykang/plato/results/test/model/lenet5_340_1127979.pth.
[INFO][10:34:45]: [Client #422] Loading a model from /data/ykang/plato/results/test/model/lenet5_422_1127978.pth.
[INFO][10:34:45]: [Client #422] Model trained.
[INFO][10:34:45]: [Client #422] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:34:45]: [Server #1127936] Received 0.24 MB of payload data from client #422 (simulated).
[INFO][10:34:45]: [Client #485] Loading a model from /data/ykang/plato/results/test/model/lenet5_485_1127977.pth.
[INFO][10:34:45]: [Client #485] Model trained.
[INFO][10:34:45]: [Client #485] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:34:45]: [Server #1127936] Received 0.24 MB of payload data from client #485 (simulated).
[INFO][10:34:45]: [Client #340] Loading a model from /data/ykang/plato/results/test/model/lenet5_340_1127979.pth.
[INFO][10:34:45]: [Client #340] Model trained.
[INFO][10:34:45]: [Client #340] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:34:45]: [Server #1127936] Received 0.24 MB of payload data from client #340 (simulated).
[INFO][10:34:45]: [Server #1127936] Selecting client #334 for training.
[INFO][10:34:45]: [Server #1127936] Sending the current model to client #334 (simulated).
[INFO][10:34:45]: [Server #1127936] Sending 0.24 MB of payload data to client #334 (simulated).
[INFO][10:34:45]: [Server #1127936] Selecting client #212 for training.
[INFO][10:34:45]: [Server #1127936] Sending the current model to client #212 (simulated).
[INFO][10:34:45]: [Server #1127936] Sending 0.24 MB of payload data to client #212 (simulated).
[INFO][10:34:45]: [Server #1127936] Selecting client #423 for training.
[INFO][10:34:45]: [Server #1127936] Sending the current model to client #423 (simulated).
[INFO][10:34:45]: [Client #334] Selected by the server.
[INFO][10:34:45]: [Client #334] Loading its data source...
[INFO][10:34:45]: [Client #334] Dataset size: 60000
[INFO][10:34:45]: [Client #334] Sampler: noniid
[INFO][10:34:45]: [Server #1127936] Sending 0.24 MB of payload data to client #423 (simulated).
[INFO][10:34:45]: [Client #423] Selected by the server.
[INFO][10:34:45]: [Client #212] Selected by the server.
[INFO][10:34:45]: [Client #423] Loading its data source...
[INFO][10:34:45]: [Client #212] Loading its data source...
[INFO][10:34:45]: [Client #423] Dataset size: 60000
[INFO][10:34:45]: [Client #423] Sampler: noniid
[INFO][10:34:45]: [Client #212] Dataset size: 60000
[INFO][10:34:45]: [Client #212] Sampler: noniid
[INFO][10:34:45]: [Client #334] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:34:45]: [Client #423] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:34:45]: [Client #212] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:34:45]: [93m[1m[Client #212] Started training in communication round #81.[0m
[INFO][10:34:45]: [93m[1m[Client #334] Started training in communication round #81.[0m
[INFO][10:34:45]: [93m[1m[Client #423] Started training in communication round #81.[0m
[INFO][10:34:47]: [Client #334] Loading the dataset.
[INFO][10:34:47]: [Client #212] Loading the dataset.
[INFO][10:34:47]: [Client #423] Loading the dataset.
[INFO][10:34:54]: [Client #212] Epoch: [1/5][0/10]	Loss: 0.007694
[INFO][10:34:54]: [Client #334] Epoch: [1/5][0/10]	Loss: 0.007624
[INFO][10:34:54]: [Client #423] Epoch: [1/5][0/10]	Loss: 0.001001
[INFO][10:34:54]: [Client #212] Epoch: [2/5][0/10]	Loss: 0.000181
[INFO][10:34:54]: [Client #334] Epoch: [2/5][0/10]	Loss: 0.026903
[INFO][10:34:54]: [Client #423] Epoch: [2/5][0/10]	Loss: 0.000012
[INFO][10:34:54]: [Client #334] Epoch: [3/5][0/10]	Loss: 0.000147
[INFO][10:34:54]: [Client #212] Epoch: [3/5][0/10]	Loss: 0.000504
[INFO][10:34:54]: [Client #423] Epoch: [3/5][0/10]	Loss: 0.000003
[INFO][10:34:54]: [Client #334] Epoch: [4/5][0/10]	Loss: 0.006352
[INFO][10:34:54]: [Client #423] Epoch: [4/5][0/10]	Loss: 0.000015
[INFO][10:34:54]: [Client #212] Epoch: [4/5][0/10]	Loss: 0.000869
[INFO][10:34:54]: [Client #334] Epoch: [5/5][0/10]	Loss: 0.064051
[INFO][10:34:54]: [Client #423] Epoch: [5/5][0/10]	Loss: 0.000037
[INFO][10:34:54]: [Client #212] Epoch: [5/5][0/10]	Loss: 0.001150
[INFO][10:34:54]: [Client #334] Model saved to /data/ykang/plato/results/test/model/lenet5_334_1127977.pth.
[INFO][10:34:54]: [Client #423] Model saved to /data/ykang/plato/results/test/model/lenet5_423_1127979.pth.
[INFO][10:34:54]: [Client #212] Model saved to /data/ykang/plato/results/test/model/lenet5_212_1127978.pth.
[INFO][10:34:55]: [Client #334] Loading a model from /data/ykang/plato/results/test/model/lenet5_334_1127977.pth.
[INFO][10:34:55]: [Client #334] Model trained.
[INFO][10:34:55]: [Client #334] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:34:55]: [Server #1127936] Received 0.24 MB of payload data from client #334 (simulated).
[INFO][10:34:55]: [Client #212] Loading a model from /data/ykang/plato/results/test/model/lenet5_212_1127978.pth.
[INFO][10:34:55]: [Client #212] Model trained.
[INFO][10:34:55]: [Client #212] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:34:55]: [Server #1127936] Received 0.24 MB of payload data from client #212 (simulated).
[INFO][10:34:55]: [Client #423] Loading a model from /data/ykang/plato/results/test/model/lenet5_423_1127979.pth.
[INFO][10:34:55]: [Client #423] Model trained.
[INFO][10:34:55]: [Client #423] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:34:55]: [Server #1127936] Received 0.24 MB of payload data from client #423 (simulated).
[INFO][10:34:55]: [Server #1127936] Selecting client #222 for training.
[INFO][10:34:55]: [Server #1127936] Sending the current model to client #222 (simulated).
[INFO][10:34:55]: [Server #1127936] Sending 0.24 MB of payload data to client #222 (simulated).
[INFO][10:34:55]: [Client #222] Selected by the server.
[INFO][10:34:55]: [Client #222] Loading its data source...
[INFO][10:34:55]: [Client #222] Dataset size: 60000
[INFO][10:34:55]: [Client #222] Sampler: noniid
[INFO][10:34:55]: [Client #222] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:34:55]: [93m[1m[Client #222] Started training in communication round #81.[0m
[INFO][10:34:57]: [Client #222] Loading the dataset.
[INFO][10:35:03]: [Client #222] Epoch: [1/5][0/10]	Loss: 0.000130
[INFO][10:35:03]: [Client #222] Epoch: [2/5][0/10]	Loss: 0.000957
[INFO][10:35:03]: [Client #222] Epoch: [3/5][0/10]	Loss: 0.000135
[INFO][10:35:03]: [Client #222] Epoch: [4/5][0/10]	Loss: 0.000057
[INFO][10:35:03]: [Client #222] Epoch: [5/5][0/10]	Loss: 0.001126
[INFO][10:35:03]: [Client #222] Model saved to /data/ykang/plato/results/test/model/lenet5_222_1127977.pth.
[INFO][10:35:04]: [Client #222] Loading a model from /data/ykang/plato/results/test/model/lenet5_222_1127977.pth.
[INFO][10:35:04]: [Client #222] Model trained.
[INFO][10:35:04]: [Client #222] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:35:04]: [Server #1127936] Received 0.24 MB of payload data from client #222 (simulated).
[INFO][10:35:04]: [Server #1127936] Adding client #473 to the list of clients for aggregation.
[INFO][10:35:04]: [Server #1127936] Adding client #499 to the list of clients for aggregation.
[INFO][10:35:04]: [Server #1127936] Adding client #303 to the list of clients for aggregation.
[INFO][10:35:04]: [Server #1127936] Adding client #352 to the list of clients for aggregation.
[INFO][10:35:04]: [Server #1127936] Adding client #183 to the list of clients for aggregation.
[INFO][10:35:04]: [Server #1127936] Adding client #437 to the list of clients for aggregation.
[INFO][10:35:04]: [Server #1127936] Adding client #237 to the list of clients for aggregation.
[INFO][10:35:04]: [Server #1127936] Adding client #423 to the list of clients for aggregation.
[INFO][10:35:04]: [Server #1127936] Adding client #422 to the list of clients for aggregation.
[INFO][10:35:04]: [Server #1127936] Adding client #222 to the list of clients for aggregation.
[INFO][10:35:04]: [Server #1127936] Aggregating 10 clients in total.
[0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  1e-05  2e-10  2e-10
 6:  6.8876e+00  6.8875e+00  9e-06  1e-10  1e-10
 7:  6.8876e+00  6.8875e+00  9e-06  1e-10  1e-12
 8:  6.8875e+00  6.8875e+00  6e-06  9e-11  9e-13
Optimal solution found.
The calculated probability is:  [0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.00259391
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.00123464 0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.3935783  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.00175126 0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.00144124 0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.00123467 0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.00137761
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.00123469 0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0016639  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347  0.0012347  0.0012347
 0.0012347  0.0012347  0.0012347  0.0012347 ]
current clients pool:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00242271 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00108308
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00215737 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00184492 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00086477 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00475988 0.00141358 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00786738 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00164582 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00216072 0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 0. 1. 0. 0. 0. 1. 0. 0. 3. 1. 0. 0. 0. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 2. 0. 1. 0.
 0. 0. 1. 0. 2. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 2. 1. 1. 0. 0.
 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 2. 1. 6. 0. 0. 2. 1. 0. 1. 1. 2. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 3. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 1. 6. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 2. 0. 0.
 1. 1. 1. 2. 0. 0. 0. 2. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 1. 2. 0. 4. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1.
 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 4. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0.
 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0.
 1. 0. 1. 0. 4. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 3. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 0. 1. 0. 6. 1. 1.
 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 6. 0. 0. 0. 1. 0. 1. 1. 2. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 2. 2. 1. 2. 0.]
local_gradient_bounds:  [INFO][10:35:06]: [Server #1127936] Global model accuracy: 96.22%

[INFO][10:35:06]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_81.pth.
[INFO][10:35:06]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_81.pth.
[INFO][10:35:06]: [93m[1m
[Server #1127936] Starting round 82/100.[0m
[0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00242271 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00108308
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00215737 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00184492 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00086477 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00475988 0.00141358 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00786738 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00164582 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00216072 0.        ]
!!!!!!The aggregation weights of this round are:  [0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002 0.1
 0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  1e-05  2e-10  2e-10
 6:  6.8876e+00  6.8875e+00  8e-06  1e-10  1e-10
 7:  6.8876e+00  6.8875e+00  8e-06  8e-11  7e-13
 8:  6.8875e+00  6.8875e+00  5e-06  7e-11  5e-13
Optimal solution found.
The calculated probability is:  [0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00186263 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125611 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00167175 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00142253 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125608 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.38251544 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.0016145  0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612 0.00125612
 0.00125612 0.00125612 0.00294984 0.00125612]
current clients pool:  [INFO][10:35:07]: [Server #1127936] Selected clients: [450 287 437 486  38 378 272 432 232  63]
[INFO][10:35:07]: [Server #1127936] Selecting client #450 for training.
[INFO][10:35:07]: [Server #1127936] Sending the current model to client #450 (simulated).
[INFO][10:35:07]: [Server #1127936] Sending 0.24 MB of payload data to client #450 (simulated).
[INFO][10:35:07]: [Server #1127936] Selecting client #287 for training.
[INFO][10:35:07]: [Server #1127936] Sending the current model to client #287 (simulated).
[INFO][10:35:07]: [Server #1127936] Sending 0.24 MB of payload data to client #287 (simulated).
[INFO][10:35:07]: [Server #1127936] Selecting client #437 for training.
[INFO][10:35:07]: [Server #1127936] Sending the current model to client #437 (simulated).
[INFO][10:35:07]: [Client #450] Selected by the server.
[INFO][10:35:07]: [Client #450] Loading its data source...
[INFO][10:35:07]: [Client #450] Dataset size: 60000
[INFO][10:35:07]: [Client #450] Sampler: noniid
[INFO][10:35:07]: [Server #1127936] Sending 0.24 MB of payload data to client #437 (simulated).
[INFO][10:35:07]: [Client #287] Selected by the server.
[INFO][10:35:07]: [Client #287] Loading its data source...
[INFO][10:35:07]: [Client #287] Dataset size: 60000
[INFO][10:35:07]: [Client #287] Sampler: noniid
[INFO][10:35:07]: [Client #450] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:35:07]: [Client #437] Selected by the server.
[INFO][10:35:07]: [Client #437] Loading its data source...
[INFO][10:35:07]: [Client #437] Dataset size: 60000
[INFO][10:35:07]: [Client #437] Sampler: noniid
[INFO][10:35:07]: [Client #437] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:35:07]: [Client #287] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:35:07]: [93m[1m[Client #287] Started training in communication round #82.[0m
[INFO][10:35:07]: [93m[1m[Client #450] Started training in communication round #82.[0m
[INFO][10:35:07]: [93m[1m[Client #437] Started training in communication round #82.[0m
[INFO][10:35:09]: [Client #437] Loading the dataset.
[INFO][10:35:09]: [Client #450] Loading the dataset.
[INFO][10:35:09]: [Client #287] Loading the dataset.
[INFO][10:35:15]: [Client #437] Epoch: [1/5][0/10]	Loss: 0.000016
[INFO][10:35:15]: [Client #287] Epoch: [1/5][0/10]	Loss: 0.008756
[INFO][10:35:15]: [Client #437] Epoch: [2/5][0/10]	Loss: 0.000035
[INFO][10:35:15]: [Client #450] Epoch: [1/5][0/10]	Loss: 0.000580
[INFO][10:35:15]: [Client #287] Epoch: [2/5][0/10]	Loss: 0.000083
[INFO][10:35:15]: [Client #437] Epoch: [3/5][0/10]	Loss: 0.000009
[INFO][10:35:15]: [Client #450] Epoch: [2/5][0/10]	Loss: 0.000433
[INFO][10:35:15]: [Client #287] Epoch: [3/5][0/10]	Loss: 0.000052
[INFO][10:35:15]: [Client #437] Epoch: [4/5][0/10]	Loss: 0.000003
[INFO][10:35:15]: [Client #450] Epoch: [3/5][0/10]	Loss: 0.003389
[INFO][10:35:15]: [Client #287] Epoch: [4/5][0/10]	Loss: 0.000050
[INFO][10:35:15]: [Client #450] Epoch: [4/5][0/10]	Loss: 0.000055
[INFO][10:35:15]: [Client #437] Epoch: [5/5][0/10]	Loss: 0.000020
[INFO][10:35:15]: [Client #287] Epoch: [5/5][0/10]	Loss: 0.000003
[INFO][10:35:15]: [Client #287] Model saved to /data/ykang/plato/results/test/model/lenet5_287_1127978.pth.
[INFO][10:35:15]: [Client #437] Model saved to /data/ykang/plato/results/test/model/lenet5_437_1127979.pth.
[INFO][10:35:15]: [Client #450] Epoch: [5/5][0/10]	Loss: 0.023167
[INFO][10:35:15]: [Client #450] Model saved to /data/ykang/plato/results/test/model/lenet5_450_1127977.pth.
[INFO][10:35:16]: [Client #287] Loading a model from /data/ykang/plato/results/test/model/lenet5_287_1127978.pth.
[INFO][10:35:16]: [Client #287] Model trained.
[INFO][10:35:16]: [Client #287] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:35:16]: [Server #1127936] Received 0.24 MB of payload data from client #287 (simulated).
[INFO][10:35:16]: [Client #437] Loading a model from /data/ykang/plato/results/test/model/lenet5_437_1127979.pth.
[INFO][10:35:16]: [Client #437] Model trained.
[INFO][10:35:16]: [Client #437] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:35:16]: [Server #1127936] Received 0.24 MB of payload data from client #437 (simulated).
[INFO][10:35:16]: [Client #450] Loading a model from /data/ykang/plato/results/test/model/lenet5_450_1127977.pth.
[INFO][10:35:16]: [Client #450] Model trained.
[INFO][10:35:16]: [Client #450] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:35:16]: [Server #1127936] Received 0.24 MB of payload data from client #450 (simulated).
[INFO][10:35:16]: [Server #1127936] Selecting client #486 for training.
[INFO][10:35:16]: [Server #1127936] Sending the current model to client #486 (simulated).
[INFO][10:35:16]: [Server #1127936] Sending 0.24 MB of payload data to client #486 (simulated).
[INFO][10:35:16]: [Server #1127936] Selecting client #38 for training.
[INFO][10:35:16]: [Server #1127936] Sending the current model to client #38 (simulated).
[INFO][10:35:16]: [Server #1127936] Sending 0.24 MB of payload data to client #38 (simulated).
[INFO][10:35:16]: [Server #1127936] Selecting client #378 for training.
[INFO][10:35:16]: [Server #1127936] Sending the current model to client #378 (simulated).
[INFO][10:35:16]: [Client #486] Selected by the server.
[INFO][10:35:16]: [Client #486] Loading its data source...
[INFO][10:35:16]: [Client #486] Dataset size: 60000
[INFO][10:35:16]: [Client #486] Sampler: noniid
[INFO][10:35:16]: [Server #1127936] Sending 0.24 MB of payload data to client #378 (simulated).
[INFO][10:35:16]: [Client #38] Selected by the server.
[INFO][10:35:16]: [Client #38] Loading its data source...
[INFO][10:35:16]: [Client #38] Dataset size: 60000
[INFO][10:35:16]: [Client #38] Sampler: noniid
[INFO][10:35:16]: [Client #378] Selected by the server.
[INFO][10:35:16]: [Client #378] Loading its data source...
[INFO][10:35:16]: [Client #378] Dataset size: 60000
[INFO][10:35:16]: [Client #378] Sampler: noniid
[INFO][10:35:16]: [Client #486] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:35:16]: [Client #38] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:35:16]: [Client #378] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:35:16]: [93m[1m[Client #486] Started training in communication round #82.[0m
[INFO][10:35:16]: [93m[1m[Client #38] Started training in communication round #82.[0m
[INFO][10:35:16]: [93m[1m[Client #378] Started training in communication round #82.[0m
[INFO][10:35:19]: [Client #486] Loading the dataset.
[INFO][10:35:19]: [Client #378] Loading the dataset.
[INFO][10:35:19]: [Client #38] Loading the dataset.
[INFO][10:35:25]: [Client #378] Epoch: [1/5][0/10]	Loss: 0.000498
[INFO][10:35:25]: [Client #486] Epoch: [1/5][0/10]	Loss: 0.001598
[INFO][10:35:25]: [Client #38] Epoch: [1/5][0/10]	Loss: 0.000016
[INFO][10:35:25]: [Client #486] Epoch: [2/5][0/10]	Loss: 0.000019
[INFO][10:35:25]: [Client #38] Epoch: [2/5][0/10]	Loss: 0.000033
[INFO][10:35:25]: [Client #378] Epoch: [2/5][0/10]	Loss: 0.001183
[INFO][10:35:25]: [Client #486] Epoch: [3/5][0/10]	Loss: 0.019984
[INFO][10:35:25]: [Client #378] Epoch: [3/5][0/10]	Loss: 0.000039
[INFO][10:35:25]: [Client #38] Epoch: [3/5][0/10]	Loss: 0.000005
[INFO][10:35:25]: [Client #486] Epoch: [4/5][0/10]	Loss: 0.000004
[INFO][10:35:25]: [Client #38] Epoch: [4/5][0/10]	Loss: 0.000004
[INFO][10:35:25]: [Client #378] Epoch: [4/5][0/10]	Loss: 0.000034
[INFO][10:35:25]: [Client #486] Epoch: [5/5][0/10]	Loss: 0.002353
[INFO][10:35:25]: [Client #38] Epoch: [5/5][0/10]	Loss: 0.000001
[INFO][10:35:25]: [Client #378] Epoch: [5/5][0/10]	Loss: 0.000061
[INFO][10:35:25]: [Client #38] Model saved to /data/ykang/plato/results/test/model/lenet5_38_1127978.pth.
[INFO][10:35:25]: [Client #486] Model saved to /data/ykang/plato/results/test/model/lenet5_486_1127977.pth.
[INFO][10:35:25]: [Client #378] Model saved to /data/ykang/plato/results/test/model/lenet5_378_1127979.pth.
[INFO][10:35:26]: [Client #38] Loading a model from /data/ykang/plato/results/test/model/lenet5_38_1127978.pth.
[INFO][10:35:26]: [Client #38] Model trained.
[INFO][10:35:26]: [Client #38] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:35:26]: [Server #1127936] Received 0.24 MB of payload data from client #38 (simulated).
[INFO][10:35:26]: [Client #486] Loading a model from /data/ykang/plato/results/test/model/lenet5_486_1127977.pth.
[INFO][10:35:26]: [Client #486] Model trained.
[INFO][10:35:26]: [Client #486] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:35:26]: [Server #1127936] Received 0.24 MB of payload data from client #486 (simulated).
[INFO][10:35:26]: [Client #378] Loading a model from /data/ykang/plato/results/test/model/lenet5_378_1127979.pth.
[INFO][10:35:26]: [Client #378] Model trained.
[INFO][10:35:26]: [Client #378] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:35:26]: [Server #1127936] Received 0.24 MB of payload data from client #378 (simulated).
[INFO][10:35:26]: [Server #1127936] Selecting client #272 for training.
[INFO][10:35:26]: [Server #1127936] Sending the current model to client #272 (simulated).
[INFO][10:35:26]: [Server #1127936] Sending 0.24 MB of payload data to client #272 (simulated).
[INFO][10:35:26]: [Server #1127936] Selecting client #432 for training.
[INFO][10:35:26]: [Server #1127936] Sending the current model to client #432 (simulated).
[INFO][10:35:26]: [Server #1127936] Sending 0.24 MB of payload data to client #432 (simulated).
[INFO][10:35:26]: [Server #1127936] Selecting client #232 for training.
[INFO][10:35:26]: [Server #1127936] Sending the current model to client #232 (simulated).
[INFO][10:35:26]: [Client #272] Selected by the server.
[INFO][10:35:26]: [Client #272] Loading its data source...
[INFO][10:35:26]: [Client #272] Dataset size: 60000
[INFO][10:35:26]: [Client #272] Sampler: noniid
[INFO][10:35:26]: [Server #1127936] Sending 0.24 MB of payload data to client #232 (simulated).
[INFO][10:35:26]: [Client #232] Selected by the server.
[INFO][10:35:26]: [Client #232] Loading its data source...
[INFO][10:35:26]: [Client #232] Dataset size: 60000
[INFO][10:35:26]: [Client #232] Sampler: noniid
[INFO][10:35:26]: [Client #432] Selected by the server.
[INFO][10:35:26]: [Client #432] Loading its data source...
[INFO][10:35:26]: [Client #432] Dataset size: 60000
[INFO][10:35:26]: [Client #432] Sampler: noniid
[INFO][10:35:26]: [Client #232] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:35:26]: [Client #272] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:35:26]: [Client #432] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:35:26]: [93m[1m[Client #432] Started training in communication round #82.[0m
[INFO][10:35:26]: [93m[1m[Client #272] Started training in communication round #82.[0m
[INFO][10:35:26]: [93m[1m[Client #232] Started training in communication round #82.[0m
[INFO][10:35:28]: [Client #432] Loading the dataset.
[INFO][10:35:28]: [Client #272] Loading the dataset.
[INFO][10:35:28]: [Client #232] Loading the dataset.
[INFO][10:35:35]: [Client #432] Epoch: [1/5][0/10]	Loss: 0.000421
[INFO][10:35:35]: [Client #272] Epoch: [1/5][0/10]	Loss: 0.001033
[INFO][10:35:35]: [Client #232] Epoch: [1/5][0/10]	Loss: 0.011896
[INFO][10:35:35]: [Client #432] Epoch: [2/5][0/10]	Loss: 0.000173
[INFO][10:35:35]: [Client #272] Epoch: [2/5][0/10]	Loss: 0.002217
[INFO][10:35:35]: [Client #432] Epoch: [3/5][0/10]	Loss: 0.000029
[INFO][10:35:35]: [Client #232] Epoch: [2/5][0/10]	Loss: 0.000009
[INFO][10:35:35]: [Client #272] Epoch: [3/5][0/10]	Loss: 0.000025
[INFO][10:35:35]: [Client #432] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:35:35]: [Client #272] Epoch: [4/5][0/10]	Loss: 0.000312
[INFO][10:35:35]: [Client #232] Epoch: [3/5][0/10]	Loss: 0.000009
[INFO][10:35:35]: [Client #432] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:35:35]: [Client #432] Model saved to /data/ykang/plato/results/test/model/lenet5_432_1127978.pth.
[INFO][10:35:35]: [Client #272] Epoch: [5/5][0/10]	Loss: 0.000011
[INFO][10:35:35]: [Client #232] Epoch: [4/5][0/10]	Loss: 0.422160
[INFO][10:35:35]: [Client #272] Model saved to /data/ykang/plato/results/test/model/lenet5_272_1127977.pth.
[INFO][10:35:35]: [Client #232] Epoch: [5/5][0/10]	Loss: 0.138859
[INFO][10:35:35]: [Client #232] Model saved to /data/ykang/plato/results/test/model/lenet5_232_1127979.pth.
[INFO][10:35:36]: [Client #432] Loading a model from /data/ykang/plato/results/test/model/lenet5_432_1127978.pth.
[INFO][10:35:36]: [Client #432] Model trained.
[INFO][10:35:36]: [Client #432] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:35:36]: [Server #1127936] Received 0.24 MB of payload data from client #432 (simulated).
[INFO][10:35:36]: [Client #272] Loading a model from /data/ykang/plato/results/test/model/lenet5_272_1127977.pth.
[INFO][10:35:36]: [Client #272] Model trained.
[INFO][10:35:36]: [Client #272] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:35:36]: [Server #1127936] Received 0.24 MB of payload data from client #272 (simulated).
[INFO][10:35:36]: [Client #232] Loading a model from /data/ykang/plato/results/test/model/lenet5_232_1127979.pth.
[INFO][10:35:36]: [Client #232] Model trained.
[INFO][10:35:36]: [Client #232] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:35:36]: [Server #1127936] Received 0.24 MB of payload data from client #232 (simulated).
[INFO][10:35:36]: [Server #1127936] Selecting client #63 for training.
[INFO][10:35:36]: [Server #1127936] Sending the current model to client #63 (simulated).
[INFO][10:35:36]: [Server #1127936] Sending 0.24 MB of payload data to client #63 (simulated).
[INFO][10:35:36]: [Client #63] Selected by the server.
[INFO][10:35:36]: [Client #63] Loading its data source...
[INFO][10:35:36]: [Client #63] Dataset size: 60000
[INFO][10:35:36]: [Client #63] Sampler: noniid
[INFO][10:35:36]: [Client #63] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:35:36]: [93m[1m[Client #63] Started training in communication round #82.[0m
[INFO][10:35:38]: [Client #63] Loading the dataset.
[INFO][10:35:44]: [Client #63] Epoch: [1/5][0/10]	Loss: 0.011818
[INFO][10:35:44]: [Client #63] Epoch: [2/5][0/10]	Loss: 0.001596
[INFO][10:35:44]: [Client #63] Epoch: [3/5][0/10]	Loss: 0.000043
[INFO][10:35:44]: [Client #63] Epoch: [4/5][0/10]	Loss: 0.000647
[INFO][10:35:44]: [Client #63] Epoch: [5/5][0/10]	Loss: 0.020331
[INFO][10:35:44]: [Client #63] Model saved to /data/ykang/plato/results/test/model/lenet5_63_1127977.pth.
[INFO][10:35:45]: [Client #63] Loading a model from /data/ykang/plato/results/test/model/lenet5_63_1127977.pth.
[INFO][10:35:45]: [Client #63] Model trained.
[INFO][10:35:45]: [Client #63] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:35:45]: [Server #1127936] Received 0.24 MB of payload data from client #63 (simulated).
[INFO][10:35:45]: [Server #1127936] Adding client #11 to the list of clients for aggregation.
[INFO][10:35:45]: [Server #1127936] Adding client #485 to the list of clients for aggregation.
[INFO][10:35:45]: [Server #1127936] Adding client #334 to the list of clients for aggregation.
[INFO][10:35:45]: [Server #1127936] Adding client #66 to the list of clients for aggregation.
[INFO][10:35:45]: [Server #1127936] Adding client #340 to the list of clients for aggregation.
[INFO][10:35:45]: [Server #1127936] Adding client #172 to the list of clients for aggregation.
[INFO][10:35:45]: [Server #1127936] Adding client #450 to the list of clients for aggregation.
[INFO][10:35:45]: [Server #1127936] Adding client #272 to the list of clients for aggregation.
[INFO][10:35:45]: [Server #1127936] Adding client #432 to the list of clients for aggregation.
[INFO][10:35:45]: [Server #1127936] Adding client #287 to the list of clients for aggregation.
[INFO][10:35:45]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 335, 336, 337, 338, 339, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00523076 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00623719
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00428317 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00116354 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00650329 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00757203 0.         0.
 0.         0.         0.         0.00056018 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00260869
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.0026528
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.0022587  0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 0. 1. 0. 0. 0. 1. 0. 0. 3. 1. 0. 0. 0. 0. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 2. 0. 1. 0.
 0. 0. 1. 0. 2. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 2. 1. 1. 0. 0.
 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 2. 1. 6. 0. 0. 2. 1. 0. 1. 1. 2. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 3. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 1. 6. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 2. 0. 0.
 1. 1. 1. 2. 0. 0. 0. 2. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 1. 2. 0. 4. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1.
 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 4. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0.
 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0.
 1. 0. 1. 0. 4. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 3. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 0. 1. 0. 6. 1. 1.
 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 6. 0. 0. 0. 1. 0. 1. 1. 2. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 2. 2. 1. 2. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00523076 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00623719
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00428317 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00116354 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00650329 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00757203 0.         0.
 0.         0.         0.         0.00056018 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00260869
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.0026528
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.0022587  0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:35:47]: [Server #1127936] Global model accuracy: 96.06%

[INFO][10:35:47]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_82.pth.
[INFO][10:35:47]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_82.pth.
[INFO][10:35:47]: [93m[1m
[Server #1127936] Starting round 83/100.[0m
[0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  3e-05  5e-10  5e-10
 6:  6.8876e+00  6.8875e+00  3e-05  3e-10  3e-10
 7:  6.8875e+00  6.8875e+00  3e-05  7e-10  2e-11
 8:  6.8875e+00  6.8875e+00  2e-05  6e-10  2e-11
 9:  6.8875e+00  6.8875e+00  1e-05  1e-09  3e-11
10:  6.8875e+00  6.8875e+00  7e-07  2e-09  5e-11
Optimal solution found.
The calculated probability is:  [5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 7.67438301e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 8.14902005e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 9.71148794e-01 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88396851e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88386073e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 8.87500993e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 6.03520847e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88395416e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88395355e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 6.54461872e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05 5.88397207e-05 5.88397207e-05
 5.88397207e-05 5.88397207e-05]
current clients pool:  [INFO][10:35:48]: [Server #1127936] Selected clients: [172 271 240 235 420 435 409 440 287 482]
[INFO][10:35:48]: [Server #1127936] Selecting client #172 for training.
[INFO][10:35:48]: [Server #1127936] Sending the current model to client #172 (simulated).
[INFO][10:35:48]: [Server #1127936] Sending 0.24 MB of payload data to client #172 (simulated).
[INFO][10:35:48]: [Server #1127936] Selecting client #271 for training.
[INFO][10:35:48]: [Server #1127936] Sending the current model to client #271 (simulated).
[INFO][10:35:48]: [Server #1127936] Sending 0.24 MB of payload data to client #271 (simulated).
[INFO][10:35:48]: [Client #172] Selected by the server.
[INFO][10:35:48]: [Client #172] Loading its data source...
[INFO][10:35:48]: [Client #172] Dataset size: 60000
[INFO][10:35:48]: [Client #172] Sampler: noniid
[INFO][10:35:48]: [Client #172] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:35:48]: [Server #1127936] Selecting client #240 for training.
[INFO][10:35:48]: [Server #1127936] Sending the current model to client #240 (simulated).
[INFO][10:35:48]: [Server #1127936] Sending 0.24 MB of payload data to client #240 (simulated).
[INFO][10:35:48]: [Client #271] Selected by the server.
[INFO][10:35:48]: [Client #271] Loading its data source...
[INFO][10:35:48]: [Client #271] Dataset size: 60000
[INFO][10:35:48]: [Client #271] Sampler: noniid
[INFO][10:35:48]: [Client #240] Selected by the server.
[INFO][10:35:48]: [Client #240] Loading its data source...
[INFO][10:35:48]: [Client #240] Dataset size: 60000
[INFO][10:35:48]: [Client #240] Sampler: noniid
[INFO][10:35:48]: [Client #271] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:35:48]: [Client #240] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:35:48]: [93m[1m[Client #172] Started training in communication round #83.[0m
[INFO][10:35:48]: [93m[1m[Client #271] Started training in communication round #83.[0m
[INFO][10:35:48]: [93m[1m[Client #240] Started training in communication round #83.[0m
[INFO][10:35:50]: [Client #240] Loading the dataset.
[INFO][10:35:50]: [Client #172] Loading the dataset.
[INFO][10:35:50]: [Client #271] Loading the dataset.
[INFO][10:35:56]: [Client #172] Epoch: [1/5][0/10]	Loss: 0.000071
[INFO][10:35:56]: [Client #240] Epoch: [1/5][0/10]	Loss: 0.000079
[INFO][10:35:56]: [Client #271] Epoch: [1/5][0/10]	Loss: 0.008394
[INFO][10:35:56]: [Client #271] Epoch: [2/5][0/10]	Loss: 0.000005
[INFO][10:35:56]: [Client #240] Epoch: [2/5][0/10]	Loss: 0.000821
[INFO][10:35:56]: [Client #172] Epoch: [2/5][0/10]	Loss: 0.000888
[INFO][10:35:56]: [Client #240] Epoch: [3/5][0/10]	Loss: 0.000208
[INFO][10:35:56]: [Client #271] Epoch: [3/5][0/10]	Loss: 0.000002
[INFO][10:35:56]: [Client #172] Epoch: [3/5][0/10]	Loss: 0.000108
[INFO][10:35:57]: [Client #240] Epoch: [4/5][0/10]	Loss: 0.000022
[INFO][10:35:57]: [Client #172] Epoch: [4/5][0/10]	Loss: 0.000001
[INFO][10:35:57]: [Client #271] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:35:57]: [Client #240] Epoch: [5/5][0/10]	Loss: 0.000319
[INFO][10:35:57]: [Client #172] Epoch: [5/5][0/10]	Loss: 0.000001
[INFO][10:35:57]: [Client #271] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:35:57]: [Client #240] Model saved to /data/ykang/plato/results/test/model/lenet5_240_1127979.pth.
[INFO][10:35:57]: [Client #172] Model saved to /data/ykang/plato/results/test/model/lenet5_172_1127977.pth.
[INFO][10:35:57]: [Client #271] Model saved to /data/ykang/plato/results/test/model/lenet5_271_1127978.pth.
[INFO][10:35:58]: [Client #240] Loading a model from /data/ykang/plato/results/test/model/lenet5_240_1127979.pth.
[INFO][10:35:58]: [Client #240] Model trained.
[INFO][10:35:58]: [Client #240] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:35:58]: [Server #1127936] Received 0.24 MB of payload data from client #240 (simulated).
[INFO][10:35:58]: [Client #172] Loading a model from /data/ykang/plato/results/test/model/lenet5_172_1127977.pth.
[INFO][10:35:58]: [Client #172] Model trained.
[INFO][10:35:58]: [Client #172] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:35:58]: [Server #1127936] Received 0.24 MB of payload data from client #172 (simulated).
[INFO][10:35:58]: [Client #271] Loading a model from /data/ykang/plato/results/test/model/lenet5_271_1127978.pth.
[INFO][10:35:58]: [Client #271] Model trained.
[INFO][10:35:58]: [Client #271] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:35:58]: [Server #1127936] Received 0.24 MB of payload data from client #271 (simulated).
[INFO][10:35:58]: [Server #1127936] Selecting client #235 for training.
[INFO][10:35:58]: [Server #1127936] Sending the current model to client #235 (simulated).
[INFO][10:35:58]: [Server #1127936] Sending 0.24 MB of payload data to client #235 (simulated).
[INFO][10:35:58]: [Server #1127936] Selecting client #420 for training.
[INFO][10:35:58]: [Server #1127936] Sending the current model to client #420 (simulated).
[INFO][10:35:58]: [Server #1127936] Sending 0.24 MB of payload data to client #420 (simulated).
[INFO][10:35:58]: [Server #1127936] Selecting client #435 for training.
[INFO][10:35:58]: [Server #1127936] Sending the current model to client #435 (simulated).
[INFO][10:35:58]: [Client #235] Selected by the server.
[INFO][10:35:58]: [Client #235] Loading its data source...
[INFO][10:35:58]: [Client #235] Dataset size: 60000
[INFO][10:35:58]: [Client #235] Sampler: noniid
[INFO][10:35:58]: [Server #1127936] Sending 0.24 MB of payload data to client #435 (simulated).
[INFO][10:35:58]: [Client #235] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:35:58]: [Client #435] Selected by the server.
[INFO][10:35:58]: [Client #435] Loading its data source...
[INFO][10:35:58]: [Client #420] Selected by the server.
[INFO][10:35:58]: [Client #435] Dataset size: 60000
[INFO][10:35:58]: [Client #435] Sampler: noniid
[INFO][10:35:58]: [Client #420] Loading its data source...
[INFO][10:35:58]: [Client #420] Dataset size: 60000
[INFO][10:35:58]: [Client #420] Sampler: noniid
[INFO][10:35:58]: [Client #435] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:35:58]: [93m[1m[Client #435] Started training in communication round #83.[0m
[INFO][10:35:58]: [Client #420] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:35:58]: [93m[1m[Client #235] Started training in communication round #83.[0m
[INFO][10:35:58]: [93m[1m[Client #420] Started training in communication round #83.[0m
[INFO][10:36:00]: [Client #435] Loading the dataset.
[INFO][10:36:00]: [Client #235] Loading the dataset.
[INFO][10:36:00]: [Client #420] Loading the dataset.
[INFO][10:36:06]: [Client #235] Epoch: [1/5][0/10]	Loss: 0.000293
[INFO][10:36:06]: [Client #435] Epoch: [1/5][0/10]	Loss: 0.000165
[INFO][10:36:06]: [Client #235] Epoch: [2/5][0/10]	Loss: 0.000213
[INFO][10:36:06]: [Client #420] Epoch: [1/5][0/10]	Loss: 0.005470
[INFO][10:36:06]: [Client #435] Epoch: [2/5][0/10]	Loss: 0.000238
[INFO][10:36:06]: [Client #235] Epoch: [3/5][0/10]	Loss: 0.000013
[INFO][10:36:06]: [Client #420] Epoch: [2/5][0/10]	Loss: 0.005616
[INFO][10:36:06]: [Client #435] Epoch: [3/5][0/10]	Loss: 0.000024
[INFO][10:36:06]: [Client #235] Epoch: [4/5][0/10]	Loss: 0.000035
[INFO][10:36:06]: [Client #420] Epoch: [3/5][0/10]	Loss: 0.000051
[INFO][10:36:06]: [Client #435] Epoch: [4/5][0/10]	Loss: 0.000008
[INFO][10:36:06]: [Client #235] Epoch: [5/5][0/10]	Loss: 0.000215
[INFO][10:36:07]: [Client #235] Model saved to /data/ykang/plato/results/test/model/lenet5_235_1127977.pth.
[INFO][10:36:07]: [Client #420] Epoch: [4/5][0/10]	Loss: 0.000078
[INFO][10:36:07]: [Client #435] Epoch: [5/5][0/10]	Loss: 0.002467
[INFO][10:36:07]: [Client #420] Epoch: [5/5][0/10]	Loss: 0.000195
[INFO][10:36:07]: [Client #435] Model saved to /data/ykang/plato/results/test/model/lenet5_435_1127979.pth.
[INFO][10:36:07]: [Client #420] Model saved to /data/ykang/plato/results/test/model/lenet5_420_1127978.pth.
[INFO][10:36:07]: [Client #235] Loading a model from /data/ykang/plato/results/test/model/lenet5_235_1127977.pth.
[INFO][10:36:07]: [Client #235] Model trained.
[INFO][10:36:07]: [Client #235] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:36:07]: [Server #1127936] Received 0.24 MB of payload data from client #235 (simulated).
[INFO][10:36:07]: [Client #435] Loading a model from /data/ykang/plato/results/test/model/lenet5_435_1127979.pth.
[INFO][10:36:07]: [Client #435] Model trained.
[INFO][10:36:07]: [Client #435] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:36:07]: [Server #1127936] Received 0.24 MB of payload data from client #435 (simulated).
[INFO][10:36:08]: [Client #420] Loading a model from /data/ykang/plato/results/test/model/lenet5_420_1127978.pth.
[INFO][10:36:08]: [Client #420] Model trained.
[INFO][10:36:08]: [Client #420] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:36:08]: [Server #1127936] Received 0.24 MB of payload data from client #420 (simulated).
[INFO][10:36:08]: [Server #1127936] Selecting client #409 for training.
[INFO][10:36:08]: [Server #1127936] Sending the current model to client #409 (simulated).
[INFO][10:36:08]: [Server #1127936] Sending 0.24 MB of payload data to client #409 (simulated).
[INFO][10:36:08]: [Server #1127936] Selecting client #440 for training.
[INFO][10:36:08]: [Server #1127936] Sending the current model to client #440 (simulated).
[INFO][10:36:08]: [Server #1127936] Sending 0.24 MB of payload data to client #440 (simulated).
[INFO][10:36:08]: [Server #1127936] Selecting client #287 for training.
[INFO][10:36:08]: [Server #1127936] Sending the current model to client #287 (simulated).
[INFO][10:36:08]: [Client #409] Selected by the server.
[INFO][10:36:08]: [Client #409] Loading its data source...
[INFO][10:36:08]: [Client #409] Dataset size: 60000
[INFO][10:36:08]: [Client #409] Sampler: noniid
[INFO][10:36:08]: [Server #1127936] Sending 0.24 MB of payload data to client #287 (simulated).
[INFO][10:36:08]: [Client #440] Selected by the server.
[INFO][10:36:08]: [Client #440] Loading its data source...
[INFO][10:36:08]: [Client #440] Dataset size: 60000
[INFO][10:36:08]: [Client #440] Sampler: noniid
[INFO][10:36:08]: [Client #287] Selected by the server.
[INFO][10:36:08]: [Client #287] Loading its data source...
[INFO][10:36:08]: [Client #287] Dataset size: 60000
[INFO][10:36:08]: [Client #287] Sampler: noniid
[INFO][10:36:08]: [Client #409] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:36:08]: [Client #440] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:36:08]: [Client #287] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:36:08]: [93m[1m[Client #409] Started training in communication round #83.[0m
[INFO][10:36:08]: [93m[1m[Client #287] Started training in communication round #83.[0m
[INFO][10:36:08]: [93m[1m[Client #440] Started training in communication round #83.[0m
[INFO][10:36:10]: [Client #409] Loading the dataset.
[INFO][10:36:10]: [Client #287] Loading the dataset.
[INFO][10:36:10]: [Client #440] Loading the dataset.
[INFO][10:36:16]: [Client #440] Epoch: [1/5][0/10]	Loss: 0.000410
[INFO][10:36:16]: [Client #287] Epoch: [1/5][0/10]	Loss: 0.001766
[INFO][10:36:16]: [Client #409] Epoch: [1/5][0/10]	Loss: 0.001859
[INFO][10:36:16]: [Client #440] Epoch: [2/5][0/10]	Loss: 0.000358
[INFO][10:36:16]: [Client #287] Epoch: [2/5][0/10]	Loss: 0.000162
[INFO][10:36:16]: [Client #440] Epoch: [3/5][0/10]	Loss: 0.000009
[INFO][10:36:16]: [Client #409] Epoch: [2/5][0/10]	Loss: 0.006520
[INFO][10:36:16]: [Client #287] Epoch: [3/5][0/10]	Loss: 0.000052
[INFO][10:36:16]: [Client #440] Epoch: [4/5][0/10]	Loss: 0.000043
[INFO][10:36:16]: [Client #409] Epoch: [3/5][0/10]	Loss: 0.000114
[INFO][10:36:16]: [Client #287] Epoch: [4/5][0/10]	Loss: 0.000046
[INFO][10:36:16]: [Client #440] Epoch: [5/5][0/10]	Loss: 0.000148
[INFO][10:36:17]: [Client #440] Model saved to /data/ykang/plato/results/test/model/lenet5_440_1127978.pth.
[INFO][10:36:17]: [Client #409] Epoch: [4/5][0/10]	Loss: 0.000156
[INFO][10:36:17]: [Client #287] Epoch: [5/5][0/10]	Loss: 0.000101
[INFO][10:36:17]: [Client #287] Model saved to /data/ykang/plato/results/test/model/lenet5_287_1127979.pth.
[INFO][10:36:17]: [Client #409] Epoch: [5/5][0/10]	Loss: 0.001426
[INFO][10:36:17]: [Client #409] Model saved to /data/ykang/plato/results/test/model/lenet5_409_1127977.pth.
[INFO][10:36:17]: [Client #440] Loading a model from /data/ykang/plato/results/test/model/lenet5_440_1127978.pth.
[INFO][10:36:17]: [Client #440] Model trained.
[INFO][10:36:17]: [Client #440] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:36:17]: [Server #1127936] Received 0.24 MB of payload data from client #440 (simulated).
[INFO][10:36:17]: [Client #287] Loading a model from /data/ykang/plato/results/test/model/lenet5_287_1127979.pth.
[INFO][10:36:17]: [Client #287] Model trained.
[INFO][10:36:17]: [Client #287] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:36:17]: [Server #1127936] Received 0.24 MB of payload data from client #287 (simulated).
[INFO][10:36:18]: [Client #409] Loading a model from /data/ykang/plato/results/test/model/lenet5_409_1127977.pth.
[INFO][10:36:18]: [Client #409] Model trained.
[INFO][10:36:18]: [Client #409] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:36:18]: [Server #1127936] Received 0.24 MB of payload data from client #409 (simulated).
[INFO][10:36:18]: [Server #1127936] Selecting client #482 for training.
[INFO][10:36:18]: [Server #1127936] Sending the current model to client #482 (simulated).
[INFO][10:36:18]: [Server #1127936] Sending 0.24 MB of payload data to client #482 (simulated).
[INFO][10:36:18]: [Client #482] Selected by the server.
[INFO][10:36:18]: [Client #482] Loading its data source...
[INFO][10:36:18]: [Client #482] Dataset size: 60000
[INFO][10:36:18]: [Client #482] Sampler: noniid
[INFO][10:36:18]: [Client #482] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:36:18]: [93m[1m[Client #482] Started training in communication round #83.[0m
[INFO][10:36:20]: [Client #482] Loading the dataset.
[INFO][10:36:25]: [Client #482] Epoch: [1/5][0/10]	Loss: 0.000298
[INFO][10:36:25]: [Client #482] Epoch: [2/5][0/10]	Loss: 0.000116
[INFO][10:36:25]: [Client #482] Epoch: [3/5][0/10]	Loss: 0.000057
[INFO][10:36:25]: [Client #482] Epoch: [4/5][0/10]	Loss: 0.000044
[INFO][10:36:25]: [Client #482] Epoch: [5/5][0/10]	Loss: 0.000046
[INFO][10:36:25]: [Client #482] Model saved to /data/ykang/plato/results/test/model/lenet5_482_1127977.pth.
[INFO][10:36:26]: [Client #482] Loading a model from /data/ykang/plato/results/test/model/lenet5_482_1127977.pth.
[INFO][10:36:26]: [Client #482] Model trained.
[INFO][10:36:26]: [Client #482] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:36:26]: [Server #1127936] Received 0.24 MB of payload data from client #482 (simulated).
[INFO][10:36:26]: [Server #1127936] Adding client #232 to the list of clients for aggregation.
[INFO][10:36:26]: [Server #1127936] Adding client #486 to the list of clients for aggregation.
[INFO][10:36:26]: [Server #1127936] Adding client #378 to the list of clients for aggregation.
[INFO][10:36:26]: [Server #1127936] Adding client #63 to the list of clients for aggregation.
[INFO][10:36:26]: [Server #1127936] Adding client #38 to the list of clients for aggregation.
[INFO][10:36:26]: [Server #1127936] Adding client #437 to the list of clients for aggregation.
[INFO][10:36:26]: [Server #1127936] Adding client #409 to the list of clients for aggregation.
[INFO][10:36:26]: [Server #1127936] Adding client #440 to the list of clients for aggregation.
[INFO][10:36:26]: [Server #1127936] Adding client #271 to the list of clients for aggregation.
[INFO][10:36:26]: [Server #1127936] Adding client #235 to the list of clients for aggregation.
[INFO][10:36:26]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00123826 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.0140447  0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01530292 0.         0.
 0.00170557 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01513585 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.0027142
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00120617 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.0013517  0.
 0.         0.00209014 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.0208362
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 0. 1. 0. 0. 0. 1. 0. 0. 3. 1. 0. 0. 0. 1. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 2. 0. 1. 0.
 0. 0. 1. 0. 2. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 2. 1. 1. 0. 0.
 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 2. 1. 6. 0. 0. 2. 1. 0. 1. 1. 2. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 3. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 1. 6. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 2. 0. 0.
 1. 1. 1. 2. 0. 0. 0. 2. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 1. 2. 0. 4. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1.
 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 4. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0.
 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0.
 1. 0. 1. 0. 4. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 3. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 0. 1. 0. 6. 1. 1.
 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 6. 0. 0. 0. 1. 0. 1. 1. 2. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 2. 2. 1. 2. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00123826 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.0140447  0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.01530292 0.         0.
 0.00170557 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01513585 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.0027142
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00120617 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.0013517  0.
 0.         0.00209014 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.0208362
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:36:28]: [Server #1127936] Global model accuracy: 96.12%

[INFO][10:36:28]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_83.pth.
[INFO][10:36:28]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_83.pth.
[INFO][10:36:28]: [93m[1m
[Server #1127936] Starting round 84/100.[0m
[0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  2e-05  4e-10  4e-10
 6:  6.8876e+00  6.8875e+00  2e-05  3e-10  3e-10
 7:  6.8875e+00  6.8875e+00  2e-05  5e-10  1e-11
 8:  6.8875e+00  6.8875e+00  1e-05  4e-10  9e-12
 9:  6.8875e+00  6.8875e+00  8e-06  8e-10  2e-11
10:  6.8875e+00  6.8875e+00  5e-07  1e-09  3e-11
Optimal solution found.
The calculated probability is:  [5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.47959345e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 2.02101394e-04 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 2.67634089e-04 5.11137563e-05 5.11137563e-05 5.11136756e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11073985e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.99342177e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137159e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.51597004e-05
 5.11137563e-05 5.11137563e-05 5.11136351e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 9.74621323e-01
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05 5.11137563e-05 5.11137563e-05
 5.11137563e-05 5.11137563e-05]
current clients pool:  [INFO][10:36:29]: [Server #1127936] Selected clients: [486  23 115 481 250  73 257 470 431 194]
[INFO][10:36:29]: [Server #1127936] Selecting client #486 for training.
[INFO][10:36:29]: [Server #1127936] Sending the current model to client #486 (simulated).
[INFO][10:36:29]: [Server #1127936] Sending 0.24 MB of payload data to client #486 (simulated).
[INFO][10:36:29]: [Server #1127936] Selecting client #23 for training.
[INFO][10:36:29]: [Server #1127936] Sending the current model to client #23 (simulated).
[INFO][10:36:29]: [Server #1127936] Sending 0.24 MB of payload data to client #23 (simulated).
[INFO][10:36:29]: [Server #1127936] Selecting client #115 for training.
[INFO][10:36:29]: [Server #1127936] Sending the current model to client #115 (simulated).
[INFO][10:36:29]: [Client #486] Selected by the server.
[INFO][10:36:29]: [Client #486] Loading its data source...
[INFO][10:36:29]: [Client #486] Dataset size: 60000
[INFO][10:36:29]: [Client #486] Sampler: noniid
[INFO][10:36:29]: [Server #1127936] Sending 0.24 MB of payload data to client #115 (simulated).
[INFO][10:36:29]: [Client #23] Selected by the server.
[INFO][10:36:29]: [Client #23] Loading its data source...
[INFO][10:36:29]: [Client #23] Dataset size: 60000
[INFO][10:36:29]: [Client #23] Sampler: noniid
[INFO][10:36:29]: [Client #115] Selected by the server.
[INFO][10:36:29]: [Client #115] Loading its data source...
[INFO][10:36:29]: [Client #115] Dataset size: 60000
[INFO][10:36:29]: [Client #115] Sampler: noniid
[INFO][10:36:29]: [Client #486] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:36:29]: [Client #23] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:36:29]: [Client #115] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:36:29]: [93m[1m[Client #23] Started training in communication round #84.[0m
[INFO][10:36:29]: [93m[1m[Client #115] Started training in communication round #84.[0m
[INFO][10:36:29]: [93m[1m[Client #486] Started training in communication round #84.[0m
[INFO][10:36:31]: [Client #23] Loading the dataset.
[INFO][10:36:31]: [Client #486] Loading the dataset.
[INFO][10:36:31]: [Client #115] Loading the dataset.
[INFO][10:36:37]: [Client #23] Epoch: [1/5][0/10]	Loss: 0.000120
[INFO][10:36:38]: [Client #486] Epoch: [1/5][0/10]	Loss: 0.000858
[INFO][10:36:38]: [Client #23] Epoch: [2/5][0/10]	Loss: 0.000331
[INFO][10:36:38]: [Client #115] Epoch: [1/5][0/10]	Loss: 0.000849
[INFO][10:36:38]: [Client #486] Epoch: [2/5][0/10]	Loss: 0.000024
[INFO][10:36:38]: [Client #115] Epoch: [2/5][0/10]	Loss: 0.000099
[INFO][10:36:38]: [Client #23] Epoch: [3/5][0/10]	Loss: 0.000045
[INFO][10:36:38]: [Client #486] Epoch: [3/5][0/10]	Loss: 0.110615
[INFO][10:36:38]: [Client #115] Epoch: [3/5][0/10]	Loss: 0.000151
[INFO][10:36:38]: [Client #486] Epoch: [4/5][0/10]	Loss: 0.000018
[INFO][10:36:38]: [Client #23] Epoch: [4/5][0/10]	Loss: 0.000526
[INFO][10:36:38]: [Client #115] Epoch: [4/5][0/10]	Loss: 0.018963
[INFO][10:36:38]: [Client #486] Epoch: [5/5][0/10]	Loss: 0.000435
[INFO][10:36:38]: [Client #486] Model saved to /data/ykang/plato/results/test/model/lenet5_486_1127977.pth.
[INFO][10:36:38]: [Client #115] Epoch: [5/5][0/10]	Loss: 0.049666
[INFO][10:36:38]: [Client #23] Epoch: [5/5][0/10]	Loss: 0.000598
[INFO][10:36:38]: [Client #115] Model saved to /data/ykang/plato/results/test/model/lenet5_115_1127979.pth.
[INFO][10:36:38]: [Client #23] Model saved to /data/ykang/plato/results/test/model/lenet5_23_1127978.pth.
[INFO][10:36:39]: [Client #486] Loading a model from /data/ykang/plato/results/test/model/lenet5_486_1127977.pth.
[INFO][10:36:39]: [Client #486] Model trained.
[INFO][10:36:39]: [Client #486] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:36:39]: [Server #1127936] Received 0.24 MB of payload data from client #486 (simulated).
[INFO][10:36:39]: [Client #23] Loading a model from /data/ykang/plato/results/test/model/lenet5_23_1127978.pth.
[INFO][10:36:39]: [Client #23] Model trained.
[INFO][10:36:39]: [Client #23] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:36:39]: [Server #1127936] Received 0.24 MB of payload data from client #23 (simulated).
[INFO][10:36:39]: [Client #115] Loading a model from /data/ykang/plato/results/test/model/lenet5_115_1127979.pth.
[INFO][10:36:39]: [Client #115] Model trained.
[INFO][10:36:39]: [Client #115] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:36:39]: [Server #1127936] Received 0.24 MB of payload data from client #115 (simulated).
[INFO][10:36:39]: [Server #1127936] Selecting client #481 for training.
[INFO][10:36:39]: [Server #1127936] Sending the current model to client #481 (simulated).
[INFO][10:36:39]: [Server #1127936] Sending 0.24 MB of payload data to client #481 (simulated).
[INFO][10:36:39]: [Server #1127936] Selecting client #250 for training.
[INFO][10:36:39]: [Server #1127936] Sending the current model to client #250 (simulated).
[INFO][10:36:39]: [Server #1127936] Sending 0.24 MB of payload data to client #250 (simulated).
[INFO][10:36:39]: [Server #1127936] Selecting client #73 for training.
[INFO][10:36:39]: [Server #1127936] Sending the current model to client #73 (simulated).
[INFO][10:36:39]: [Client #481] Selected by the server.
[INFO][10:36:39]: [Client #481] Loading its data source...
[INFO][10:36:39]: [Client #481] Dataset size: 60000
[INFO][10:36:39]: [Client #481] Sampler: noniid
[INFO][10:36:39]: [Server #1127936] Sending 0.24 MB of payload data to client #73 (simulated).
[INFO][10:36:39]: [Client #250] Selected by the server.
[INFO][10:36:39]: [Client #250] Loading its data source...
[INFO][10:36:39]: [Client #250] Dataset size: 60000
[INFO][10:36:39]: [Client #250] Sampler: noniid
[INFO][10:36:39]: [Client #73] Selected by the server.
[INFO][10:36:39]: [Client #73] Loading its data source...
[INFO][10:36:39]: [Client #73] Dataset size: 60000
[INFO][10:36:39]: [Client #73] Sampler: noniid
[INFO][10:36:39]: [Client #481] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:36:39]: [Client #73] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:36:39]: [93m[1m[Client #481] Started training in communication round #84.[0m
[INFO][10:36:39]: [Client #250] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:36:39]: [93m[1m[Client #73] Started training in communication round #84.[0m
[INFO][10:36:39]: [93m[1m[Client #250] Started training in communication round #84.[0m
[INFO][10:36:41]: [Client #73] Loading the dataset.
[INFO][10:36:41]: [Client #481] Loading the dataset.
[INFO][10:36:41]: [Client #250] Loading the dataset.
[INFO][10:36:47]: [Client #73] Epoch: [1/5][0/10]	Loss: 0.002812
[INFO][10:36:47]: [Client #250] Epoch: [1/5][0/10]	Loss: 0.005344
[INFO][10:36:47]: [Client #481] Epoch: [1/5][0/10]	Loss: 0.000821
[INFO][10:36:47]: [Client #73] Epoch: [2/5][0/10]	Loss: 0.000066
[INFO][10:36:47]: [Client #250] Epoch: [2/5][0/10]	Loss: 0.000102
[INFO][10:36:47]: [Client #481] Epoch: [2/5][0/10]	Loss: 0.000063
[INFO][10:36:47]: [Client #73] Epoch: [3/5][0/10]	Loss: 0.000328
[INFO][10:36:48]: [Client #250] Epoch: [3/5][0/10]	Loss: 0.000145
[INFO][10:36:48]: [Client #481] Epoch: [3/5][0/10]	Loss: 0.023652
[INFO][10:36:48]: [Client #73] Epoch: [4/5][0/10]	Loss: 0.000003
[INFO][10:36:48]: [Client #250] Epoch: [4/5][0/10]	Loss: 0.018937
[INFO][10:36:48]: [Client #73] Epoch: [5/5][0/10]	Loss: 0.000121
[INFO][10:36:48]: [Client #481] Epoch: [4/5][0/10]	Loss: 0.000248
[INFO][10:36:48]: [Client #73] Model saved to /data/ykang/plato/results/test/model/lenet5_73_1127979.pth.
[INFO][10:36:48]: [Client #250] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:36:48]: [Client #481] Epoch: [5/5][0/10]	Loss: 0.000348
[INFO][10:36:48]: [Client #250] Model saved to /data/ykang/plato/results/test/model/lenet5_250_1127978.pth.
[INFO][10:36:48]: [Client #481] Model saved to /data/ykang/plato/results/test/model/lenet5_481_1127977.pth.
[INFO][10:36:49]: [Client #73] Loading a model from /data/ykang/plato/results/test/model/lenet5_73_1127979.pth.
[INFO][10:36:49]: [Client #73] Model trained.
[INFO][10:36:49]: [Client #73] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:36:49]: [Server #1127936] Received 0.24 MB of payload data from client #73 (simulated).
[INFO][10:36:49]: [Client #250] Loading a model from /data/ykang/plato/results/test/model/lenet5_250_1127978.pth.
[INFO][10:36:49]: [Client #250] Model trained.
[INFO][10:36:49]: [Client #250] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:36:49]: [Server #1127936] Received 0.24 MB of payload data from client #250 (simulated).
[INFO][10:36:49]: [Client #481] Loading a model from /data/ykang/plato/results/test/model/lenet5_481_1127977.pth.
[INFO][10:36:49]: [Client #481] Model trained.
[INFO][10:36:49]: [Client #481] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:36:49]: [Server #1127936] Received 0.24 MB of payload data from client #481 (simulated).
[INFO][10:36:49]: [Server #1127936] Selecting client #257 for training.
[INFO][10:36:49]: [Server #1127936] Sending the current model to client #257 (simulated).
[INFO][10:36:49]: [Server #1127936] Sending 0.24 MB of payload data to client #257 (simulated).
[INFO][10:36:49]: [Server #1127936] Selecting client #470 for training.
[INFO][10:36:49]: [Server #1127936] Sending the current model to client #470 (simulated).
[INFO][10:36:49]: [Server #1127936] Sending 0.24 MB of payload data to client #470 (simulated).
[INFO][10:36:49]: [Server #1127936] Selecting client #431 for training.
[INFO][10:36:49]: [Server #1127936] Sending the current model to client #431 (simulated).
[INFO][10:36:49]: [Client #257] Selected by the server.
[INFO][10:36:49]: [Client #257] Loading its data source...
[INFO][10:36:49]: [Client #257] Dataset size: 60000
[INFO][10:36:49]: [Client #257] Sampler: noniid
[INFO][10:36:49]: [Server #1127936] Sending 0.24 MB of payload data to client #431 (simulated).
[INFO][10:36:49]: [Client #470] Selected by the server.
[INFO][10:36:49]: [Client #470] Loading its data source...
[INFO][10:36:49]: [Client #431] Selected by the server.
[INFO][10:36:49]: [Client #470] Dataset size: 60000
[INFO][10:36:49]: [Client #431] Loading its data source...
[INFO][10:36:49]: [Client #470] Sampler: noniid
[INFO][10:36:49]: [Client #431] Dataset size: 60000
[INFO][10:36:49]: [Client #431] Sampler: noniid
[INFO][10:36:49]: [Client #257] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:36:49]: [Client #431] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:36:49]: [Client #470] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:36:49]: [93m[1m[Client #257] Started training in communication round #84.[0m
[INFO][10:36:49]: [93m[1m[Client #470] Started training in communication round #84.[0m
[INFO][10:36:49]: [93m[1m[Client #431] Started training in communication round #84.[0m
[INFO][10:36:51]: [Client #470] Loading the dataset.
[INFO][10:36:51]: [Client #431] Loading the dataset.
[INFO][10:36:51]: [Client #257] Loading the dataset.
[INFO][10:36:57]: [Client #257] Epoch: [1/5][0/10]	Loss: 0.000287
[INFO][10:36:57]: [Client #470] Epoch: [1/5][0/10]	Loss: 0.003977
[INFO][10:36:57]: [Client #431] Epoch: [1/5][0/10]	Loss: 0.000409
[INFO][10:36:57]: [Client #257] Epoch: [2/5][0/10]	Loss: 0.000139
[INFO][10:36:57]: [Client #470] Epoch: [2/5][0/10]	Loss: 0.000125
[INFO][10:36:57]: [Client #431] Epoch: [2/5][0/10]	Loss: 0.000148
[INFO][10:36:57]: [Client #257] Epoch: [3/5][0/10]	Loss: 0.000082
[INFO][10:36:57]: [Client #470] Epoch: [3/5][0/10]	Loss: 0.000088
[INFO][10:36:58]: [Client #431] Epoch: [3/5][0/10]	Loss: 0.000009
[INFO][10:36:58]: [Client #257] Epoch: [4/5][0/10]	Loss: 0.003253
[INFO][10:36:58]: [Client #470] Epoch: [4/5][0/10]	Loss: 0.000034
[INFO][10:36:58]: [Client #257] Epoch: [5/5][0/10]	Loss: 0.080852
[INFO][10:36:58]: [Client #431] Epoch: [4/5][0/10]	Loss: 0.000005
[INFO][10:36:58]: [Client #470] Epoch: [5/5][0/10]	Loss: 0.000617
[INFO][10:36:58]: [Client #257] Model saved to /data/ykang/plato/results/test/model/lenet5_257_1127977.pth.
[INFO][10:36:58]: [Client #470] Model saved to /data/ykang/plato/results/test/model/lenet5_470_1127978.pth.
[INFO][10:36:58]: [Client #431] Epoch: [5/5][0/10]	Loss: 0.000001
[INFO][10:36:58]: [Client #431] Model saved to /data/ykang/plato/results/test/model/lenet5_431_1127979.pth.
[INFO][10:36:58]: [Client #257] Loading a model from /data/ykang/plato/results/test/model/lenet5_257_1127977.pth.
[INFO][10:36:58]: [Client #257] Model trained.
[INFO][10:36:58]: [Client #257] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:36:58]: [Server #1127936] Received 0.24 MB of payload data from client #257 (simulated).
[INFO][10:36:59]: [Client #470] Loading a model from /data/ykang/plato/results/test/model/lenet5_470_1127978.pth.
[INFO][10:36:59]: [Client #470] Model trained.
[INFO][10:36:59]: [Client #470] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:36:59]: [Server #1127936] Received 0.24 MB of payload data from client #470 (simulated).
[INFO][10:36:59]: [Client #431] Loading a model from /data/ykang/plato/results/test/model/lenet5_431_1127979.pth.
[INFO][10:36:59]: [Client #431] Model trained.
[INFO][10:36:59]: [Client #431] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:36:59]: [Server #1127936] Received 0.24 MB of payload data from client #431 (simulated).
[INFO][10:36:59]: [Server #1127936] Selecting client #194 for training.
[INFO][10:36:59]: [Server #1127936] Sending the current model to client #194 (simulated).
[INFO][10:36:59]: [Server #1127936] Sending 0.24 MB of payload data to client #194 (simulated).
[INFO][10:36:59]: [Client #194] Selected by the server.
[INFO][10:36:59]: [Client #194] Loading its data source...
[INFO][10:36:59]: [Client #194] Dataset size: 60000
[INFO][10:36:59]: [Client #194] Sampler: noniid
[INFO][10:36:59]: [Client #194] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:36:59]: [93m[1m[Client #194] Started training in communication round #84.[0m
[INFO][10:37:01]: [Client #194] Loading the dataset.
[INFO][10:37:06]: [Client #194] Epoch: [1/5][0/10]	Loss: 0.001055
[INFO][10:37:06]: [Client #194] Epoch: [2/5][0/10]	Loss: 0.000210
[INFO][10:37:06]: [Client #194] Epoch: [3/5][0/10]	Loss: 0.000023
[INFO][10:37:06]: [Client #194] Epoch: [4/5][0/10]	Loss: 0.000382
[INFO][10:37:07]: [Client #194] Epoch: [5/5][0/10]	Loss: 0.000077
[INFO][10:37:07]: [Client #194] Model saved to /data/ykang/plato/results/test/model/lenet5_194_1127977.pth.
[INFO][10:37:07]: [Client #194] Loading a model from /data/ykang/plato/results/test/model/lenet5_194_1127977.pth.
[INFO][10:37:07]: [Client #194] Model trained.
[INFO][10:37:07]: [Client #194] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:37:07]: [Server #1127936] Received 0.24 MB of payload data from client #194 (simulated).
[INFO][10:37:07]: [Server #1127936] Adding client #287 to the list of clients for aggregation.
[INFO][10:37:07]: [Server #1127936] Adding client #420 to the list of clients for aggregation.
[INFO][10:37:07]: [Server #1127936] Adding client #435 to the list of clients for aggregation.
[INFO][10:37:07]: [Server #1127936] Adding client #240 to the list of clients for aggregation.
[INFO][10:37:07]: [Server #1127936] Adding client #212 to the list of clients for aggregation.
[INFO][10:37:07]: [Server #1127936] Adding client #275 to the list of clients for aggregation.
[INFO][10:37:07]: [Server #1127936] Adding client #482 to the list of clients for aggregation.
[INFO][10:37:07]: [Server #1127936] Adding client #41 to the list of clients for aggregation.
[INFO][10:37:07]: [Server #1127936] Adding client #367 to the list of clients for aggregation.
[INFO][10:37:07]: [Server #1127936] Adding client #115 to the list of clients for aggregation.
[INFO][10:37:07]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00784183 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00682694 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01622725 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00255484
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00104625 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00872048 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00790462 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00387518
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.0023294  0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00183831 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 0. 1. 0. 0. 0. 1. 0. 0. 3. 1. 0. 0. 0. 1. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 2. 0. 1. 0.
 0. 0. 1. 0. 2. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 2. 1. 1. 0. 0.
 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 2. 1. 6. 0. 0. 2. 1. 0. 1. 1. 2. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 3. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 1. 6. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 2. 0. 0.
 1. 1. 1. 2. 0. 0. 0. 2. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 1. 2. 0. 4. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1.
 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 4. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0.
 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0.
 1. 0. 1. 0. 4. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 3. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.
 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 0. 1. 0. 6. 1. 1.
 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 6. 0. 0. 0. 1. 0. 1. 1. 2. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 2. 2. 1. 2. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00784183 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00682694 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01622725 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00255484
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00104625 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00872048 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00790462 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00387518
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.0023294  0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00183831 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:37:09]: [Server #1127936] Global model accuracy: 96.17%

[INFO][10:37:09]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_84.pth.
[INFO][10:37:09]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_84.pth.
[INFO][10:37:09]: [93m[1m
[Server #1127936] Starting round 85/100.[0m
[0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  4e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  5e-05  9e-10  9e-10
 6:  6.8876e+00  6.8875e+00  5e-05  4e-10  4e-10
 7:  6.8875e+00  6.8875e+00  4e-05  9e-10  4e-11
 8:  6.8875e+00  6.8875e+00  3e-05  9e-10  5e-11
 9:  6.8875e+00  6.8875e+00  1e-05  7e-09  4e-10
10:  6.8875e+00  6.8875e+00  4e-06  1e-09  4e-11
Optimal solution found.
The calculated probability is:  [1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 4.86114686e-02
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70131797e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 6.83843155e-01
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.79999157e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.86915549e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 2.09286110e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.84593204e-01 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.85559947e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.79082842e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.77118652e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04 1.70133498e-04 1.70133498e-04
 1.70133498e-04 1.70133498e-04]
current clients pool:  [INFO][10:37:10]: [Server #1127936] Selected clients: [212 367  63  41 363  25 456 164 400 189]
[INFO][10:37:10]: [Server #1127936] Selecting client #212 for training.
[INFO][10:37:10]: [Server #1127936] Sending the current model to client #212 (simulated).
[INFO][10:37:10]: [Server #1127936] Sending 0.24 MB of payload data to client #212 (simulated).
[INFO][10:37:10]: [Server #1127936] Selecting client #367 for training.
[INFO][10:37:10]: [Server #1127936] Sending the current model to client #367 (simulated).
[INFO][10:37:10]: [Server #1127936] Sending 0.24 MB of payload data to client #367 (simulated).
[INFO][10:37:10]: [Server #1127936] Selecting client #63 for training.
[INFO][10:37:10]: [Server #1127936] Sending the current model to client #63 (simulated).
[INFO][10:37:10]: [Client #212] Selected by the server.
[INFO][10:37:10]: [Client #212] Loading its data source...
[INFO][10:37:10]: [Client #212] Dataset size: 60000
[INFO][10:37:10]: [Client #212] Sampler: noniid
[INFO][10:37:10]: [Server #1127936] Sending 0.24 MB of payload data to client #63 (simulated).
[INFO][10:37:10]: [Client #367] Selected by the server.
[INFO][10:37:10]: [Client #367] Loading its data source...
[INFO][10:37:10]: [Client #367] Dataset size: 60000
[INFO][10:37:10]: [Client #367] Sampler: noniid
[INFO][10:37:10]: [Client #63] Selected by the server.
[INFO][10:37:10]: [Client #212] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:37:10]: [Client #63] Loading its data source...
[INFO][10:37:10]: [Client #63] Dataset size: 60000
[INFO][10:37:10]: [Client #63] Sampler: noniid
[INFO][10:37:10]: [Client #367] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:37:10]: [93m[1m[Client #212] Started training in communication round #85.[0m
[INFO][10:37:10]: [Client #63] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:37:10]: [93m[1m[Client #367] Started training in communication round #85.[0m
[INFO][10:37:10]: [93m[1m[Client #63] Started training in communication round #85.[0m
[INFO][10:37:12]: [Client #63] Loading the dataset.
[INFO][10:37:12]: [Client #367] Loading the dataset.
[INFO][10:37:12]: [Client #212] Loading the dataset.
[INFO][10:37:18]: [Client #63] Epoch: [1/5][0/10]	Loss: 0.002795
[INFO][10:37:18]: [Client #367] Epoch: [1/5][0/10]	Loss: 0.001790
[INFO][10:37:18]: [Client #63] Epoch: [2/5][0/10]	Loss: 0.000470
[INFO][10:37:18]: [Client #212] Epoch: [1/5][0/10]	Loss: 0.002745
[INFO][10:37:19]: [Client #367] Epoch: [2/5][0/10]	Loss: 0.001169
[INFO][10:37:19]: [Client #63] Epoch: [3/5][0/10]	Loss: 0.000094
[INFO][10:37:19]: [Client #367] Epoch: [3/5][0/10]	Loss: 0.000001
[INFO][10:37:19]: [Client #212] Epoch: [2/5][0/10]	Loss: 0.000037
[INFO][10:37:19]: [Client #63] Epoch: [4/5][0/10]	Loss: 0.000715
[INFO][10:37:19]: [Client #367] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:37:19]: [Client #212] Epoch: [3/5][0/10]	Loss: 0.000001
[INFO][10:37:19]: [Client #63] Epoch: [5/5][0/10]	Loss: 0.007834
[INFO][10:37:19]: [Client #63] Model saved to /data/ykang/plato/results/test/model/lenet5_63_1127979.pth.
[INFO][10:37:19]: [Client #212] Epoch: [4/5][0/10]	Loss: 0.000016
[INFO][10:37:19]: [Client #367] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:37:19]: [Client #367] Model saved to /data/ykang/plato/results/test/model/lenet5_367_1127978.pth.
[INFO][10:37:19]: [Client #212] Epoch: [5/5][0/10]	Loss: 0.000042
[INFO][10:37:19]: [Client #212] Model saved to /data/ykang/plato/results/test/model/lenet5_212_1127977.pth.
[INFO][10:37:20]: [Client #63] Loading a model from /data/ykang/plato/results/test/model/lenet5_63_1127979.pth.
[INFO][10:37:20]: [Client #63] Model trained.
[INFO][10:37:20]: [Client #63] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:37:20]: [Server #1127936] Received 0.24 MB of payload data from client #63 (simulated).
[INFO][10:37:20]: [Client #367] Loading a model from /data/ykang/plato/results/test/model/lenet5_367_1127978.pth.
[INFO][10:37:20]: [Client #367] Model trained.
[INFO][10:37:20]: [Client #367] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:37:20]: [Server #1127936] Received 0.24 MB of payload data from client #367 (simulated).
[INFO][10:37:20]: [Client #212] Loading a model from /data/ykang/plato/results/test/model/lenet5_212_1127977.pth.
[INFO][10:37:20]: [Client #212] Model trained.
[INFO][10:37:20]: [Client #212] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:37:20]: [Server #1127936] Received 0.24 MB of payload data from client #212 (simulated).
[INFO][10:37:20]: [Server #1127936] Selecting client #41 for training.
[INFO][10:37:20]: [Server #1127936] Sending the current model to client #41 (simulated).
[INFO][10:37:20]: [Server #1127936] Sending 0.24 MB of payload data to client #41 (simulated).
[INFO][10:37:20]: [Server #1127936] Selecting client #363 for training.
[INFO][10:37:20]: [Server #1127936] Sending the current model to client #363 (simulated).
[INFO][10:37:20]: [Server #1127936] Sending 0.24 MB of payload data to client #363 (simulated).
[INFO][10:37:20]: [Server #1127936] Selecting client #25 for training.
[INFO][10:37:20]: [Server #1127936] Sending the current model to client #25 (simulated).
[INFO][10:37:20]: [Client #41] Selected by the server.
[INFO][10:37:20]: [Client #41] Loading its data source...
[INFO][10:37:20]: [Client #41] Dataset size: 60000
[INFO][10:37:20]: [Client #41] Sampler: noniid
[INFO][10:37:20]: [Server #1127936] Sending 0.24 MB of payload data to client #25 (simulated).
[INFO][10:37:20]: [Client #363] Selected by the server.
[INFO][10:37:20]: [Client #363] Loading its data source...
[INFO][10:37:20]: [Client #363] Dataset size: 60000
[INFO][10:37:20]: [Client #363] Sampler: noniid
[INFO][10:37:20]: [Client #25] Selected by the server.
[INFO][10:37:20]: [Client #25] Loading its data source...
[INFO][10:37:20]: [Client #25] Dataset size: 60000
[INFO][10:37:20]: [Client #25] Sampler: noniid
[INFO][10:37:20]: [Client #41] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:37:20]: [Client #25] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:37:20]: [Client #363] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:37:20]: [93m[1m[Client #41] Started training in communication round #85.[0m
[INFO][10:37:20]: [93m[1m[Client #25] Started training in communication round #85.[0m
[INFO][10:37:20]: [93m[1m[Client #363] Started training in communication round #85.[0m
[INFO][10:37:22]: [Client #363] Loading the dataset.
[INFO][10:37:22]: [Client #41] Loading the dataset.
[INFO][10:37:22]: [Client #25] Loading the dataset.
[INFO][10:37:28]: [Client #363] Epoch: [1/5][0/10]	Loss: 0.002588
[INFO][10:37:28]: [Client #41] Epoch: [1/5][0/10]	Loss: 0.002543
[INFO][10:37:28]: [Client #363] Epoch: [2/5][0/10]	Loss: 0.000231
[INFO][10:37:28]: [Client #41] Epoch: [2/5][0/10]	Loss: 0.000196
[INFO][10:37:28]: [Client #25] Epoch: [1/5][0/10]	Loss: 0.001814
[INFO][10:37:28]: [Client #41] Epoch: [3/5][0/10]	Loss: 0.000878
[INFO][10:37:28]: [Client #363] Epoch: [3/5][0/10]	Loss: 0.000006
[INFO][10:37:29]: [Client #41] Epoch: [4/5][0/10]	Loss: 0.016270
[INFO][10:37:29]: [Client #25] Epoch: [2/5][0/10]	Loss: 0.000689
[INFO][10:37:29]: [Client #363] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:37:29]: [Client #25] Epoch: [3/5][0/10]	Loss: 0.000015
[INFO][10:37:29]: [Client #41] Epoch: [5/5][0/10]	Loss: 0.009283
[INFO][10:37:29]: [Client #41] Model saved to /data/ykang/plato/results/test/model/lenet5_41_1127977.pth.
[INFO][10:37:29]: [Client #363] Epoch: [5/5][0/10]	Loss: 0.000177
[INFO][10:37:29]: [Client #363] Model saved to /data/ykang/plato/results/test/model/lenet5_363_1127978.pth.
[INFO][10:37:29]: [Client #25] Epoch: [4/5][0/10]	Loss: 0.000006
[INFO][10:37:29]: [Client #25] Epoch: [5/5][0/10]	Loss: 0.000271
[INFO][10:37:29]: [Client #25] Model saved to /data/ykang/plato/results/test/model/lenet5_25_1127979.pth.
[INFO][10:37:29]: [Client #41] Loading a model from /data/ykang/plato/results/test/model/lenet5_41_1127977.pth.
[INFO][10:37:29]: [Client #41] Model trained.
[INFO][10:37:29]: [Client #41] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:37:29]: [Server #1127936] Received 0.24 MB of payload data from client #41 (simulated).
[INFO][10:37:30]: [Client #363] Loading a model from /data/ykang/plato/results/test/model/lenet5_363_1127978.pth.
[INFO][10:37:30]: [Client #363] Model trained.
[INFO][10:37:30]: [Client #363] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:37:30]: [Server #1127936] Received 0.24 MB of payload data from client #363 (simulated).
[INFO][10:37:30]: [Client #25] Loading a model from /data/ykang/plato/results/test/model/lenet5_25_1127979.pth.
[INFO][10:37:30]: [Client #25] Model trained.
[INFO][10:37:30]: [Client #25] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:37:30]: [Server #1127936] Received 0.24 MB of payload data from client #25 (simulated).
[INFO][10:37:30]: [Server #1127936] Selecting client #456 for training.
[INFO][10:37:30]: [Server #1127936] Sending the current model to client #456 (simulated).
[INFO][10:37:30]: [Server #1127936] Sending 0.24 MB of payload data to client #456 (simulated).
[INFO][10:37:30]: [Server #1127936] Selecting client #164 for training.
[INFO][10:37:30]: [Server #1127936] Sending the current model to client #164 (simulated).
[INFO][10:37:30]: [Server #1127936] Sending 0.24 MB of payload data to client #164 (simulated).
[INFO][10:37:30]: [Server #1127936] Selecting client #400 for training.
[INFO][10:37:30]: [Server #1127936] Sending the current model to client #400 (simulated).
[INFO][10:37:30]: [Client #456] Selected by the server.
[INFO][10:37:30]: [Client #456] Loading its data source...
[INFO][10:37:30]: [Client #456] Dataset size: 60000
[INFO][10:37:30]: [Client #456] Sampler: noniid
[INFO][10:37:30]: [Server #1127936] Sending 0.24 MB of payload data to client #400 (simulated).
[INFO][10:37:30]: [Client #164] Selected by the server.
[INFO][10:37:30]: [Client #164] Loading its data source...
[INFO][10:37:30]: [Client #164] Dataset size: 60000
[INFO][10:37:30]: [Client #164] Sampler: noniid
[INFO][10:37:30]: [Client #400] Selected by the server.
[INFO][10:37:30]: [Client #400] Loading its data source...
[INFO][10:37:30]: [Client #400] Dataset size: 60000
[INFO][10:37:30]: [Client #400] Sampler: noniid
[INFO][10:37:30]: [Client #456] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:37:30]: [Client #164] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:37:30]: [93m[1m[Client #456] Started training in communication round #85.[0m
[INFO][10:37:30]: [Client #400] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:37:30]: [93m[1m[Client #164] Started training in communication round #85.[0m
[INFO][10:37:30]: [93m[1m[Client #400] Started training in communication round #85.[0m
[INFO][10:37:32]: [Client #456] Loading the dataset.
[INFO][10:37:32]: [Client #164] Loading the dataset.
[INFO][10:37:32]: [Client #400] Loading the dataset.
[INFO][10:37:38]: [Client #456] Epoch: [1/5][0/10]	Loss: 0.001730
[INFO][10:37:38]: [Client #400] Epoch: [1/5][0/10]	Loss: 0.000817
[INFO][10:37:38]: [Client #456] Epoch: [2/5][0/10]	Loss: 0.000445
[INFO][10:37:38]: [Client #164] Epoch: [1/5][0/10]	Loss: 0.008972
[INFO][10:37:38]: [Client #456] Epoch: [3/5][0/10]	Loss: 0.000022
[INFO][10:37:38]: [Client #400] Epoch: [2/5][0/10]	Loss: 0.004764
[INFO][10:37:38]: [Client #164] Epoch: [2/5][0/10]	Loss: 0.001541
[INFO][10:37:39]: [Client #456] Epoch: [4/5][0/10]	Loss: 0.000252
[INFO][10:37:39]: [Client #164] Epoch: [3/5][0/10]	Loss: 0.000000
[INFO][10:37:39]: [Client #400] Epoch: [3/5][0/10]	Loss: 0.000075
[INFO][10:37:39]: [Client #456] Epoch: [5/5][0/10]	Loss: 0.001293
[INFO][10:37:39]: [Client #164] Epoch: [4/5][0/10]	Loss: 0.000095
[INFO][10:37:39]: [Client #456] Model saved to /data/ykang/plato/results/test/model/lenet5_456_1127977.pth.
[INFO][10:37:39]: [Client #400] Epoch: [4/5][0/10]	Loss: 0.000120
[INFO][10:37:39]: [Client #164] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:37:39]: [Client #164] Model saved to /data/ykang/plato/results/test/model/lenet5_164_1127978.pth.
[INFO][10:37:39]: [Client #400] Epoch: [5/5][0/10]	Loss: 0.000015
[INFO][10:37:39]: [Client #400] Model saved to /data/ykang/plato/results/test/model/lenet5_400_1127979.pth.
[INFO][10:37:40]: [Client #456] Loading a model from /data/ykang/plato/results/test/model/lenet5_456_1127977.pth.
[INFO][10:37:40]: [Client #456] Model trained.
[INFO][10:37:40]: [Client #456] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:37:40]: [Server #1127936] Received 0.24 MB of payload data from client #456 (simulated).
[INFO][10:37:40]: [Client #164] Loading a model from /data/ykang/plato/results/test/model/lenet5_164_1127978.pth.
[INFO][10:37:40]: [Client #164] Model trained.
[INFO][10:37:40]: [Client #164] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:37:40]: [Server #1127936] Received 0.24 MB of payload data from client #164 (simulated).
[INFO][10:37:40]: [Client #400] Loading a model from /data/ykang/plato/results/test/model/lenet5_400_1127979.pth.
[INFO][10:37:40]: [Client #400] Model trained.
[INFO][10:37:40]: [Client #400] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:37:40]: [Server #1127936] Received 0.24 MB of payload data from client #400 (simulated).
[INFO][10:37:40]: [Server #1127936] Selecting client #189 for training.
[INFO][10:37:40]: [Server #1127936] Sending the current model to client #189 (simulated).
[INFO][10:37:40]: [Server #1127936] Sending 0.24 MB of payload data to client #189 (simulated).
[INFO][10:37:40]: [Client #189] Selected by the server.
[INFO][10:37:40]: [Client #189] Loading its data source...
[INFO][10:37:40]: [Client #189] Dataset size: 60000
[INFO][10:37:40]: [Client #189] Sampler: noniid
[INFO][10:37:40]: [Client #189] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:37:40]: [93m[1m[Client #189] Started training in communication round #85.[0m
[INFO][10:37:42]: [Client #189] Loading the dataset.
[INFO][10:37:47]: [Client #189] Epoch: [1/5][0/10]	Loss: 0.000622
[INFO][10:37:47]: [Client #189] Epoch: [2/5][0/10]	Loss: 0.000041
[INFO][10:37:47]: [Client #189] Epoch: [3/5][0/10]	Loss: 0.000004
[INFO][10:37:47]: [Client #189] Epoch: [4/5][0/10]	Loss: 0.000166
[INFO][10:37:47]: [Client #189] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:37:47]: [Client #189] Model saved to /data/ykang/plato/results/test/model/lenet5_189_1127977.pth.
[INFO][10:37:48]: [Client #189] Loading a model from /data/ykang/plato/results/test/model/lenet5_189_1127977.pth.
[INFO][10:37:48]: [Client #189] Model trained.
[INFO][10:37:48]: [Client #189] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:37:48]: [Server #1127936] Received 0.24 MB of payload data from client #189 (simulated).
[INFO][10:37:48]: [Server #1127936] Adding client #73 to the list of clients for aggregation.
[INFO][10:37:48]: [Server #1127936] Adding client #23 to the list of clients for aggregation.
[INFO][10:37:48]: [Server #1127936] Adding client #470 to the list of clients for aggregation.
[INFO][10:37:48]: [Server #1127936] Adding client #481 to the list of clients for aggregation.
[INFO][10:37:48]: [Server #1127936] Adding client #431 to the list of clients for aggregation.
[INFO][10:37:48]: [Server #1127936] Adding client #257 to the list of clients for aggregation.
[INFO][10:37:48]: [Server #1127936] Adding client #486 to the list of clients for aggregation.
[INFO][10:37:48]: [Server #1127936] Adding client #194 to the list of clients for aggregation.
[INFO][10:37:48]: [Server #1127936] Adding client #189 to the list of clients for aggregation.
[INFO][10:37:48]: [Server #1127936] Adding client #400 to the list of clients for aggregation.
[INFO][10:37:48]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 251, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 482, 483, 484, 485, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00250159 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00900989 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00808457 0.         0.         0.
 0.         0.00233355 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00270809 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.0015339  0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00303177 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00701563 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00639436 0.         0.         0.         0.         0.01282011
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 0. 1. 0. 0. 0. 1. 0. 0. 3. 1. 0. 0. 0. 1. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 2. 0. 1. 0.
 1. 0. 1. 0. 2. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 2. 1. 1. 0. 0.
 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 2. 1. 6. 0. 0. 2. 1. 0. 1. 1. 2. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 3. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 1. 6. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 2. 0. 0.
 1. 1. 1. 2. 0. 0. 0. 2. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 1. 2. 0. 4. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1.
 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 4. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0.
 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0.
 1. 0. 1. 0. 4. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 3. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0.
 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 0. 1. 0. 6. 1. 1.
 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 6. 0. 0. 1. 1. 0. 1. 1. 2. 0. 1. 1. 0. 1.
 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 2. 2. 1. 2. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00250159 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00900989 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00808457 0.         0.         0.
 0.         0.00233355 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00270809 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.0015339  0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00303177 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00701563 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00639436 0.         0.         0.         0.         0.01282011
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:37:50]: [Server #1127936] Global model accuracy: 96.16%

[INFO][10:37:50]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_85.pth.
[INFO][10:37:50]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_85.pth.
[INFO][10:37:50]: [93m[1m
[Server #1127936] Starting round 86/100.[0m
[INFO][10:37:51]: [Server #1127936] Selected clients: [352 416 486 259 481 101  34  98 134 299]
[INFO][10:37:51]: [Server #1127936] Selecting client #352 for training.
[INFO][10:37:51]: [Server #1127936] Sending the current model to client #352 (simulated).
[INFO][10:37:51]: [Server #1127936] Sending 0.24 MB of payload data to client #352 (simulated).
[INFO][10:37:51]: [Server #1127936] Selecting client #416 for training.
[INFO][10:37:51]: [Server #1127936] Sending the current model to client #416 (simulated).
[INFO][10:37:51]: [Server #1127936] Sending 0.24 MB of payload data to client #416 (simulated).
[INFO][10:37:51]: [Server #1127936] Selecting client #486 for training.
[INFO][10:37:51]: [Server #1127936] Sending the current model to client #486 (simulated).
[INFO][10:37:51]: [Client #352] Selected by the server.
[INFO][10:37:51]: [Client #352] Loading its data source...
[INFO][10:37:51]: [Client #352] Dataset size: 60000
[INFO][10:37:51]: [Client #352] Sampler: noniid
[INFO][10:37:51]: [Server #1127936] Sending 0.24 MB of payload data to client #486 (simulated).
[INFO][10:37:51]: [Client #416] Selected by the server.
[INFO][10:37:51]: [Client #416] Loading its data source...
[INFO][10:37:51]: [Client #416] Dataset size: 60000
[INFO][10:37:51]: [Client #416] Sampler: noniid
[INFO][10:37:51]: [Client #352] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:37:51]: [Client #416] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:37:51]: [93m[1m[Client #352] Started training in communication round #86.[0m
[INFO][10:37:51]: [93m[1m[Client #416] Started training in communication round #86.[0m
[INFO][10:37:51]: [Client #486] Selected by the server.
[INFO][10:37:51]: [Client #486] Loading its data source...
[INFO][10:37:51]: [Client #486] Dataset size: 60000
[INFO][10:37:51]: [Client #486] Sampler: noniid
[INFO][10:37:51]: [Client #486] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:37:51]: [93m[1m[Client #486] Started training in communication round #86.[0m
[INFO][10:37:53]: [Client #352] Loading the dataset.
[INFO][10:37:53]: [Client #416] Loading the dataset.
[INFO][10:37:53]: [Client #486] Loading the dataset.
[INFO][10:38:01]: [Client #352] Epoch: [1/5][0/10]	Loss: 0.000927
[INFO][10:38:01]: [Client #416] Epoch: [1/5][0/10]	Loss: 0.000594
[INFO][10:38:01]: [Client #486] Epoch: [1/5][0/10]	Loss: 0.001685
[INFO][10:38:01]: [Client #352] Epoch: [2/5][0/10]	Loss: 0.003130
[INFO][10:38:01]: [Client #416] Epoch: [2/5][0/10]	Loss: 0.000323
[INFO][10:38:01]: [Client #486] Epoch: [2/5][0/10]	Loss: 0.000041
[INFO][10:38:01]: [Client #352] Epoch: [3/5][0/10]	Loss: 0.000089
[INFO][10:38:01]: [Client #416] Epoch: [3/5][0/10]	Loss: 0.000006
[INFO][10:38:01]: [Client #486] Epoch: [3/5][0/10]	Loss: 0.045863
[INFO][10:38:01]: [Client #352] Epoch: [4/5][0/10]	Loss: 0.000081
[INFO][10:38:01]: [Client #416] Epoch: [4/5][0/10]	Loss: 0.000003
[INFO][10:38:01]: [Client #352] Epoch: [5/5][0/10]	Loss: 0.000049
[INFO][10:38:01]: [Client #486] Epoch: [4/5][0/10]	Loss: 0.000022
[INFO][10:38:01]: [Client #416] Epoch: [5/5][0/10]	Loss: 0.000003
[INFO][10:38:01]: [Client #352] Model saved to /data/ykang/plato/results/test/model/lenet5_352_1127977.pth.
[INFO][10:38:01]: [Client #416] Model saved to /data/ykang/plato/results/test/model/lenet5_416_1127978.pth.
[INFO][10:38:01]: [Client #486] Epoch: [5/5][0/10]	Loss: 0.000143
[INFO][10:38:01]: [Client #486] Model saved to /data/ykang/plato/results/test/model/lenet5_486_1127979.pth.
[INFO][10:38:02]: [Client #352] Loading a model from /data/ykang/plato/results/test/model/lenet5_352_1127977.pth.
[INFO][10:38:02]: [Client #352] Model trained.
[INFO][10:38:02]: [Client #352] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:38:02]: [Server #1127936] Received 0.24 MB of payload data from client #352 (simulated).
[INFO][10:38:02]: [Client #416] Loading a model from /data/ykang/plato/results/test/model/lenet5_416_1127978.pth.
[INFO][10:38:02]: [Client #416] Model trained.
[INFO][10:38:02]: [Client #416] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:38:02]: [Server #1127936] Received 0.24 MB of payload data from client #416 (simulated).
[INFO][10:38:02]: [Client #486] Loading a model from /data/ykang/plato/results/test/model/lenet5_486_1127979.pth.
[INFO][10:38:02]: [Client #486] Model trained.
[INFO][10:38:02]: [Client #486] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:38:02]: [Server #1127936] Received 0.24 MB of payload data from client #486 (simulated).
[INFO][10:38:02]: [Server #1127936] Selecting client #259 for training.
[INFO][10:38:02]: [Server #1127936] Sending the current model to client #259 (simulated).
[INFO][10:38:02]: [Server #1127936] Sending 0.24 MB of payload data to client #259 (simulated).
[INFO][10:38:02]: [Server #1127936] Selecting client #481 for training.
[INFO][10:38:02]: [Server #1127936] Sending the current model to client #481 (simulated).
[INFO][10:38:02]: [Server #1127936] Sending 0.24 MB of payload data to client #481 (simulated).
[INFO][10:38:02]: [Server #1127936] Selecting client #101 for training.
[INFO][10:38:02]: [Server #1127936] Sending the current model to client #101 (simulated).
[INFO][10:38:02]: [Client #259] Selected by the server.
[INFO][10:38:02]: [Client #259] Loading its data source...
[INFO][10:38:02]: [Client #259] Dataset size: 60000
[INFO][10:38:02]: [Client #259] Sampler: noniid
[INFO][10:38:02]: [Server #1127936] Sending 0.24 MB of payload data to client #101 (simulated).
[INFO][10:38:02]: [Client #481] Selected by the server.
[INFO][10:38:02]: [Client #481] Loading its data source...
[INFO][10:38:02]: [Client #481] Dataset size: 60000
[INFO][10:38:02]: [Client #101] Selected by the server.
[INFO][10:38:02]: [Client #481] Sampler: noniid
[INFO][10:38:02]: [Client #101] Loading its data source...
[INFO][10:38:02]: [Client #101] Dataset size: 60000
[INFO][10:38:02]: [Client #101] Sampler: noniid
[INFO][10:38:02]: [Client #259] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:38:02]: [Client #481] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:38:02]: [93m[1m[Client #259] Started training in communication round #86.[0m
[INFO][10:38:02]: [93m[1m[Client #481] Started training in communication round #86.[0m
[INFO][10:38:02]: [Client #101] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:38:02]: [93m[1m[Client #101] Started training in communication round #86.[0m
[INFO][10:38:04]: [Client #481] Loading the dataset.
[INFO][10:38:04]: [Client #259] Loading the dataset.
[INFO][10:38:04]: [Client #101] Loading the dataset.
[INFO][10:38:10]: [Client #481] Epoch: [1/5][0/10]	Loss: 0.001503
[INFO][10:38:10]: [Client #259] Epoch: [1/5][0/10]	Loss: 0.001081
[INFO][10:38:10]: [Client #481] Epoch: [2/5][0/10]	Loss: 0.000063
[INFO][10:38:11]: [Client #101] Epoch: [1/5][0/10]	Loss: 0.006175
[INFO][10:38:11]: [Client #259] Epoch: [2/5][0/10]	Loss: 0.003413
[INFO][10:38:11]: [Client #481] Epoch: [3/5][0/10]	Loss: 0.017219
[INFO][10:38:11]: [Client #101] Epoch: [2/5][0/10]	Loss: 0.000008
[INFO][10:38:11]: [Client #481] Epoch: [4/5][0/10]	Loss: 0.088864
[INFO][10:38:11]: [Client #259] Epoch: [3/5][0/10]	Loss: 0.000247
[INFO][10:38:11]: [Client #101] Epoch: [3/5][0/10]	Loss: 0.000066
[INFO][10:38:11]: [Client #481] Epoch: [5/5][0/10]	Loss: 0.004035
[INFO][10:38:11]: [Client #259] Epoch: [4/5][0/10]	Loss: 0.000043
[INFO][10:38:11]: [Client #101] Epoch: [4/5][0/10]	Loss: 0.000972
[INFO][10:38:11]: [Client #481] Model saved to /data/ykang/plato/results/test/model/lenet5_481_1127978.pth.
[INFO][10:38:11]: [Client #259] Epoch: [5/5][0/10]	Loss: 0.023399
[INFO][10:38:11]: [Client #101] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:38:11]: [Client #259] Model saved to /data/ykang/plato/results/test/model/lenet5_259_1127977.pth.
[INFO][10:38:11]: [Client #101] Model saved to /data/ykang/plato/results/test/model/lenet5_101_1127979.pth.
[INFO][10:38:12]: [Client #481] Loading a model from /data/ykang/plato/results/test/model/lenet5_481_1127978.pth.
[INFO][10:38:12]: [Client #481] Model trained.
[INFO][10:38:12]: [Client #481] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:38:12]: [Server #1127936] Received 0.24 MB of payload data from client #481 (simulated).
[INFO][10:38:12]: [Client #259] Loading a model from /data/ykang/plato/results/test/model/lenet5_259_1127977.pth.
[INFO][10:38:12]: [Client #259] Model trained.
[INFO][10:38:12]: [Client #259] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:38:12]: [Server #1127936] Received 0.24 MB of payload data from client #259 (simulated).
[INFO][10:38:12]: [Client #101] Loading a model from /data/ykang/plato/results/test/model/lenet5_101_1127979.pth.
[INFO][10:38:12]: [Client #101] Model trained.
[INFO][10:38:12]: [Client #101] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:38:12]: [Server #1127936] Received 0.24 MB of payload data from client #101 (simulated).
[INFO][10:38:12]: [Server #1127936] Selecting client #34 for training.
[INFO][10:38:12]: [Server #1127936] Sending the current model to client #34 (simulated).
[INFO][10:38:12]: [Server #1127936] Sending 0.24 MB of payload data to client #34 (simulated).
[INFO][10:38:12]: [Server #1127936] Selecting client #98 for training.
[INFO][10:38:12]: [Server #1127936] Sending the current model to client #98 (simulated).
[INFO][10:38:12]: [Server #1127936] Sending 0.24 MB of payload data to client #98 (simulated).
[INFO][10:38:12]: [Server #1127936] Selecting client #134 for training.
[INFO][10:38:12]: [Server #1127936] Sending the current model to client #134 (simulated).
[INFO][10:38:12]: [Client #34] Selected by the server.
[INFO][10:38:12]: [Client #34] Loading its data source...
[INFO][10:38:12]: [Client #34] Dataset size: 60000
[INFO][10:38:12]: [Client #34] Sampler: noniid
[INFO][10:38:12]: [Server #1127936] Sending 0.24 MB of payload data to client #134 (simulated).
[INFO][10:38:12]: [Client #98] Selected by the server.
[INFO][10:38:12]: [Client #98] Loading its data source...
[INFO][10:38:12]: [Client #134] Selected by the server.
[INFO][10:38:12]: [Client #98] Dataset size: 60000
[INFO][10:38:12]: [Client #98] Sampler: noniid
[INFO][10:38:12]: [Client #134] Loading its data source...
[INFO][10:38:12]: [Client #134] Dataset size: 60000
[INFO][10:38:12]: [Client #134] Sampler: noniid
[INFO][10:38:12]: [Client #34] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:38:12]: [Client #98] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:38:12]: [Client #134] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:38:12]: [93m[1m[Client #98] Started training in communication round #86.[0m
[INFO][10:38:12]: [93m[1m[Client #34] Started training in communication round #86.[0m
[INFO][10:38:12]: [93m[1m[Client #134] Started training in communication round #86.[0m
[INFO][10:38:14]: [Client #98] Loading the dataset.
[INFO][10:38:14]: [Client #34] Loading the dataset.
[INFO][10:38:14]: [Client #134] Loading the dataset.
[INFO][10:38:20]: [Client #98] Epoch: [1/5][0/10]	Loss: 0.001023
[INFO][10:38:20]: [Client #134] Epoch: [1/5][0/10]	Loss: 0.000123
[INFO][10:38:20]: [Client #34] Epoch: [1/5][0/10]	Loss: 0.006165
[INFO][10:38:20]: [Client #98] Epoch: [2/5][0/10]	Loss: 0.001157
[INFO][10:38:20]: [Client #134] Epoch: [2/5][0/10]	Loss: 0.000676
[INFO][10:38:20]: [Client #34] Epoch: [2/5][0/10]	Loss: 0.000317
[INFO][10:38:20]: [Client #98] Epoch: [3/5][0/10]	Loss: 0.000019
[INFO][10:38:20]: [Client #134] Epoch: [3/5][0/10]	Loss: 0.000012
[INFO][10:38:21]: [Client #134] Epoch: [4/5][0/10]	Loss: 0.000004
[INFO][10:38:21]: [Client #34] Epoch: [3/5][0/10]	Loss: 0.000005
[INFO][10:38:21]: [Client #98] Epoch: [4/5][0/10]	Loss: 0.000353
[INFO][10:38:21]: [Client #134] Epoch: [5/5][0/10]	Loss: 0.000080
[INFO][10:38:21]: [Client #134] Model saved to /data/ykang/plato/results/test/model/lenet5_134_1127979.pth.
[INFO][10:38:21]: [Client #34] Epoch: [4/5][0/10]	Loss: 0.000003
[INFO][10:38:21]: [Client #98] Epoch: [5/5][0/10]	Loss: 0.000871
[INFO][10:38:21]: [Client #98] Model saved to /data/ykang/plato/results/test/model/lenet5_98_1127978.pth.
[INFO][10:38:21]: [Client #34] Epoch: [5/5][0/10]	Loss: 0.000008
[INFO][10:38:21]: [Client #34] Model saved to /data/ykang/plato/results/test/model/lenet5_34_1127977.pth.
[INFO][10:38:21]: [Client #134] Loading a model from /data/ykang/plato/results/test/model/lenet5_134_1127979.pth.
[INFO][10:38:21]: [Client #134] Model trained.
[INFO][10:38:21]: [Client #134] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:38:21]: [Server #1127936] Received 0.24 MB of payload data from client #134 (simulated).
[INFO][10:38:22]: [Client #98] Loading a model from /data/ykang/plato/results/test/model/lenet5_98_1127978.pth.
[INFO][10:38:22]: [Client #98] Model trained.
[INFO][10:38:22]: [Client #98] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:38:22]: [Server #1127936] Received 0.24 MB of payload data from client #98 (simulated).
[INFO][10:38:22]: [Client #34] Loading a model from /data/ykang/plato/results/test/model/lenet5_34_1127977.pth.
[INFO][10:38:22]: [Client #34] Model trained.
[INFO][10:38:22]: [Client #34] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:38:22]: [Server #1127936] Received 0.24 MB of payload data from client #34 (simulated).
[INFO][10:38:22]: [Server #1127936] Selecting client #299 for training.
[INFO][10:38:22]: [Server #1127936] Sending the current model to client #299 (simulated).
[INFO][10:38:22]: [Server #1127936] Sending 0.24 MB of payload data to client #299 (simulated).
[INFO][10:38:22]: [Client #299] Selected by the server.
[INFO][10:38:22]: [Client #299] Loading its data source...
[INFO][10:38:22]: [Client #299] Dataset size: 60000
[INFO][10:38:22]: [Client #299] Sampler: noniid
[INFO][10:38:22]: [Client #299] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:38:22]: [93m[1m[Client #299] Started training in communication round #86.[0m
[INFO][10:38:24]: [Client #299] Loading the dataset.
[INFO][10:38:29]: [Client #299] Epoch: [1/5][0/10]	Loss: 0.009043
[INFO][10:38:30]: [Client #299] Epoch: [2/5][0/10]	Loss: 0.000081
[INFO][10:38:30]: [Client #299] Epoch: [3/5][0/10]	Loss: 0.000396
[INFO][10:38:30]: [Client #299] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:38:30]: [Client #299] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:38:30]: [Client #299] Model saved to /data/ykang/plato/results/test/model/lenet5_299_1127977.pth.
[INFO][10:38:30]: [Client #299] Loading a model from /data/ykang/plato/results/test/model/lenet5_299_1127977.pth.
[INFO][10:38:30]: [Client #299] Model trained.
[INFO][10:38:30]: [Client #299] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:38:30]: [Server #1127936] Received 0.24 MB of payload data from client #299 (simulated).
[INFO][10:38:30]: [Server #1127936] Adding client #250 to the list of clients for aggregation.
[INFO][10:38:30]: [Server #1127936] Adding client #63 to the list of clients for aggregation.
[INFO][10:38:30]: [Server #1127936] Adding client #456 to the list of clients for aggregation.
[INFO][10:38:30]: [Server #1127936] Adding client #164 to the list of clients for aggregation.
[INFO][10:38:30]: [Server #1127936] Adding client #25 to the list of clients for aggregation.
[INFO][10:38:30]: [Server #1127936] Adding client #363 to the list of clients for aggregation.
[INFO][10:38:30]: [Server #1127936] Adding client #134 to the list of clients for aggregation.
[INFO][10:38:30]: [Server #1127936] Adding client #259 to the list of clients for aggregation.
[INFO][10:38:30]: [Server #1127936] Adding client #481 to the list of clients for aggregation.
[INFO][10:38:30]: [Server #1127936] Adding client #101 to the list of clients for aggregation.
[INFO][10:38:30]: [Server #1127936] Aggregating 10 clients in total.
[0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  2e-05  3e-10  3e-10
 6:  6.8876e+00  6.8875e+00  1e-05  2e-10  2e-10
 7:  6.8875e+00  6.8875e+00  1e-05  2e-10  3e-12
 8:  6.8875e+00  6.8875e+00  8e-06  2e-10  3e-12
 9:  6.8875e+00  6.8875e+00  5e-06  3e-10  4e-12
Optimal solution found.
The calculated probability is:  [0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.0009102  0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00259853 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072252 0.00072256 0.00072256
 0.00072256 0.00072256 0.00089469 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00092998 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00096273 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00167485 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00150457 0.00072256 0.00072256 0.00072256
 0.00072256 0.64225024 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256 0.00072256
 0.00072256 0.00072256 0.00072256 0.00072256]
current clients pool:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00138646 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00731902 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.001984   0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00234245 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01168873 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00677653 0.         0.
 0.         0.         0.         0.         0.         0.
 0.00400457 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00846397 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00549137
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01242381 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 1. 1. 0. 0. 0. 1. 0. 0. 3. 1. 0. 0. 0. 1. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 2. 0. 1. 0.
 1. 0. 1. 0. 2. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 2. 1. 1. 0. 0.
 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 2. 1. 6. 0. 0. 2. 1. 0. 1. 1. 2. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 3. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 1. 6. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 2. 0. 0.
 1. 1. 1. 2. 0. 0. 0. 2. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 1. 2. 0. 4. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1.
 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 4. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0.
 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0.
 1. 0. 1. 0. 4. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 3. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0.
 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 0. 1. 0. 6. 1. 1.
 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 6. 0. 0. 1. 1. 0. 1. 1. 2. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 2. 2. 1. 2. 0.]
local_gradient_bounds:  [INFO][10:38:33]: [Server #1127936] Global model accuracy: 96.10%

[INFO][10:38:33]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_86.pth.
[INFO][10:38:33]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_86.pth.
[INFO][10:38:33]: [93m[1m
[Server #1127936] Starting round 87/100.[0m
[0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00138646 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00731902 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.001984   0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00234245 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01168873 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00677653 0.         0.
 0.         0.         0.         0.         0.         0.
 0.00400457 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00846397 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00549137
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01242381 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  2e-05  3e-10  3e-10
 6:  6.8876e+00  6.8875e+00  1e-05  2e-10  2e-10
 7:  6.8875e+00  6.8875e+00  1e-05  3e-10  4e-12
 8:  6.8875e+00  6.8875e+00  8e-06  2e-10  3e-12
 9:  6.8875e+00  6.8875e+00  5e-06  3e-10  5e-12
Optimal solution found.
The calculated probability is:  [0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.0008138  0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00165297 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00566928 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.63736549 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.0007256  0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00204415 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00126006 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072552 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561 0.00072561
 0.00072561 0.00072561 0.00072561 0.00072561]
current clients pool:  [INFO][10:38:33]: [Server #1127936] Selected clients: [324 173 443 250 476 150  55 143 383 318]
[INFO][10:38:33]: [Server #1127936] Selecting client #324 for training.
[INFO][10:38:33]: [Server #1127936] Sending the current model to client #324 (simulated).
[INFO][10:38:33]: [Server #1127936] Sending 0.24 MB of payload data to client #324 (simulated).
[INFO][10:38:33]: [Server #1127936] Selecting client #173 for training.
[INFO][10:38:33]: [Server #1127936] Sending the current model to client #173 (simulated).
[INFO][10:38:33]: [Server #1127936] Sending 0.24 MB of payload data to client #173 (simulated).
[INFO][10:38:33]: [Server #1127936] Selecting client #443 for training.
[INFO][10:38:33]: [Server #1127936] Sending the current model to client #443 (simulated).
[INFO][10:38:33]: [Client #324] Selected by the server.
[INFO][10:38:33]: [Client #324] Loading its data source...
[INFO][10:38:33]: [Client #324] Dataset size: 60000
[INFO][10:38:33]: [Client #324] Sampler: noniid
[INFO][10:38:33]: [Server #1127936] Sending 0.24 MB of payload data to client #443 (simulated).
[INFO][10:38:33]: [Client #173] Selected by the server.
[INFO][10:38:33]: [Client #173] Loading its data source...
[INFO][10:38:33]: [Client #173] Dataset size: 60000
[INFO][10:38:33]: [Client #173] Sampler: noniid
[INFO][10:38:33]: [Client #324] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:38:33]: [Client #443] Selected by the server.
[INFO][10:38:33]: [Client #443] Loading its data source...
[INFO][10:38:33]: [Client #443] Dataset size: 60000
[INFO][10:38:33]: [Client #443] Sampler: noniid
[INFO][10:38:33]: [Client #173] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:38:33]: [Client #443] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:38:33]: [93m[1m[Client #443] Started training in communication round #87.[0m
[INFO][10:38:33]: [93m[1m[Client #173] Started training in communication round #87.[0m
[INFO][10:38:33]: [93m[1m[Client #324] Started training in communication round #87.[0m
[INFO][10:38:35]: [Client #173] Loading the dataset.
[INFO][10:38:35]: [Client #443] Loading the dataset.
[INFO][10:38:35]: [Client #324] Loading the dataset.
[INFO][10:38:42]: [Client #173] Epoch: [1/5][0/10]	Loss: 0.001572
[INFO][10:38:42]: [Client #173] Epoch: [2/5][0/10]	Loss: 0.000111
[INFO][10:38:42]: [Client #324] Epoch: [1/5][0/10]	Loss: 0.021706
[INFO][10:38:42]: [Client #443] Epoch: [1/5][0/10]	Loss: 0.000907
[INFO][10:38:42]: [Client #173] Epoch: [3/5][0/10]	Loss: 0.000048
[INFO][10:38:42]: [Client #443] Epoch: [2/5][0/10]	Loss: 0.002297
[INFO][10:38:42]: [Client #443] Epoch: [3/5][0/10]	Loss: 0.000316
[INFO][10:38:42]: [Client #324] Epoch: [2/5][0/10]	Loss: 0.000000
[INFO][10:38:42]: [Client #173] Epoch: [4/5][0/10]	Loss: 0.000869
[INFO][10:38:42]: [Client #443] Epoch: [4/5][0/10]	Loss: 0.001033
[INFO][10:38:42]: [Client #324] Epoch: [3/5][0/10]	Loss: 0.000583
[INFO][10:38:42]: [Client #443] Epoch: [5/5][0/10]	Loss: 0.000059
[INFO][10:38:42]: [Client #173] Epoch: [5/5][0/10]	Loss: 0.000122
[INFO][10:38:42]: [Client #324] Epoch: [4/5][0/10]	Loss: 0.000341
[INFO][10:38:42]: [Client #173] Model saved to /data/ykang/plato/results/test/model/lenet5_173_1127978.pth.
[INFO][10:38:42]: [Client #443] Model saved to /data/ykang/plato/results/test/model/lenet5_443_1127979.pth.
[INFO][10:38:42]: [Client #324] Epoch: [5/5][0/10]	Loss: 0.001014
[INFO][10:38:42]: [Client #324] Model saved to /data/ykang/plato/results/test/model/lenet5_324_1127977.pth.
[INFO][10:38:43]: [Client #443] Loading a model from /data/ykang/plato/results/test/model/lenet5_443_1127979.pth.
[INFO][10:38:43]: [Client #443] Model trained.
[INFO][10:38:43]: [Client #443] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:38:43]: [Server #1127936] Received 0.24 MB of payload data from client #443 (simulated).
[INFO][10:38:43]: [Client #173] Loading a model from /data/ykang/plato/results/test/model/lenet5_173_1127978.pth.
[INFO][10:38:43]: [Client #173] Model trained.
[INFO][10:38:43]: [Client #173] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:38:43]: [Server #1127936] Received 0.24 MB of payload data from client #173 (simulated).
[INFO][10:38:43]: [Client #324] Loading a model from /data/ykang/plato/results/test/model/lenet5_324_1127977.pth.
[INFO][10:38:43]: [Client #324] Model trained.
[INFO][10:38:43]: [Client #324] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:38:43]: [Server #1127936] Received 0.24 MB of payload data from client #324 (simulated).
[INFO][10:38:43]: [Server #1127936] Selecting client #250 for training.
[INFO][10:38:43]: [Server #1127936] Sending the current model to client #250 (simulated).
[INFO][10:38:43]: [Server #1127936] Sending 0.24 MB of payload data to client #250 (simulated).
[INFO][10:38:43]: [Server #1127936] Selecting client #476 for training.
[INFO][10:38:43]: [Server #1127936] Sending the current model to client #476 (simulated).
[INFO][10:38:43]: [Server #1127936] Sending 0.24 MB of payload data to client #476 (simulated).
[INFO][10:38:43]: [Server #1127936] Selecting client #150 for training.
[INFO][10:38:43]: [Server #1127936] Sending the current model to client #150 (simulated).
[INFO][10:38:43]: [Client #250] Selected by the server.
[INFO][10:38:43]: [Client #250] Loading its data source...
[INFO][10:38:43]: [Client #250] Dataset size: 60000
[INFO][10:38:43]: [Client #250] Sampler: noniid
[INFO][10:38:43]: [Server #1127936] Sending 0.24 MB of payload data to client #150 (simulated).
[INFO][10:38:43]: [Client #150] Selected by the server.
[INFO][10:38:43]: [Client #150] Loading its data source...
[INFO][10:38:43]: [Client #150] Dataset size: 60000
[INFO][10:38:43]: [Client #150] Sampler: noniid
[INFO][10:38:43]: [Client #476] Selected by the server.
[INFO][10:38:43]: [Client #476] Loading its data source...
[INFO][10:38:43]: [Client #476] Dataset size: 60000
[INFO][10:38:43]: [Client #476] Sampler: noniid
[INFO][10:38:43]: [Client #250] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:38:43]: [Client #150] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:38:43]: [93m[1m[Client #150] Started training in communication round #87.[0m
[INFO][10:38:43]: [Client #476] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:38:43]: [93m[1m[Client #250] Started training in communication round #87.[0m
[INFO][10:38:43]: [93m[1m[Client #476] Started training in communication round #87.[0m
[INFO][10:38:45]: [Client #476] Loading the dataset.
[INFO][10:38:45]: [Client #150] Loading the dataset.
[INFO][10:38:45]: [Client #250] Loading the dataset.
[INFO][10:38:51]: [Client #250] Epoch: [1/5][0/10]	Loss: 0.000194
[INFO][10:38:51]: [Client #250] Epoch: [2/5][0/10]	Loss: 0.000079
[INFO][10:38:51]: [Client #476] Epoch: [1/5][0/10]	Loss: 0.000194
[INFO][10:38:51]: [Client #150] Epoch: [1/5][0/10]	Loss: 0.003373
[INFO][10:38:52]: [Client #150] Epoch: [2/5][0/10]	Loss: 0.000118
[INFO][10:38:52]: [Client #250] Epoch: [3/5][0/10]	Loss: 0.000146
[INFO][10:38:52]: [Client #476] Epoch: [2/5][0/10]	Loss: 0.000079
[INFO][10:38:52]: [Client #250] Epoch: [4/5][0/10]	Loss: 0.000008
[INFO][10:38:52]: [Client #150] Epoch: [3/5][0/10]	Loss: 0.000025
[INFO][10:38:52]: [Client #476] Epoch: [3/5][0/10]	Loss: 0.000075
[INFO][10:38:52]: [Client #250] Epoch: [5/5][0/10]	Loss: 0.000002
[INFO][10:38:52]: [Client #150] Epoch: [4/5][0/10]	Loss: 0.000036
[INFO][10:38:52]: [Client #250] Model saved to /data/ykang/plato/results/test/model/lenet5_250_1127977.pth.
[INFO][10:38:52]: [Client #150] Epoch: [5/5][0/10]	Loss: 0.000181
[INFO][10:38:52]: [Client #476] Epoch: [4/5][0/10]	Loss: 0.000002
[INFO][10:38:52]: [Client #150] Model saved to /data/ykang/plato/results/test/model/lenet5_150_1127979.pth.
[INFO][10:38:52]: [Client #476] Epoch: [5/5][0/10]	Loss: 0.000001
[INFO][10:38:52]: [Client #476] Model saved to /data/ykang/plato/results/test/model/lenet5_476_1127978.pth.
[INFO][10:38:53]: [Client #250] Loading a model from /data/ykang/plato/results/test/model/lenet5_250_1127977.pth.
[INFO][10:38:53]: [Client #250] Model trained.
[INFO][10:38:53]: [Client #250] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:38:53]: [Server #1127936] Received 0.24 MB of payload data from client #250 (simulated).
[INFO][10:38:53]: [Client #150] Loading a model from /data/ykang/plato/results/test/model/lenet5_150_1127979.pth.
[INFO][10:38:53]: [Client #150] Model trained.
[INFO][10:38:53]: [Client #150] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:38:53]: [Server #1127936] Received 0.24 MB of payload data from client #150 (simulated).
[INFO][10:38:53]: [Client #476] Loading a model from /data/ykang/plato/results/test/model/lenet5_476_1127978.pth.
[INFO][10:38:53]: [Client #476] Model trained.
[INFO][10:38:53]: [Client #476] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:38:53]: [Server #1127936] Received 0.24 MB of payload data from client #476 (simulated).
[INFO][10:38:53]: [Server #1127936] Selecting client #55 for training.
[INFO][10:38:53]: [Server #1127936] Sending the current model to client #55 (simulated).
[INFO][10:38:53]: [Server #1127936] Sending 0.24 MB of payload data to client #55 (simulated).
[INFO][10:38:53]: [Server #1127936] Selecting client #143 for training.
[INFO][10:38:53]: [Server #1127936] Sending the current model to client #143 (simulated).
[INFO][10:38:53]: [Server #1127936] Sending 0.24 MB of payload data to client #143 (simulated).
[INFO][10:38:53]: [Server #1127936] Selecting client #383 for training.
[INFO][10:38:53]: [Server #1127936] Sending the current model to client #383 (simulated).
[INFO][10:38:53]: [Client #55] Selected by the server.
[INFO][10:38:53]: [Client #55] Loading its data source...
[INFO][10:38:53]: [Client #55] Dataset size: 60000
[INFO][10:38:53]: [Client #55] Sampler: noniid
[INFO][10:38:53]: [Server #1127936] Sending 0.24 MB of payload data to client #383 (simulated).
[INFO][10:38:53]: [Client #143] Selected by the server.
[INFO][10:38:53]: [Client #143] Loading its data source...
[INFO][10:38:53]: [Client #143] Dataset size: 60000
[INFO][10:38:53]: [Client #383] Selected by the server.
[INFO][10:38:53]: [Client #143] Sampler: noniid
[INFO][10:38:53]: [Client #383] Loading its data source...
[INFO][10:38:53]: [Client #383] Dataset size: 60000
[INFO][10:38:53]: [Client #383] Sampler: noniid
[INFO][10:38:53]: [Client #55] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:38:53]: [Client #383] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:38:53]: [Client #143] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:38:53]: [93m[1m[Client #55] Started training in communication round #87.[0m
[INFO][10:38:53]: [93m[1m[Client #383] Started training in communication round #87.[0m
[INFO][10:38:53]: [93m[1m[Client #143] Started training in communication round #87.[0m
[INFO][10:38:55]: [Client #55] Loading the dataset.
[INFO][10:38:55]: [Client #383] Loading the dataset.
[INFO][10:38:55]: [Client #143] Loading the dataset.
[INFO][10:39:01]: [Client #55] Epoch: [1/5][0/10]	Loss: 0.000839
[INFO][10:39:01]: [Client #383] Epoch: [1/5][0/10]	Loss: 0.001299
[INFO][10:39:01]: [Client #55] Epoch: [2/5][0/10]	Loss: 0.000002
[INFO][10:39:01]: [Client #143] Epoch: [1/5][0/10]	Loss: 0.000645
[INFO][10:39:01]: [Client #55] Epoch: [3/5][0/10]	Loss: 0.000020
[INFO][10:39:01]: [Client #383] Epoch: [2/5][0/10]	Loss: 0.000006
[INFO][10:39:01]: [Client #143] Epoch: [2/5][0/10]	Loss: 0.000103
[INFO][10:39:01]: [Client #55] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:39:01]: [Client #383] Epoch: [3/5][0/10]	Loss: 0.000039
[INFO][10:39:01]: [Client #143] Epoch: [3/5][0/10]	Loss: 0.000007
[INFO][10:39:02]: [Client #55] Epoch: [5/5][0/10]	Loss: 0.001306
[INFO][10:39:02]: [Client #383] Epoch: [4/5][0/10]	Loss: 0.009862
[INFO][10:39:02]: [Client #143] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:39:02]: [Client #55] Model saved to /data/ykang/plato/results/test/model/lenet5_55_1127977.pth.
[INFO][10:39:02]: [Client #383] Epoch: [5/5][0/10]	Loss: 0.063790
[INFO][10:39:02]: [Client #143] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:39:02]: [Client #383] Model saved to /data/ykang/plato/results/test/model/lenet5_383_1127979.pth.
[INFO][10:39:02]: [Client #143] Model saved to /data/ykang/plato/results/test/model/lenet5_143_1127978.pth.
[INFO][10:39:02]: [Client #55] Loading a model from /data/ykang/plato/results/test/model/lenet5_55_1127977.pth.
[INFO][10:39:02]: [Client #55] Model trained.
[INFO][10:39:02]: [Client #55] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:39:02]: [Server #1127936] Received 0.24 MB of payload data from client #55 (simulated).
[INFO][10:39:03]: [Client #383] Loading a model from /data/ykang/plato/results/test/model/lenet5_383_1127979.pth.
[INFO][10:39:03]: [Client #383] Model trained.
[INFO][10:39:03]: [Client #383] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:39:03]: [Server #1127936] Received 0.24 MB of payload data from client #383 (simulated).
[INFO][10:39:03]: [Client #143] Loading a model from /data/ykang/plato/results/test/model/lenet5_143_1127978.pth.
[INFO][10:39:03]: [Client #143] Model trained.
[INFO][10:39:03]: [Client #143] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:39:03]: [Server #1127936] Received 0.24 MB of payload data from client #143 (simulated).
[INFO][10:39:03]: [Server #1127936] Selecting client #318 for training.
[INFO][10:39:03]: [Server #1127936] Sending the current model to client #318 (simulated).
[INFO][10:39:03]: [Server #1127936] Sending 0.24 MB of payload data to client #318 (simulated).
[INFO][10:39:03]: [Client #318] Selected by the server.
[INFO][10:39:03]: [Client #318] Loading its data source...
[INFO][10:39:03]: [Client #318] Dataset size: 60000
[INFO][10:39:03]: [Client #318] Sampler: noniid
[INFO][10:39:03]: [Client #318] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:39:03]: [93m[1m[Client #318] Started training in communication round #87.[0m
[INFO][10:39:05]: [Client #318] Loading the dataset.
[INFO][10:39:10]: [Client #318] Epoch: [1/5][0/10]	Loss: 0.001395
[INFO][10:39:10]: [Client #318] Epoch: [2/5][0/10]	Loss: 0.000011
[INFO][10:39:10]: [Client #318] Epoch: [3/5][0/10]	Loss: 0.000001
[INFO][10:39:10]: [Client #318] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:39:11]: [Client #318] Epoch: [5/5][0/10]	Loss: 0.000554
[INFO][10:39:11]: [Client #318] Model saved to /data/ykang/plato/results/test/model/lenet5_318_1127977.pth.
[INFO][10:39:11]: [Client #318] Loading a model from /data/ykang/plato/results/test/model/lenet5_318_1127977.pth.
[INFO][10:39:11]: [Client #318] Model trained.
[INFO][10:39:11]: [Client #318] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:39:11]: [Server #1127936] Received 0.24 MB of payload data from client #318 (simulated).
[INFO][10:39:11]: [Server #1127936] Adding client #416 to the list of clients for aggregation.
[INFO][10:39:11]: [Server #1127936] Adding client #34 to the list of clients for aggregation.
[INFO][10:39:11]: [Server #1127936] Adding client #98 to the list of clients for aggregation.
[INFO][10:39:11]: [Server #1127936] Adding client #486 to the list of clients for aggregation.
[INFO][10:39:11]: [Server #1127936] Adding client #352 to the list of clients for aggregation.
[INFO][10:39:11]: [Server #1127936] Adding client #150 to the list of clients for aggregation.
[INFO][10:39:11]: [Server #1127936] Adding client #476 to the list of clients for aggregation.
[INFO][10:39:11]: [Server #1127936] Adding client #143 to the list of clients for aggregation.
[INFO][10:39:11]: [Server #1127936] Adding client #324 to the list of clients for aggregation.
[INFO][10:39:11]: [Server #1127936] Adding client #318 to the list of clients for aggregation.
[INFO][10:39:11]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00135951 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00363223 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00387499 0.
 0.         0.         0.         0.         0.         0.00182509
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01201342
 0.         0.         0.         0.         0.         0.0349015
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00072113 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00143598 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00223897 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00972455
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 1. 1. 0. 0. 0. 1. 0. 0. 3. 1. 0. 0. 0. 1. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 2. 0. 1. 0.
 1. 0. 1. 0. 2. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 2. 1. 1. 0. 0.
 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 2. 1. 6. 0. 0. 2. 1. 0. 1. 1. 2. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 3. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 1. 6. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 2. 0. 0.
 1. 1. 1. 2. 0. 0. 0. 2. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 1. 2. 0. 4. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1.
 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 4. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0.
 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0.
 1. 0. 1. 0. 4. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 3. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0.
 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 0. 1. 0. 6. 1. 1.
 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 6. 0. 0. 1. 1. 0. 1. 1. 2. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 2. 2. 1. 2. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00135951 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00363223 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00387499 0.
 0.         0.         0.         0.         0.         0.00182509
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01201342
 0.         0.         0.         0.         0.         0.0349015
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00072113 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00143598 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00223897 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00972455
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:39:14]: [Server #1127936] Global model accuracy: 96.41%

[INFO][10:39:14]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_87.pth.
[INFO][10:39:14]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_87.pth.
[INFO][10:39:14]: [93m[1m
[Server #1127936] Starting round 88/100.[0m
[INFO][10:39:14]: [Server #1127936] Selected clients: [486 429 192 253  24 348 353 475  36  87]
[INFO][10:39:14]: [Server #1127936] Selecting client #486 for training.
[INFO][10:39:14]: [Server #1127936] Sending the current model to client #486 (simulated).
[INFO][10:39:14]: [Server #1127936] Sending 0.24 MB of payload data to client #486 (simulated).
[INFO][10:39:14]: [Server #1127936] Selecting client #429 for training.
[INFO][10:39:14]: [Server #1127936] Sending the current model to client #429 (simulated).
[INFO][10:39:14]: [Server #1127936] Sending 0.24 MB of payload data to client #429 (simulated).
[INFO][10:39:14]: [Server #1127936] Selecting client #192 for training.
[INFO][10:39:14]: [Server #1127936] Sending the current model to client #192 (simulated).
[INFO][10:39:14]: [Client #486] Selected by the server.
[INFO][10:39:14]: [Client #486] Loading its data source...
[INFO][10:39:14]: [Client #486] Dataset size: 60000
[INFO][10:39:14]: [Client #486] Sampler: noniid
[INFO][10:39:14]: [Server #1127936] Sending 0.24 MB of payload data to client #192 (simulated).
[INFO][10:39:14]: [Client #429] Selected by the server.
[INFO][10:39:14]: [Client #429] Loading its data source...
[INFO][10:39:14]: [Client #429] Dataset size: 60000
[INFO][10:39:14]: [Client #429] Sampler: noniid
[INFO][10:39:14]: [Client #486] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:39:14]: [Client #192] Selected by the server.
[INFO][10:39:14]: [Client #192] Loading its data source...
[INFO][10:39:14]: [Client #192] Dataset size: 60000
[INFO][10:39:14]: [Client #192] Sampler: noniid
[INFO][10:39:14]: [Client #192] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:39:14]: [Client #429] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:39:14]: [93m[1m[Client #429] Started training in communication round #88.[0m
[INFO][10:39:14]: [93m[1m[Client #486] Started training in communication round #88.[0m
[INFO][10:39:14]: [93m[1m[Client #192] Started training in communication round #88.[0m
[INFO][10:39:16]: [Client #486] Loading the dataset.
[INFO][10:39:16]: [Client #192] Loading the dataset.
[INFO][10:39:16]: [Client #429] Loading the dataset.
[INFO][10:39:22]: [Client #486] Epoch: [1/5][0/10]	Loss: 0.000503
[INFO][10:39:23]: [Client #192] Epoch: [1/5][0/10]	Loss: 0.000294
[INFO][10:39:23]: [Client #429] Epoch: [1/5][0/10]	Loss: 0.001431
[INFO][10:39:23]: [Client #486] Epoch: [2/5][0/10]	Loss: 0.000070
[INFO][10:39:23]: [Client #192] Epoch: [2/5][0/10]	Loss: 0.000089
[INFO][10:39:23]: [Client #429] Epoch: [2/5][0/10]	Loss: 0.000852
[INFO][10:39:23]: [Client #192] Epoch: [3/5][0/10]	Loss: 0.000011
[INFO][10:39:23]: [Client #486] Epoch: [3/5][0/10]	Loss: 0.001723
[INFO][10:39:23]: [Client #429] Epoch: [3/5][0/10]	Loss: 0.000096
[INFO][10:39:23]: [Client #192] Epoch: [4/5][0/10]	Loss: 0.000002
[INFO][10:39:23]: [Client #486] Epoch: [4/5][0/10]	Loss: 0.000056
[INFO][10:39:23]: [Client #429] Epoch: [4/5][0/10]	Loss: 0.000020
[INFO][10:39:23]: [Client #192] Epoch: [5/5][0/10]	Loss: 0.000051
[INFO][10:39:23]: [Client #429] Epoch: [5/5][0/10]	Loss: 0.000226
[INFO][10:39:23]: [Client #486] Epoch: [5/5][0/10]	Loss: 0.000926
[INFO][10:39:23]: [Client #192] Model saved to /data/ykang/plato/results/test/model/lenet5_192_1127979.pth.
[INFO][10:39:23]: [Client #429] Model saved to /data/ykang/plato/results/test/model/lenet5_429_1127978.pth.
[INFO][10:39:23]: [Client #486] Model saved to /data/ykang/plato/results/test/model/lenet5_486_1127977.pth.
[INFO][10:39:24]: [Client #192] Loading a model from /data/ykang/plato/results/test/model/lenet5_192_1127979.pth.
[INFO][10:39:24]: [Client #192] Model trained.
[INFO][10:39:24]: [Client #192] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:39:24]: [Server #1127936] Received 0.24 MB of payload data from client #192 (simulated).
[INFO][10:39:24]: [Client #486] Loading a model from /data/ykang/plato/results/test/model/lenet5_486_1127977.pth.
[INFO][10:39:24]: [Client #486] Model trained.
[INFO][10:39:24]: [Client #486] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:39:24]: [Server #1127936] Received 0.24 MB of payload data from client #486 (simulated).
[INFO][10:39:24]: [Client #429] Loading a model from /data/ykang/plato/results/test/model/lenet5_429_1127978.pth.
[INFO][10:39:24]: [Client #429] Model trained.
[INFO][10:39:24]: [Client #429] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:39:24]: [Server #1127936] Received 0.24 MB of payload data from client #429 (simulated).
[INFO][10:39:24]: [Server #1127936] Selecting client #253 for training.
[INFO][10:39:24]: [Server #1127936] Sending the current model to client #253 (simulated).
[INFO][10:39:24]: [Server #1127936] Sending 0.24 MB of payload data to client #253 (simulated).
[INFO][10:39:24]: [Server #1127936] Selecting client #24 for training.
[INFO][10:39:24]: [Server #1127936] Sending the current model to client #24 (simulated).
[INFO][10:39:24]: [Server #1127936] Sending 0.24 MB of payload data to client #24 (simulated).
[INFO][10:39:24]: [Server #1127936] Selecting client #348 for training.
[INFO][10:39:24]: [Server #1127936] Sending the current model to client #348 (simulated).
[INFO][10:39:24]: [Client #253] Selected by the server.
[INFO][10:39:24]: [Client #253] Loading its data source...
[INFO][10:39:24]: [Client #253] Dataset size: 60000
[INFO][10:39:24]: [Client #253] Sampler: noniid
[INFO][10:39:24]: [Server #1127936] Sending 0.24 MB of payload data to client #348 (simulated).
[INFO][10:39:24]: [Client #348] Selected by the server.
[INFO][10:39:24]: [Client #348] Loading its data source...
[INFO][10:39:24]: [Client #348] Dataset size: 60000
[INFO][10:39:24]: [Client #348] Sampler: noniid
[INFO][10:39:24]: [Client #24] Selected by the server.
[INFO][10:39:24]: [Client #24] Loading its data source...
[INFO][10:39:24]: [Client #24] Dataset size: 60000
[INFO][10:39:24]: [Client #24] Sampler: noniid
[INFO][10:39:24]: [Client #348] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:39:24]: [Client #24] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:39:24]: [93m[1m[Client #348] Started training in communication round #88.[0m
[INFO][10:39:24]: [93m[1m[Client #24] Started training in communication round #88.[0m
[INFO][10:39:24]: [Client #253] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:39:24]: [93m[1m[Client #253] Started training in communication round #88.[0m
[INFO][10:39:26]: [Client #24] Loading the dataset.
[INFO][10:39:26]: [Client #348] Loading the dataset.
[INFO][10:39:26]: [Client #253] Loading the dataset.
[INFO][10:39:32]: [Client #348] Epoch: [1/5][0/10]	Loss: 0.012190
[INFO][10:39:32]: [Client #253] Epoch: [1/5][0/10]	Loss: 0.001132
[INFO][10:39:32]: [Client #24] Epoch: [1/5][0/10]	Loss: 0.000758
[INFO][10:39:32]: [Client #348] Epoch: [2/5][0/10]	Loss: 0.001941
[INFO][10:39:32]: [Client #253] Epoch: [2/5][0/10]	Loss: 0.000063
[INFO][10:39:32]: [Client #24] Epoch: [2/5][0/10]	Loss: 0.001153
[INFO][10:39:33]: [Client #348] Epoch: [3/5][0/10]	Loss: 0.000040
[INFO][10:39:33]: [Client #253] Epoch: [3/5][0/10]	Loss: 0.000074
[INFO][10:39:33]: [Client #24] Epoch: [3/5][0/10]	Loss: 0.000022
[INFO][10:39:33]: [Client #348] Epoch: [4/5][0/10]	Loss: 0.000017
[INFO][10:39:33]: [Client #348] Epoch: [5/5][0/10]	Loss: 0.000008
[INFO][10:39:33]: [Client #253] Epoch: [4/5][0/10]	Loss: 0.002806
[INFO][10:39:33]: [Client #348] Model saved to /data/ykang/plato/results/test/model/lenet5_348_1127979.pth.
[INFO][10:39:33]: [Client #24] Epoch: [4/5][0/10]	Loss: 0.000002
[INFO][10:39:33]: [Client #253] Epoch: [5/5][0/10]	Loss: 0.027741
[INFO][10:39:33]: [Client #24] Epoch: [5/5][0/10]	Loss: 0.000036
[INFO][10:39:33]: [Client #253] Model saved to /data/ykang/plato/results/test/model/lenet5_253_1127977.pth.
[INFO][10:39:33]: [Client #24] Model saved to /data/ykang/plato/results/test/model/lenet5_24_1127978.pth.
[INFO][10:39:34]: [Client #348] Loading a model from /data/ykang/plato/results/test/model/lenet5_348_1127979.pth.
[INFO][10:39:34]: [Client #348] Model trained.
[INFO][10:39:34]: [Client #348] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:39:34]: [Server #1127936] Received 0.24 MB of payload data from client #348 (simulated).
[INFO][10:39:34]: [Client #253] Loading a model from /data/ykang/plato/results/test/model/lenet5_253_1127977.pth.
[INFO][10:39:34]: [Client #253] Model trained.
[INFO][10:39:34]: [Client #253] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:39:34]: [Server #1127936] Received 0.24 MB of payload data from client #253 (simulated).
[INFO][10:39:34]: [Client #24] Loading a model from /data/ykang/plato/results/test/model/lenet5_24_1127978.pth.
[INFO][10:39:34]: [Client #24] Model trained.
[INFO][10:39:34]: [Client #24] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:39:34]: [Server #1127936] Received 0.24 MB of payload data from client #24 (simulated).
[INFO][10:39:34]: [Server #1127936] Selecting client #353 for training.
[INFO][10:39:34]: [Server #1127936] Sending the current model to client #353 (simulated).
[INFO][10:39:34]: [Server #1127936] Sending 0.24 MB of payload data to client #353 (simulated).
[INFO][10:39:34]: [Server #1127936] Selecting client #475 for training.
[INFO][10:39:34]: [Server #1127936] Sending the current model to client #475 (simulated).
[INFO][10:39:34]: [Server #1127936] Sending 0.24 MB of payload data to client #475 (simulated).
[INFO][10:39:34]: [Server #1127936] Selecting client #36 for training.
[INFO][10:39:34]: [Server #1127936] Sending the current model to client #36 (simulated).
[INFO][10:39:34]: [Client #353] Selected by the server.
[INFO][10:39:34]: [Client #353] Loading its data source...
[INFO][10:39:34]: [Client #353] Dataset size: 60000
[INFO][10:39:34]: [Client #353] Sampler: noniid
[INFO][10:39:34]: [Server #1127936] Sending 0.24 MB of payload data to client #36 (simulated).
[INFO][10:39:34]: [Client #475] Selected by the server.
[INFO][10:39:34]: [Client #475] Loading its data source...
[INFO][10:39:34]: [Client #475] Dataset size: 60000
[INFO][10:39:34]: [Client #475] Sampler: noniid
[INFO][10:39:34]: [Client #36] Selected by the server.
[INFO][10:39:34]: [Client #36] Loading its data source...
[INFO][10:39:34]: [Client #36] Dataset size: 60000
[INFO][10:39:34]: [Client #36] Sampler: noniid
[INFO][10:39:34]: [Client #353] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:39:34]: [Client #475] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:39:34]: [Client #36] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:39:34]: [93m[1m[Client #353] Started training in communication round #88.[0m
[INFO][10:39:34]: [93m[1m[Client #36] Started training in communication round #88.[0m
[INFO][10:39:34]: [93m[1m[Client #475] Started training in communication round #88.[0m
[INFO][10:39:36]: [Client #353] Loading the dataset.
[INFO][10:39:36]: [Client #475] Loading the dataset.
[INFO][10:39:36]: [Client #36] Loading the dataset.
[INFO][10:39:42]: [Client #475] Epoch: [1/5][0/10]	Loss: 0.000647
[INFO][10:39:42]: [Client #36] Epoch: [1/5][0/10]	Loss: 0.001289
[INFO][10:39:42]: [Client #353] Epoch: [1/5][0/10]	Loss: 0.000597
[INFO][10:39:42]: [Client #475] Epoch: [2/5][0/10]	Loss: 0.000344
[INFO][10:39:42]: [Client #36] Epoch: [2/5][0/10]	Loss: 0.000379
[INFO][10:39:42]: [Client #353] Epoch: [2/5][0/10]	Loss: 0.000108
[INFO][10:39:43]: [Client #36] Epoch: [3/5][0/10]	Loss: 0.000032
[INFO][10:39:43]: [Client #475] Epoch: [3/5][0/10]	Loss: 0.000000
[INFO][10:39:43]: [Client #475] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:39:43]: [Client #353] Epoch: [3/5][0/10]	Loss: 0.000183
[INFO][10:39:43]: [Client #36] Epoch: [4/5][0/10]	Loss: 0.000007
[INFO][10:39:43]: [Client #353] Epoch: [4/5][0/10]	Loss: 0.000032
[INFO][10:39:43]: [Client #475] Epoch: [5/5][0/10]	Loss: 0.000003
[INFO][10:39:43]: [Client #36] Epoch: [5/5][0/10]	Loss: 0.000003
[INFO][10:39:43]: [Client #353] Epoch: [5/5][0/10]	Loss: 0.000003
[INFO][10:39:43]: [Client #475] Model saved to /data/ykang/plato/results/test/model/lenet5_475_1127978.pth.
[INFO][10:39:43]: [Client #36] Model saved to /data/ykang/plato/results/test/model/lenet5_36_1127979.pth.
[INFO][10:39:43]: [Client #353] Model saved to /data/ykang/plato/results/test/model/lenet5_353_1127977.pth.
[INFO][10:39:44]: [Client #36] Loading a model from /data/ykang/plato/results/test/model/lenet5_36_1127979.pth.
[INFO][10:39:44]: [Client #36] Model trained.
[INFO][10:39:44]: [Client #36] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:39:44]: [Server #1127936] Received 0.24 MB of payload data from client #36 (simulated).
[INFO][10:39:44]: [Client #475] Loading a model from /data/ykang/plato/results/test/model/lenet5_475_1127978.pth.
[INFO][10:39:44]: [Client #475] Model trained.
[INFO][10:39:44]: [Client #475] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:39:44]: [Server #1127936] Received 0.24 MB of payload data from client #475 (simulated).
[INFO][10:39:44]: [Client #353] Loading a model from /data/ykang/plato/results/test/model/lenet5_353_1127977.pth.
[INFO][10:39:44]: [Client #353] Model trained.
[INFO][10:39:44]: [Client #353] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:39:44]: [Server #1127936] Received 0.24 MB of payload data from client #353 (simulated).
[INFO][10:39:44]: [Server #1127936] Selecting client #87 for training.
[INFO][10:39:44]: [Server #1127936] Sending the current model to client #87 (simulated).
[INFO][10:39:44]: [Server #1127936] Sending 0.24 MB of payload data to client #87 (simulated).
[INFO][10:39:44]: [Client #87] Selected by the server.
[INFO][10:39:44]: [Client #87] Loading its data source...
[INFO][10:39:44]: [Client #87] Dataset size: 60000
[INFO][10:39:44]: [Client #87] Sampler: noniid
[INFO][10:39:44]: [Client #87] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:39:44]: [93m[1m[Client #87] Started training in communication round #88.[0m
[INFO][10:39:46]: [Client #87] Loading the dataset.
[INFO][10:39:52]: [Client #87] Epoch: [1/5][0/10]	Loss: 0.000759
[INFO][10:39:52]: [Client #87] Epoch: [2/5][0/10]	Loss: 0.000464
[INFO][10:39:52]: [Client #87] Epoch: [3/5][0/10]	Loss: 0.000052
[INFO][10:39:52]: [Client #87] Epoch: [4/5][0/10]	Loss: 0.000076
[INFO][10:39:52]: [Client #87] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:39:52]: [Client #87] Model saved to /data/ykang/plato/results/test/model/lenet5_87_1127977.pth.
[INFO][10:39:53]: [Client #87] Loading a model from /data/ykang/plato/results/test/model/lenet5_87_1127977.pth.
[INFO][10:39:53]: [Client #87] Model trained.
[INFO][10:39:53]: [Client #87] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:39:53]: [Server #1127936] Received 0.24 MB of payload data from client #87 (simulated).
[INFO][10:39:53]: [Server #1127936] Adding client #55 to the list of clients for aggregation.
[INFO][10:39:53]: [Server #1127936] Adding client #443 to the list of clients for aggregation.
[INFO][10:39:53]: [Server #1127936] Adding client #173 to the list of clients for aggregation.
[INFO][10:39:53]: [Server #1127936] Adding client #212 to the list of clients for aggregation.
[INFO][10:39:53]: [Server #1127936] Adding client #192 to the list of clients for aggregation.
[INFO][10:39:53]: [Server #1127936] Adding client #353 to the list of clients for aggregation.
[INFO][10:39:53]: [Server #1127936] Adding client #36 to the list of clients for aggregation.
[INFO][10:39:53]: [Server #1127936] Adding client #253 to the list of clients for aggregation.
[INFO][10:39:53]: [Server #1127936] Adding client #429 to the list of clients for aggregation.
[INFO][10:39:53]: [Server #1127936] Adding client #87 to the list of clients for aggregation.
[INFO][10:39:53]: [Server #1127936] Aggregating 10 clients in total.
[0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  1e-05  2e-10  2e-10
 6:  6.8876e+00  6.8875e+00  1e-05  1e-10  1e-10
 7:  6.8875e+00  6.8875e+00  1e-05  1e-10  1e-12
 8:  6.8875e+00  6.8875e+00  6e-06  1e-10  1e-12
Optimal solution found.
The calculated probability is:  [0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00144515 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00202678
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123053 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123054 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123035 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.0012289  0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00133602 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00145939 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123054 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.39691987 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055 0.00123055
 0.00123055 0.00123055 0.00123055 0.00123055]
current clients pool:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00398261
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.0121527  0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.0035405  0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00191544 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00175694
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01706382 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.0043518  0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00208848 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.0015333  0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00278592 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 1. 1. 0. 0. 0. 1. 0. 0. 3. 1. 0. 0. 0. 1. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 2. 0. 1. 0.
 1. 0. 1. 0. 2. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 2. 1. 1. 0. 0.
 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 2. 1. 6. 0. 0. 2. 1. 0. 1. 1. 2. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 3. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 1. 6. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 2. 0. 0.
 1. 1. 1. 2. 0. 0. 0. 2. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 1. 2. 0. 4. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1.
 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 4. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0.
 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0.
 1. 0. 1. 0. 4. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 3. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0.
 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 0. 1. 0. 6. 1. 1.
 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 6. 0. 0. 1. 1. 0. 1. 1. 2. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 2. 2. 1. 2. 0.]
local_gradient_bounds:  [INFO][10:39:55]: [Server #1127936] Global model accuracy: 96.21%

[INFO][10:39:55]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_88.pth.
[INFO][10:39:55]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_88.pth.
[INFO][10:39:55]: [93m[1m
[Server #1127936] Starting round 89/100.[0m
[0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00398261
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.0121527  0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.0035405  0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00191544 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00175694
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01706382 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.0043518  0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00208848 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.0015333  0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00278592 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.002 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  4e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  6e-05  9e-10  9e-10
 6:  6.8876e+00  6.8875e+00  5e-05  5e-10  5e-10
 7:  6.8875e+00  6.8875e+00  5e-05  1e-09  7e-11
 8:  6.8875e+00  6.8875e+00  4e-05  1e-09  8e-11
 9:  6.8875e+00  6.8875e+00  1e-05  1e-08  6e-10
10:  6.8875e+00  6.8875e+00  4e-06  5e-09  3e-10
Optimal solution found.
The calculated probability is:  [1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55606479e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 2.11428462e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55606591e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.62373175e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55606911e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 9.23835545e-01 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55606374e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55606868e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55606936e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.65645473e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04 1.55607016e-04 1.55607016e-04
 1.55607016e-04 1.55607016e-04]
current clients pool:  [INFO][10:39:55]: [Server #1127936] Selected clients: [212 269 375 114  88  20  74 426  54 112]
[INFO][10:39:55]: [Server #1127936] Selecting client #212 for training.
[INFO][10:39:55]: [Server #1127936] Sending the current model to client #212 (simulated).
[INFO][10:39:55]: [Server #1127936] Sending 0.24 MB of payload data to client #212 (simulated).
[INFO][10:39:55]: [Server #1127936] Selecting client #269 for training.
[INFO][10:39:55]: [Server #1127936] Sending the current model to client #269 (simulated).
[INFO][10:39:55]: [Server #1127936] Sending 0.24 MB of payload data to client #269 (simulated).
[INFO][10:39:55]: [Server #1127936] Selecting client #375 for training.
[INFO][10:39:55]: [Server #1127936] Sending the current model to client #375 (simulated).
[INFO][10:39:55]: [Client #212] Selected by the server.
[INFO][10:39:55]: [Client #212] Loading its data source...
[INFO][10:39:55]: [Client #212] Dataset size: 60000
[INFO][10:39:55]: [Client #212] Sampler: noniid
[INFO][10:39:55]: [Server #1127936] Sending 0.24 MB of payload data to client #375 (simulated).
[INFO][10:39:55]: [Client #269] Selected by the server.
[INFO][10:39:55]: [Client #269] Loading its data source...
[INFO][10:39:55]: [Client #269] Dataset size: 60000
[INFO][10:39:55]: [Client #269] Sampler: noniid
[INFO][10:39:55]: [Client #375] Selected by the server.
[INFO][10:39:55]: [Client #212] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:39:55]: [Client #375] Loading its data source...
[INFO][10:39:55]: [Client #375] Dataset size: 60000
[INFO][10:39:55]: [Client #375] Sampler: noniid
[INFO][10:39:55]: [Client #269] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:39:55]: [Client #375] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:39:56]: [93m[1m[Client #375] Started training in communication round #89.[0m
[INFO][10:39:56]: [93m[1m[Client #212] Started training in communication round #89.[0m
[INFO][10:39:56]: [93m[1m[Client #269] Started training in communication round #89.[0m
[INFO][10:39:58]: [Client #375] Loading the dataset.
[INFO][10:39:58]: [Client #269] Loading the dataset.
[INFO][10:39:58]: [Client #212] Loading the dataset.
[INFO][10:40:04]: [Client #375] Epoch: [1/5][0/10]	Loss: 0.007739
[INFO][10:40:04]: [Client #212] Epoch: [1/5][0/10]	Loss: 0.001586
[INFO][10:40:04]: [Client #375] Epoch: [2/5][0/10]	Loss: 0.000984
[INFO][10:40:04]: [Client #269] Epoch: [1/5][0/10]	Loss: 0.001025
[INFO][10:40:04]: [Client #212] Epoch: [2/5][0/10]	Loss: 0.000022
[INFO][10:40:04]: [Client #375] Epoch: [3/5][0/10]	Loss: 0.000006
[INFO][10:40:04]: [Client #269] Epoch: [2/5][0/10]	Loss: 0.000003
[INFO][10:40:04]: [Client #212] Epoch: [3/5][0/10]	Loss: 0.000007
[INFO][10:40:04]: [Client #375] Epoch: [4/5][0/10]	Loss: 0.000033
[INFO][10:40:04]: [Client #269] Epoch: [3/5][0/10]	Loss: 0.000023
[INFO][10:40:04]: [Client #212] Epoch: [4/5][0/10]	Loss: 0.000156
[INFO][10:40:04]: [Client #375] Epoch: [5/5][0/10]	Loss: 0.000514
[INFO][10:40:04]: [Client #269] Epoch: [4/5][0/10]	Loss: 0.000027
[INFO][10:40:04]: [Client #375] Model saved to /data/ykang/plato/results/test/model/lenet5_375_1127979.pth.
[INFO][10:40:04]: [Client #212] Epoch: [5/5][0/10]	Loss: 0.000327
[INFO][10:40:04]: [Client #269] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:40:04]: [Client #212] Model saved to /data/ykang/plato/results/test/model/lenet5_212_1127977.pth.
[INFO][10:40:04]: [Client #269] Model saved to /data/ykang/plato/results/test/model/lenet5_269_1127978.pth.
[INFO][10:40:05]: [Client #375] Loading a model from /data/ykang/plato/results/test/model/lenet5_375_1127979.pth.
[INFO][10:40:05]: [Client #375] Model trained.
[INFO][10:40:05]: [Client #375] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:40:05]: [Server #1127936] Received 0.24 MB of payload data from client #375 (simulated).
[INFO][10:40:05]: [Client #269] Loading a model from /data/ykang/plato/results/test/model/lenet5_269_1127978.pth.
[INFO][10:40:05]: [Client #269] Model trained.
[INFO][10:40:05]: [Client #269] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:40:05]: [Server #1127936] Received 0.24 MB of payload data from client #269 (simulated).
[INFO][10:40:05]: [Client #212] Loading a model from /data/ykang/plato/results/test/model/lenet5_212_1127977.pth.
[INFO][10:40:05]: [Client #212] Model trained.
[INFO][10:40:05]: [Client #212] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:40:05]: [Server #1127936] Received 0.24 MB of payload data from client #212 (simulated).
[INFO][10:40:05]: [Server #1127936] Selecting client #114 for training.
[INFO][10:40:05]: [Server #1127936] Sending the current model to client #114 (simulated).
[INFO][10:40:05]: [Server #1127936] Sending 0.24 MB of payload data to client #114 (simulated).
[INFO][10:40:05]: [Server #1127936] Selecting client #88 for training.
[INFO][10:40:05]: [Server #1127936] Sending the current model to client #88 (simulated).
[INFO][10:40:05]: [Server #1127936] Sending 0.24 MB of payload data to client #88 (simulated).
[INFO][10:40:05]: [Server #1127936] Selecting client #20 for training.
[INFO][10:40:05]: [Server #1127936] Sending the current model to client #20 (simulated).
[INFO][10:40:05]: [Client #114] Selected by the server.
[INFO][10:40:05]: [Client #114] Loading its data source...
[INFO][10:40:05]: [Client #114] Dataset size: 60000
[INFO][10:40:05]: [Client #114] Sampler: noniid
[INFO][10:40:05]: [Server #1127936] Sending 0.24 MB of payload data to client #20 (simulated).
[INFO][10:40:05]: [Client #114] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:40:05]: [Client #20] Selected by the server.
[INFO][10:40:05]: [Client #88] Selected by the server.
[INFO][10:40:05]: [Client #20] Loading its data source...
[INFO][10:40:05]: [Client #88] Loading its data source...
[INFO][10:40:05]: [Client #20] Dataset size: 60000
[INFO][10:40:05]: [Client #88] Dataset size: 60000
[INFO][10:40:05]: [Client #20] Sampler: noniid
[INFO][10:40:05]: [Client #88] Sampler: noniid
[INFO][10:40:05]: [Client #20] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:40:05]: [Client #88] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:40:05]: [93m[1m[Client #114] Started training in communication round #89.[0m
[INFO][10:40:05]: [93m[1m[Client #88] Started training in communication round #89.[0m
[INFO][10:40:05]: [93m[1m[Client #20] Started training in communication round #89.[0m
[INFO][10:40:08]: [Client #114] Loading the dataset.
[INFO][10:40:08]: [Client #88] Loading the dataset.
[INFO][10:40:08]: [Client #20] Loading the dataset.
[INFO][10:40:14]: [Client #20] Epoch: [1/5][0/10]	Loss: 0.000073
[INFO][10:40:14]: [Client #114] Epoch: [1/5][0/10]	Loss: 0.000049
[INFO][10:40:14]: [Client #88] Epoch: [1/5][0/10]	Loss: 0.003711
[INFO][10:40:14]: [Client #20] Epoch: [2/5][0/10]	Loss: 0.000093
[INFO][10:40:14]: [Client #114] Epoch: [2/5][0/10]	Loss: 0.000068
[INFO][10:40:14]: [Client #88] Epoch: [2/5][0/10]	Loss: 0.000848
[INFO][10:40:14]: [Client #20] Epoch: [3/5][0/10]	Loss: 0.000067
[INFO][10:40:14]: [Client #88] Epoch: [3/5][0/10]	Loss: 0.000059
[INFO][10:40:14]: [Client #114] Epoch: [3/5][0/10]	Loss: 0.000022
[INFO][10:40:14]: [Client #20] Epoch: [4/5][0/10]	Loss: 0.000267
[INFO][10:40:14]: [Client #88] Epoch: [4/5][0/10]	Loss: 0.068059
[INFO][10:40:14]: [Client #20] Epoch: [5/5][0/10]	Loss: 0.000302
[INFO][10:40:14]: [Client #114] Epoch: [4/5][0/10]	Loss: 0.000018
[INFO][10:40:14]: [Client #20] Model saved to /data/ykang/plato/results/test/model/lenet5_20_1127979.pth.
[INFO][10:40:14]: [Client #88] Epoch: [5/5][0/10]	Loss: 0.003200
[INFO][10:40:14]: [Client #114] Epoch: [5/5][0/10]	Loss: 0.000109
[INFO][10:40:14]: [Client #88] Model saved to /data/ykang/plato/results/test/model/lenet5_88_1127978.pth.
[INFO][10:40:14]: [Client #114] Model saved to /data/ykang/plato/results/test/model/lenet5_114_1127977.pth.
[INFO][10:40:15]: [Client #20] Loading a model from /data/ykang/plato/results/test/model/lenet5_20_1127979.pth.
[INFO][10:40:15]: [Client #20] Model trained.
[INFO][10:40:15]: [Client #20] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:40:15]: [Server #1127936] Received 0.24 MB of payload data from client #20 (simulated).
[INFO][10:40:15]: [Client #88] Loading a model from /data/ykang/plato/results/test/model/lenet5_88_1127978.pth.
[INFO][10:40:15]: [Client #88] Model trained.
[INFO][10:40:15]: [Client #88] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:40:15]: [Server #1127936] Received 0.24 MB of payload data from client #88 (simulated).
[INFO][10:40:15]: [Client #114] Loading a model from /data/ykang/plato/results/test/model/lenet5_114_1127977.pth.
[INFO][10:40:15]: [Client #114] Model trained.
[INFO][10:40:15]: [Client #114] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:40:15]: [Server #1127936] Received 0.24 MB of payload data from client #114 (simulated).
[INFO][10:40:15]: [Server #1127936] Selecting client #74 for training.
[INFO][10:40:15]: [Server #1127936] Sending the current model to client #74 (simulated).
[INFO][10:40:15]: [Server #1127936] Sending 0.24 MB of payload data to client #74 (simulated).
[INFO][10:40:15]: [Server #1127936] Selecting client #426 for training.
[INFO][10:40:15]: [Server #1127936] Sending the current model to client #426 (simulated).
[INFO][10:40:15]: [Server #1127936] Sending 0.24 MB of payload data to client #426 (simulated).
[INFO][10:40:15]: [Server #1127936] Selecting client #54 for training.
[INFO][10:40:15]: [Server #1127936] Sending the current model to client #54 (simulated).
[INFO][10:40:15]: [Client #74] Selected by the server.
[INFO][10:40:15]: [Client #74] Loading its data source...
[INFO][10:40:15]: [Client #74] Dataset size: 60000
[INFO][10:40:15]: [Client #74] Sampler: noniid
[INFO][10:40:15]: [Server #1127936] Sending 0.24 MB of payload data to client #54 (simulated).
[INFO][10:40:15]: [Client #426] Selected by the server.
[INFO][10:40:15]: [Client #426] Loading its data source...
[INFO][10:40:15]: [Client #426] Dataset size: 60000
[INFO][10:40:15]: [Client #426] Sampler: noniid
[INFO][10:40:15]: [Client #54] Selected by the server.
[INFO][10:40:15]: [Client #54] Loading its data source...
[INFO][10:40:15]: [Client #54] Dataset size: 60000
[INFO][10:40:15]: [Client #54] Sampler: noniid
[INFO][10:40:15]: [Client #74] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:40:15]: [Client #426] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:40:15]: [Client #54] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:40:15]: [93m[1m[Client #74] Started training in communication round #89.[0m
[INFO][10:40:15]: [93m[1m[Client #426] Started training in communication round #89.[0m
[INFO][10:40:15]: [93m[1m[Client #54] Started training in communication round #89.[0m
[INFO][10:40:17]: [Client #426] Loading the dataset.
[INFO][10:40:17]: [Client #74] Loading the dataset.
[INFO][10:40:17]: [Client #54] Loading the dataset.
[INFO][10:40:24]: [Client #54] Epoch: [1/5][0/10]	Loss: 0.011678
[INFO][10:40:24]: [Client #74] Epoch: [1/5][0/10]	Loss: 0.000191
[INFO][10:40:24]: [Client #426] Epoch: [1/5][0/10]	Loss: 0.004279
[INFO][10:40:24]: [Client #54] Epoch: [2/5][0/10]	Loss: 0.000658
[INFO][10:40:24]: [Client #426] Epoch: [2/5][0/10]	Loss: 0.000214
[INFO][10:40:24]: [Client #74] Epoch: [2/5][0/10]	Loss: 0.000242
[INFO][10:40:24]: [Client #54] Epoch: [3/5][0/10]	Loss: 0.000101
[INFO][10:40:24]: [Client #426] Epoch: [3/5][0/10]	Loss: 0.000141
[INFO][10:40:24]: [Client #74] Epoch: [3/5][0/10]	Loss: 0.000294
[INFO][10:40:24]: [Client #426] Epoch: [4/5][0/10]	Loss: 0.000308
[INFO][10:40:24]: [Client #74] Epoch: [4/5][0/10]	Loss: 0.000042
[INFO][10:40:24]: [Client #54] Epoch: [4/5][0/10]	Loss: 0.014485
[INFO][10:40:24]: [Client #426] Epoch: [5/5][0/10]	Loss: 0.003921
[INFO][10:40:24]: [Client #74] Epoch: [5/5][0/10]	Loss: 0.000091
[INFO][10:40:24]: [Client #54] Epoch: [5/5][0/10]	Loss: 0.000337
[INFO][10:40:24]: [Client #74] Model saved to /data/ykang/plato/results/test/model/lenet5_74_1127977.pth.
[INFO][10:40:24]: [Client #426] Model saved to /data/ykang/plato/results/test/model/lenet5_426_1127978.pth.
[INFO][10:40:24]: [Client #54] Model saved to /data/ykang/plato/results/test/model/lenet5_54_1127979.pth.
[INFO][10:40:25]: [Client #426] Loading a model from /data/ykang/plato/results/test/model/lenet5_426_1127978.pth.
[INFO][10:40:25]: [Client #426] Model trained.
[INFO][10:40:25]: [Client #426] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:40:25]: [Server #1127936] Received 0.24 MB of payload data from client #426 (simulated).
[INFO][10:40:25]: [Client #74] Loading a model from /data/ykang/plato/results/test/model/lenet5_74_1127977.pth.
[INFO][10:40:25]: [Client #74] Model trained.
[INFO][10:40:25]: [Client #74] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:40:25]: [Server #1127936] Received 0.24 MB of payload data from client #74 (simulated).
[INFO][10:40:25]: [Client #54] Loading a model from /data/ykang/plato/results/test/model/lenet5_54_1127979.pth.
[INFO][10:40:25]: [Client #54] Model trained.
[INFO][10:40:25]: [Client #54] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:40:25]: [Server #1127936] Received 0.24 MB of payload data from client #54 (simulated).
[INFO][10:40:25]: [Server #1127936] Selecting client #112 for training.
[INFO][10:40:25]: [Server #1127936] Sending the current model to client #112 (simulated).
[INFO][10:40:25]: [Server #1127936] Sending 0.24 MB of payload data to client #112 (simulated).
[INFO][10:40:25]: [Client #112] Selected by the server.
[INFO][10:40:25]: [Client #112] Loading its data source...
[INFO][10:40:25]: [Client #112] Dataset size: 60000
[INFO][10:40:25]: [Client #112] Sampler: noniid
[INFO][10:40:25]: [Client #112] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:40:25]: [93m[1m[Client #112] Started training in communication round #89.[0m
[INFO][10:40:27]: [Client #112] Loading the dataset.
[INFO][10:40:33]: [Client #112] Epoch: [1/5][0/10]	Loss: 0.004118
[INFO][10:40:33]: [Client #112] Epoch: [2/5][0/10]	Loss: 0.000093
[INFO][10:40:33]: [Client #112] Epoch: [3/5][0/10]	Loss: 0.000096
[INFO][10:40:33]: [Client #112] Epoch: [4/5][0/10]	Loss: 0.001190
[INFO][10:40:33]: [Client #112] Epoch: [5/5][0/10]	Loss: 0.001219
[INFO][10:40:33]: [Client #112] Model saved to /data/ykang/plato/results/test/model/lenet5_112_1127977.pth.
[INFO][10:40:34]: [Client #112] Loading a model from /data/ykang/plato/results/test/model/lenet5_112_1127977.pth.
[INFO][10:40:34]: [Client #112] Model trained.
[INFO][10:40:34]: [Client #112] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:40:34]: [Server #1127936] Received 0.24 MB of payload data from client #112 (simulated).
[INFO][10:40:34]: [Server #1127936] Adding client #250 to the list of clients for aggregation.
[INFO][10:40:34]: [Server #1127936] Adding client #348 to the list of clients for aggregation.
[INFO][10:40:34]: [Server #1127936] Adding client #486 to the list of clients for aggregation.
[INFO][10:40:34]: [Server #1127936] Adding client #24 to the list of clients for aggregation.
[INFO][10:40:34]: [Server #1127936] Adding client #172 to the list of clients for aggregation.
[INFO][10:40:34]: [Server #1127936] Adding client #375 to the list of clients for aggregation.
[INFO][10:40:34]: [Server #1127936] Adding client #426 to the list of clients for aggregation.
[INFO][10:40:34]: [Server #1127936] Adding client #112 to the list of clients for aggregation.
[INFO][10:40:34]: [Server #1127936] Adding client #20 to the list of clients for aggregation.
[INFO][10:40:34]: [Server #1127936] Adding client #54 to the list of clients for aggregation.
[INFO][10:40:34]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.0004451  0.         0.         0.         0.00135802
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00399561
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00040313 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00338924 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00390109 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00249693
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00612425 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00078395
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00289022
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 1. 1. 0. 0. 0. 1. 0. 0. 3. 1. 0. 0. 0. 1. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 2. 0. 1. 0.
 1. 0. 1. 0. 2. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 2. 1. 1. 0. 0.
 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 2. 1. 6. 0. 0. 2. 1. 0. 1. 1. 2. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 3. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 1. 6. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 2. 0. 0.
 1. 1. 1. 2. 0. 0. 0. 2. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 1. 2. 0. 4. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1.
 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 4. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0.
 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0.
 1. 0. 1. 0. 4. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 3. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0.
 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 0. 1. 0. 6. 1. 1.
 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 6. 0. 0. 1. 1. 0. 1. 1. 2. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 2. 2. 1. 2. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.0004451  0.         0.         0.         0.00135802
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00399561
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00040313 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00338924 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00390109 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00249693
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00612425 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00078395
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00289022
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:40:36]: [Server #1127936] Global model accuracy: 96.44%

[INFO][10:40:36]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_89.pth.
[INFO][10:40:36]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_89.pth.
[INFO][10:40:36]: [93m[1m
[Server #1127936] Starting round 90/100.[0m
[0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  2e-05  4e-10  4e-10
 6:  6.8876e+00  6.8875e+00  2e-05  2e-10  2e-10
 7:  6.8875e+00  6.8875e+00  2e-05  5e-10  1e-11
 8:  6.8875e+00  6.8875e+00  1e-05  4e-10  9e-12
 9:  6.8875e+00  6.8875e+00  8e-06  7e-10  2e-11
10:  6.8875e+00  6.8875e+00  5e-07  1e-09  2e-11
Optimal solution found.
The calculated probability is:  [4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471883e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 5.29452145e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89467589e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471893e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 9.76006436e-01
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 8.60892103e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 5.68325657e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89461722e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471770e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 5.83093688e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05 4.89471937e-05 4.89471937e-05
 4.89471937e-05 4.89471937e-05]
current clients pool:  [INFO][10:40:37]: [Server #1127936] Selected clients: [172 388  69 214 138 152 337 259  97 236]
[INFO][10:40:37]: [Server #1127936] Selecting client #172 for training.
[INFO][10:40:37]: [Server #1127936] Sending the current model to client #172 (simulated).
[INFO][10:40:37]: [Server #1127936] Sending 0.24 MB of payload data to client #172 (simulated).
[INFO][10:40:37]: [Server #1127936] Selecting client #388 for training.
[INFO][10:40:37]: [Server #1127936] Sending the current model to client #388 (simulated).
[INFO][10:40:37]: [Server #1127936] Sending 0.24 MB of payload data to client #388 (simulated).
[INFO][10:40:37]: [Server #1127936] Selecting client #69 for training.
[INFO][10:40:37]: [Server #1127936] Sending the current model to client #69 (simulated).
[INFO][10:40:37]: [Client #172] Selected by the server.
[INFO][10:40:37]: [Client #172] Loading its data source...
[INFO][10:40:37]: [Client #172] Dataset size: 60000
[INFO][10:40:37]: [Client #172] Sampler: noniid
[INFO][10:40:37]: [Server #1127936] Sending 0.24 MB of payload data to client #69 (simulated).
[INFO][10:40:37]: [Client #388] Selected by the server.
[INFO][10:40:37]: [Client #388] Loading its data source...
[INFO][10:40:37]: [Client #69] Selected by the server.
[INFO][10:40:37]: [Client #388] Dataset size: 60000
[INFO][10:40:37]: [Client #69] Loading its data source...
[INFO][10:40:37]: [Client #388] Sampler: noniid
[INFO][10:40:37]: [Client #69] Dataset size: 60000
[INFO][10:40:37]: [Client #172] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:40:37]: [Client #69] Sampler: noniid
[INFO][10:40:37]: [Client #388] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:40:37]: [Client #69] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:40:37]: [93m[1m[Client #388] Started training in communication round #90.[0m
[INFO][10:40:37]: [93m[1m[Client #172] Started training in communication round #90.[0m
[INFO][10:40:37]: [93m[1m[Client #69] Started training in communication round #90.[0m
[INFO][10:40:39]: [Client #69] Loading the dataset.
[INFO][10:40:39]: [Client #172] Loading the dataset.
[INFO][10:40:39]: [Client #388] Loading the dataset.
[INFO][10:40:45]: [Client #69] Epoch: [1/5][0/10]	Loss: 0.000082
[INFO][10:40:45]: [Client #388] Epoch: [1/5][0/10]	Loss: 0.000023
[INFO][10:40:45]: [Client #172] Epoch: [1/5][0/10]	Loss: 0.000068
[INFO][10:40:45]: [Client #69] Epoch: [2/5][0/10]	Loss: 0.000090
[INFO][10:40:45]: [Client #172] Epoch: [2/5][0/10]	Loss: 0.000182
[INFO][10:40:45]: [Client #388] Epoch: [2/5][0/10]	Loss: 0.000858
[INFO][10:40:45]: [Client #69] Epoch: [3/5][0/10]	Loss: 0.000052
[INFO][10:40:45]: [Client #388] Epoch: [3/5][0/10]	Loss: 0.000040
[INFO][10:40:45]: [Client #172] Epoch: [3/5][0/10]	Loss: 0.000071
[INFO][10:40:45]: [Client #69] Epoch: [4/5][0/10]	Loss: 0.000130
[INFO][10:40:45]: [Client #388] Epoch: [4/5][0/10]	Loss: 0.000028
[INFO][10:40:45]: [Client #172] Epoch: [4/5][0/10]	Loss: 0.000001
[INFO][10:40:46]: [Client #69] Epoch: [5/5][0/10]	Loss: 0.000002
[INFO][10:40:46]: [Client #172] Epoch: [5/5][0/10]	Loss: 0.000001
[INFO][10:40:46]: [Client #69] Model saved to /data/ykang/plato/results/test/model/lenet5_69_1127979.pth.
[INFO][10:40:46]: [Client #388] Epoch: [5/5][0/10]	Loss: 0.000305
[INFO][10:40:46]: [Client #172] Model saved to /data/ykang/plato/results/test/model/lenet5_172_1127977.pth.
[INFO][10:40:46]: [Client #388] Model saved to /data/ykang/plato/results/test/model/lenet5_388_1127978.pth.
[INFO][10:40:46]: [Client #69] Loading a model from /data/ykang/plato/results/test/model/lenet5_69_1127979.pth.
[INFO][10:40:46]: [Client #69] Model trained.
[INFO][10:40:46]: [Client #69] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:40:46]: [Server #1127936] Received 0.24 MB of payload data from client #69 (simulated).
[INFO][10:40:46]: [Client #388] Loading a model from /data/ykang/plato/results/test/model/lenet5_388_1127978.pth.
[INFO][10:40:46]: [Client #388] Model trained.
[INFO][10:40:46]: [Client #388] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:40:46]: [Server #1127936] Received 0.24 MB of payload data from client #388 (simulated).
[INFO][10:40:47]: [Client #172] Loading a model from /data/ykang/plato/results/test/model/lenet5_172_1127977.pth.
[INFO][10:40:47]: [Client #172] Model trained.
[INFO][10:40:47]: [Client #172] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:40:47]: [Server #1127936] Received 0.24 MB of payload data from client #172 (simulated).
[INFO][10:40:47]: [Server #1127936] Selecting client #214 for training.
[INFO][10:40:47]: [Server #1127936] Sending the current model to client #214 (simulated).
[INFO][10:40:47]: [Server #1127936] Sending 0.24 MB of payload data to client #214 (simulated).
[INFO][10:40:47]: [Server #1127936] Selecting client #138 for training.
[INFO][10:40:47]: [Server #1127936] Sending the current model to client #138 (simulated).
[INFO][10:40:47]: [Server #1127936] Sending 0.24 MB of payload data to client #138 (simulated).
[INFO][10:40:47]: [Server #1127936] Selecting client #152 for training.
[INFO][10:40:47]: [Server #1127936] Sending the current model to client #152 (simulated).
[INFO][10:40:47]: [Client #214] Selected by the server.
[INFO][10:40:47]: [Client #214] Loading its data source...
[INFO][10:40:47]: [Client #214] Dataset size: 60000
[INFO][10:40:47]: [Client #214] Sampler: noniid
[INFO][10:40:47]: [Server #1127936] Sending 0.24 MB of payload data to client #152 (simulated).
[INFO][10:40:47]: [Client #138] Selected by the server.
[INFO][10:40:47]: [Client #138] Loading its data source...
[INFO][10:40:47]: [Client #152] Selected by the server.
[INFO][10:40:47]: [Client #138] Dataset size: 60000
[INFO][10:40:47]: [Client #152] Loading its data source...
[INFO][10:40:47]: [Client #138] Sampler: noniid
[INFO][10:40:47]: [Client #152] Dataset size: 60000
[INFO][10:40:47]: [Client #152] Sampler: noniid
[INFO][10:40:47]: [Client #214] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:40:47]: [Client #152] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:40:47]: [Client #138] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:40:47]: [93m[1m[Client #214] Started training in communication round #90.[0m
[INFO][10:40:47]: [93m[1m[Client #152] Started training in communication round #90.[0m
[INFO][10:40:47]: [93m[1m[Client #138] Started training in communication round #90.[0m
[INFO][10:40:49]: [Client #152] Loading the dataset.
[INFO][10:40:49]: [Client #214] Loading the dataset.
[INFO][10:40:49]: [Client #138] Loading the dataset.
[INFO][10:40:55]: [Client #152] Epoch: [1/5][0/10]	Loss: 0.000645
[INFO][10:40:55]: [Client #214] Epoch: [1/5][0/10]	Loss: 0.000067
[INFO][10:40:55]: [Client #138] Epoch: [1/5][0/10]	Loss: 0.002267
[INFO][10:40:55]: [Client #152] Epoch: [2/5][0/10]	Loss: 0.000571
[INFO][10:40:55]: [Client #214] Epoch: [2/5][0/10]	Loss: 0.000065
[INFO][10:40:55]: [Client #152] Epoch: [3/5][0/10]	Loss: 0.000033
[INFO][10:40:55]: [Client #138] Epoch: [2/5][0/10]	Loss: 0.000099
[INFO][10:40:55]: [Client #214] Epoch: [3/5][0/10]	Loss: 0.000016
[INFO][10:40:55]: [Client #138] Epoch: [3/5][0/10]	Loss: 0.000001
[INFO][10:40:55]: [Client #152] Epoch: [4/5][0/10]	Loss: 0.000029
[INFO][10:40:55]: [Client #214] Epoch: [4/5][0/10]	Loss: 0.000006
[INFO][10:40:55]: [Client #152] Epoch: [5/5][0/10]	Loss: 0.000011
[INFO][10:40:55]: [Client #138] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:40:55]: [Client #152] Model saved to /data/ykang/plato/results/test/model/lenet5_152_1127979.pth.
[INFO][10:40:55]: [Client #214] Epoch: [5/5][0/10]	Loss: 0.000064
[INFO][10:40:55]: [Client #138] Epoch: [5/5][0/10]	Loss: 0.000008
[INFO][10:40:55]: [Client #214] Model saved to /data/ykang/plato/results/test/model/lenet5_214_1127977.pth.
[INFO][10:40:55]: [Client #138] Model saved to /data/ykang/plato/results/test/model/lenet5_138_1127978.pth.
[INFO][10:40:56]: [Client #152] Loading a model from /data/ykang/plato/results/test/model/lenet5_152_1127979.pth.
[INFO][10:40:56]: [Client #152] Model trained.
[INFO][10:40:56]: [Client #152] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:40:56]: [Server #1127936] Received 0.24 MB of payload data from client #152 (simulated).
[INFO][10:40:56]: [Client #138] Loading a model from /data/ykang/plato/results/test/model/lenet5_138_1127978.pth.
[INFO][10:40:56]: [Client #138] Model trained.
[INFO][10:40:56]: [Client #138] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:40:56]: [Server #1127936] Received 0.24 MB of payload data from client #138 (simulated).
[INFO][10:40:56]: [Client #214] Loading a model from /data/ykang/plato/results/test/model/lenet5_214_1127977.pth.
[INFO][10:40:56]: [Client #214] Model trained.
[INFO][10:40:56]: [Client #214] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:40:56]: [Server #1127936] Received 0.24 MB of payload data from client #214 (simulated).
[INFO][10:40:56]: [Server #1127936] Selecting client #337 for training.
[INFO][10:40:56]: [Server #1127936] Sending the current model to client #337 (simulated).
[INFO][10:40:56]: [Server #1127936] Sending 0.24 MB of payload data to client #337 (simulated).
[INFO][10:40:56]: [Server #1127936] Selecting client #259 for training.
[INFO][10:40:56]: [Server #1127936] Sending the current model to client #259 (simulated).
[INFO][10:40:56]: [Server #1127936] Sending 0.24 MB of payload data to client #259 (simulated).
[INFO][10:40:56]: [Server #1127936] Selecting client #97 for training.
[INFO][10:40:56]: [Server #1127936] Sending the current model to client #97 (simulated).
[INFO][10:40:56]: [Client #337] Selected by the server.
[INFO][10:40:56]: [Client #337] Loading its data source...
[INFO][10:40:56]: [Client #337] Dataset size: 60000
[INFO][10:40:56]: [Client #337] Sampler: noniid
[INFO][10:40:56]: [Server #1127936] Sending 0.24 MB of payload data to client #97 (simulated).
[INFO][10:40:56]: [Client #259] Selected by the server.
[INFO][10:40:56]: [Client #259] Loading its data source...
[INFO][10:40:56]: [Client #259] Dataset size: 60000
[INFO][10:40:56]: [Client #97] Selected by the server.
[INFO][10:40:56]: [Client #259] Sampler: noniid
[INFO][10:40:56]: [Client #97] Loading its data source...
[INFO][10:40:56]: [Client #97] Dataset size: 60000
[INFO][10:40:56]: [Client #97] Sampler: noniid
[INFO][10:40:56]: [Client #337] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:40:56]: [Client #97] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:40:56]: [Client #259] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:40:56]: [93m[1m[Client #97] Started training in communication round #90.[0m
[INFO][10:40:56]: [93m[1m[Client #259] Started training in communication round #90.[0m
[INFO][10:40:56]: [93m[1m[Client #337] Started training in communication round #90.[0m
[INFO][10:40:58]: [Client #97] Loading the dataset.
[INFO][10:40:58]: [Client #337] Loading the dataset.
[INFO][10:40:58]: [Client #259] Loading the dataset.
[INFO][10:41:04]: [Client #97] Epoch: [1/5][0/10]	Loss: 0.002634
[INFO][10:41:04]: [Client #259] Epoch: [1/5][0/10]	Loss: 0.000095
[INFO][10:41:04]: [Client #337] Epoch: [1/5][0/10]	Loss: 0.000027
[INFO][10:41:04]: [Client #97] Epoch: [2/5][0/10]	Loss: 0.000901
[INFO][10:41:05]: [Client #259] Epoch: [2/5][0/10]	Loss: 0.000690
[INFO][10:41:05]: [Client #337] Epoch: [2/5][0/10]	Loss: 0.000057
[INFO][10:41:05]: [Client #259] Epoch: [3/5][0/10]	Loss: 0.000268
[INFO][10:41:05]: [Client #337] Epoch: [3/5][0/10]	Loss: 0.000334
[INFO][10:41:05]: [Client #97] Epoch: [3/5][0/10]	Loss: 0.000001
[INFO][10:41:05]: [Client #259] Epoch: [4/5][0/10]	Loss: 0.000034
[INFO][10:41:05]: [Client #337] Epoch: [4/5][0/10]	Loss: 0.000006
[INFO][10:41:05]: [Client #97] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:41:05]: [Client #259] Epoch: [5/5][0/10]	Loss: 0.000117
[INFO][10:41:05]: [Client #337] Epoch: [5/5][0/10]	Loss: 0.000180
[INFO][10:41:05]: [Client #259] Model saved to /data/ykang/plato/results/test/model/lenet5_259_1127978.pth.
[INFO][10:41:05]: [Client #97] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:41:05]: [Client #337] Model saved to /data/ykang/plato/results/test/model/lenet5_337_1127977.pth.
[INFO][10:41:05]: [Client #97] Model saved to /data/ykang/plato/results/test/model/lenet5_97_1127979.pth.
[INFO][10:41:06]: [Client #259] Loading a model from /data/ykang/plato/results/test/model/lenet5_259_1127978.pth.
[INFO][10:41:06]: [Client #259] Model trained.
[INFO][10:41:06]: [Client #259] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:41:06]: [Server #1127936] Received 0.24 MB of payload data from client #259 (simulated).
[INFO][10:41:06]: [Client #97] Loading a model from /data/ykang/plato/results/test/model/lenet5_97_1127979.pth.
[INFO][10:41:06]: [Client #97] Model trained.
[INFO][10:41:06]: [Client #97] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:41:06]: [Server #1127936] Received 0.24 MB of payload data from client #97 (simulated).
[INFO][10:41:06]: [Client #337] Loading a model from /data/ykang/plato/results/test/model/lenet5_337_1127977.pth.
[INFO][10:41:06]: [Client #337] Model trained.
[INFO][10:41:06]: [Client #337] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:41:06]: [Server #1127936] Received 0.24 MB of payload data from client #337 (simulated).
[INFO][10:41:06]: [Server #1127936] Selecting client #236 for training.
[INFO][10:41:06]: [Server #1127936] Sending the current model to client #236 (simulated).
[INFO][10:41:06]: [Server #1127936] Sending 0.24 MB of payload data to client #236 (simulated).
[INFO][10:41:06]: [Client #236] Selected by the server.
[INFO][10:41:06]: [Client #236] Loading its data source...
[INFO][10:41:06]: [Client #236] Dataset size: 60000
[INFO][10:41:06]: [Client #236] Sampler: noniid
[INFO][10:41:06]: [Client #236] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:41:06]: [93m[1m[Client #236] Started training in communication round #90.[0m
[INFO][10:41:08]: [Client #236] Loading the dataset.
[INFO][10:41:13]: [Client #236] Epoch: [1/5][0/10]	Loss: 0.002634
[INFO][10:41:13]: [Client #236] Epoch: [2/5][0/10]	Loss: 0.000388
[INFO][10:41:13]: [Client #236] Epoch: [3/5][0/10]	Loss: 0.000023
[INFO][10:41:13]: [Client #236] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:41:13]: [Client #236] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:41:13]: [Client #236] Model saved to /data/ykang/plato/results/test/model/lenet5_236_1127977.pth.
[INFO][10:41:14]: [Client #236] Loading a model from /data/ykang/plato/results/test/model/lenet5_236_1127977.pth.
[INFO][10:41:14]: [Client #236] Model trained.
[INFO][10:41:14]: [Client #236] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:41:14]: [Server #1127936] Received 0.24 MB of payload data from client #236 (simulated).
[INFO][10:41:14]: [Server #1127936] Adding client #269 to the list of clients for aggregation.
[INFO][10:41:14]: [Server #1127936] Adding client #114 to the list of clients for aggregation.
[INFO][10:41:14]: [Server #1127936] Adding client #74 to the list of clients for aggregation.
[INFO][10:41:14]: [Server #1127936] Adding client #475 to the list of clients for aggregation.
[INFO][10:41:14]: [Server #1127936] Adding client #214 to the list of clients for aggregation.
[INFO][10:41:14]: [Server #1127936] Adding client #259 to the list of clients for aggregation.
[INFO][10:41:14]: [Server #1127936] Adding client #337 to the list of clients for aggregation.
[INFO][10:41:14]: [Server #1127936] Adding client #138 to the list of clients for aggregation.
[INFO][10:41:14]: [Server #1127936] Adding client #388 to the list of clients for aggregation.
[INFO][10:41:14]: [Server #1127936] Adding client #236 to the list of clients for aggregation.
[INFO][10:41:14]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00092643 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00257081
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00345582
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00160503 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00810811 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00077103 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00214051 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00076278 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00141208 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00759988 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 1. 1. 0. 0. 0. 1. 0. 0. 3. 1. 0. 0. 0. 1. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 2. 0. 1. 0.
 1. 1. 1. 0. 2. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 2. 1. 1. 0. 0.
 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 2. 1. 6. 0. 0. 2. 1. 0. 1. 1. 2. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 3. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 1. 6. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 2. 0. 0.
 1. 1. 1. 2. 0. 0. 0. 2. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 1. 2. 0. 4. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1.
 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 4. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0.
 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0.
 1. 0. 1. 0. 4. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 3. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0.
 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 0. 1. 0. 6. 1. 1.
 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 6. 0. 0. 1. 1. 0. 1. 1. 2. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 2. 2. 1. 2. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00092643 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00257081
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00345582
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00160503 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00810811 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00077103 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00214051 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00076278 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00141208 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00759988 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:41:16]: [Server #1127936] Global model accuracy: 96.10%

[INFO][10:41:16]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_90.pth.
[INFO][10:41:16]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_90.pth.
[INFO][10:41:16]: [93m[1m
[Server #1127936] Starting round 91/100.[0m
[INFO][10:41:17]: [Server #1127936] Selected clients: [472 475 316 354 261 476  66 142 401 332]
[INFO][10:41:17]: [Server #1127936] Selecting client #472 for training.
[INFO][10:41:17]: [Server #1127936] Sending the current model to client #472 (simulated).
[INFO][10:41:17]: [Server #1127936] Sending 0.24 MB of payload data to client #472 (simulated).
[INFO][10:41:17]: [Server #1127936] Selecting client #475 for training.
[INFO][10:41:17]: [Server #1127936] Sending the current model to client #475 (simulated).
[INFO][10:41:17]: [Server #1127936] Sending 0.24 MB of payload data to client #475 (simulated).
[INFO][10:41:17]: [Server #1127936] Selecting client #316 for training.
[INFO][10:41:17]: [Server #1127936] Sending the current model to client #316 (simulated).
[INFO][10:41:17]: [Client #472] Selected by the server.
[INFO][10:41:17]: [Client #472] Loading its data source...
[INFO][10:41:17]: [Client #472] Dataset size: 60000
[INFO][10:41:17]: [Client #472] Sampler: noniid
[INFO][10:41:17]: [Server #1127936] Sending 0.24 MB of payload data to client #316 (simulated).
[INFO][10:41:17]: [Client #475] Selected by the server.
[INFO][10:41:17]: [Client #475] Loading its data source...
[INFO][10:41:17]: [Client #316] Selected by the server.
[INFO][10:41:17]: [Client #475] Dataset size: 60000
[INFO][10:41:17]: [Client #316] Loading its data source...
[INFO][10:41:17]: [Client #475] Sampler: noniid
[INFO][10:41:17]: [Client #316] Dataset size: 60000
[INFO][10:41:17]: [Client #316] Sampler: noniid
[INFO][10:41:17]: [Client #472] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:41:17]: [Client #316] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:41:17]: [Client #475] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:41:17]: [93m[1m[Client #472] Started training in communication round #91.[0m
[INFO][10:41:17]: [93m[1m[Client #316] Started training in communication round #91.[0m
[INFO][10:41:17]: [93m[1m[Client #475] Started training in communication round #91.[0m
[INFO][10:41:19]: [Client #316] Loading the dataset.
[INFO][10:41:19]: [Client #475] Loading the dataset.
[INFO][10:41:19]: [Client #472] Loading the dataset.
[INFO][10:41:25]: [Client #316] Epoch: [1/5][0/10]	Loss: 0.000061
[INFO][10:41:25]: [Client #475] Epoch: [1/5][0/10]	Loss: 0.000105
[INFO][10:41:25]: [Client #472] Epoch: [1/5][0/10]	Loss: 0.001359
[INFO][10:41:25]: [Client #316] Epoch: [2/5][0/10]	Loss: 0.000411
[INFO][10:41:25]: [Client #472] Epoch: [2/5][0/10]	Loss: 0.001451
[INFO][10:41:25]: [Client #475] Epoch: [2/5][0/10]	Loss: 0.001607
[INFO][10:41:25]: [Client #316] Epoch: [3/5][0/10]	Loss: 0.000029
[INFO][10:41:25]: [Client #475] Epoch: [3/5][0/10]	Loss: 0.000000
[INFO][10:41:25]: [Client #472] Epoch: [3/5][0/10]	Loss: 0.000030
[INFO][10:41:25]: [Client #316] Epoch: [4/5][0/10]	Loss: 0.000035
[INFO][10:41:25]: [Client #475] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:41:25]: [Client #472] Epoch: [4/5][0/10]	Loss: 0.000782
[INFO][10:41:25]: [Client #316] Epoch: [5/5][0/10]	Loss: 0.000018
[INFO][10:41:25]: [Client #472] Epoch: [5/5][0/10]	Loss: 0.005019
[INFO][10:41:25]: [Client #316] Model saved to /data/ykang/plato/results/test/model/lenet5_316_1127979.pth.
[INFO][10:41:25]: [Client #475] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:41:25]: [Client #472] Model saved to /data/ykang/plato/results/test/model/lenet5_472_1127977.pth.
[INFO][10:41:25]: [Client #475] Model saved to /data/ykang/plato/results/test/model/lenet5_475_1127978.pth.
[INFO][10:41:27]: [Client #316] Loading a model from /data/ykang/plato/results/test/model/lenet5_316_1127979.pth.
[INFO][10:41:27]: [Client #316] Model trained.
[INFO][10:41:27]: [Client #316] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:41:27]: [Server #1127936] Received 0.24 MB of payload data from client #316 (simulated).
[INFO][10:41:27]: [Client #472] Loading a model from /data/ykang/plato/results/test/model/lenet5_472_1127977.pth.
[INFO][10:41:27]: [Client #472] Model trained.
[INFO][10:41:27]: [Client #472] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:41:27]: [Server #1127936] Received 0.24 MB of payload data from client #472 (simulated).
[INFO][10:41:27]: [Client #475] Loading a model from /data/ykang/plato/results/test/model/lenet5_475_1127978.pth.
[INFO][10:41:27]: [Client #475] Model trained.
[INFO][10:41:27]: [Client #475] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:41:27]: [Server #1127936] Received 0.24 MB of payload data from client #475 (simulated).
[INFO][10:41:27]: [Server #1127936] Selecting client #354 for training.
[INFO][10:41:27]: [Server #1127936] Sending the current model to client #354 (simulated).
[INFO][10:41:27]: [Server #1127936] Sending 0.24 MB of payload data to client #354 (simulated).
[INFO][10:41:27]: [Server #1127936] Selecting client #261 for training.
[INFO][10:41:27]: [Server #1127936] Sending the current model to client #261 (simulated).
[INFO][10:41:27]: [Server #1127936] Sending 0.24 MB of payload data to client #261 (simulated).
[INFO][10:41:27]: [Server #1127936] Selecting client #476 for training.
[INFO][10:41:27]: [Server #1127936] Sending the current model to client #476 (simulated).
[INFO][10:41:27]: [Client #354] Selected by the server.
[INFO][10:41:27]: [Client #354] Loading its data source...
[INFO][10:41:27]: [Client #354] Dataset size: 60000
[INFO][10:41:27]: [Client #354] Sampler: noniid
[INFO][10:41:27]: [Server #1127936] Sending 0.24 MB of payload data to client #476 (simulated).
[INFO][10:41:27]: [Client #261] Selected by the server.
[INFO][10:41:27]: [Client #261] Loading its data source...
[INFO][10:41:27]: [Client #476] Selected by the server.
[INFO][10:41:27]: [Client #261] Dataset size: 60000
[INFO][10:41:27]: [Client #476] Loading its data source...
[INFO][10:41:27]: [Client #261] Sampler: noniid
[INFO][10:41:27]: [Client #476] Dataset size: 60000
[INFO][10:41:27]: [Client #476] Sampler: noniid
[INFO][10:41:27]: [Client #354] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:41:27]: [Client #476] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:41:27]: [93m[1m[Client #354] Started training in communication round #91.[0m
[INFO][10:41:27]: [93m[1m[Client #476] Started training in communication round #91.[0m
[INFO][10:41:27]: [Client #261] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:41:27]: [93m[1m[Client #261] Started training in communication round #91.[0m
[INFO][10:41:29]: [Client #354] Loading the dataset.
[INFO][10:41:29]: [Client #261] Loading the dataset.
[INFO][10:41:29]: [Client #476] Loading the dataset.
[INFO][10:41:36]: [Client #261] Epoch: [1/5][0/10]	Loss: 0.000868
[INFO][10:41:36]: [Client #354] Epoch: [1/5][0/10]	Loss: 0.000976
[INFO][10:41:36]: [Client #476] Epoch: [1/5][0/10]	Loss: 0.000201
[INFO][10:41:36]: [Client #261] Epoch: [2/5][0/10]	Loss: 0.000155
[INFO][10:41:36]: [Client #354] Epoch: [2/5][0/10]	Loss: 0.000424
[INFO][10:41:36]: [Client #476] Epoch: [2/5][0/10]	Loss: 0.000292
[INFO][10:41:36]: [Client #261] Epoch: [3/5][0/10]	Loss: 0.000017
[INFO][10:41:36]: [Client #354] Epoch: [3/5][0/10]	Loss: 0.000007
[INFO][10:41:36]: [Client #476] Epoch: [3/5][0/10]	Loss: 0.000032
[INFO][10:41:36]: [Client #261] Epoch: [4/5][0/10]	Loss: 0.000019
[INFO][10:41:36]: [Client #476] Epoch: [4/5][0/10]	Loss: 0.000001
[INFO][10:41:36]: [Client #354] Epoch: [4/5][0/10]	Loss: 0.000002
[INFO][10:41:36]: [Client #261] Epoch: [5/5][0/10]	Loss: 0.001158
[INFO][10:41:36]: [Client #476] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:41:36]: [Client #261] Model saved to /data/ykang/plato/results/test/model/lenet5_261_1127978.pth.
[INFO][10:41:36]: [Client #354] Epoch: [5/5][0/10]	Loss: 0.001422
[INFO][10:41:36]: [Client #476] Model saved to /data/ykang/plato/results/test/model/lenet5_476_1127979.pth.
[INFO][10:41:36]: [Client #354] Model saved to /data/ykang/plato/results/test/model/lenet5_354_1127977.pth.
[INFO][10:41:37]: [Client #261] Loading a model from /data/ykang/plato/results/test/model/lenet5_261_1127978.pth.
[INFO][10:41:37]: [Client #261] Model trained.
[INFO][10:41:37]: [Client #261] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:41:37]: [Server #1127936] Received 0.24 MB of payload data from client #261 (simulated).
[INFO][10:41:37]: [Client #476] Loading a model from /data/ykang/plato/results/test/model/lenet5_476_1127979.pth.
[INFO][10:41:37]: [Client #476] Model trained.
[INFO][10:41:37]: [Client #476] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:41:37]: [Server #1127936] Received 0.24 MB of payload data from client #476 (simulated).
[INFO][10:41:37]: [Client #354] Loading a model from /data/ykang/plato/results/test/model/lenet5_354_1127977.pth.
[INFO][10:41:37]: [Client #354] Model trained.
[INFO][10:41:37]: [Client #354] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:41:37]: [Server #1127936] Received 0.24 MB of payload data from client #354 (simulated).
[INFO][10:41:37]: [Server #1127936] Selecting client #66 for training.
[INFO][10:41:37]: [Server #1127936] Sending the current model to client #66 (simulated).
[INFO][10:41:37]: [Server #1127936] Sending 0.24 MB of payload data to client #66 (simulated).
[INFO][10:41:37]: [Server #1127936] Selecting client #142 for training.
[INFO][10:41:37]: [Server #1127936] Sending the current model to client #142 (simulated).
[INFO][10:41:37]: [Server #1127936] Sending 0.24 MB of payload data to client #142 (simulated).
[INFO][10:41:37]: [Server #1127936] Selecting client #401 for training.
[INFO][10:41:37]: [Server #1127936] Sending the current model to client #401 (simulated).
[INFO][10:41:37]: [Client #66] Selected by the server.
[INFO][10:41:37]: [Client #66] Loading its data source...
[INFO][10:41:37]: [Client #66] Dataset size: 60000
[INFO][10:41:37]: [Client #66] Sampler: noniid
[INFO][10:41:37]: [Server #1127936] Sending 0.24 MB of payload data to client #401 (simulated).
[INFO][10:41:37]: [Client #142] Selected by the server.
[INFO][10:41:37]: [Client #142] Loading its data source...
[INFO][10:41:37]: [Client #142] Dataset size: 60000
[INFO][10:41:37]: [Client #142] Sampler: noniid
[INFO][10:41:37]: [Client #401] Selected by the server.
[INFO][10:41:37]: [Client #401] Loading its data source...
[INFO][10:41:37]: [Client #401] Dataset size: 60000
[INFO][10:41:37]: [Client #401] Sampler: noniid
[INFO][10:41:37]: [Client #66] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:41:37]: [Client #142] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:41:37]: [Client #401] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:41:37]: [93m[1m[Client #142] Started training in communication round #91.[0m
[INFO][10:41:37]: [93m[1m[Client #401] Started training in communication round #91.[0m
[INFO][10:41:37]: [93m[1m[Client #66] Started training in communication round #91.[0m
[INFO][10:41:39]: [Client #142] Loading the dataset.
[INFO][10:41:39]: [Client #401] Loading the dataset.
[INFO][10:41:39]: [Client #66] Loading the dataset.
[INFO][10:41:45]: [Client #142] Epoch: [1/5][0/10]	Loss: 0.004059
[INFO][10:41:45]: [Client #401] Epoch: [1/5][0/10]	Loss: 0.000091
[INFO][10:41:46]: [Client #142] Epoch: [2/5][0/10]	Loss: 0.000740
[INFO][10:41:46]: [Client #66] Epoch: [1/5][0/10]	Loss: 0.000930
[INFO][10:41:46]: [Client #401] Epoch: [2/5][0/10]	Loss: 0.000982
[INFO][10:41:46]: [Client #142] Epoch: [3/5][0/10]	Loss: 0.000046
[INFO][10:41:46]: [Client #66] Epoch: [2/5][0/10]	Loss: 0.000168
[INFO][10:41:46]: [Client #401] Epoch: [3/5][0/10]	Loss: 0.000021
[INFO][10:41:46]: [Client #66] Epoch: [3/5][0/10]	Loss: 0.000048
[INFO][10:41:46]: [Client #142] Epoch: [4/5][0/10]	Loss: 0.000066
[INFO][10:41:46]: [Client #401] Epoch: [4/5][0/10]	Loss: 0.000001
[INFO][10:41:46]: [Client #66] Epoch: [4/5][0/10]	Loss: 0.023814
[INFO][10:41:46]: [Client #401] Epoch: [5/5][0/10]	Loss: 0.000245
[INFO][10:41:46]: [Client #142] Epoch: [5/5][0/10]	Loss: 0.000087
[INFO][10:41:46]: [Client #401] Model saved to /data/ykang/plato/results/test/model/lenet5_401_1127979.pth.
[INFO][10:41:46]: [Client #142] Model saved to /data/ykang/plato/results/test/model/lenet5_142_1127978.pth.
[INFO][10:41:46]: [Client #66] Epoch: [5/5][0/10]	Loss: 0.020273
[INFO][10:41:46]: [Client #66] Model saved to /data/ykang/plato/results/test/model/lenet5_66_1127977.pth.
[INFO][10:41:47]: [Client #401] Loading a model from /data/ykang/plato/results/test/model/lenet5_401_1127979.pth.
[INFO][10:41:47]: [Client #401] Model trained.
[INFO][10:41:47]: [Client #401] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:41:47]: [Server #1127936] Received 0.24 MB of payload data from client #401 (simulated).
[INFO][10:41:47]: [Client #142] Loading a model from /data/ykang/plato/results/test/model/lenet5_142_1127978.pth.
[INFO][10:41:47]: [Client #142] Model trained.
[INFO][10:41:47]: [Client #142] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:41:47]: [Server #1127936] Received 0.24 MB of payload data from client #142 (simulated).
[INFO][10:41:47]: [Client #66] Loading a model from /data/ykang/plato/results/test/model/lenet5_66_1127977.pth.
[INFO][10:41:47]: [Client #66] Model trained.
[INFO][10:41:47]: [Client #66] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:41:47]: [Server #1127936] Received 0.24 MB of payload data from client #66 (simulated).
[INFO][10:41:47]: [Server #1127936] Selecting client #332 for training.
[INFO][10:41:47]: [Server #1127936] Sending the current model to client #332 (simulated).
[INFO][10:41:47]: [Server #1127936] Sending 0.24 MB of payload data to client #332 (simulated).
[INFO][10:41:47]: [Client #332] Selected by the server.
[INFO][10:41:47]: [Client #332] Loading its data source...
[INFO][10:41:47]: [Client #332] Dataset size: 60000
[INFO][10:41:47]: [Client #332] Sampler: noniid
[INFO][10:41:47]: [Client #332] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:41:47]: [93m[1m[Client #332] Started training in communication round #91.[0m
[INFO][10:41:49]: [Client #332] Loading the dataset.
[INFO][10:41:55]: [Client #332] Epoch: [1/5][0/10]	Loss: 0.000701
[INFO][10:41:55]: [Client #332] Epoch: [2/5][0/10]	Loss: 0.000001
[INFO][10:41:55]: [Client #332] Epoch: [3/5][0/10]	Loss: 0.000001
[INFO][10:41:55]: [Client #332] Epoch: [4/5][0/10]	Loss: 0.000078
[INFO][10:41:55]: [Client #332] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:41:55]: [Client #332] Model saved to /data/ykang/plato/results/test/model/lenet5_332_1127977.pth.
[INFO][10:41:56]: [Client #332] Loading a model from /data/ykang/plato/results/test/model/lenet5_332_1127977.pth.
[INFO][10:41:56]: [Client #332] Model trained.
[INFO][10:41:56]: [Client #332] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:41:56]: [Server #1127936] Received 0.24 MB of payload data from client #332 (simulated).
[INFO][10:41:56]: [Server #1127936] Adding client #41 to the list of clients for aggregation.
[INFO][10:41:56]: [Server #1127936] Adding client #367 to the list of clients for aggregation.
[INFO][10:41:56]: [Server #1127936] Adding client #97 to the list of clients for aggregation.
[INFO][10:41:56]: [Server #1127936] Adding client #476 to the list of clients for aggregation.
[INFO][10:41:56]: [Server #1127936] Adding client #142 to the list of clients for aggregation.
[INFO][10:41:56]: [Server #1127936] Adding client #354 to the list of clients for aggregation.
[INFO][10:41:56]: [Server #1127936] Adding client #472 to the list of clients for aggregation.
[INFO][10:41:56]: [Server #1127936] Adding client #316 to the list of clients for aggregation.
[INFO][10:41:56]: [Server #1127936] Adding client #66 to the list of clients for aggregation.
[INFO][10:41:56]: [Server #1127936] Adding client #261 to the list of clients for aggregation.
[INFO][10:41:56]: [Server #1127936] Aggregating 10 clients in total.
[0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  2e-05  3e-10  3e-10
 6:  6.8876e+00  6.8875e+00  2e-05  2e-10  2e-10
 7:  6.8875e+00  6.8875e+00  2e-05  3e-10  5e-12
 8:  6.8875e+00  6.8875e+00  9e-06  3e-10  4e-12
 9:  6.8875e+00  6.8875e+00  6e-06  4e-10  7e-12
Optimal solution found.
The calculated probability is:  [0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00078508
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00089426 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.0007343  0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.0007343  0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073427 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.0007343
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00086294 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.0007343  0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.0007343
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.64058552 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431 0.00073431
 0.00073431 0.00073431 0.00073431 0.00073431]
current clients pool:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.0202426  0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00206458
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00202449 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.0023886  0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00647794 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00167183 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00662071
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01274513 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00181289 0.         0.
 0.         0.00156592 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 1. 1. 0. 0. 0. 1. 0. 0. 3. 1. 0. 0. 0. 1. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 2. 0. 1. 0.
 1. 1. 1. 0. 2. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 2. 1. 1. 0. 0.
 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 2. 1. 6. 0. 0. 2. 1. 0. 1. 1. 2. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 3. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 1. 6. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 2. 0. 0.
 1. 1. 1. 2. 0. 0. 0. 2. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 1. 2. 0. 4. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1.
 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 4. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0.
 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0.
 1. 0. 1. 0. 4. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 3. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0.
 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 0. 1. 0. 6. 1. 1.
 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 6. 0. 0. 1. 1. 0. 1. 1. 2. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 2. 2. 1. 2. 0.]
local_gradient_bounds:  [INFO][10:41:58]: [Server #1127936] Global model accuracy: 95.09%

[INFO][10:41:58]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_91.pth.
[INFO][10:41:58]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_91.pth.
[INFO][10:41:58]: [93m[1m
[Server #1127936] Starting round 92/100.[0m
[0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.0202426  0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00206458
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00202449 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.0023886  0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00647794 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00167183 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00662071
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01274513 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00181289 0.         0.
 0.         0.00156592 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8871e+00  4e-04  8e-09  8e-09
 5:  6.8876e+00  6.8874e+00  1e-04  2e-09  2e-09
 6:  6.8875e+00  6.8874e+00  1e-04  2e-09  2e-10
 7:  6.8875e+00  6.8874e+00  1e-04  2e-09  3e-10
 8:  6.8875e+00  6.8874e+00  7e-05  3e-08  4e-09
 9:  6.8875e+00  6.8874e+00  4e-05  3e-08  4e-09
10:  6.8874e+00  6.8874e+00  3e-06  3e-08  3e-09
Optimal solution found.
The calculated probability is:  [4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 9.75273862e-01 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792679e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 5.13637685e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792582e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99790163e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792777e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99790038e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 3.34857771e-04
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792744e-05 4.99792963e-05
 4.99792963e-05 4.99792799e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05 4.99792963e-05 4.99792963e-05
 4.99792963e-05 4.99792963e-05]
current clients pool:  [INFO][10:41:58]: [Server #1127936] Selected clients: [ 41 157 155 441  64 321 141 414 186 367]
[INFO][10:41:58]: [Server #1127936] Selecting client #41 for training.
[INFO][10:41:58]: [Server #1127936] Sending the current model to client #41 (simulated).
[INFO][10:41:58]: [Server #1127936] Sending 0.24 MB of payload data to client #41 (simulated).
[INFO][10:41:58]: [Server #1127936] Selecting client #157 for training.
[INFO][10:41:58]: [Server #1127936] Sending the current model to client #157 (simulated).
[INFO][10:41:58]: [Server #1127936] Sending 0.24 MB of payload data to client #157 (simulated).
[INFO][10:41:58]: [Server #1127936] Selecting client #155 for training.
[INFO][10:41:58]: [Server #1127936] Sending the current model to client #155 (simulated).
[INFO][10:41:58]: [Client #41] Selected by the server.
[INFO][10:41:58]: [Client #41] Loading its data source...
[INFO][10:41:58]: [Client #41] Dataset size: 60000
[INFO][10:41:58]: [Client #41] Sampler: noniid
[INFO][10:41:58]: [Server #1127936] Sending 0.24 MB of payload data to client #155 (simulated).
[INFO][10:41:58]: [Client #157] Selected by the server.
[INFO][10:41:58]: [Client #157] Loading its data source...
[INFO][10:41:58]: [Client #157] Dataset size: 60000
[INFO][10:41:58]: [Client #157] Sampler: noniid
[INFO][10:41:58]: [Client #41] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:41:58]: [Client #155] Selected by the server.
[INFO][10:41:58]: [Client #155] Loading its data source...
[INFO][10:41:58]: [Client #155] Dataset size: 60000
[INFO][10:41:58]: [Client #155] Sampler: noniid
[INFO][10:41:58]: [Client #157] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:41:58]: [Client #155] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:41:58]: [93m[1m[Client #155] Started training in communication round #92.[0m
[INFO][10:41:58]: [93m[1m[Client #157] Started training in communication round #92.[0m
[INFO][10:41:58]: [93m[1m[Client #41] Started training in communication round #92.[0m
[INFO][10:42:01]: [Client #157] Loading the dataset.
[INFO][10:42:01]: [Client #155] Loading the dataset.
[INFO][10:42:01]: [Client #41] Loading the dataset.
[INFO][10:42:07]: [Client #157] Epoch: [1/5][0/10]	Loss: 0.000608
[INFO][10:42:07]: [Client #155] Epoch: [1/5][0/10]	Loss: 0.000066
[INFO][10:42:07]: [Client #157] Epoch: [2/5][0/10]	Loss: 0.003738
[INFO][10:42:07]: [Client #155] Epoch: [2/5][0/10]	Loss: 0.000138
[INFO][10:42:07]: [Client #41] Epoch: [1/5][0/10]	Loss: 0.000284
[INFO][10:42:07]: [Client #157] Epoch: [3/5][0/10]	Loss: 0.000007
[INFO][10:42:07]: [Client #155] Epoch: [3/5][0/10]	Loss: 0.000013
[INFO][10:42:07]: [Client #157] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:42:07]: [Client #41] Epoch: [2/5][0/10]	Loss: 0.000231
[INFO][10:42:07]: [Client #155] Epoch: [4/5][0/10]	Loss: 0.000001
[INFO][10:42:07]: [Client #41] Epoch: [3/5][0/10]	Loss: 0.016174
[INFO][10:42:07]: [Client #155] Epoch: [5/5][0/10]	Loss: 0.000081
[INFO][10:42:07]: [Client #157] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:42:07]: [Client #41] Epoch: [4/5][0/10]	Loss: 0.007973
[INFO][10:42:07]: [Client #155] Model saved to /data/ykang/plato/results/test/model/lenet5_155_1127979.pth.
[INFO][10:42:07]: [Client #157] Model saved to /data/ykang/plato/results/test/model/lenet5_157_1127978.pth.
[INFO][10:42:07]: [Client #41] Epoch: [5/5][0/10]	Loss: 0.020858
[INFO][10:42:07]: [Client #41] Model saved to /data/ykang/plato/results/test/model/lenet5_41_1127977.pth.
[INFO][10:42:08]: [Client #155] Loading a model from /data/ykang/plato/results/test/model/lenet5_155_1127979.pth.
[INFO][10:42:08]: [Client #155] Model trained.
[INFO][10:42:08]: [Client #155] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:42:08]: [Server #1127936] Received 0.24 MB of payload data from client #155 (simulated).
[INFO][10:42:08]: [Client #157] Loading a model from /data/ykang/plato/results/test/model/lenet5_157_1127978.pth.
[INFO][10:42:08]: [Client #157] Model trained.
[INFO][10:42:08]: [Client #157] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:42:08]: [Server #1127936] Received 0.24 MB of payload data from client #157 (simulated).
[INFO][10:42:08]: [Client #41] Loading a model from /data/ykang/plato/results/test/model/lenet5_41_1127977.pth.
[INFO][10:42:08]: [Client #41] Model trained.
[INFO][10:42:08]: [Client #41] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:42:08]: [Server #1127936] Received 0.24 MB of payload data from client #41 (simulated).
[INFO][10:42:08]: [Server #1127936] Selecting client #441 for training.
[INFO][10:42:08]: [Server #1127936] Sending the current model to client #441 (simulated).
[INFO][10:42:08]: [Server #1127936] Sending 0.24 MB of payload data to client #441 (simulated).
[INFO][10:42:08]: [Server #1127936] Selecting client #64 for training.
[INFO][10:42:08]: [Server #1127936] Sending the current model to client #64 (simulated).
[INFO][10:42:08]: [Server #1127936] Sending 0.24 MB of payload data to client #64 (simulated).
[INFO][10:42:08]: [Server #1127936] Selecting client #321 for training.
[INFO][10:42:08]: [Server #1127936] Sending the current model to client #321 (simulated).
[INFO][10:42:08]: [Client #441] Selected by the server.
[INFO][10:42:08]: [Client #441] Loading its data source...
[INFO][10:42:08]: [Client #441] Dataset size: 60000
[INFO][10:42:08]: [Client #441] Sampler: noniid
[INFO][10:42:08]: [Server #1127936] Sending 0.24 MB of payload data to client #321 (simulated).
[INFO][10:42:08]: [Client #64] Selected by the server.
[INFO][10:42:08]: [Client #64] Loading its data source...
[INFO][10:42:08]: [Client #321] Selected by the server.
[INFO][10:42:08]: [Client #64] Dataset size: 60000
[INFO][10:42:08]: [Client #321] Loading its data source...
[INFO][10:42:08]: [Client #64] Sampler: noniid
[INFO][10:42:08]: [Client #321] Dataset size: 60000
[INFO][10:42:08]: [Client #321] Sampler: noniid
[INFO][10:42:08]: [Client #441] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:42:08]: [Client #321] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:42:08]: [Client #64] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:42:08]: [93m[1m[Client #441] Started training in communication round #92.[0m
[INFO][10:42:08]: [93m[1m[Client #64] Started training in communication round #92.[0m
[INFO][10:42:08]: [93m[1m[Client #321] Started training in communication round #92.[0m
[INFO][10:42:10]: [Client #441] Loading the dataset.
[INFO][10:42:10]: [Client #321] Loading the dataset.
[INFO][10:42:10]: [Client #64] Loading the dataset.
[INFO][10:42:16]: [Client #441] Epoch: [1/5][0/10]	Loss: 0.000950
[INFO][10:42:16]: [Client #64] Epoch: [1/5][0/10]	Loss: 0.002115
[INFO][10:42:16]: [Client #441] Epoch: [2/5][0/10]	Loss: 0.000569
[INFO][10:42:16]: [Client #321] Epoch: [1/5][0/10]	Loss: 0.000950
[INFO][10:42:16]: [Client #64] Epoch: [2/5][0/10]	Loss: 0.000076
[INFO][10:42:16]: [Client #441] Epoch: [3/5][0/10]	Loss: 0.000005
[INFO][10:42:17]: [Client #321] Epoch: [2/5][0/10]	Loss: 0.000900
[INFO][10:42:17]: [Client #441] Epoch: [4/5][0/10]	Loss: 0.000001
[INFO][10:42:17]: [Client #64] Epoch: [3/5][0/10]	Loss: 0.000029
[INFO][10:42:17]: [Client #321] Epoch: [3/5][0/10]	Loss: 0.000005
[INFO][10:42:17]: [Client #64] Epoch: [4/5][0/10]	Loss: 0.000016
[INFO][10:42:17]: [Client #441] Epoch: [5/5][0/10]	Loss: 0.000021
[INFO][10:42:17]: [Client #441] Model saved to /data/ykang/plato/results/test/model/lenet5_441_1127977.pth.
[INFO][10:42:17]: [Client #321] Epoch: [4/5][0/10]	Loss: 0.000002
[INFO][10:42:17]: [Client #64] Epoch: [5/5][0/10]	Loss: 0.000206
[INFO][10:42:17]: [Client #64] Model saved to /data/ykang/plato/results/test/model/lenet5_64_1127978.pth.
[INFO][10:42:17]: [Client #321] Epoch: [5/5][0/10]	Loss: 0.000090
[INFO][10:42:17]: [Client #321] Model saved to /data/ykang/plato/results/test/model/lenet5_321_1127979.pth.
[INFO][10:42:18]: [Client #441] Loading a model from /data/ykang/plato/results/test/model/lenet5_441_1127977.pth.
[INFO][10:42:18]: [Client #441] Model trained.
[INFO][10:42:18]: [Client #441] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:42:18]: [Server #1127936] Received 0.24 MB of payload data from client #441 (simulated).
[INFO][10:42:18]: [Client #64] Loading a model from /data/ykang/plato/results/test/model/lenet5_64_1127978.pth.
[INFO][10:42:18]: [Client #64] Model trained.
[INFO][10:42:18]: [Client #64] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:42:18]: [Server #1127936] Received 0.24 MB of payload data from client #64 (simulated).
[INFO][10:42:18]: [Client #321] Loading a model from /data/ykang/plato/results/test/model/lenet5_321_1127979.pth.
[INFO][10:42:18]: [Client #321] Model trained.
[INFO][10:42:18]: [Client #321] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:42:18]: [Server #1127936] Received 0.24 MB of payload data from client #321 (simulated).
[INFO][10:42:18]: [Server #1127936] Selecting client #141 for training.
[INFO][10:42:18]: [Server #1127936] Sending the current model to client #141 (simulated).
[INFO][10:42:18]: [Server #1127936] Sending 0.24 MB of payload data to client #141 (simulated).
[INFO][10:42:18]: [Server #1127936] Selecting client #414 for training.
[INFO][10:42:18]: [Server #1127936] Sending the current model to client #414 (simulated).
[INFO][10:42:18]: [Server #1127936] Sending 0.24 MB of payload data to client #414 (simulated).
[INFO][10:42:18]: [Server #1127936] Selecting client #186 for training.
[INFO][10:42:18]: [Server #1127936] Sending the current model to client #186 (simulated).
[INFO][10:42:18]: [Client #141] Selected by the server.
[INFO][10:42:18]: [Client #141] Loading its data source...
[INFO][10:42:18]: [Client #141] Dataset size: 60000
[INFO][10:42:18]: [Client #141] Sampler: noniid
[INFO][10:42:18]: [Server #1127936] Sending 0.24 MB of payload data to client #186 (simulated).
[INFO][10:42:18]: [Client #186] Selected by the server.
[INFO][10:42:18]: [Client #414] Selected by the server.
[INFO][10:42:18]: [Client #186] Loading its data source...
[INFO][10:42:18]: [Client #414] Loading its data source...
[INFO][10:42:18]: [Client #186] Dataset size: 60000
[INFO][10:42:18]: [Client #414] Dataset size: 60000
[INFO][10:42:18]: [Client #186] Sampler: noniid
[INFO][10:42:18]: [Client #414] Sampler: noniid
[INFO][10:42:18]: [Client #141] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:42:18]: [93m[1m[Client #141] Started training in communication round #92.[0m
[INFO][10:42:18]: [Client #186] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:42:18]: [Client #414] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:42:18]: [93m[1m[Client #414] Started training in communication round #92.[0m
[INFO][10:42:18]: [93m[1m[Client #186] Started training in communication round #92.[0m
[INFO][10:42:20]: [Client #414] Loading the dataset.
[INFO][10:42:20]: [Client #141] Loading the dataset.
[INFO][10:42:20]: [Client #186] Loading the dataset.
[INFO][10:42:26]: [Client #186] Epoch: [1/5][0/10]	Loss: 0.002864
[INFO][10:42:26]: [Client #141] Epoch: [1/5][0/10]	Loss: 0.001393
[INFO][10:42:26]: [Client #414] Epoch: [1/5][0/10]	Loss: 0.000201
[INFO][10:42:26]: [Client #186] Epoch: [2/5][0/10]	Loss: 0.000031
[INFO][10:42:26]: [Client #141] Epoch: [2/5][0/10]	Loss: 0.000542
[INFO][10:42:26]: [Client #414] Epoch: [2/5][0/10]	Loss: 0.000097
[INFO][10:42:26]: [Client #186] Epoch: [3/5][0/10]	Loss: 0.000006
[INFO][10:42:26]: [Client #141] Epoch: [3/5][0/10]	Loss: 0.000011
[INFO][10:42:26]: [Client #414] Epoch: [3/5][0/10]	Loss: 0.000020
[INFO][10:42:26]: [Client #186] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:42:26]: [Client #414] Epoch: [4/5][0/10]	Loss: 0.000073
[INFO][10:42:26]: [Client #141] Epoch: [4/5][0/10]	Loss: 0.000015
[INFO][10:42:27]: [Client #186] Epoch: [5/5][0/10]	Loss: 0.000012
[INFO][10:42:27]: [Client #141] Epoch: [5/5][0/10]	Loss: 0.000096
[INFO][10:42:27]: [Client #414] Epoch: [5/5][0/10]	Loss: 0.000015
[INFO][10:42:27]: [Client #186] Model saved to /data/ykang/plato/results/test/model/lenet5_186_1127979.pth.
[INFO][10:42:27]: [Client #141] Model saved to /data/ykang/plato/results/test/model/lenet5_141_1127977.pth.
[INFO][10:42:27]: [Client #414] Model saved to /data/ykang/plato/results/test/model/lenet5_414_1127978.pth.
[INFO][10:42:27]: [Client #414] Loading a model from /data/ykang/plato/results/test/model/lenet5_414_1127978.pth.
[INFO][10:42:27]: [Client #414] Model trained.
[INFO][10:42:27]: [Client #414] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:42:27]: [Server #1127936] Received 0.24 MB of payload data from client #414 (simulated).
[INFO][10:42:28]: [Client #186] Loading a model from /data/ykang/plato/results/test/model/lenet5_186_1127979.pth.
[INFO][10:42:28]: [Client #186] Model trained.
[INFO][10:42:28]: [Client #186] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:42:28]: [Server #1127936] Received 0.24 MB of payload data from client #186 (simulated).
[INFO][10:42:28]: [Client #141] Loading a model from /data/ykang/plato/results/test/model/lenet5_141_1127977.pth.
[INFO][10:42:28]: [Client #141] Model trained.
[INFO][10:42:28]: [Client #141] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:42:28]: [Server #1127936] Received 0.24 MB of payload data from client #141 (simulated).
[INFO][10:42:28]: [Server #1127936] Selecting client #367 for training.
[INFO][10:42:28]: [Server #1127936] Sending the current model to client #367 (simulated).
[INFO][10:42:28]: [Server #1127936] Sending 0.24 MB of payload data to client #367 (simulated).
[INFO][10:42:28]: [Client #367] Selected by the server.
[INFO][10:42:28]: [Client #367] Loading its data source...
[INFO][10:42:28]: [Client #367] Dataset size: 60000
[INFO][10:42:28]: [Client #367] Sampler: noniid
[INFO][10:42:28]: [Client #367] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:42:28]: [93m[1m[Client #367] Started training in communication round #92.[0m
[INFO][10:42:30]: [Client #367] Loading the dataset.
[INFO][10:42:35]: [Client #367] Epoch: [1/5][0/10]	Loss: 0.000140
[INFO][10:42:35]: [Client #367] Epoch: [2/5][0/10]	Loss: 0.000134
[INFO][10:42:35]: [Client #367] Epoch: [3/5][0/10]	Loss: 0.000001
[INFO][10:42:35]: [Client #367] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:42:36]: [Client #367] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:42:36]: [Client #367] Model saved to /data/ykang/plato/results/test/model/lenet5_367_1127977.pth.
[INFO][10:42:36]: [Client #367] Loading a model from /data/ykang/plato/results/test/model/lenet5_367_1127977.pth.
[INFO][10:42:36]: [Client #367] Model trained.
[INFO][10:42:36]: [Client #367] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:42:36]: [Server #1127936] Received 0.24 MB of payload data from client #367 (simulated).
[INFO][10:42:36]: [Server #1127936] Adding client #332 to the list of clients for aggregation.
[INFO][10:42:36]: [Server #1127936] Adding client #299 to the list of clients for aggregation.
[INFO][10:42:36]: [Server #1127936] Adding client #212 to the list of clients for aggregation.
[INFO][10:42:36]: [Server #1127936] Adding client #69 to the list of clients for aggregation.
[INFO][10:42:36]: [Server #1127936] Adding client #88 to the list of clients for aggregation.
[INFO][10:42:36]: [Server #1127936] Adding client #157 to the list of clients for aggregation.
[INFO][10:42:36]: [Server #1127936] Adding client #155 to the list of clients for aggregation.
[INFO][10:42:36]: [Server #1127936] Adding client #64 to the list of clients for aggregation.
[INFO][10:42:36]: [Server #1127936] Adding client #321 to the list of clients for aggregation.
[INFO][10:42:36]: [Server #1127936] Adding client #141 to the list of clients for aggregation.
[INFO][10:42:36]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00828662 0.         0.
 0.         0.         0.00191509 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00910653 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00294305 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00359537 0.
 0.00385494 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01586042 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.0174258  0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00122062 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00463511 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 1. 1. 0. 0. 0. 1. 0. 0. 3. 1. 0. 0. 0. 1. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 2. 0. 1. 0.
 1. 1. 1. 0. 2. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 2. 1. 1. 0. 0.
 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 2. 1. 6. 0. 0. 2. 1. 0. 1. 1. 2. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 3. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 1. 6. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 2. 0. 0.
 1. 1. 1. 2. 0. 0. 0. 2. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 1. 2. 0. 4. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1.
 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 4. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 6. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0.
 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0.
 1. 0. 1. 0. 4. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 3. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0.
 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 0. 1. 0. 6. 1. 1.
 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 6. 0. 0. 1. 1. 0. 1. 1. 2. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 2. 2. 1. 2. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00828662 0.         0.
 0.         0.         0.00191509 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00910653 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00294305 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00359537 0.
 0.00385494 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01586042 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.0174258  0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00122062 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00463511 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:42:39]: [Server #1127936] Global model accuracy: 96.11%

[INFO][10:42:39]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_92.pth.
[INFO][10:42:39]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_92.pth.
[INFO][10:42:39]: [93m[1m
[Server #1127936] Starting round 93/100.[0m
[0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8871e+00  4e-04  7e-09  7e-09
 5:  6.8876e+00  6.8874e+00  1e-04  2e-09  2e-09
 6:  6.8875e+00  6.8874e+00  1e-04  1e-09  1e-10
 7:  6.8875e+00  6.8874e+00  8e-05  8e-09  9e-10
 8:  6.8875e+00  6.8874e+00  6e-05  1e-08  1e-09
 9:  6.8875e+00  6.8875e+00  1e-05  4e-08  5e-09
10:  6.8875e+00  6.8875e+00  2e-06  9e-09  1e-09
Optimal solution found.
The calculated probability is:  [4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30240378e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.52188399e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 6.53584725e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30243654e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30243421e-05 4.30244127e-05 4.30243316e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 1.03565545e-04
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 9.78873310e-01 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244046e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.57076732e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05 4.30244127e-05 4.30244127e-05
 4.30244127e-05 4.30244127e-05]
current clients pool:  [INFO][10:42:39]: [Server #1127936] Selected clients: [299 436 351 293 408 454   2 199 278 364]
[INFO][10:42:39]: [Server #1127936] Selecting client #299 for training.
[INFO][10:42:39]: [Server #1127936] Sending the current model to client #299 (simulated).
[INFO][10:42:39]: [Server #1127936] Sending 0.24 MB of payload data to client #299 (simulated).
[INFO][10:42:39]: [Server #1127936] Selecting client #436 for training.
[INFO][10:42:39]: [Server #1127936] Sending the current model to client #436 (simulated).
[INFO][10:42:39]: [Server #1127936] Sending 0.24 MB of payload data to client #436 (simulated).
[INFO][10:42:39]: [Server #1127936] Selecting client #351 for training.
[INFO][10:42:39]: [Server #1127936] Sending the current model to client #351 (simulated).
[INFO][10:42:39]: [Client #299] Selected by the server.
[INFO][10:42:39]: [Client #299] Loading its data source...
[INFO][10:42:39]: [Client #299] Dataset size: 60000
[INFO][10:42:39]: [Client #299] Sampler: noniid
[INFO][10:42:39]: [Server #1127936] Sending 0.24 MB of payload data to client #351 (simulated).
[INFO][10:42:39]: [Client #436] Selected by the server.
[INFO][10:42:39]: [Client #436] Loading its data source...
[INFO][10:42:39]: [Client #436] Dataset size: 60000
[INFO][10:42:39]: [Client #436] Sampler: noniid
[INFO][10:42:39]: [Client #299] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:42:39]: [Client #351] Selected by the server.
[INFO][10:42:39]: [Client #351] Loading its data source...
[INFO][10:42:39]: [Client #351] Dataset size: 60000
[INFO][10:42:39]: [Client #351] Sampler: noniid
[INFO][10:42:39]: [Client #351] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:42:39]: [Client #436] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:42:39]: [93m[1m[Client #299] Started training in communication round #93.[0m
[INFO][10:42:39]: [93m[1m[Client #436] Started training in communication round #93.[0m
[INFO][10:42:39]: [93m[1m[Client #351] Started training in communication round #93.[0m
[INFO][10:42:41]: [Client #351] Loading the dataset.
[INFO][10:42:41]: [Client #436] Loading the dataset.
[INFO][10:42:41]: [Client #299] Loading the dataset.
[INFO][10:42:47]: [Client #436] Epoch: [1/5][0/10]	Loss: 0.002953
[INFO][10:42:47]: [Client #351] Epoch: [1/5][0/10]	Loss: 0.005618
[INFO][10:42:47]: [Client #299] Epoch: [1/5][0/10]	Loss: 0.003871
[INFO][10:42:47]: [Client #436] Epoch: [2/5][0/10]	Loss: 0.003904
[INFO][10:42:48]: [Client #351] Epoch: [2/5][0/10]	Loss: 0.002164
[INFO][10:42:48]: [Client #299] Epoch: [2/5][0/10]	Loss: 0.000173
[INFO][10:42:48]: [Client #436] Epoch: [3/5][0/10]	Loss: 0.000010
[INFO][10:42:48]: [Client #351] Epoch: [3/5][0/10]	Loss: 0.000198
[INFO][10:42:48]: [Client #436] Epoch: [4/5][0/10]	Loss: 0.000125
[INFO][10:42:48]: [Client #299] Epoch: [3/5][0/10]	Loss: 0.000397
[INFO][10:42:48]: [Client #351] Epoch: [4/5][0/10]	Loss: 0.000004
[INFO][10:42:48]: [Client #436] Epoch: [5/5][0/10]	Loss: 0.000010
[INFO][10:42:48]: [Client #299] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:42:48]: [Client #436] Model saved to /data/ykang/plato/results/test/model/lenet5_436_1127978.pth.
[INFO][10:42:48]: [Client #351] Epoch: [5/5][0/10]	Loss: 0.036792
[INFO][10:42:48]: [Client #351] Model saved to /data/ykang/plato/results/test/model/lenet5_351_1127979.pth.
[INFO][10:42:48]: [Client #299] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:42:48]: [Client #299] Model saved to /data/ykang/plato/results/test/model/lenet5_299_1127977.pth.
[INFO][10:42:49]: [Client #436] Loading a model from /data/ykang/plato/results/test/model/lenet5_436_1127978.pth.
[INFO][10:42:49]: [Client #436] Model trained.
[INFO][10:42:49]: [Client #436] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:42:49]: [Server #1127936] Received 0.24 MB of payload data from client #436 (simulated).
[INFO][10:42:49]: [Client #351] Loading a model from /data/ykang/plato/results/test/model/lenet5_351_1127979.pth.
[INFO][10:42:49]: [Client #351] Model trained.
[INFO][10:42:49]: [Client #351] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:42:49]: [Server #1127936] Received 0.24 MB of payload data from client #351 (simulated).
[INFO][10:42:49]: [Client #299] Loading a model from /data/ykang/plato/results/test/model/lenet5_299_1127977.pth.
[INFO][10:42:49]: [Client #299] Model trained.
[INFO][10:42:49]: [Client #299] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:42:49]: [Server #1127936] Received 0.24 MB of payload data from client #299 (simulated).
[INFO][10:42:49]: [Server #1127936] Selecting client #293 for training.
[INFO][10:42:49]: [Server #1127936] Sending the current model to client #293 (simulated).
[INFO][10:42:49]: [Server #1127936] Sending 0.24 MB of payload data to client #293 (simulated).
[INFO][10:42:49]: [Server #1127936] Selecting client #408 for training.
[INFO][10:42:49]: [Server #1127936] Sending the current model to client #408 (simulated).
[INFO][10:42:49]: [Server #1127936] Sending 0.24 MB of payload data to client #408 (simulated).
[INFO][10:42:49]: [Server #1127936] Selecting client #454 for training.
[INFO][10:42:49]: [Server #1127936] Sending the current model to client #454 (simulated).
[INFO][10:42:49]: [Client #293] Selected by the server.
[INFO][10:42:49]: [Client #293] Loading its data source...
[INFO][10:42:49]: [Client #293] Dataset size: 60000
[INFO][10:42:49]: [Client #293] Sampler: noniid
[INFO][10:42:49]: [Server #1127936] Sending 0.24 MB of payload data to client #454 (simulated).
[INFO][10:42:49]: [Client #408] Selected by the server.
[INFO][10:42:49]: [Client #408] Loading its data source...
[INFO][10:42:49]: [Client #454] Selected by the server.
[INFO][10:42:49]: [Client #408] Dataset size: 60000
[INFO][10:42:49]: [Client #454] Loading its data source...
[INFO][10:42:49]: [Client #408] Sampler: noniid
[INFO][10:42:49]: [Client #454] Dataset size: 60000
[INFO][10:42:49]: [Client #454] Sampler: noniid
[INFO][10:42:49]: [Client #293] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:42:49]: [Client #454] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:42:49]: [Client #408] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:42:49]: [93m[1m[Client #293] Started training in communication round #93.[0m
[INFO][10:42:49]: [93m[1m[Client #408] Started training in communication round #93.[0m
[INFO][10:42:49]: [93m[1m[Client #454] Started training in communication round #93.[0m
[INFO][10:42:51]: [Client #293] Loading the dataset.
[INFO][10:42:51]: [Client #408] Loading the dataset.
[INFO][10:42:51]: [Client #454] Loading the dataset.
[INFO][10:42:57]: [Client #293] Epoch: [1/5][0/10]	Loss: 0.002937
[INFO][10:42:57]: [Client #454] Epoch: [1/5][0/10]	Loss: 0.000407
[INFO][10:42:57]: [Client #408] Epoch: [1/5][0/10]	Loss: 0.000410
[INFO][10:42:57]: [Client #293] Epoch: [2/5][0/10]	Loss: 0.001087
[INFO][10:42:57]: [Client #454] Epoch: [2/5][0/10]	Loss: 0.001591
[INFO][10:42:57]: [Client #408] Epoch: [2/5][0/10]	Loss: 0.000143
[INFO][10:42:57]: [Client #293] Epoch: [3/5][0/10]	Loss: 0.000056
[INFO][10:42:57]: [Client #454] Epoch: [3/5][0/10]	Loss: 0.000016
[INFO][10:42:57]: [Client #408] Epoch: [3/5][0/10]	Loss: 0.000002
[INFO][10:42:57]: [Client #454] Epoch: [4/5][0/10]	Loss: 0.000128
[INFO][10:42:57]: [Client #293] Epoch: [4/5][0/10]	Loss: 0.000780
[INFO][10:42:57]: [Client #408] Epoch: [4/5][0/10]	Loss: 0.000004
[INFO][10:42:58]: [Client #293] Epoch: [5/5][0/10]	Loss: 0.000896
[INFO][10:42:58]: [Client #454] Epoch: [5/5][0/10]	Loss: 0.005236
[INFO][10:42:58]: [Client #408] Epoch: [5/5][0/10]	Loss: 0.000001
[INFO][10:42:58]: [Client #454] Model saved to /data/ykang/plato/results/test/model/lenet5_454_1127979.pth.
[INFO][10:42:58]: [Client #293] Model saved to /data/ykang/plato/results/test/model/lenet5_293_1127977.pth.
[INFO][10:42:58]: [Client #408] Model saved to /data/ykang/plato/results/test/model/lenet5_408_1127978.pth.
[INFO][10:42:58]: [Client #454] Loading a model from /data/ykang/plato/results/test/model/lenet5_454_1127979.pth.
[INFO][10:42:58]: [Client #454] Model trained.
[INFO][10:42:58]: [Client #454] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:42:58]: [Server #1127936] Received 0.24 MB of payload data from client #454 (simulated).
[INFO][10:42:58]: [Client #293] Loading a model from /data/ykang/plato/results/test/model/lenet5_293_1127977.pth.
[INFO][10:42:59]: [Client #293] Model trained.
[INFO][10:42:59]: [Client #293] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:42:59]: [Server #1127936] Received 0.24 MB of payload data from client #293 (simulated).
[INFO][10:42:59]: [Client #408] Loading a model from /data/ykang/plato/results/test/model/lenet5_408_1127978.pth.
[INFO][10:42:59]: [Client #408] Model trained.
[INFO][10:42:59]: [Client #408] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:42:59]: [Server #1127936] Received 0.24 MB of payload data from client #408 (simulated).
[INFO][10:42:59]: [Server #1127936] Selecting client #2 for training.
[INFO][10:42:59]: [Server #1127936] Sending the current model to client #2 (simulated).
[INFO][10:42:59]: [Server #1127936] Sending 0.24 MB of payload data to client #2 (simulated).
[INFO][10:42:59]: [Server #1127936] Selecting client #199 for training.
[INFO][10:42:59]: [Server #1127936] Sending the current model to client #199 (simulated).
[INFO][10:42:59]: [Server #1127936] Sending 0.24 MB of payload data to client #199 (simulated).
[INFO][10:42:59]: [Server #1127936] Selecting client #278 for training.
[INFO][10:42:59]: [Server #1127936] Sending the current model to client #278 (simulated).
[INFO][10:42:59]: [Client #2] Selected by the server.
[INFO][10:42:59]: [Client #2] Loading its data source...
[INFO][10:42:59]: [Client #2] Dataset size: 60000
[INFO][10:42:59]: [Client #2] Sampler: noniid
[INFO][10:42:59]: [Server #1127936] Sending 0.24 MB of payload data to client #278 (simulated).
[INFO][10:42:59]: [Client #2] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:42:59]: [Client #199] Selected by the server.
[INFO][10:42:59]: [Client #199] Loading its data source...
[INFO][10:42:59]: [Client #199] Dataset size: 60000
[INFO][10:42:59]: [Client #199] Sampler: noniid
[INFO][10:42:59]: [Client #278] Selected by the server.
[INFO][10:42:59]: [Client #278] Loading its data source...
[INFO][10:42:59]: [Client #278] Dataset size: 60000
[INFO][10:42:59]: [Client #278] Sampler: noniid
[INFO][10:42:59]: [Client #199] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:42:59]: [Client #278] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:42:59]: [93m[1m[Client #2] Started training in communication round #93.[0m
[INFO][10:42:59]: [93m[1m[Client #278] Started training in communication round #93.[0m
[INFO][10:42:59]: [93m[1m[Client #199] Started training in communication round #93.[0m
[INFO][10:43:01]: [Client #278] Loading the dataset.
[INFO][10:43:01]: [Client #2] Loading the dataset.
[INFO][10:43:01]: [Client #199] Loading the dataset.
[INFO][10:43:07]: [Client #278] Epoch: [1/5][0/10]	Loss: 0.001184
[INFO][10:43:07]: [Client #199] Epoch: [1/5][0/10]	Loss: 0.005701
[INFO][10:43:07]: [Client #278] Epoch: [2/5][0/10]	Loss: 0.000931
[INFO][10:43:07]: [Client #2] Epoch: [1/5][0/10]	Loss: 0.004365
[INFO][10:43:07]: [Client #199] Epoch: [2/5][0/10]	Loss: 0.000028
[INFO][10:43:07]: [Client #2] Epoch: [2/5][0/10]	Loss: 0.000001
[INFO][10:43:07]: [Client #278] Epoch: [3/5][0/10]	Loss: 0.000006
[INFO][10:43:07]: [Client #199] Epoch: [3/5][0/10]	Loss: 0.000004
[INFO][10:43:07]: [Client #278] Epoch: [4/5][0/10]	Loss: 0.000002
[INFO][10:43:07]: [Client #199] Epoch: [4/5][0/10]	Loss: 0.001182
[INFO][10:43:07]: [Client #2] Epoch: [3/5][0/10]	Loss: 0.609530
[INFO][10:43:07]: [Client #278] Epoch: [5/5][0/10]	Loss: 0.000202
[INFO][10:43:07]: [Client #199] Epoch: [5/5][0/10]	Loss: 0.001567
[INFO][10:43:07]: [Client #2] Epoch: [4/5][0/10]	Loss: 0.000003
[INFO][10:43:07]: [Client #278] Model saved to /data/ykang/plato/results/test/model/lenet5_278_1127979.pth.
[INFO][10:43:07]: [Client #199] Model saved to /data/ykang/plato/results/test/model/lenet5_199_1127978.pth.
[INFO][10:43:07]: [Client #2] Epoch: [5/5][0/10]	Loss: 0.411353
[INFO][10:43:07]: [Client #2] Model saved to /data/ykang/plato/results/test/model/lenet5_2_1127977.pth.
[INFO][10:43:08]: [Client #278] Loading a model from /data/ykang/plato/results/test/model/lenet5_278_1127979.pth.
[INFO][10:43:08]: [Client #278] Model trained.
[INFO][10:43:08]: [Client #278] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:43:08]: [Server #1127936] Received 0.24 MB of payload data from client #278 (simulated).
[INFO][10:43:08]: [Client #199] Loading a model from /data/ykang/plato/results/test/model/lenet5_199_1127978.pth.
[INFO][10:43:08]: [Client #199] Model trained.
[INFO][10:43:08]: [Client #199] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:43:08]: [Server #1127936] Received 0.24 MB of payload data from client #199 (simulated).
[INFO][10:43:08]: [Client #2] Loading a model from /data/ykang/plato/results/test/model/lenet5_2_1127977.pth.
[INFO][10:43:08]: [Client #2] Model trained.
[INFO][10:43:08]: [Client #2] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:43:08]: [Server #1127936] Received 0.24 MB of payload data from client #2 (simulated).
[INFO][10:43:08]: [Server #1127936] Selecting client #364 for training.
[INFO][10:43:08]: [Server #1127936] Sending the current model to client #364 (simulated).
[INFO][10:43:08]: [Server #1127936] Sending 0.24 MB of payload data to client #364 (simulated).
[INFO][10:43:08]: [Client #364] Selected by the server.
[INFO][10:43:08]: [Client #364] Loading its data source...
[INFO][10:43:08]: [Client #364] Dataset size: 60000
[INFO][10:43:08]: [Client #364] Sampler: noniid
[INFO][10:43:08]: [Client #364] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:43:08]: [93m[1m[Client #364] Started training in communication round #93.[0m
[INFO][10:43:10]: [Client #364] Loading the dataset.
[INFO][10:43:16]: [Client #364] Epoch: [1/5][0/10]	Loss: 0.000131
[INFO][10:43:16]: [Client #364] Epoch: [2/5][0/10]	Loss: 0.000139
[INFO][10:43:16]: [Client #364] Epoch: [3/5][0/10]	Loss: 0.000003
[INFO][10:43:16]: [Client #364] Epoch: [4/5][0/10]	Loss: 0.000740
[INFO][10:43:16]: [Client #364] Epoch: [5/5][0/10]	Loss: 0.000001
[INFO][10:43:16]: [Client #364] Model saved to /data/ykang/plato/results/test/model/lenet5_364_1127977.pth.
[INFO][10:43:17]: [Client #364] Loading a model from /data/ykang/plato/results/test/model/lenet5_364_1127977.pth.
[INFO][10:43:17]: [Client #364] Model trained.
[INFO][10:43:17]: [Client #364] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:43:17]: [Server #1127936] Received 0.24 MB of payload data from client #364 (simulated).
[INFO][10:43:17]: [Server #1127936] Adding client #441 to the list of clients for aggregation.
[INFO][10:43:17]: [Server #1127936] Adding client #414 to the list of clients for aggregation.
[INFO][10:43:17]: [Server #1127936] Adding client #475 to the list of clients for aggregation.
[INFO][10:43:17]: [Server #1127936] Adding client #186 to the list of clients for aggregation.
[INFO][10:43:17]: [Server #1127936] Adding client #383 to the list of clients for aggregation.
[INFO][10:43:17]: [Server #1127936] Adding client #152 to the list of clients for aggregation.
[INFO][10:43:17]: [Server #1127936] Adding client #401 to the list of clients for aggregation.
[INFO][10:43:17]: [Server #1127936] Adding client #408 to the list of clients for aggregation.
[INFO][10:43:17]: [Server #1127936] Adding client #436 to the list of clients for aggregation.
[INFO][10:43:17]: [Server #1127936] Adding client #278 to the list of clients for aggregation.
[INFO][10:43:17]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.0010062  0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00634579
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00088674 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00225491 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00053039 0.
 0.         0.         0.         0.         0.         0.00461006
 0.         0.         0.         0.         0.         0.00235092
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00143152 0.         0.
 0.         0.         0.00110671 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01200779 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 1. 1. 0. 0. 0. 1. 0. 0. 3. 1. 0. 0. 0. 1. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 2. 0. 1. 0.
 1. 1. 1. 0. 2. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 2. 1. 1. 0. 0.
 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 2. 1. 6. 0. 0. 2. 1. 0. 1. 1. 2. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 3. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 1. 6. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 2. 0. 0.
 1. 1. 1. 2. 0. 0. 0. 2. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 1. 2. 0. 4. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1.
 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 4. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 6. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0.
 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0.
 1. 0. 1. 0. 4. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 6. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 2. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0.
 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 2. 1. 1. 1. 0. 0. 0. 1. 0. 6. 1. 1.
 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 6. 0. 0. 1. 1. 0. 1. 1. 2. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 2. 2. 1. 2. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.0010062  0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00634579
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00088674 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00225491 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00053039 0.
 0.         0.         0.         0.         0.         0.00461006
 0.         0.         0.         0.         0.         0.00235092
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00143152 0.         0.
 0.         0.         0.00110671 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01200779 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:43:19]: [Server #1127936] Global model accuracy: 96.26%

[INFO][10:43:19]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_93.pth.
[INFO][10:43:19]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_93.pth.
[INFO][10:43:19]: [93m[1m
[Server #1127936] Starting round 94/100.[0m
[0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002 0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  3e-05  5e-10  5e-10
 6:  6.8876e+00  6.8875e+00  2e-05  3e-10  3e-10
 7:  6.8875e+00  6.8875e+00  2e-05  6e-10  2e-11
 8:  6.8875e+00  6.8875e+00  2e-05  5e-10  1e-11
 9:  6.8875e+00  6.8875e+00  9e-06  1e-09  3e-11
10:  6.8875e+00  6.8875e+00  7e-07  2e-09  4e-11
Optimal solution found.
The calculated probability is:  [5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 6.56249975e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 8.04035584e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62058833e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 1.53860593e-04 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.91949500e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62053355e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 6.32824636e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62058495e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.93314578e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 9.72370851e-01 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05 5.62059044e-05 5.62059044e-05
 5.62059044e-05 5.62059044e-05]
current clients pool:  [INFO][10:43:20]: [Server #1127936] Selected clients: [475   4  98 160  75 300 306 250 327  97]
[INFO][10:43:20]: [Server #1127936] Selecting client #475 for training.
[INFO][10:43:20]: [Server #1127936] Sending the current model to client #475 (simulated).
[INFO][10:43:20]: [Server #1127936] Sending 0.24 MB of payload data to client #475 (simulated).
[INFO][10:43:20]: [Server #1127936] Selecting client #4 for training.
[INFO][10:43:20]: [Server #1127936] Sending the current model to client #4 (simulated).
[INFO][10:43:20]: [Server #1127936] Sending 0.24 MB of payload data to client #4 (simulated).
[INFO][10:43:20]: [Server #1127936] Selecting client #98 for training.
[INFO][10:43:20]: [Server #1127936] Sending the current model to client #98 (simulated).
[INFO][10:43:20]: [Client #475] Selected by the server.
[INFO][10:43:20]: [Client #475] Loading its data source...
[INFO][10:43:20]: [Client #475] Dataset size: 60000
[INFO][10:43:20]: [Client #475] Sampler: noniid
[INFO][10:43:20]: [Server #1127936] Sending 0.24 MB of payload data to client #98 (simulated).
[INFO][10:43:20]: [Client #4] Selected by the server.
[INFO][10:43:20]: [Client #4] Loading its data source...
[INFO][10:43:20]: [Client #475] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:43:20]: [Client #4] Dataset size: 60000
[INFO][10:43:20]: [Client #4] Sampler: noniid
[INFO][10:43:20]: [Client #4] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:43:20]: [93m[1m[Client #475] Started training in communication round #94.[0m
[INFO][10:43:20]: [Client #98] Selected by the server.
[INFO][10:43:20]: [Client #98] Loading its data source...
[INFO][10:43:20]: [Client #98] Dataset size: 60000
[INFO][10:43:20]: [Client #98] Sampler: noniid
[INFO][10:43:20]: [Client #98] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:43:20]: [93m[1m[Client #4] Started training in communication round #94.[0m
[INFO][10:43:20]: [93m[1m[Client #98] Started training in communication round #94.[0m
[INFO][10:43:22]: [Client #98] Loading the dataset.
[INFO][10:43:22]: [Client #4] Loading the dataset.
[INFO][10:43:22]: [Client #475] Loading the dataset.
[INFO][10:43:28]: [Client #98] Epoch: [1/5][0/10]	Loss: 0.001721
[INFO][10:43:28]: [Client #4] Epoch: [1/5][0/10]	Loss: 0.000324
[INFO][10:43:28]: [Client #98] Epoch: [2/5][0/10]	Loss: 0.000161
[INFO][10:43:28]: [Client #98] Epoch: [3/5][0/10]	Loss: 0.000007
[INFO][10:43:28]: [Client #4] Epoch: [2/5][0/10]	Loss: 0.000320
[INFO][10:43:28]: [Client #475] Epoch: [1/5][0/10]	Loss: 0.000015
[INFO][10:43:28]: [Client #98] Epoch: [4/5][0/10]	Loss: 0.000073
[INFO][10:43:28]: [Client #475] Epoch: [2/5][0/10]	Loss: 0.000161
[INFO][10:43:28]: [Client #4] Epoch: [3/5][0/10]	Loss: 0.000061
[INFO][10:43:28]: [Client #98] Epoch: [5/5][0/10]	Loss: 0.000799
[INFO][10:43:28]: [Client #98] Model saved to /data/ykang/plato/results/test/model/lenet5_98_1127979.pth.
[INFO][10:43:29]: [Client #475] Epoch: [3/5][0/10]	Loss: 0.000001
[INFO][10:43:29]: [Client #4] Epoch: [4/5][0/10]	Loss: 0.000010
[INFO][10:43:29]: [Client #475] Epoch: [4/5][0/10]	Loss: 0.000001
[INFO][10:43:29]: [Client #4] Epoch: [5/5][0/10]	Loss: 0.000095
[INFO][10:43:29]: [Client #4] Model saved to /data/ykang/plato/results/test/model/lenet5_4_1127978.pth.
[INFO][10:43:29]: [Client #475] Epoch: [5/5][0/10]	Loss: 0.000055
[INFO][10:43:29]: [Client #475] Model saved to /data/ykang/plato/results/test/model/lenet5_475_1127977.pth.
[INFO][10:43:29]: [Client #98] Loading a model from /data/ykang/plato/results/test/model/lenet5_98_1127979.pth.
[INFO][10:43:29]: [Client #98] Model trained.
[INFO][10:43:29]: [Client #98] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:43:29]: [Server #1127936] Received 0.24 MB of payload data from client #98 (simulated).
[INFO][10:43:29]: [Client #4] Loading a model from /data/ykang/plato/results/test/model/lenet5_4_1127978.pth.
[INFO][10:43:29]: [Client #4] Model trained.
[INFO][10:43:29]: [Client #4] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:43:29]: [Server #1127936] Received 0.24 MB of payload data from client #4 (simulated).
[INFO][10:43:30]: [Client #475] Loading a model from /data/ykang/plato/results/test/model/lenet5_475_1127977.pth.
[INFO][10:43:30]: [Client #475] Model trained.
[INFO][10:43:30]: [Client #475] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:43:30]: [Server #1127936] Received 0.24 MB of payload data from client #475 (simulated).
[INFO][10:43:30]: [Server #1127936] Selecting client #160 for training.
[INFO][10:43:30]: [Server #1127936] Sending the current model to client #160 (simulated).
[INFO][10:43:30]: [Server #1127936] Sending 0.24 MB of payload data to client #160 (simulated).
[INFO][10:43:30]: [Server #1127936] Selecting client #75 for training.
[INFO][10:43:30]: [Server #1127936] Sending the current model to client #75 (simulated).
[INFO][10:43:30]: [Server #1127936] Sending 0.24 MB of payload data to client #75 (simulated).
[INFO][10:43:30]: [Server #1127936] Selecting client #300 for training.
[INFO][10:43:30]: [Server #1127936] Sending the current model to client #300 (simulated).
[INFO][10:43:30]: [Client #160] Selected by the server.
[INFO][10:43:30]: [Client #160] Loading its data source...
[INFO][10:43:30]: [Client #160] Dataset size: 60000
[INFO][10:43:30]: [Client #160] Sampler: noniid
[INFO][10:43:30]: [Server #1127936] Sending 0.24 MB of payload data to client #300 (simulated).
[INFO][10:43:30]: [Client #300] Selected by the server.
[INFO][10:43:30]: [Client #300] Loading its data source...
[INFO][10:43:30]: [Client #300] Dataset size: 60000
[INFO][10:43:30]: [Client #300] Sampler: noniid
[INFO][10:43:30]: [Client #75] Selected by the server.
[INFO][10:43:30]: [Client #75] Loading its data source...
[INFO][10:43:30]: [Client #75] Dataset size: 60000
[INFO][10:43:30]: [Client #75] Sampler: noniid
[INFO][10:43:30]: [Client #160] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:43:30]: [Client #300] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:43:30]: [Client #75] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:43:30]: [93m[1m[Client #300] Started training in communication round #94.[0m
[INFO][10:43:30]: [93m[1m[Client #160] Started training in communication round #94.[0m
[INFO][10:43:30]: [93m[1m[Client #75] Started training in communication round #94.[0m
[INFO][10:43:32]: [Client #300] Loading the dataset.
[INFO][10:43:32]: [Client #75] Loading the dataset.
[INFO][10:43:32]: [Client #160] Loading the dataset.
[INFO][10:43:38]: [Client #160] Epoch: [1/5][0/10]	Loss: 0.000123
[INFO][10:43:38]: [Client #300] Epoch: [1/5][0/10]	Loss: 0.005267
[INFO][10:43:38]: [Client #75] Epoch: [1/5][0/10]	Loss: 0.000376
[INFO][10:43:38]: [Client #160] Epoch: [2/5][0/10]	Loss: 0.000165
[INFO][10:43:38]: [Client #75] Epoch: [2/5][0/10]	Loss: 0.000431
[INFO][10:43:38]: [Client #300] Epoch: [2/5][0/10]	Loss: 0.000049
[INFO][10:43:38]: [Client #160] Epoch: [3/5][0/10]	Loss: 0.000034
[INFO][10:43:38]: [Client #75] Epoch: [3/5][0/10]	Loss: 0.000006
[INFO][10:43:38]: [Client #300] Epoch: [3/5][0/10]	Loss: 0.000008
[INFO][10:43:38]: [Client #160] Epoch: [4/5][0/10]	Loss: 0.000044
[INFO][10:43:38]: [Client #75] Epoch: [4/5][0/10]	Loss: 0.000015
[INFO][10:43:38]: [Client #300] Epoch: [4/5][0/10]	Loss: 0.000006
[INFO][10:43:38]: [Client #160] Epoch: [5/5][0/10]	Loss: 0.000110
[INFO][10:43:38]: [Client #300] Epoch: [5/5][0/10]	Loss: 0.000021
[INFO][10:43:38]: [Client #75] Epoch: [5/5][0/10]	Loss: 0.000108
[INFO][10:43:38]: [Client #160] Model saved to /data/ykang/plato/results/test/model/lenet5_160_1127977.pth.
[INFO][10:43:38]: [Client #300] Model saved to /data/ykang/plato/results/test/model/lenet5_300_1127979.pth.
[INFO][10:43:38]: [Client #75] Model saved to /data/ykang/plato/results/test/model/lenet5_75_1127978.pth.
[INFO][10:43:39]: [Client #160] Loading a model from /data/ykang/plato/results/test/model/lenet5_160_1127977.pth.
[INFO][10:43:39]: [Client #160] Model trained.
[INFO][10:43:39]: [Client #160] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:43:39]: [Server #1127936] Received 0.24 MB of payload data from client #160 (simulated).
[INFO][10:43:39]: [Client #300] Loading a model from /data/ykang/plato/results/test/model/lenet5_300_1127979.pth.
[INFO][10:43:39]: [Client #300] Model trained.
[INFO][10:43:39]: [Client #300] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:43:39]: [Server #1127936] Received 0.24 MB of payload data from client #300 (simulated).
[INFO][10:43:39]: [Client #75] Loading a model from /data/ykang/plato/results/test/model/lenet5_75_1127978.pth.
[INFO][10:43:39]: [Client #75] Model trained.
[INFO][10:43:39]: [Client #75] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:43:39]: [Server #1127936] Received 0.24 MB of payload data from client #75 (simulated).
[INFO][10:43:39]: [Server #1127936] Selecting client #306 for training.
[INFO][10:43:39]: [Server #1127936] Sending the current model to client #306 (simulated).
[INFO][10:43:39]: [Server #1127936] Sending 0.24 MB of payload data to client #306 (simulated).
[INFO][10:43:39]: [Server #1127936] Selecting client #250 for training.
[INFO][10:43:39]: [Server #1127936] Sending the current model to client #250 (simulated).
[INFO][10:43:39]: [Server #1127936] Sending 0.24 MB of payload data to client #250 (simulated).
[INFO][10:43:39]: [Server #1127936] Selecting client #327 for training.
[INFO][10:43:39]: [Server #1127936] Sending the current model to client #327 (simulated).
[INFO][10:43:39]: [Client #306] Selected by the server.
[INFO][10:43:39]: [Client #306] Loading its data source...
[INFO][10:43:39]: [Client #306] Dataset size: 60000
[INFO][10:43:39]: [Client #306] Sampler: noniid
[INFO][10:43:39]: [Server #1127936] Sending 0.24 MB of payload data to client #327 (simulated).
[INFO][10:43:39]: [Client #327] Selected by the server.
[INFO][10:43:39]: [Client #327] Loading its data source...
[INFO][10:43:39]: [Client #327] Dataset size: 60000
[INFO][10:43:39]: [Client #327] Sampler: noniid
[INFO][10:43:39]: [Client #250] Selected by the server.
[INFO][10:43:39]: [Client #250] Loading its data source...
[INFO][10:43:39]: [Client #250] Dataset size: 60000
[INFO][10:43:39]: [Client #250] Sampler: noniid
[INFO][10:43:39]: [Client #327] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:43:39]: [Client #306] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:43:39]: [Client #250] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:43:39]: [93m[1m[Client #327] Started training in communication round #94.[0m
[INFO][10:43:39]: [93m[1m[Client #250] Started training in communication round #94.[0m
[INFO][10:43:39]: [93m[1m[Client #306] Started training in communication round #94.[0m
[INFO][10:43:41]: [Client #306] Loading the dataset.
[INFO][10:43:42]: [Client #250] Loading the dataset.
[INFO][10:43:42]: [Client #327] Loading the dataset.
[INFO][10:43:48]: [Client #306] Epoch: [1/5][0/10]	Loss: 0.004578
[INFO][10:43:48]: [Client #250] Epoch: [1/5][0/10]	Loss: 0.000375
[INFO][10:43:48]: [Client #327] Epoch: [1/5][0/10]	Loss: 0.001836
[INFO][10:43:48]: [Client #306] Epoch: [2/5][0/10]	Loss: 0.001516
[INFO][10:43:48]: [Client #250] Epoch: [2/5][0/10]	Loss: 0.000303
[INFO][10:43:48]: [Client #327] Epoch: [2/5][0/10]	Loss: 0.000001
[INFO][10:43:48]: [Client #306] Epoch: [3/5][0/10]	Loss: 0.000025
[INFO][10:43:48]: [Client #250] Epoch: [3/5][0/10]	Loss: 0.000024
[INFO][10:43:48]: [Client #327] Epoch: [3/5][0/10]	Loss: 0.000004
[INFO][10:43:48]: [Client #306] Epoch: [4/5][0/10]	Loss: 0.000009
[INFO][10:43:48]: [Client #327] Epoch: [4/5][0/10]	Loss: 0.000001
[INFO][10:43:48]: [Client #250] Epoch: [4/5][0/10]	Loss: 0.000007
[INFO][10:43:48]: [Client #306] Epoch: [5/5][0/10]	Loss: 0.000006
[INFO][10:43:48]: [Client #327] Epoch: [5/5][0/10]	Loss: 0.000072
[INFO][10:43:48]: [Client #306] Model saved to /data/ykang/plato/results/test/model/lenet5_306_1127977.pth.
[INFO][10:43:48]: [Client #327] Model saved to /data/ykang/plato/results/test/model/lenet5_327_1127979.pth.
[INFO][10:43:48]: [Client #250] Epoch: [5/5][0/10]	Loss: 0.000001
[INFO][10:43:48]: [Client #250] Model saved to /data/ykang/plato/results/test/model/lenet5_250_1127978.pth.
[INFO][10:43:49]: [Client #327] Loading a model from /data/ykang/plato/results/test/model/lenet5_327_1127979.pth.
[INFO][10:43:49]: [Client #327] Model trained.
[INFO][10:43:49]: [Client #327] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:43:49]: [Server #1127936] Received 0.24 MB of payload data from client #327 (simulated).
[INFO][10:43:49]: [Client #306] Loading a model from /data/ykang/plato/results/test/model/lenet5_306_1127977.pth.
[INFO][10:43:49]: [Client #306] Model trained.
[INFO][10:43:49]: [Client #306] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:43:49]: [Server #1127936] Received 0.24 MB of payload data from client #306 (simulated).
[INFO][10:43:49]: [Client #250] Loading a model from /data/ykang/plato/results/test/model/lenet5_250_1127978.pth.
[INFO][10:43:49]: [Client #250] Model trained.
[INFO][10:43:49]: [Client #250] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:43:49]: [Server #1127936] Received 0.24 MB of payload data from client #250 (simulated).
[INFO][10:43:49]: [Server #1127936] Selecting client #97 for training.
[INFO][10:43:49]: [Server #1127936] Sending the current model to client #97 (simulated).
[INFO][10:43:49]: [Server #1127936] Sending 0.24 MB of payload data to client #97 (simulated).
[INFO][10:43:49]: [Client #97] Selected by the server.
[INFO][10:43:49]: [Client #97] Loading its data source...
[INFO][10:43:49]: [Client #97] Dataset size: 60000
[INFO][10:43:49]: [Client #97] Sampler: noniid
[INFO][10:43:49]: [Client #97] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:43:49]: [93m[1m[Client #97] Started training in communication round #94.[0m
[INFO][10:43:51]: [Client #97] Loading the dataset.
[INFO][10:43:57]: [Client #97] Epoch: [1/5][0/10]	Loss: 0.005039
[INFO][10:43:57]: [Client #97] Epoch: [2/5][0/10]	Loss: 0.000602
[INFO][10:43:57]: [Client #97] Epoch: [3/5][0/10]	Loss: 0.000001
[INFO][10:43:57]: [Client #97] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:43:57]: [Client #97] Epoch: [5/5][0/10]	Loss: 0.000001
[INFO][10:43:57]: [Client #97] Model saved to /data/ykang/plato/results/test/model/lenet5_97_1127977.pth.
[INFO][10:43:58]: [Client #97] Loading a model from /data/ykang/plato/results/test/model/lenet5_97_1127977.pth.
[INFO][10:43:58]: [Client #97] Model trained.
[INFO][10:43:58]: [Client #97] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:43:58]: [Server #1127936] Received 0.24 MB of payload data from client #97 (simulated).
[INFO][10:43:58]: [Server #1127936] Adding client #364 to the list of clients for aggregation.
[INFO][10:43:58]: [Server #1127936] Adding client #293 to the list of clients for aggregation.
[INFO][10:43:58]: [Server #1127936] Adding client #199 to the list of clients for aggregation.
[INFO][10:43:58]: [Server #1127936] Adding client #2 to the list of clients for aggregation.
[INFO][10:43:58]: [Server #1127936] Adding client #351 to the list of clients for aggregation.
[INFO][10:43:58]: [Server #1127936] Adding client #327 to the list of clients for aggregation.
[INFO][10:43:58]: [Server #1127936] Adding client #300 to the list of clients for aggregation.
[INFO][10:43:58]: [Server #1127936] Adding client #4 to the list of clients for aggregation.
[INFO][10:43:58]: [Server #1127936] Adding client #75 to the list of clients for aggregation.
[INFO][10:43:58]: [Server #1127936] Adding client #306 to the list of clients for aggregation.
[INFO][10:43:58]: [Server #1127936] Aggregating 10 clients in total.
[1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 294, 295, 296, 297, 298, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.02253659 0.         0.00025979 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00018939 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00575837 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00112192 0.
 0.         0.         0.         0.         0.         0.00131402
 0.         0.         0.         0.         0.         0.00161994
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00193907 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00620852 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00546259 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 1. 1. 0. 0. 0. 1. 0. 0. 3. 1. 0. 0. 0. 1. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 2. 0. 1. 0.
 1. 1. 0. 0. 2. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 2. 1. 1. 0. 0.
 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 2. 1. 6. 0. 0. 2. 1. 0. 1. 1. 2. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 3. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 1. 6. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 2. 0. 0.
 1. 1. 1. 2. 0. 0. 1. 2. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 1. 2. 0. 4. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1.
 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 4. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 6. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0.
 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0.
 1. 0. 1. 1. 4. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 6. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 2. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0.
 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 2. 1. 1. 1. 0. 0. 0. 1. 0. 6. 1. 1.
 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 6. 0. 0. 1. 1. 0. 1. 1. 2. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 2. 2. 1. 2. 0.]
local_gradient_bounds:  [0.         0.02253659 0.         0.00025979 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00018939 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00575837 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00112192 0.
 0.         0.         0.         0.         0.         0.00131402
 0.         0.         0.         0.         0.         0.00161994
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00193907 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00620852 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00546259 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:44:00]: [Server #1127936] Global model accuracy: 96.59%

[INFO][10:44:00]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_94.pth.
[INFO][10:44:00]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_94.pth.
[INFO][10:44:00]: [93m[1m
[Server #1127936] Starting round 95/100.[0m
[0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  3e-05  4e-10  4e-10
 6:  6.8876e+00  6.8875e+00  2e-05  3e-10  3e-10
 7:  6.8875e+00  6.8875e+00  2e-05  6e-10  1e-11
 8:  6.8875e+00  6.8875e+00  1e-05  5e-10  1e-11
 9:  6.8875e+00  6.8875e+00  9e-06  9e-10  2e-11
10:  6.8875e+00  6.8875e+00  6e-07  1e-09  3e-11
Optimal solution found.
The calculated probability is:  [5.32092010e-05 9.73911367e-01 5.32092010e-05 5.32091992e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092000e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 7.49462370e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.64070267e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32091545e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32091304e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32090998e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 7.74038773e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 7.34132163e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05 5.32092010e-05 5.32092010e-05
 5.32092010e-05 5.32092010e-05]
current clients pool:  [INFO][10:44:00]: [Server #1127936] Selected clients: [  2 229 313 376  56 314 196 337 104 266]
[INFO][10:44:00]: [Server #1127936] Selecting client #2 for training.
[INFO][10:44:00]: [Server #1127936] Sending the current model to client #2 (simulated).
[INFO][10:44:00]: [Server #1127936] Sending 0.24 MB of payload data to client #2 (simulated).
[INFO][10:44:00]: [Server #1127936] Selecting client #229 for training.
[INFO][10:44:00]: [Server #1127936] Sending the current model to client #229 (simulated).
[INFO][10:44:00]: [Server #1127936] Sending 0.24 MB of payload data to client #229 (simulated).
[INFO][10:44:00]: [Server #1127936] Selecting client #313 for training.
[INFO][10:44:00]: [Server #1127936] Sending the current model to client #313 (simulated).
[INFO][10:44:00]: [Client #2] Selected by the server.
[INFO][10:44:00]: [Client #2] Loading its data source...
[INFO][10:44:00]: [Client #2] Dataset size: 60000
[INFO][10:44:00]: [Client #2] Sampler: noniid
[INFO][10:44:00]: [Server #1127936] Sending 0.24 MB of payload data to client #313 (simulated).
[INFO][10:44:00]: [Client #229] Selected by the server.
[INFO][10:44:00]: [Client #229] Loading its data source...
[INFO][10:44:00]: [Client #229] Dataset size: 60000
[INFO][10:44:00]: [Client #229] Sampler: noniid
[INFO][10:44:00]: [Client #2] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:44:00]: [Client #229] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:44:00]: [93m[1m[Client #2] Started training in communication round #95.[0m
[INFO][10:44:00]: [93m[1m[Client #229] Started training in communication round #95.[0m
[INFO][10:44:01]: [Client #313] Selected by the server.
[INFO][10:44:01]: [Client #313] Loading its data source...
[INFO][10:44:01]: [Client #313] Dataset size: 60000
[INFO][10:44:01]: [Client #313] Sampler: noniid
[INFO][10:44:01]: [Client #313] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:44:01]: [93m[1m[Client #313] Started training in communication round #95.[0m
[INFO][10:44:03]: [Client #313] Loading the dataset.
[INFO][10:44:03]: [Client #229] Loading the dataset.
[INFO][10:44:03]: [Client #2] Loading the dataset.
[INFO][10:44:09]: [Client #229] Epoch: [1/5][0/10]	Loss: 0.000209
[INFO][10:44:09]: [Client #2] Epoch: [1/5][0/10]	Loss: 0.000499
[INFO][10:44:09]: [Client #313] Epoch: [1/5][0/10]	Loss: 0.000747
[INFO][10:44:09]: [Client #229] Epoch: [2/5][0/10]	Loss: 0.001466
[INFO][10:44:09]: [Client #2] Epoch: [2/5][0/10]	Loss: 0.000038
[INFO][10:44:09]: [Client #313] Epoch: [2/5][0/10]	Loss: 0.000157
[INFO][10:44:09]: [Client #229] Epoch: [3/5][0/10]	Loss: 0.000038
[INFO][10:44:09]: [Client #2] Epoch: [3/5][0/10]	Loss: 0.000014
[INFO][10:44:09]: [Client #313] Epoch: [3/5][0/10]	Loss: 0.000001
[INFO][10:44:09]: [Client #229] Epoch: [4/5][0/10]	Loss: 0.000029
[INFO][10:44:09]: [Client #313] Epoch: [4/5][0/10]	Loss: 0.000007
[INFO][10:44:09]: [Client #2] Epoch: [4/5][0/10]	Loss: 0.000028
[INFO][10:44:09]: [Client #229] Epoch: [5/5][0/10]	Loss: 0.000654
[INFO][10:44:09]: [Client #2] Epoch: [5/5][0/10]	Loss: 0.000252
[INFO][10:44:09]: [Client #313] Epoch: [5/5][0/10]	Loss: 0.000025
[INFO][10:44:09]: [Client #229] Model saved to /data/ykang/plato/results/test/model/lenet5_229_1127978.pth.
[INFO][10:44:09]: [Client #2] Model saved to /data/ykang/plato/results/test/model/lenet5_2_1127977.pth.
[INFO][10:44:09]: [Client #313] Model saved to /data/ykang/plato/results/test/model/lenet5_313_1127979.pth.
[INFO][10:44:10]: [Client #229] Loading a model from /data/ykang/plato/results/test/model/lenet5_229_1127978.pth.
[INFO][10:44:10]: [Client #229] Model trained.
[INFO][10:44:10]: [Client #229] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:44:10]: [Server #1127936] Received 0.24 MB of payload data from client #229 (simulated).
[INFO][10:44:10]: [Client #2] Loading a model from /data/ykang/plato/results/test/model/lenet5_2_1127977.pth.
[INFO][10:44:10]: [Client #2] Model trained.
[INFO][10:44:10]: [Client #2] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:44:10]: [Server #1127936] Received 0.24 MB of payload data from client #2 (simulated).
[INFO][10:44:10]: [Client #313] Loading a model from /data/ykang/plato/results/test/model/lenet5_313_1127979.pth.
[INFO][10:44:10]: [Client #313] Model trained.
[INFO][10:44:10]: [Client #313] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:44:10]: [Server #1127936] Received 0.24 MB of payload data from client #313 (simulated).
[INFO][10:44:10]: [Server #1127936] Selecting client #376 for training.
[INFO][10:44:10]: [Server #1127936] Sending the current model to client #376 (simulated).
[INFO][10:44:10]: [Server #1127936] Sending 0.24 MB of payload data to client #376 (simulated).
[INFO][10:44:10]: [Server #1127936] Selecting client #56 for training.
[INFO][10:44:10]: [Server #1127936] Sending the current model to client #56 (simulated).
[INFO][10:44:10]: [Server #1127936] Sending 0.24 MB of payload data to client #56 (simulated).
[INFO][10:44:10]: [Server #1127936] Selecting client #314 for training.
[INFO][10:44:10]: [Server #1127936] Sending the current model to client #314 (simulated).
[INFO][10:44:10]: [Client #376] Selected by the server.
[INFO][10:44:10]: [Client #376] Loading its data source...
[INFO][10:44:10]: [Client #376] Dataset size: 60000
[INFO][10:44:10]: [Client #376] Sampler: noniid
[INFO][10:44:10]: [Server #1127936] Sending 0.24 MB of payload data to client #314 (simulated).
[INFO][10:44:10]: [Client #376] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:44:10]: [Client #56] Selected by the server.
[INFO][10:44:10]: [Client #56] Loading its data source...
[INFO][10:44:10]: [Client #314] Selected by the server.
[INFO][10:44:10]: [Client #56] Dataset size: 60000
[INFO][10:44:10]: [Client #314] Loading its data source...
[INFO][10:44:10]: [Client #56] Sampler: noniid
[INFO][10:44:10]: [Client #314] Dataset size: 60000
[INFO][10:44:10]: [Client #314] Sampler: noniid
[INFO][10:44:10]: [Client #314] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:44:10]: [Client #56] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:44:10]: [93m[1m[Client #376] Started training in communication round #95.[0m
[INFO][10:44:10]: [93m[1m[Client #56] Started training in communication round #95.[0m
[INFO][10:44:10]: [93m[1m[Client #314] Started training in communication round #95.[0m
[INFO][10:44:12]: [Client #376] Loading the dataset.
[INFO][10:44:13]: [Client #56] Loading the dataset.
[INFO][10:44:13]: [Client #314] Loading the dataset.
[INFO][10:44:19]: [Client #56] Epoch: [1/5][0/10]	Loss: 0.000281
[INFO][10:44:19]: [Client #376] Epoch: [1/5][0/10]	Loss: 0.000124
[INFO][10:44:19]: [Client #56] Epoch: [2/5][0/10]	Loss: 0.000704
[INFO][10:44:19]: [Client #314] Epoch: [1/5][0/10]	Loss: 0.000166
[INFO][10:44:19]: [Client #56] Epoch: [3/5][0/10]	Loss: 0.000081
[INFO][10:44:19]: [Client #376] Epoch: [2/5][0/10]	Loss: 0.000554
[INFO][10:44:19]: [Client #314] Epoch: [2/5][0/10]	Loss: 0.001200
[INFO][10:44:19]: [Client #56] Epoch: [4/5][0/10]	Loss: 0.000060
[INFO][10:44:19]: [Client #376] Epoch: [3/5][0/10]	Loss: 0.000027
[INFO][10:44:19]: [Client #314] Epoch: [3/5][0/10]	Loss: 0.000081
[INFO][10:44:19]: [Client #56] Epoch: [5/5][0/10]	Loss: 0.000086
[INFO][10:44:19]: [Client #376] Epoch: [4/5][0/10]	Loss: 0.000129
[INFO][10:44:19]: [Client #314] Epoch: [4/5][0/10]	Loss: 0.000122
[INFO][10:44:19]: [Client #56] Model saved to /data/ykang/plato/results/test/model/lenet5_56_1127978.pth.
[INFO][10:44:19]: [Client #376] Epoch: [5/5][0/10]	Loss: 0.000012
[INFO][10:44:19]: [Client #376] Model saved to /data/ykang/plato/results/test/model/lenet5_376_1127977.pth.
[INFO][10:44:19]: [Client #314] Epoch: [5/5][0/10]	Loss: 0.000058
[INFO][10:44:19]: [Client #314] Model saved to /data/ykang/plato/results/test/model/lenet5_314_1127979.pth.
[INFO][10:44:20]: [Client #56] Loading a model from /data/ykang/plato/results/test/model/lenet5_56_1127978.pth.
[INFO][10:44:20]: [Client #56] Model trained.
[INFO][10:44:20]: [Client #56] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:44:20]: [Server #1127936] Received 0.24 MB of payload data from client #56 (simulated).
[INFO][10:44:20]: [Client #376] Loading a model from /data/ykang/plato/results/test/model/lenet5_376_1127977.pth.
[INFO][10:44:20]: [Client #376] Model trained.
[INFO][10:44:20]: [Client #376] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:44:20]: [Server #1127936] Received 0.24 MB of payload data from client #376 (simulated).
[INFO][10:44:20]: [Client #314] Loading a model from /data/ykang/plato/results/test/model/lenet5_314_1127979.pth.
[INFO][10:44:20]: [Client #314] Model trained.
[INFO][10:44:20]: [Client #314] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:44:20]: [Server #1127936] Received 0.24 MB of payload data from client #314 (simulated).
[INFO][10:44:20]: [Server #1127936] Selecting client #196 for training.
[INFO][10:44:20]: [Server #1127936] Sending the current model to client #196 (simulated).
[INFO][10:44:20]: [Server #1127936] Sending 0.24 MB of payload data to client #196 (simulated).
[INFO][10:44:20]: [Server #1127936] Selecting client #337 for training.
[INFO][10:44:20]: [Server #1127936] Sending the current model to client #337 (simulated).
[INFO][10:44:20]: [Server #1127936] Sending 0.24 MB of payload data to client #337 (simulated).
[INFO][10:44:20]: [Server #1127936] Selecting client #104 for training.
[INFO][10:44:20]: [Server #1127936] Sending the current model to client #104 (simulated).
[INFO][10:44:20]: [Client #196] Selected by the server.
[INFO][10:44:20]: [Client #196] Loading its data source...
[INFO][10:44:20]: [Client #196] Dataset size: 60000
[INFO][10:44:20]: [Client #196] Sampler: noniid
[INFO][10:44:20]: [Server #1127936] Sending 0.24 MB of payload data to client #104 (simulated).
[INFO][10:44:20]: [Client #337] Selected by the server.
[INFO][10:44:20]: [Client #104] Selected by the server.
[INFO][10:44:20]: [Client #337] Loading its data source...
[INFO][10:44:20]: [Client #104] Loading its data source...
[INFO][10:44:20]: [Client #337] Dataset size: 60000
[INFO][10:44:20]: [Client #104] Dataset size: 60000
[INFO][10:44:20]: [Client #337] Sampler: noniid
[INFO][10:44:20]: [Client #104] Sampler: noniid
[INFO][10:44:20]: [Client #196] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:44:20]: [Client #104] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:44:20]: [Client #337] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:44:20]: [93m[1m[Client #196] Started training in communication round #95.[0m
[INFO][10:44:20]: [93m[1m[Client #337] Started training in communication round #95.[0m
[INFO][10:44:20]: [93m[1m[Client #104] Started training in communication round #95.[0m
[INFO][10:44:22]: [Client #196] Loading the dataset.
[INFO][10:44:22]: [Client #104] Loading the dataset.
[INFO][10:44:22]: [Client #337] Loading the dataset.
[INFO][10:44:28]: [Client #196] Epoch: [1/5][0/10]	Loss: 0.000429
[INFO][10:44:28]: [Client #337] Epoch: [1/5][0/10]	Loss: 0.000087
[INFO][10:44:28]: [Client #104] Epoch: [1/5][0/10]	Loss: 0.000095
[INFO][10:44:28]: [Client #196] Epoch: [2/5][0/10]	Loss: 0.000443
[INFO][10:44:28]: [Client #337] Epoch: [2/5][0/10]	Loss: 0.000222
[INFO][10:44:29]: [Client #104] Epoch: [2/5][0/10]	Loss: 0.000063
[INFO][10:44:29]: [Client #196] Epoch: [3/5][0/10]	Loss: 0.000109
[INFO][10:44:29]: [Client #337] Epoch: [3/5][0/10]	Loss: 0.000356
[INFO][10:44:29]: [Client #104] Epoch: [3/5][0/10]	Loss: 0.000005
[INFO][10:44:29]: [Client #196] Epoch: [4/5][0/10]	Loss: 0.000003
[INFO][10:44:29]: [Client #337] Epoch: [4/5][0/10]	Loss: 0.000003
[INFO][10:44:29]: [Client #196] Epoch: [5/5][0/10]	Loss: 0.000043
[INFO][10:44:29]: [Client #104] Epoch: [4/5][0/10]	Loss: 0.000001
[INFO][10:44:29]: [Client #196] Model saved to /data/ykang/plato/results/test/model/lenet5_196_1127977.pth.
[INFO][10:44:29]: [Client #337] Epoch: [5/5][0/10]	Loss: 0.000254
[INFO][10:44:29]: [Client #104] Epoch: [5/5][0/10]	Loss: 0.000001
[INFO][10:44:29]: [Client #337] Model saved to /data/ykang/plato/results/test/model/lenet5_337_1127978.pth.
[INFO][10:44:29]: [Client #104] Model saved to /data/ykang/plato/results/test/model/lenet5_104_1127979.pth.
[INFO][10:44:30]: [Client #196] Loading a model from /data/ykang/plato/results/test/model/lenet5_196_1127977.pth.
[INFO][10:44:30]: [Client #196] Model trained.
[INFO][10:44:30]: [Client #196] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:44:30]: [Server #1127936] Received 0.24 MB of payload data from client #196 (simulated).
[INFO][10:44:30]: [Client #337] Loading a model from /data/ykang/plato/results/test/model/lenet5_337_1127978.pth.
[INFO][10:44:30]: [Client #337] Model trained.
[INFO][10:44:30]: [Client #337] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:44:30]: [Server #1127936] Received 0.24 MB of payload data from client #337 (simulated).
[INFO][10:44:30]: [Client #104] Loading a model from /data/ykang/plato/results/test/model/lenet5_104_1127979.pth.
[INFO][10:44:30]: [Client #104] Model trained.
[INFO][10:44:30]: [Client #104] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:44:30]: [Server #1127936] Received 0.24 MB of payload data from client #104 (simulated).
[INFO][10:44:30]: [Server #1127936] Selecting client #266 for training.
[INFO][10:44:30]: [Server #1127936] Sending the current model to client #266 (simulated).
[INFO][10:44:30]: [Server #1127936] Sending 0.24 MB of payload data to client #266 (simulated).
[INFO][10:44:30]: [Client #266] Selected by the server.
[INFO][10:44:30]: [Client #266] Loading its data source...
[INFO][10:44:30]: [Client #266] Dataset size: 60000
[INFO][10:44:30]: [Client #266] Sampler: noniid
[INFO][10:44:30]: [Client #266] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:44:30]: [93m[1m[Client #266] Started training in communication round #95.[0m
[INFO][10:44:32]: [Client #266] Loading the dataset.
[INFO][10:44:38]: [Client #266] Epoch: [1/5][0/10]	Loss: 0.000438
[INFO][10:44:38]: [Client #266] Epoch: [2/5][0/10]	Loss: 0.001337
[INFO][10:44:38]: [Client #266] Epoch: [3/5][0/10]	Loss: 0.000005
[INFO][10:44:38]: [Client #266] Epoch: [4/5][0/10]	Loss: 0.000001
[INFO][10:44:38]: [Client #266] Epoch: [5/5][0/10]	Loss: 0.000035
[INFO][10:44:38]: [Client #266] Model saved to /data/ykang/plato/results/test/model/lenet5_266_1127977.pth.
[INFO][10:44:39]: [Client #266] Loading a model from /data/ykang/plato/results/test/model/lenet5_266_1127977.pth.
[INFO][10:44:39]: [Client #266] Model trained.
[INFO][10:44:39]: [Client #266] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:44:39]: [Server #1127936] Received 0.24 MB of payload data from client #266 (simulated).
[INFO][10:44:39]: [Server #1127936] Adding client #98 to the list of clients for aggregation.
[INFO][10:44:39]: [Server #1127936] Adding client #160 to the list of clients for aggregation.
[INFO][10:44:39]: [Server #1127936] Adding client #97 to the list of clients for aggregation.
[INFO][10:44:39]: [Server #1127936] Adding client #56 to the list of clients for aggregation.
[INFO][10:44:39]: [Server #1127936] Adding client #314 to the list of clients for aggregation.
[INFO][10:44:39]: [Server #1127936] Adding client #313 to the list of clients for aggregation.
[INFO][10:44:39]: [Server #1127936] Adding client #250 to the list of clients for aggregation.
[INFO][10:44:39]: [Server #1127936] Adding client #266 to the list of clients for aggregation.
[INFO][10:44:39]: [Server #1127936] Adding client #337 to the list of clients for aggregation.
[INFO][10:44:39]: [Server #1127936] Adding client #104 to the list of clients for aggregation.
[INFO][10:44:39]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00056954 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00191293 0.00321657 0.         0.         0.         0.
 0.         0.00229467 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00283333 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00193101 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00200965 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00785266 0.00014976 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.0011608  0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 1. 1. 0. 0. 0. 1. 0. 0. 3. 1. 0. 0. 0. 1. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 2. 0. 1. 0.
 1. 1. 0. 0. 2. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 2. 1. 1. 0. 0.
 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 2. 1. 6. 0. 0. 2. 1. 0. 1. 1. 2. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 3. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 1. 6. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 2. 0. 0.
 1. 1. 1. 2. 0. 0. 1. 2. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 4. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1.
 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 4. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 6. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0.
 1. 0. 1. 1. 4. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 6. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 2. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0.
 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 2. 1. 1. 1. 0. 0. 0. 1. 0. 6. 1. 1.
 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 6. 0. 0. 1. 1. 0. 1. 1. 2. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 2. 2. 1. 2. 0.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00056954 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00191293 0.00321657 0.         0.         0.         0.
 0.         0.00229467 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00283333 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00193101 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00200965 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00785266 0.00014976 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.0011608  0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:44:41]: [Server #1127936] Global model accuracy: 96.29%

[INFO][10:44:41]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_95.pth.
[INFO][10:44:41]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_95.pth.
[INFO][10:44:41]: [93m[1m
[Server #1127936] Starting round 96/100.[0m
[INFO][10:44:41]: [Server #1127936] Selected clients: [ 87  11 244  93 319 268 388 197 495 258]
[INFO][10:44:41]: [Server #1127936] Selecting client #87 for training.
[INFO][10:44:41]: [Server #1127936] Sending the current model to client #87 (simulated).
[INFO][10:44:41]: [Server #1127936] Sending 0.24 MB of payload data to client #87 (simulated).
[INFO][10:44:41]: [Server #1127936] Selecting client #11 for training.
[INFO][10:44:41]: [Server #1127936] Sending the current model to client #11 (simulated).
[INFO][10:44:41]: [Server #1127936] Sending 0.24 MB of payload data to client #11 (simulated).
[INFO][10:44:41]: [Server #1127936] Selecting client #244 for training.
[INFO][10:44:41]: [Server #1127936] Sending the current model to client #244 (simulated).
[INFO][10:44:41]: [Client #87] Selected by the server.
[INFO][10:44:41]: [Client #87] Loading its data source...
[INFO][10:44:41]: [Client #87] Dataset size: 60000
[INFO][10:44:41]: [Client #87] Sampler: noniid
[INFO][10:44:41]: [Server #1127936] Sending 0.24 MB of payload data to client #244 (simulated).
[INFO][10:44:41]: [Client #11] Selected by the server.
[INFO][10:44:41]: [Client #11] Loading its data source...
[INFO][10:44:41]: [Client #11] Dataset size: 60000
[INFO][10:44:41]: [Client #11] Sampler: noniid
[INFO][10:44:41]: [Client #244] Selected by the server.
[INFO][10:44:41]: [Client #244] Loading its data source...
[INFO][10:44:41]: [Client #244] Dataset size: 60000
[INFO][10:44:41]: [Client #87] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:44:41]: [Client #244] Sampler: noniid
[INFO][10:44:41]: [Client #244] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:44:41]: [Client #11] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:44:41]: [93m[1m[Client #244] Started training in communication round #96.[0m
[INFO][10:44:41]: [93m[1m[Client #87] Started training in communication round #96.[0m
[INFO][10:44:41]: [93m[1m[Client #11] Started training in communication round #96.[0m
[INFO][10:44:43]: [Client #11] Loading the dataset.
[INFO][10:44:43]: [Client #87] Loading the dataset.
[INFO][10:44:43]: [Client #244] Loading the dataset.
[INFO][10:44:49]: [Client #87] Epoch: [1/5][0/10]	Loss: 0.000145
[INFO][10:44:49]: [Client #244] Epoch: [1/5][0/10]	Loss: 0.000112
[INFO][10:44:49]: [Client #11] Epoch: [1/5][0/10]	Loss: 0.000207
[INFO][10:44:49]: [Client #87] Epoch: [2/5][0/10]	Loss: 0.000152
[INFO][10:44:49]: [Client #11] Epoch: [2/5][0/10]	Loss: 0.000747
[INFO][10:44:49]: [Client #244] Epoch: [2/5][0/10]	Loss: 0.000398
[INFO][10:44:49]: [Client #87] Epoch: [3/5][0/10]	Loss: 0.000051
[INFO][10:44:49]: [Client #11] Epoch: [3/5][0/10]	Loss: 0.000191
[INFO][10:44:49]: [Client #244] Epoch: [3/5][0/10]	Loss: 0.000216
[INFO][10:44:49]: [Client #87] Epoch: [4/5][0/10]	Loss: 0.000178
[INFO][10:44:49]: [Client #11] Epoch: [4/5][0/10]	Loss: 0.000026
[INFO][10:44:49]: [Client #244] Epoch: [4/5][0/10]	Loss: 0.002122
[INFO][10:44:49]: [Client #11] Epoch: [5/5][0/10]	Loss: 0.000015
[INFO][10:44:50]: [Client #87] Epoch: [5/5][0/10]	Loss: 0.000001
[INFO][10:44:50]: [Client #244] Epoch: [5/5][0/10]	Loss: 0.003042
[INFO][10:44:50]: [Client #11] Model saved to /data/ykang/plato/results/test/model/lenet5_11_1127978.pth.
[INFO][10:44:50]: [Client #87] Model saved to /data/ykang/plato/results/test/model/lenet5_87_1127977.pth.
[INFO][10:44:50]: [Client #244] Model saved to /data/ykang/plato/results/test/model/lenet5_244_1127979.pth.
[INFO][10:44:50]: [Client #11] Loading a model from /data/ykang/plato/results/test/model/lenet5_11_1127978.pth.
[INFO][10:44:50]: [Client #11] Model trained.
[INFO][10:44:50]: [Client #11] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:44:50]: [Server #1127936] Received 0.24 MB of payload data from client #11 (simulated).
[INFO][10:44:50]: [Client #87] Loading a model from /data/ykang/plato/results/test/model/lenet5_87_1127977.pth.
[INFO][10:44:50]: [Client #87] Model trained.
[INFO][10:44:50]: [Client #87] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:44:50]: [Server #1127936] Received 0.24 MB of payload data from client #87 (simulated).
[INFO][10:44:50]: [Client #244] Loading a model from /data/ykang/plato/results/test/model/lenet5_244_1127979.pth.
[INFO][10:44:50]: [Client #244] Model trained.
[INFO][10:44:50]: [Client #244] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:44:50]: [Server #1127936] Received 0.24 MB of payload data from client #244 (simulated).
[INFO][10:44:50]: [Server #1127936] Selecting client #93 for training.
[INFO][10:44:50]: [Server #1127936] Sending the current model to client #93 (simulated).
[INFO][10:44:51]: [Server #1127936] Sending 0.24 MB of payload data to client #93 (simulated).
[INFO][10:44:51]: [Server #1127936] Selecting client #319 for training.
[INFO][10:44:51]: [Server #1127936] Sending the current model to client #319 (simulated).
[INFO][10:44:51]: [Server #1127936] Sending 0.24 MB of payload data to client #319 (simulated).
[INFO][10:44:51]: [Server #1127936] Selecting client #268 for training.
[INFO][10:44:51]: [Server #1127936] Sending the current model to client #268 (simulated).
[INFO][10:44:51]: [Client #93] Selected by the server.
[INFO][10:44:51]: [Client #93] Loading its data source...
[INFO][10:44:51]: [Client #93] Dataset size: 60000
[INFO][10:44:51]: [Client #93] Sampler: noniid
[INFO][10:44:51]: [Server #1127936] Sending 0.24 MB of payload data to client #268 (simulated).
[INFO][10:44:51]: [Client #268] Selected by the server.
[INFO][10:44:51]: [Client #268] Loading its data source...
[INFO][10:44:51]: [Client #268] Dataset size: 60000
[INFO][10:44:51]: [Client #268] Sampler: noniid
[INFO][10:44:51]: [Client #319] Selected by the server.
[INFO][10:44:51]: [Client #319] Loading its data source...
[INFO][10:44:51]: [Client #319] Dataset size: 60000
[INFO][10:44:51]: [Client #319] Sampler: noniid
[INFO][10:44:51]: [Client #93] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:44:51]: [Client #319] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:44:51]: [93m[1m[Client #93] Started training in communication round #96.[0m
[INFO][10:44:51]: [93m[1m[Client #319] Started training in communication round #96.[0m
[INFO][10:44:51]: [Client #268] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:44:51]: [93m[1m[Client #268] Started training in communication round #96.[0m
[INFO][10:44:53]: [Client #93] Loading the dataset.
[INFO][10:44:53]: [Client #268] Loading the dataset.
[INFO][10:44:53]: [Client #319] Loading the dataset.
[INFO][10:44:59]: [Client #268] Epoch: [1/5][0/10]	Loss: 0.000114
[INFO][10:45:00]: [Client #319] Epoch: [1/5][0/10]	Loss: 0.003340
[INFO][10:45:00]: [Client #93] Epoch: [1/5][0/10]	Loss: 0.000117
[INFO][10:45:00]: [Client #268] Epoch: [2/5][0/10]	Loss: 0.000077
[INFO][10:45:00]: [Client #319] Epoch: [2/5][0/10]	Loss: 0.000477
[INFO][10:45:00]: [Client #93] Epoch: [2/5][0/10]	Loss: 0.000326
[INFO][10:45:00]: [Client #268] Epoch: [3/5][0/10]	Loss: 0.000016
[INFO][10:45:00]: [Client #319] Epoch: [3/5][0/10]	Loss: 0.000010
[INFO][10:45:00]: [Client #268] Epoch: [4/5][0/10]	Loss: 0.000002
[INFO][10:45:00]: [Client #93] Epoch: [3/5][0/10]	Loss: 0.000176
[INFO][10:45:00]: [Client #319] Epoch: [4/5][0/10]	Loss: 0.000003
[INFO][10:45:00]: [Client #268] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:45:00]: [Client #93] Epoch: [4/5][0/10]	Loss: 0.000002
[INFO][10:45:00]: [Client #319] Epoch: [5/5][0/10]	Loss: 0.000476
[INFO][10:45:00]: [Client #268] Model saved to /data/ykang/plato/results/test/model/lenet5_268_1127979.pth.
[INFO][10:45:00]: [Client #319] Model saved to /data/ykang/plato/results/test/model/lenet5_319_1127978.pth.
[INFO][10:45:00]: [Client #93] Epoch: [5/5][0/10]	Loss: 0.000086
[INFO][10:45:00]: [Client #93] Model saved to /data/ykang/plato/results/test/model/lenet5_93_1127977.pth.
[INFO][10:45:01]: [Client #268] Loading a model from /data/ykang/plato/results/test/model/lenet5_268_1127979.pth.
[INFO][10:45:01]: [Client #268] Model trained.
[INFO][10:45:01]: [Client #268] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:45:01]: [Server #1127936] Received 0.24 MB of payload data from client #268 (simulated).
[INFO][10:45:01]: [Client #319] Loading a model from /data/ykang/plato/results/test/model/lenet5_319_1127978.pth.
[INFO][10:45:01]: [Client #319] Model trained.
[INFO][10:45:01]: [Client #319] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:45:01]: [Server #1127936] Received 0.24 MB of payload data from client #319 (simulated).
[INFO][10:45:01]: [Client #93] Loading a model from /data/ykang/plato/results/test/model/lenet5_93_1127977.pth.
[INFO][10:45:01]: [Client #93] Model trained.
[INFO][10:45:01]: [Client #93] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:45:01]: [Server #1127936] Received 0.24 MB of payload data from client #93 (simulated).
[INFO][10:45:01]: [Server #1127936] Selecting client #388 for training.
[INFO][10:45:01]: [Server #1127936] Sending the current model to client #388 (simulated).
[INFO][10:45:01]: [Server #1127936] Sending 0.24 MB of payload data to client #388 (simulated).
[INFO][10:45:01]: [Server #1127936] Selecting client #197 for training.
[INFO][10:45:01]: [Server #1127936] Sending the current model to client #197 (simulated).
[INFO][10:45:01]: [Server #1127936] Sending 0.24 MB of payload data to client #197 (simulated).
[INFO][10:45:01]: [Server #1127936] Selecting client #495 for training.
[INFO][10:45:01]: [Server #1127936] Sending the current model to client #495 (simulated).
[INFO][10:45:01]: [Client #388] Selected by the server.
[INFO][10:45:01]: [Client #388] Loading its data source...
[INFO][10:45:01]: [Client #388] Dataset size: 60000
[INFO][10:45:01]: [Client #388] Sampler: noniid
[INFO][10:45:01]: [Server #1127936] Sending 0.24 MB of payload data to client #495 (simulated).
[INFO][10:45:01]: [Client #197] Selected by the server.
[INFO][10:45:01]: [Client #197] Loading its data source...
[INFO][10:45:01]: [Client #495] Selected by the server.
[INFO][10:45:01]: [Client #197] Dataset size: 60000
[INFO][10:45:01]: [Client #197] Sampler: noniid
[INFO][10:45:01]: [Client #495] Loading its data source...
[INFO][10:45:01]: [Client #495] Dataset size: 60000
[INFO][10:45:01]: [Client #495] Sampler: noniid
[INFO][10:45:01]: [Client #388] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:45:01]: [Client #197] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:45:01]: [Client #495] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:45:01]: [93m[1m[Client #197] Started training in communication round #96.[0m
[INFO][10:45:01]: [93m[1m[Client #388] Started training in communication round #96.[0m
[INFO][10:45:01]: [93m[1m[Client #495] Started training in communication round #96.[0m
[INFO][10:45:04]: [Client #495] Loading the dataset.
[INFO][10:45:04]: [Client #197] Loading the dataset.
[INFO][10:45:04]: [Client #388] Loading the dataset.
[INFO][10:45:10]: [Client #495] Epoch: [1/5][0/10]	Loss: 0.003389
[INFO][10:45:10]: [Client #197] Epoch: [1/5][0/10]	Loss: 0.000114
[INFO][10:45:10]: [Client #388] Epoch: [1/5][0/10]	Loss: 0.000033
[INFO][10:45:10]: [Client #495] Epoch: [2/5][0/10]	Loss: 0.000485
[INFO][10:45:10]: [Client #197] Epoch: [2/5][0/10]	Loss: 0.000100
[INFO][10:45:10]: [Client #388] Epoch: [2/5][0/10]	Loss: 0.001741
[INFO][10:45:10]: [Client #495] Epoch: [3/5][0/10]	Loss: 0.000040
[INFO][10:45:10]: [Client #197] Epoch: [3/5][0/10]	Loss: 0.000007
[INFO][10:45:10]: [Client #388] Epoch: [3/5][0/10]	Loss: 0.000029
[INFO][10:45:10]: [Client #495] Epoch: [4/5][0/10]	Loss: 0.000026
[INFO][10:45:10]: [Client #197] Epoch: [4/5][0/10]	Loss: 0.000047
[INFO][10:45:10]: [Client #495] Epoch: [5/5][0/10]	Loss: 0.000174
[INFO][10:45:10]: [Client #388] Epoch: [4/5][0/10]	Loss: 0.000064
[INFO][10:45:10]: [Client #197] Epoch: [5/5][0/10]	Loss: 0.000073
[INFO][10:45:10]: [Client #495] Model saved to /data/ykang/plato/results/test/model/lenet5_495_1127979.pth.
[INFO][10:45:10]: [Client #388] Epoch: [5/5][0/10]	Loss: 0.000701
[INFO][10:45:10]: [Client #197] Model saved to /data/ykang/plato/results/test/model/lenet5_197_1127978.pth.
[INFO][10:45:10]: [Client #388] Model saved to /data/ykang/plato/results/test/model/lenet5_388_1127977.pth.
[INFO][10:45:11]: [Client #495] Loading a model from /data/ykang/plato/results/test/model/lenet5_495_1127979.pth.
[INFO][10:45:11]: [Client #495] Model trained.
[INFO][10:45:11]: [Client #495] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:45:11]: [Server #1127936] Received 0.24 MB of payload data from client #495 (simulated).
[INFO][10:45:11]: [Client #197] Loading a model from /data/ykang/plato/results/test/model/lenet5_197_1127978.pth.
[INFO][10:45:11]: [Client #197] Model trained.
[INFO][10:45:11]: [Client #197] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:45:11]: [Server #1127936] Received 0.24 MB of payload data from client #197 (simulated).
[INFO][10:45:11]: [Client #388] Loading a model from /data/ykang/plato/results/test/model/lenet5_388_1127977.pth.
[INFO][10:45:11]: [Client #388] Model trained.
[INFO][10:45:11]: [Client #388] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:45:11]: [Server #1127936] Received 0.24 MB of payload data from client #388 (simulated).
[INFO][10:45:11]: [Server #1127936] Selecting client #258 for training.
[INFO][10:45:11]: [Server #1127936] Sending the current model to client #258 (simulated).
[INFO][10:45:11]: [Server #1127936] Sending 0.24 MB of payload data to client #258 (simulated).
[INFO][10:45:11]: [Client #258] Selected by the server.
[INFO][10:45:11]: [Client #258] Loading its data source...
[INFO][10:45:11]: [Client #258] Dataset size: 60000
[INFO][10:45:11]: [Client #258] Sampler: noniid
[INFO][10:45:11]: [Client #258] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:45:11]: [93m[1m[Client #258] Started training in communication round #96.[0m
[INFO][10:45:13]: [Client #258] Loading the dataset.
[INFO][10:45:19]: [Client #258] Epoch: [1/5][0/10]	Loss: 0.003884
[INFO][10:45:19]: [Client #258] Epoch: [2/5][0/10]	Loss: 0.001371
[INFO][10:45:19]: [Client #258] Epoch: [3/5][0/10]	Loss: 0.000006
[INFO][10:45:19]: [Client #258] Epoch: [4/5][0/10]	Loss: 0.000004
[INFO][10:45:19]: [Client #258] Epoch: [5/5][0/10]	Loss: 0.000254
[INFO][10:45:19]: [Client #258] Model saved to /data/ykang/plato/results/test/model/lenet5_258_1127977.pth.
[INFO][10:45:20]: [Client #258] Loading a model from /data/ykang/plato/results/test/model/lenet5_258_1127977.pth.
[INFO][10:45:20]: [Client #258] Model trained.
[INFO][10:45:20]: [Client #258] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:45:20]: [Server #1127936] Received 0.24 MB of payload data from client #258 (simulated).
[INFO][10:45:20]: [Server #1127936] Adding client #376 to the list of clients for aggregation.
[INFO][10:45:20]: [Server #1127936] Adding client #2 to the list of clients for aggregation.
[INFO][10:45:20]: [Server #1127936] Adding client #172 to the list of clients for aggregation.
[INFO][10:45:20]: [Server #1127936] Adding client #475 to the list of clients for aggregation.
[INFO][10:45:20]: [Server #1127936] Adding client #229 to the list of clients for aggregation.
[INFO][10:45:20]: [Server #1127936] Adding client #197 to the list of clients for aggregation.
[INFO][10:45:20]: [Server #1127936] Adding client #244 to the list of clients for aggregation.
[INFO][10:45:20]: [Server #1127936] Adding client #319 to the list of clients for aggregation.
[INFO][10:45:20]: [Server #1127936] Adding client #495 to the list of clients for aggregation.
[INFO][10:45:20]: [Server #1127936] Adding client #11 to the list of clients for aggregation.
[INFO][10:45:20]: [Server #1127936] Aggregating 10 clients in total.
[0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  6e-06  1e-10  1e-10
Optimal solution found.
The calculated probability is:  [0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.0020533  0.00206199
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00205943 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00205342 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068 0.00204068
 0.00204068 0.00204068 0.00204068 0.00204068]
current clients pool:  [1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.00649508 0.         0.         0.         0.
 0.         0.         0.         0.         0.00082917 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00147204 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00589638 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00084208 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.000461   0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00206078 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.0014111  0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00121352 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00155137 0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 1. 1. 0. 0. 0. 1. 0. 0. 3. 1. 0. 0. 0. 1. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 2. 0. 1. 0.
 1. 1. 0. 0. 2. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 2. 1. 1. 0. 0.
 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 2. 1. 6. 0. 0. 2. 1. 0. 1. 1. 2. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 3. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 1. 6. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 2. 0. 0.
 1. 1. 1. 2. 0. 0. 1. 2. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1.
 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 4. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1.
 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 4. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1.
 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 6. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0.
 1. 0. 1. 1. 4. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 6. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 2. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0.
 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 2. 1. 1. 1. 0. 0. 0. 1. 0. 6. 1. 1.
 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 6. 0. 0. 1. 1. 0. 1. 1. 2. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 2. 2. 1. 2. 0.]
local_gradient_bounds:  [INFO][10:45:22]: [Server #1127936] Global model accuracy: 96.64%

[INFO][10:45:22]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_96.pth.
[INFO][10:45:22]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_96.pth.
[INFO][10:45:22]: [93m[1m
[Server #1127936] Starting round 97/100.[0m
[0.         0.00649508 0.         0.         0.         0.
 0.         0.         0.         0.         0.00082917 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00147204 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00589638 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00084208 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.000461   0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00206078 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.0014111  0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00121352 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00155137 0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  1e-05  2e-10  2e-10
 6:  6.8876e+00  6.8875e+00  9e-06  1e-10  1e-10
 7:  6.8876e+00  6.8875e+00  9e-06  1e-10  1e-12
 8:  6.8875e+00  6.8875e+00  5e-06  9e-11  8e-13
Optimal solution found.
The calculated probability is:  [0.00123751 0.00506744 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.39012753 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123746 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00137725 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00149038 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00174443 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751 0.00123751
 0.00123751 0.00123751 0.00123751 0.00123751]
current clients pool:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 259, 260, 261, 262, 263, 264, 265, 266, 267, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
[INFO][10:45:22]: [Server #1127936] Selected clients: [172 295 153  22 263  12 278 145 289 368]
[INFO][10:45:22]: [Server #1127936] Selecting client #172 for training.
[INFO][10:45:22]: [Server #1127936] Sending the current model to client #172 (simulated).
[INFO][10:45:22]: [Server #1127936] Sending 0.24 MB of payload data to client #172 (simulated).
[INFO][10:45:22]: [Server #1127936] Selecting client #295 for training.
[INFO][10:45:22]: [Server #1127936] Sending the current model to client #295 (simulated).
[INFO][10:45:22]: [Server #1127936] Sending 0.24 MB of payload data to client #295 (simulated).
[INFO][10:45:22]: [Server #1127936] Selecting client #153 for training.
[INFO][10:45:22]: [Server #1127936] Sending the current model to client #153 (simulated).
[INFO][10:45:22]: [Client #172] Selected by the server.
[INFO][10:45:22]: [Client #172] Loading its data source...
[INFO][10:45:22]: [Client #172] Dataset size: 60000
[INFO][10:45:22]: [Client #172] Sampler: noniid
[INFO][10:45:22]: [Server #1127936] Sending 0.24 MB of payload data to client #153 (simulated).
[INFO][10:45:22]: [Client #295] Selected by the server.
[INFO][10:45:22]: [Client #295] Loading its data source...
[INFO][10:45:22]: [Client #295] Dataset size: 60000
[INFO][10:45:22]: [Client #295] Sampler: noniid
[INFO][10:45:22]: [Client #172] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:45:22]: [Client #153] Selected by the server.
[INFO][10:45:22]: [Client #153] Loading its data source...
[INFO][10:45:22]: [Client #153] Dataset size: 60000
[INFO][10:45:22]: [Client #153] Sampler: noniid
[INFO][10:45:22]: [Client #295] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:45:22]: [Client #153] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:45:22]: [93m[1m[Client #153] Started training in communication round #97.[0m
[INFO][10:45:23]: [93m[1m[Client #172] Started training in communication round #97.[0m
[INFO][10:45:23]: [93m[1m[Client #295] Started training in communication round #97.[0m
[INFO][10:45:25]: [Client #153] Loading the dataset.
[INFO][10:45:25]: [Client #295] Loading the dataset.
[INFO][10:45:25]: [Client #172] Loading the dataset.
[INFO][10:45:31]: [Client #295] Epoch: [1/5][0/10]	Loss: 0.005048
[INFO][10:45:31]: [Client #153] Epoch: [1/5][0/10]	Loss: 0.000024
[INFO][10:45:31]: [Client #172] Epoch: [1/5][0/10]	Loss: 0.000033
[INFO][10:45:31]: [Client #295] Epoch: [2/5][0/10]	Loss: 0.000004
[INFO][10:45:31]: [Client #172] Epoch: [2/5][0/10]	Loss: 0.000233
[INFO][10:45:31]: [Client #153] Epoch: [2/5][0/10]	Loss: 0.001203
[INFO][10:45:31]: [Client #295] Epoch: [3/5][0/10]	Loss: 0.000002
[INFO][10:45:31]: [Client #172] Epoch: [3/5][0/10]	Loss: 0.000143
[INFO][10:45:31]: [Client #153] Epoch: [3/5][0/10]	Loss: 0.000003
[INFO][10:45:31]: [Client #295] Epoch: [4/5][0/10]	Loss: 0.000008
[INFO][10:45:31]: [Client #153] Epoch: [4/5][0/10]	Loss: 0.000012
[INFO][10:45:31]: [Client #172] Epoch: [4/5][0/10]	Loss: 0.000001
[INFO][10:45:31]: [Client #295] Epoch: [5/5][0/10]	Loss: 0.000227
[INFO][10:45:31]: [Client #295] Model saved to /data/ykang/plato/results/test/model/lenet5_295_1127978.pth.
[INFO][10:45:31]: [Client #172] Epoch: [5/5][0/10]	Loss: 0.000001
[INFO][10:45:31]: [Client #153] Epoch: [5/5][0/10]	Loss: 0.000004
[INFO][10:45:31]: [Client #172] Model saved to /data/ykang/plato/results/test/model/lenet5_172_1127977.pth.
[INFO][10:45:31]: [Client #153] Model saved to /data/ykang/plato/results/test/model/lenet5_153_1127979.pth.
[INFO][10:45:32]: [Client #295] Loading a model from /data/ykang/plato/results/test/model/lenet5_295_1127978.pth.
[INFO][10:45:32]: [Client #295] Model trained.
[INFO][10:45:32]: [Client #295] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:45:32]: [Server #1127936] Received 0.24 MB of payload data from client #295 (simulated).
[INFO][10:45:32]: [Client #153] Loading a model from /data/ykang/plato/results/test/model/lenet5_153_1127979.pth.
[INFO][10:45:32]: [Client #153] Model trained.
[INFO][10:45:32]: [Client #153] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:45:32]: [Server #1127936] Received 0.24 MB of payload data from client #153 (simulated).
[INFO][10:45:32]: [Client #172] Loading a model from /data/ykang/plato/results/test/model/lenet5_172_1127977.pth.
[INFO][10:45:32]: [Client #172] Model trained.
[INFO][10:45:32]: [Client #172] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:45:32]: [Server #1127936] Received 0.24 MB of payload data from client #172 (simulated).
[INFO][10:45:32]: [Server #1127936] Selecting client #22 for training.
[INFO][10:45:32]: [Server #1127936] Sending the current model to client #22 (simulated).
[INFO][10:45:32]: [Server #1127936] Sending 0.24 MB of payload data to client #22 (simulated).
[INFO][10:45:32]: [Server #1127936] Selecting client #263 for training.
[INFO][10:45:32]: [Server #1127936] Sending the current model to client #263 (simulated).
[INFO][10:45:32]: [Server #1127936] Sending 0.24 MB of payload data to client #263 (simulated).
[INFO][10:45:32]: [Server #1127936] Selecting client #12 for training.
[INFO][10:45:32]: [Server #1127936] Sending the current model to client #12 (simulated).
[INFO][10:45:32]: [Client #22] Selected by the server.
[INFO][10:45:32]: [Client #22] Loading its data source...
[INFO][10:45:32]: [Client #22] Dataset size: 60000
[INFO][10:45:32]: [Client #22] Sampler: noniid
[INFO][10:45:32]: [Server #1127936] Sending 0.24 MB of payload data to client #12 (simulated).
[INFO][10:45:32]: [Client #263] Selected by the server.
[INFO][10:45:32]: [Client #263] Loading its data source...
[INFO][10:45:32]: [Client #12] Selected by the server.
[INFO][10:45:32]: [Client #263] Dataset size: 60000
[INFO][10:45:32]: [Client #263] Sampler: noniid
[INFO][10:45:32]: [Client #12] Loading its data source...
[INFO][10:45:32]: [Client #12] Dataset size: 60000
[INFO][10:45:32]: [Client #12] Sampler: noniid
[INFO][10:45:32]: [Client #22] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:45:32]: [Client #263] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:45:32]: [Client #12] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:45:32]: [93m[1m[Client #22] Started training in communication round #97.[0m
[INFO][10:45:32]: [93m[1m[Client #12] Started training in communication round #97.[0m
[INFO][10:45:32]: [93m[1m[Client #263] Started training in communication round #97.[0m
[INFO][10:45:34]: [Client #12] Loading the dataset.
[INFO][10:45:34]: [Client #263] Loading the dataset.
[INFO][10:45:34]: [Client #22] Loading the dataset.
[INFO][10:45:40]: [Client #12] Epoch: [1/5][0/10]	Loss: 0.003938
[INFO][10:45:40]: [Client #22] Epoch: [1/5][0/10]	Loss: 0.000576
[INFO][10:45:40]: [Client #263] Epoch: [1/5][0/10]	Loss: 0.000834
[INFO][10:45:40]: [Client #12] Epoch: [2/5][0/10]	Loss: 0.000117
[INFO][10:45:40]: [Client #22] Epoch: [2/5][0/10]	Loss: 0.004681
[INFO][10:45:40]: [Client #12] Epoch: [3/5][0/10]	Loss: 0.000222
[INFO][10:45:40]: [Client #263] Epoch: [2/5][0/10]	Loss: 0.000197
[INFO][10:45:41]: [Client #22] Epoch: [3/5][0/10]	Loss: 0.000005
[INFO][10:45:41]: [Client #263] Epoch: [3/5][0/10]	Loss: 0.000053
[INFO][10:45:41]: [Client #12] Epoch: [4/5][0/10]	Loss: 0.000019
[INFO][10:45:41]: [Client #263] Epoch: [4/5][0/10]	Loss: 0.000132
[INFO][10:45:41]: [Client #22] Epoch: [4/5][0/10]	Loss: 0.000129
[INFO][10:45:41]: [Client #12] Epoch: [5/5][0/10]	Loss: 0.003438
[INFO][10:45:41]: [Client #12] Model saved to /data/ykang/plato/results/test/model/lenet5_12_1127979.pth.
[INFO][10:45:41]: [Client #263] Epoch: [5/5][0/10]	Loss: 0.000074
[INFO][10:45:41]: [Client #22] Epoch: [5/5][0/10]	Loss: 0.000694
[INFO][10:45:41]: [Client #22] Model saved to /data/ykang/plato/results/test/model/lenet5_22_1127977.pth.
[INFO][10:45:41]: [Client #263] Model saved to /data/ykang/plato/results/test/model/lenet5_263_1127978.pth.
[INFO][10:45:42]: [Client #12] Loading a model from /data/ykang/plato/results/test/model/lenet5_12_1127979.pth.
[INFO][10:45:42]: [Client #12] Model trained.
[INFO][10:45:42]: [Client #12] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:45:42]: [Server #1127936] Received 0.24 MB of payload data from client #12 (simulated).
[INFO][10:45:42]: [Client #263] Loading a model from /data/ykang/plato/results/test/model/lenet5_263_1127978.pth.
[INFO][10:45:42]: [Client #263] Model trained.
[INFO][10:45:42]: [Client #263] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:45:42]: [Server #1127936] Received 0.24 MB of payload data from client #263 (simulated).
[INFO][10:45:42]: [Client #22] Loading a model from /data/ykang/plato/results/test/model/lenet5_22_1127977.pth.
[INFO][10:45:42]: [Client #22] Model trained.
[INFO][10:45:42]: [Client #22] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:45:42]: [Server #1127936] Received 0.24 MB of payload data from client #22 (simulated).
[INFO][10:45:42]: [Server #1127936] Selecting client #278 for training.
[INFO][10:45:42]: [Server #1127936] Sending the current model to client #278 (simulated).
[INFO][10:45:42]: [Server #1127936] Sending 0.24 MB of payload data to client #278 (simulated).
[INFO][10:45:42]: [Server #1127936] Selecting client #145 for training.
[INFO][10:45:42]: [Server #1127936] Sending the current model to client #145 (simulated).
[INFO][10:45:42]: [Server #1127936] Sending 0.24 MB of payload data to client #145 (simulated).
[INFO][10:45:42]: [Server #1127936] Selecting client #289 for training.
[INFO][10:45:42]: [Server #1127936] Sending the current model to client #289 (simulated).
[INFO][10:45:42]: [Client #278] Selected by the server.
[INFO][10:45:42]: [Client #278] Loading its data source...
[INFO][10:45:42]: [Client #278] Dataset size: 60000
[INFO][10:45:42]: [Client #278] Sampler: noniid
[INFO][10:45:42]: [Server #1127936] Sending 0.24 MB of payload data to client #289 (simulated).
[INFO][10:45:42]: [Client #145] Selected by the server.
[INFO][10:45:42]: [Client #145] Loading its data source...
[INFO][10:45:42]: [Client #289] Selected by the server.
[INFO][10:45:42]: [Client #145] Dataset size: 60000
[INFO][10:45:42]: [Client #289] Loading its data source...
[INFO][10:45:42]: [Client #145] Sampler: noniid
[INFO][10:45:42]: [Client #289] Dataset size: 60000
[INFO][10:45:42]: [Client #289] Sampler: noniid
[INFO][10:45:42]: [Client #278] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:45:42]: [Client #145] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:45:42]: [Client #289] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:45:42]: [93m[1m[Client #278] Started training in communication round #97.[0m
[INFO][10:45:42]: [93m[1m[Client #289] Started training in communication round #97.[0m
[INFO][10:45:42]: [93m[1m[Client #145] Started training in communication round #97.[0m
[INFO][10:45:44]: [Client #278] Loading the dataset.
[INFO][10:45:44]: [Client #145] Loading the dataset.
[INFO][10:45:44]: [Client #289] Loading the dataset.
[INFO][10:45:50]: [Client #278] Epoch: [1/5][0/10]	Loss: 0.000219
[INFO][10:45:50]: [Client #145] Epoch: [1/5][0/10]	Loss: 0.000033
[INFO][10:45:50]: [Client #289] Epoch: [1/5][0/10]	Loss: 0.002020
[INFO][10:45:50]: [Client #278] Epoch: [2/5][0/10]	Loss: 0.001955
[INFO][10:45:50]: [Client #145] Epoch: [2/5][0/10]	Loss: 0.000250
[INFO][10:45:50]: [Client #278] Epoch: [3/5][0/10]	Loss: 0.000008
[INFO][10:45:50]: [Client #289] Epoch: [2/5][0/10]	Loss: 0.005161
[INFO][10:45:50]: [Client #145] Epoch: [3/5][0/10]	Loss: 0.000486
[INFO][10:45:50]: [Client #278] Epoch: [4/5][0/10]	Loss: 0.000002
[INFO][10:45:50]: [Client #289] Epoch: [3/5][0/10]	Loss: 0.000012
[INFO][10:45:50]: [Client #145] Epoch: [4/5][0/10]	Loss: 0.000172
[INFO][10:45:50]: [Client #278] Epoch: [5/5][0/10]	Loss: 0.000074
[INFO][10:45:50]: [Client #278] Model saved to /data/ykang/plato/results/test/model/lenet5_278_1127977.pth.
[INFO][10:45:51]: [Client #145] Epoch: [5/5][0/10]	Loss: 0.000020
[INFO][10:45:51]: [Client #289] Epoch: [4/5][0/10]	Loss: 0.000165
[INFO][10:45:51]: [Client #145] Model saved to /data/ykang/plato/results/test/model/lenet5_145_1127978.pth.
[INFO][10:45:51]: [Client #289] Epoch: [5/5][0/10]	Loss: 0.001207
[INFO][10:45:51]: [Client #289] Model saved to /data/ykang/plato/results/test/model/lenet5_289_1127979.pth.
[INFO][10:45:51]: [Client #278] Loading a model from /data/ykang/plato/results/test/model/lenet5_278_1127977.pth.
[INFO][10:45:51]: [Client #278] Model trained.
[INFO][10:45:51]: [Client #278] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:45:51]: [Server #1127936] Received 0.24 MB of payload data from client #278 (simulated).
[INFO][10:45:51]: [Client #145] Loading a model from /data/ykang/plato/results/test/model/lenet5_145_1127978.pth.
[INFO][10:45:51]: [Client #145] Model trained.
[INFO][10:45:51]: [Client #145] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:45:51]: [Server #1127936] Received 0.24 MB of payload data from client #145 (simulated).
[INFO][10:45:51]: [Client #289] Loading a model from /data/ykang/plato/results/test/model/lenet5_289_1127979.pth.
[INFO][10:45:51]: [Client #289] Model trained.
[INFO][10:45:51]: [Client #289] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:45:51]: [Server #1127936] Received 0.24 MB of payload data from client #289 (simulated).
[INFO][10:45:51]: [Server #1127936] Selecting client #368 for training.
[INFO][10:45:51]: [Server #1127936] Sending the current model to client #368 (simulated).
[INFO][10:45:51]: [Server #1127936] Sending 0.24 MB of payload data to client #368 (simulated).
[INFO][10:45:51]: [Client #368] Selected by the server.
[INFO][10:45:51]: [Client #368] Loading its data source...
[INFO][10:45:51]: [Client #368] Dataset size: 60000
[INFO][10:45:51]: [Client #368] Sampler: noniid
[INFO][10:45:51]: [Client #368] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:45:51]: [93m[1m[Client #368] Started training in communication round #97.[0m
[INFO][10:45:53]: [Client #368] Loading the dataset.
[INFO][10:45:59]: [Client #368] Epoch: [1/5][0/10]	Loss: 0.000187
[INFO][10:45:59]: [Client #368] Epoch: [2/5][0/10]	Loss: 0.000580
[INFO][10:45:59]: [Client #368] Epoch: [3/5][0/10]	Loss: 0.000151
[INFO][10:45:59]: [Client #368] Epoch: [4/5][0/10]	Loss: 0.000008
[INFO][10:45:59]: [Client #368] Epoch: [5/5][0/10]	Loss: 0.000018
[INFO][10:45:59]: [Client #368] Model saved to /data/ykang/plato/results/test/model/lenet5_368_1127977.pth.
[INFO][10:46:00]: [Client #368] Loading a model from /data/ykang/plato/results/test/model/lenet5_368_1127977.pth.
[INFO][10:46:00]: [Client #368] Model trained.
[INFO][10:46:00]: [Client #368] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:46:00]: [Server #1127936] Received 0.24 MB of payload data from client #368 (simulated).
[INFO][10:46:00]: [Server #1127936] Adding client #87 to the list of clients for aggregation.
[INFO][10:46:00]: [Server #1127936] Adding client #93 to the list of clients for aggregation.
[INFO][10:46:00]: [Server #1127936] Adding client #268 to the list of clients for aggregation.
[INFO][10:46:00]: [Server #1127936] Adding client #388 to the list of clients for aggregation.
[INFO][10:46:00]: [Server #1127936] Adding client #258 to the list of clients for aggregation.
[INFO][10:46:00]: [Server #1127936] Adding client #196 to the list of clients for aggregation.
[INFO][10:46:00]: [Server #1127936] Adding client #145 to the list of clients for aggregation.
[INFO][10:46:00]: [Server #1127936] Adding client #295 to the list of clients for aggregation.
[INFO][10:46:00]: [Server #1127936] Adding client #278 to the list of clients for aggregation.
[INFO][10:46:00]: [Server #1127936] Adding client #368 to the list of clients for aggregation.
[INFO][10:46:00]: [Server #1127936] Aggregating 10 clients in total.
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00672107 0.         0.         0.
 0.         0.         0.00114984 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.0032632  0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00301111 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00137736
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.0022942  0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00150654 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00372435 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00045233 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00188388 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 1. 1. 0. 0. 0. 1. 0. 0. 3. 1. 0. 0. 0. 1. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 2. 0. 1. 0.
 1. 1. 0. 0. 2. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 3. 1. 1. 0. 2. 1. 1. 0. 0.
 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 2. 1. 6. 0. 0. 2. 1. 0. 1. 1. 2. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 3. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 1. 6. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 2. 0. 0.
 1. 1. 1. 2. 0. 0. 1. 2. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1.
 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 4. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1.
 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 4. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1.
 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 6. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0.
 1. 0. 1. 1. 4. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 6. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 2. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0.
 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 2. 1. 1. 1. 0. 0. 0. 1. 0. 6. 1. 1.
 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 6. 0. 0. 1. 1. 0. 1. 1. 2. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 2. 2. 1. 2. 0.]
local_gradient_bounds:  [INFO][10:46:02]: [Server #1127936] Global model accuracy: 96.20%

[INFO][10:46:02]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_97.pth.
[INFO][10:46:02]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_97.pth.
[INFO][10:46:02]: [93m[1m
[Server #1127936] Starting round 98/100.[0m
[0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00672107 0.         0.         0.
 0.         0.         0.00114984 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.0032632  0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00301111 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00137736
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.0022942  0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00150654 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00372435 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00045233 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00188388 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
[INFO][10:46:03]: [Server #1127936] Selected clients: [220 333 120 500 122 471 109 249 411 359]
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8872e+00  3e-04  6e-09  6e-09
 5:  6.8876e+00  6.8875e+00  1e-05  2e-10  2e-10
 6:  6.8876e+00  6.8875e+00  7e-06  9e-11  9e-11
Optimal solution found.
The calculated probability is:  [0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00413715
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00216301
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203167 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00357537 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00219312 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00233239 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203167
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00226617 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168 0.00203168
 0.00203168 0.00203168 0.00203168 0.00203168]
current clients pool:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 290, 291, 292, 293, 294, 295, 296, 297, 298, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  [INFO][10:46:03]: [Server #1127936] Selecting client #220 for training.
[INFO][10:46:03]: [Server #1127936] Sending the current model to client #220 (simulated).
[INFO][10:46:03]: [Server #1127936] Sending 0.24 MB of payload data to client #220 (simulated).
[INFO][10:46:03]: [Server #1127936] Selecting client #333 for training.
[INFO][10:46:03]: [Server #1127936] Sending the current model to client #333 (simulated).
[INFO][10:46:03]: [Server #1127936] Sending 0.24 MB of payload data to client #333 (simulated).
[INFO][10:46:03]: [Server #1127936] Selecting client #120 for training.
[INFO][10:46:03]: [Server #1127936] Sending the current model to client #120 (simulated).
[INFO][10:46:03]: [Client #220] Selected by the server.
[INFO][10:46:03]: [Client #220] Loading its data source...
[INFO][10:46:03]: [Client #220] Dataset size: 60000
[INFO][10:46:03]: [Client #220] Sampler: noniid
[INFO][10:46:03]: [Server #1127936] Sending 0.24 MB of payload data to client #120 (simulated).
[INFO][10:46:03]: [Client #333] Selected by the server.
[INFO][10:46:03]: [Client #333] Loading its data source...
[INFO][10:46:03]: [Client #333] Dataset size: 60000
[INFO][10:46:03]: [Client #333] Sampler: noniid
[INFO][10:46:03]: [Client #220] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:46:03]: [Client #120] Selected by the server.
[INFO][10:46:03]: [Client #120] Loading its data source...
[INFO][10:46:03]: [Client #120] Dataset size: 60000
[INFO][10:46:03]: [Client #120] Sampler: noniid
[INFO][10:46:03]: [Client #120] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:46:03]: [Client #333] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:46:03]: [93m[1m[Client #220] Started training in communication round #98.[0m
[INFO][10:46:03]: [93m[1m[Client #333] Started training in communication round #98.[0m
[INFO][10:46:03]: [93m[1m[Client #120] Started training in communication round #98.[0m
[INFO][10:46:05]: [Client #220] Loading the dataset.
[INFO][10:46:05]: [Client #120] Loading the dataset.
[INFO][10:46:05]: [Client #333] Loading the dataset.
[INFO][10:46:11]: [Client #220] Epoch: [1/5][0/10]	Loss: 0.000011
[INFO][10:46:11]: [Client #120] Epoch: [1/5][0/10]	Loss: 0.000844
[INFO][10:46:11]: [Client #120] Epoch: [2/5][0/10]	Loss: 0.000558
[INFO][10:46:11]: [Client #220] Epoch: [2/5][0/10]	Loss: 0.000072
[INFO][10:46:11]: [Client #333] Epoch: [1/5][0/10]	Loss: 0.004388
[INFO][10:46:11]: [Client #333] Epoch: [2/5][0/10]	Loss: 0.000332
[INFO][10:46:11]: [Client #120] Epoch: [3/5][0/10]	Loss: 0.000000
[INFO][10:46:11]: [Client #220] Epoch: [3/5][0/10]	Loss: 0.001177
[INFO][10:46:11]: [Client #333] Epoch: [3/5][0/10]	Loss: 0.000002
[INFO][10:46:11]: [Client #120] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:46:11]: [Client #220] Epoch: [4/5][0/10]	Loss: 0.000024
[INFO][10:46:11]: [Client #333] Epoch: [4/5][0/10]	Loss: 0.000049
[INFO][10:46:11]: [Client #120] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:46:11]: [Client #333] Epoch: [5/5][0/10]	Loss: 0.005256
[INFO][10:46:11]: [Client #220] Epoch: [5/5][0/10]	Loss: 0.064146
[INFO][10:46:11]: [Client #120] Model saved to /data/ykang/plato/results/test/model/lenet5_120_1127979.pth.
[INFO][10:46:11]: [Client #333] Model saved to /data/ykang/plato/results/test/model/lenet5_333_1127978.pth.
[INFO][10:46:11]: [Client #220] Model saved to /data/ykang/plato/results/test/model/lenet5_220_1127977.pth.
[INFO][10:46:12]: [Client #120] Loading a model from /data/ykang/plato/results/test/model/lenet5_120_1127979.pth.
[INFO][10:46:12]: [Client #120] Model trained.
[INFO][10:46:12]: [Client #120] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:46:12]: [Server #1127936] Received 0.24 MB of payload data from client #120 (simulated).
[INFO][10:46:12]: [Client #333] Loading a model from /data/ykang/plato/results/test/model/lenet5_333_1127978.pth.
[INFO][10:46:12]: [Client #333] Model trained.
[INFO][10:46:12]: [Client #333] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:46:12]: [Server #1127936] Received 0.24 MB of payload data from client #333 (simulated).
[INFO][10:46:12]: [Client #220] Loading a model from /data/ykang/plato/results/test/model/lenet5_220_1127977.pth.
[INFO][10:46:12]: [Client #220] Model trained.
[INFO][10:46:12]: [Client #220] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:46:12]: [Server #1127936] Received 0.24 MB of payload data from client #220 (simulated).
[INFO][10:46:12]: [Server #1127936] Selecting client #500 for training.
[INFO][10:46:12]: [Server #1127936] Sending the current model to client #500 (simulated).
[INFO][10:46:12]: [Server #1127936] Sending 0.24 MB of payload data to client #500 (simulated).
[INFO][10:46:12]: [Server #1127936] Selecting client #122 for training.
[INFO][10:46:12]: [Server #1127936] Sending the current model to client #122 (simulated).
[INFO][10:46:12]: [Server #1127936] Sending 0.24 MB of payload data to client #122 (simulated).
[INFO][10:46:12]: [Server #1127936] Selecting client #471 for training.
[INFO][10:46:12]: [Server #1127936] Sending the current model to client #471 (simulated).
[INFO][10:46:12]: [Client #500] Selected by the server.
[INFO][10:46:12]: [Client #500] Loading its data source...
[INFO][10:46:12]: [Client #500] Dataset size: 60000
[INFO][10:46:12]: [Client #500] Sampler: noniid
[INFO][10:46:12]: [Server #1127936] Sending 0.24 MB of payload data to client #471 (simulated).
[INFO][10:46:12]: [Client #122] Selected by the server.
[INFO][10:46:12]: [Client #122] Loading its data source...
[INFO][10:46:12]: [Client #122] Dataset size: 60000
[INFO][10:46:12]: [Client #122] Sampler: noniid
[INFO][10:46:12]: [Client #471] Selected by the server.
[INFO][10:46:12]: [Client #471] Loading its data source...
[INFO][10:46:12]: [Client #471] Dataset size: 60000
[INFO][10:46:12]: [Client #471] Sampler: noniid
[INFO][10:46:12]: [Client #500] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:46:12]: [Client #122] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:46:12]: [Client #471] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:46:12]: [93m[1m[Client #471] Started training in communication round #98.[0m
[INFO][10:46:12]: [93m[1m[Client #500] Started training in communication round #98.[0m
[INFO][10:46:12]: [93m[1m[Client #122] Started training in communication round #98.[0m
[INFO][10:46:14]: [Client #122] Loading the dataset.
[INFO][10:46:14]: [Client #500] Loading the dataset.
[INFO][10:46:14]: [Client #471] Loading the dataset.
[INFO][10:46:20]: [Client #122] Epoch: [1/5][0/10]	Loss: 0.001524
[INFO][10:46:20]: [Client #500] Epoch: [1/5][0/10]	Loss: 0.003753
[INFO][10:46:20]: [Client #122] Epoch: [2/5][0/10]	Loss: 0.000682
[INFO][10:46:20]: [Client #471] Epoch: [1/5][0/10]	Loss: 0.000035
[INFO][10:46:21]: [Client #122] Epoch: [3/5][0/10]	Loss: 0.000031
[INFO][10:46:21]: [Client #500] Epoch: [2/5][0/10]	Loss: 0.000331
[INFO][10:46:21]: [Client #471] Epoch: [2/5][0/10]	Loss: 0.000470
[INFO][10:46:21]: [Client #122] Epoch: [4/5][0/10]	Loss: 0.000620
[INFO][10:46:21]: [Client #471] Epoch: [3/5][0/10]	Loss: 0.000092
[INFO][10:46:21]: [Client #500] Epoch: [3/5][0/10]	Loss: 0.000035
[INFO][10:46:21]: [Client #122] Epoch: [5/5][0/10]	Loss: 0.000166
[INFO][10:46:21]: [Client #471] Epoch: [4/5][0/10]	Loss: 0.000015
[INFO][10:46:21]: [Client #500] Epoch: [4/5][0/10]	Loss: 0.000170
[INFO][10:46:21]: [Client #122] Model saved to /data/ykang/plato/results/test/model/lenet5_122_1127978.pth.
[INFO][10:46:21]: [Client #471] Epoch: [5/5][0/10]	Loss: 0.000221
[INFO][10:46:21]: [Client #500] Epoch: [5/5][0/10]	Loss: 0.000097
[INFO][10:46:21]: [Client #471] Model saved to /data/ykang/plato/results/test/model/lenet5_471_1127979.pth.
[INFO][10:46:21]: [Client #500] Model saved to /data/ykang/plato/results/test/model/lenet5_500_1127977.pth.
[INFO][10:46:22]: [Client #122] Loading a model from /data/ykang/plato/results/test/model/lenet5_122_1127978.pth.
[INFO][10:46:22]: [Client #122] Model trained.
[INFO][10:46:22]: [Client #122] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:46:22]: [Server #1127936] Received 0.24 MB of payload data from client #122 (simulated).
[INFO][10:46:22]: [Client #471] Loading a model from /data/ykang/plato/results/test/model/lenet5_471_1127979.pth.
[INFO][10:46:22]: [Client #471] Model trained.
[INFO][10:46:22]: [Client #471] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:46:22]: [Server #1127936] Received 0.24 MB of payload data from client #471 (simulated).
[INFO][10:46:22]: [Client #500] Loading a model from /data/ykang/plato/results/test/model/lenet5_500_1127977.pth.
[INFO][10:46:22]: [Client #500] Model trained.
[INFO][10:46:22]: [Client #500] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:46:22]: [Server #1127936] Received 0.24 MB of payload data from client #500 (simulated).
[INFO][10:46:22]: [Server #1127936] Selecting client #109 for training.
[INFO][10:46:22]: [Server #1127936] Sending the current model to client #109 (simulated).
[INFO][10:46:22]: [Server #1127936] Sending 0.24 MB of payload data to client #109 (simulated).
[INFO][10:46:22]: [Server #1127936] Selecting client #249 for training.
[INFO][10:46:22]: [Server #1127936] Sending the current model to client #249 (simulated).
[INFO][10:46:22]: [Server #1127936] Sending 0.24 MB of payload data to client #249 (simulated).
[INFO][10:46:22]: [Server #1127936] Selecting client #411 for training.
[INFO][10:46:22]: [Server #1127936] Sending the current model to client #411 (simulated).
[INFO][10:46:22]: [Client #109] Selected by the server.
[INFO][10:46:22]: [Client #109] Loading its data source...
[INFO][10:46:22]: [Client #109] Dataset size: 60000
[INFO][10:46:22]: [Client #109] Sampler: noniid
[INFO][10:46:22]: [Server #1127936] Sending 0.24 MB of payload data to client #411 (simulated).
[INFO][10:46:22]: [Client #249] Selected by the server.
[INFO][10:46:22]: [Client #249] Loading its data source...
[INFO][10:46:22]: [Client #249] Dataset size: 60000
[INFO][10:46:22]: [Client #411] Selected by the server.
[INFO][10:46:22]: [Client #249] Sampler: noniid
[INFO][10:46:22]: [Client #411] Loading its data source...
[INFO][10:46:22]: [Client #411] Dataset size: 60000
[INFO][10:46:22]: [Client #411] Sampler: noniid
[INFO][10:46:22]: [Client #109] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:46:22]: [Client #249] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:46:22]: [Client #411] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:46:22]: [93m[1m[Client #109] Started training in communication round #98.[0m
[INFO][10:46:22]: [93m[1m[Client #249] Started training in communication round #98.[0m
[INFO][10:46:22]: [93m[1m[Client #411] Started training in communication round #98.[0m
[INFO][10:46:24]: [Client #249] Loading the dataset.
[INFO][10:46:24]: [Client #109] Loading the dataset.
[INFO][10:46:24]: [Client #411] Loading the dataset.
[INFO][10:46:30]: [Client #249] Epoch: [1/5][0/10]	Loss: 0.000041
[INFO][10:46:30]: [Client #109] Epoch: [1/5][0/10]	Loss: 0.003934
[INFO][10:46:30]: [Client #249] Epoch: [2/5][0/10]	Loss: 0.000406
[INFO][10:46:30]: [Client #411] Epoch: [1/5][0/10]	Loss: 0.000022
[INFO][10:46:30]: [Client #109] Epoch: [2/5][0/10]	Loss: 0.000003
[INFO][10:46:30]: [Client #249] Epoch: [3/5][0/10]	Loss: 0.000042
[INFO][10:46:30]: [Client #411] Epoch: [2/5][0/10]	Loss: 0.000264
[INFO][10:46:30]: [Client #109] Epoch: [3/5][0/10]	Loss: 0.000000
[INFO][10:46:30]: [Client #249] Epoch: [4/5][0/10]	Loss: 0.000015
[INFO][10:46:30]: [Client #109] Epoch: [4/5][0/10]	Loss: 0.000001
[INFO][10:46:31]: [Client #411] Epoch: [3/5][0/10]	Loss: 0.000026
[INFO][10:46:31]: [Client #249] Epoch: [5/5][0/10]	Loss: 0.000016
[INFO][10:46:31]: [Client #109] Epoch: [5/5][0/10]	Loss: 0.000025
[INFO][10:46:31]: [Client #249] Model saved to /data/ykang/plato/results/test/model/lenet5_249_1127978.pth.
[INFO][10:46:31]: [Client #109] Model saved to /data/ykang/plato/results/test/model/lenet5_109_1127977.pth.
[INFO][10:46:31]: [Client #411] Epoch: [4/5][0/10]	Loss: 0.000040
[INFO][10:46:31]: [Client #411] Epoch: [5/5][0/10]	Loss: 0.000032
[INFO][10:46:31]: [Client #411] Model saved to /data/ykang/plato/results/test/model/lenet5_411_1127979.pth.
[INFO][10:46:31]: [Client #249] Loading a model from /data/ykang/plato/results/test/model/lenet5_249_1127978.pth.
[INFO][10:46:31]: [Client #249] Model trained.
[INFO][10:46:31]: [Client #249] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:46:31]: [Server #1127936] Received 0.24 MB of payload data from client #249 (simulated).
[INFO][10:46:31]: [Client #109] Loading a model from /data/ykang/plato/results/test/model/lenet5_109_1127977.pth.
[INFO][10:46:31]: [Client #109] Model trained.
[INFO][10:46:31]: [Client #109] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:46:31]: [Server #1127936] Received 0.24 MB of payload data from client #109 (simulated).
[INFO][10:46:32]: [Client #411] Loading a model from /data/ykang/plato/results/test/model/lenet5_411_1127979.pth.
[INFO][10:46:32]: [Client #411] Model trained.
[INFO][10:46:32]: [Client #411] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:46:32]: [Server #1127936] Received 0.24 MB of payload data from client #411 (simulated).
[INFO][10:46:32]: [Server #1127936] Selecting client #359 for training.
[INFO][10:46:32]: [Server #1127936] Sending the current model to client #359 (simulated).
[INFO][10:46:32]: [Server #1127936] Sending 0.24 MB of payload data to client #359 (simulated).
[INFO][10:46:32]: [Client #359] Selected by the server.
[INFO][10:46:32]: [Client #359] Loading its data source...
[INFO][10:46:32]: [Client #359] Dataset size: 60000
[INFO][10:46:32]: [Client #359] Sampler: noniid
[INFO][10:46:32]: [Client #359] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:46:32]: [93m[1m[Client #359] Started training in communication round #98.[0m
[INFO][10:46:34]: [Client #359] Loading the dataset.
[INFO][10:46:39]: [Client #359] Epoch: [1/5][0/10]	Loss: 0.000479
[INFO][10:46:39]: [Client #359] Epoch: [2/5][0/10]	Loss: 0.000218
[INFO][10:46:39]: [Client #359] Epoch: [3/5][0/10]	Loss: 0.000003
[INFO][10:46:39]: [Client #359] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:46:39]: [Client #359] Epoch: [5/5][0/10]	Loss: 0.000003
[INFO][10:46:39]: [Client #359] Model saved to /data/ykang/plato/results/test/model/lenet5_359_1127977.pth.
[INFO][10:46:40]: [Client #359] Loading a model from /data/ykang/plato/results/test/model/lenet5_359_1127977.pth.
[INFO][10:46:40]: [Client #359] Model trained.
[INFO][10:46:40]: [Client #359] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:46:40]: [Server #1127936] Received 0.24 MB of payload data from client #359 (simulated).
[INFO][10:46:40]: [Server #1127936] Adding client #153 to the list of clients for aggregation.
[INFO][10:46:40]: [Server #1127936] Adding client #289 to the list of clients for aggregation.
[INFO][10:46:40]: [Server #1127936] Adding client #12 to the list of clients for aggregation.
[INFO][10:46:40]: [Server #1127936] Adding client #263 to the list of clients for aggregation.
[INFO][10:46:40]: [Server #1127936] Adding client #367 to the list of clients for aggregation.
[INFO][10:46:40]: [Server #1127936] Adding client #41 to the list of clients for aggregation.
[INFO][10:46:40]: [Server #1127936] Adding client #220 to the list of clients for aggregation.
[INFO][10:46:40]: [Server #1127936] Adding client #122 to the list of clients for aggregation.
[INFO][10:46:40]: [Server #1127936] Adding client #411 to the list of clients for aggregation.
[INFO][10:46:40]: [Server #1127936] Adding client #120 to the list of clients for aggregation.
[INFO][10:46:40]: [Server #1127936] Aggregating 10 clients in total.
<class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.0006072
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01690814 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.02595164
 0.         0.00313581 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.0004163  0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00613927 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00354864 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01004255 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00225043 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00026656 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 1. 1. 0. 0. 0. 1. 0. 0. 3. 1. 0. 0. 0. 1. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 2. 0. 1. 0.
 1. 1. 0. 0. 2. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 3. 1. 1. 0. 2. 1. 1. 0. 0.
 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 2. 1. 6. 0. 0. 2. 1. 0. 1. 1. 2. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 3. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 1. 6. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 2. 0. 0.
 1. 1. 1. 2. 0. 0. 1. 2. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1.
 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 4. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1.
 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 4. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1.
 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 6. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0.
 1. 0. 1. 1. 4. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 6. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 2. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0.
 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 2. 1. 1. 1. 0. 0. 0. 1. 0. 6. 1. 1.
 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 6. 0. 0. 1. 1. 0. 1. 1. 2. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 2. 2. 1. 2. 0.]
local_gradient_bounds:  [INFO][10:46:42]: [Server #1127936] Global model accuracy: 96.06%

[INFO][10:46:42]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_98.pth.
[INFO][10:46:42]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_98.pth.
[INFO][10:46:42]: [93m[1m
[Server #1127936] Starting round 99/100.[0m
[0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.0006072
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01690814 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.02595164
 0.         0.00313581 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.0004163  0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00613927 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00354864 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01004255 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00225043 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00026656 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.002 0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8871e+00  4e-04  7e-09  7e-09
 5:  6.8876e+00  6.8874e+00  1e-04  2e-09  2e-09
 6:  6.8875e+00  6.8874e+00  1e-04  9e-10  9e-11
 7:  6.8875e+00  6.8874e+00  8e-05  8e-09  8e-10
 8:  6.8875e+00  6.8875e+00  6e-05  1e-08  1e-09
 9:  6.8875e+00  6.8875e+00  1e-05  4e-08  4e-09
10:  6.8875e+00  6.8875e+00  2e-06  9e-09  1e-09
Optimal solution found.
The calculated probability is:  [5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.08048591e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 9.75329405e-01
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04037220e-05 5.04080892e-05 5.04080254e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.06794681e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04078447e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.28159371e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.78511514e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 6.09329133e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080887e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05 5.04080892e-05 5.04080892e-05
 5.04080892e-05 5.04080892e-05]
current clients pool:  [INFO][10:46:43]: [Server #1127936] Selected clients: [ 41  57 366 272 303  15  74 142   3 335]
[INFO][10:46:43]: [Server #1127936] Selecting client #41 for training.
[INFO][10:46:43]: [Server #1127936] Sending the current model to client #41 (simulated).
[INFO][10:46:43]: [Server #1127936] Sending 0.24 MB of payload data to client #41 (simulated).
[INFO][10:46:43]: [Server #1127936] Selecting client #57 for training.
[INFO][10:46:43]: [Server #1127936] Sending the current model to client #57 (simulated).
[INFO][10:46:43]: [Server #1127936] Sending 0.24 MB of payload data to client #57 (simulated).
[INFO][10:46:43]: [Server #1127936] Selecting client #366 for training.
[INFO][10:46:43]: [Server #1127936] Sending the current model to client #366 (simulated).
[INFO][10:46:43]: [Client #41] Selected by the server.
[INFO][10:46:43]: [Client #41] Loading its data source...
[INFO][10:46:43]: [Client #41] Dataset size: 60000
[INFO][10:46:43]: [Client #41] Sampler: noniid
[INFO][10:46:43]: [Server #1127936] Sending 0.24 MB of payload data to client #366 (simulated).
[INFO][10:46:43]: [Client #57] Selected by the server.
[INFO][10:46:43]: [Client #57] Loading its data source...
[INFO][10:46:43]: [Client #57] Dataset size: 60000
[INFO][10:46:43]: [Client #57] Sampler: noniid
[INFO][10:46:43]: [Client #366] Selected by the server.
[INFO][10:46:43]: [Client #366] Loading its data source...
[INFO][10:46:43]: [Client #366] Dataset size: 60000
[INFO][10:46:43]: [Client #366] Sampler: noniid
[INFO][10:46:43]: [Client #366] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:46:43]: [Client #57] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:46:43]: [93m[1m[Client #366] Started training in communication round #99.[0m
[INFO][10:46:43]: [Client #41] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:46:43]: [93m[1m[Client #57] Started training in communication round #99.[0m
[INFO][10:46:43]: [93m[1m[Client #41] Started training in communication round #99.[0m
[INFO][10:46:45]: [Client #41] Loading the dataset.
[INFO][10:46:45]: [Client #57] Loading the dataset.
[INFO][10:46:45]: [Client #366] Loading the dataset.
[INFO][10:46:51]: [Client #41] Epoch: [1/5][0/10]	Loss: 0.000643
[INFO][10:46:51]: [Client #366] Epoch: [1/5][0/10]	Loss: 0.006233
[INFO][10:46:51]: [Client #57] Epoch: [1/5][0/10]	Loss: 0.000779
[INFO][10:46:51]: [Client #41] Epoch: [2/5][0/10]	Loss: 0.000183
[INFO][10:46:51]: [Client #366] Epoch: [2/5][0/10]	Loss: 0.000146
[INFO][10:46:51]: [Client #57] Epoch: [2/5][0/10]	Loss: 0.000034
[INFO][10:46:51]: [Client #41] Epoch: [3/5][0/10]	Loss: 0.007142
[INFO][10:46:51]: [Client #57] Epoch: [3/5][0/10]	Loss: 0.000002
[INFO][10:46:51]: [Client #366] Epoch: [3/5][0/10]	Loss: 0.000142
[INFO][10:46:51]: [Client #57] Epoch: [4/5][0/10]	Loss: 0.013137
[INFO][10:46:52]: [Client #366] Epoch: [4/5][0/10]	Loss: 0.000011
[INFO][10:46:52]: [Client #41] Epoch: [4/5][0/10]	Loss: 0.010816
[INFO][10:46:52]: [Client #41] Epoch: [5/5][0/10]	Loss: 0.013070
[INFO][10:46:52]: [Client #57] Epoch: [5/5][0/10]	Loss: 0.000570
[INFO][10:46:52]: [Client #366] Epoch: [5/5][0/10]	Loss: 0.001837
[INFO][10:46:52]: [Client #41] Model saved to /data/ykang/plato/results/test/model/lenet5_41_1127977.pth.
[INFO][10:46:52]: [Client #57] Model saved to /data/ykang/plato/results/test/model/lenet5_57_1127978.pth.
[INFO][10:46:52]: [Client #366] Model saved to /data/ykang/plato/results/test/model/lenet5_366_1127979.pth.
[INFO][10:46:52]: [Client #41] Loading a model from /data/ykang/plato/results/test/model/lenet5_41_1127977.pth.
[INFO][10:46:52]: [Client #41] Model trained.
[INFO][10:46:52]: [Client #41] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:46:52]: [Server #1127936] Received 0.24 MB of payload data from client #41 (simulated).
[INFO][10:46:53]: [Client #57] Loading a model from /data/ykang/plato/results/test/model/lenet5_57_1127978.pth.
[INFO][10:46:53]: [Client #57] Model trained.
[INFO][10:46:53]: [Client #57] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:46:53]: [Server #1127936] Received 0.24 MB of payload data from client #57 (simulated).
[INFO][10:46:53]: [Client #366] Loading a model from /data/ykang/plato/results/test/model/lenet5_366_1127979.pth.
[INFO][10:46:53]: [Client #366] Model trained.
[INFO][10:46:53]: [Client #366] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:46:53]: [Server #1127936] Received 0.24 MB of payload data from client #366 (simulated).
[INFO][10:46:53]: [Server #1127936] Selecting client #272 for training.
[INFO][10:46:53]: [Server #1127936] Sending the current model to client #272 (simulated).
[INFO][10:46:53]: [Server #1127936] Sending 0.24 MB of payload data to client #272 (simulated).
[INFO][10:46:53]: [Server #1127936] Selecting client #303 for training.
[INFO][10:46:53]: [Server #1127936] Sending the current model to client #303 (simulated).
[INFO][10:46:53]: [Server #1127936] Sending 0.24 MB of payload data to client #303 (simulated).
[INFO][10:46:53]: [Server #1127936] Selecting client #15 for training.
[INFO][10:46:53]: [Server #1127936] Sending the current model to client #15 (simulated).
[INFO][10:46:53]: [Client #272] Selected by the server.
[INFO][10:46:53]: [Client #272] Loading its data source...
[INFO][10:46:53]: [Client #272] Dataset size: 60000
[INFO][10:46:53]: [Client #272] Sampler: noniid
[INFO][10:46:53]: [Server #1127936] Sending 0.24 MB of payload data to client #15 (simulated).
[INFO][10:46:53]: [Client #272] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:46:53]: [Client #303] Selected by the server.
[INFO][10:46:53]: [Client #303] Loading its data source...
[INFO][10:46:53]: [Client #303] Dataset size: 60000
[INFO][10:46:53]: [Client #303] Sampler: noniid
[INFO][10:46:53]: [Client #15] Selected by the server.
[INFO][10:46:53]: [Client #15] Loading its data source...
[INFO][10:46:53]: [Client #15] Dataset size: 60000
[INFO][10:46:53]: [Client #15] Sampler: noniid
[INFO][10:46:53]: [Client #303] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:46:53]: [Client #15] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:46:53]: [93m[1m[Client #272] Started training in communication round #99.[0m
[INFO][10:46:53]: [93m[1m[Client #303] Started training in communication round #99.[0m
[INFO][10:46:53]: [93m[1m[Client #15] Started training in communication round #99.[0m
[INFO][10:46:55]: [Client #303] Loading the dataset.
[INFO][10:46:55]: [Client #15] Loading the dataset.
[INFO][10:46:55]: [Client #272] Loading the dataset.
[INFO][10:47:01]: [Client #272] Epoch: [1/5][0/10]	Loss: 0.000786
[INFO][10:47:01]: [Client #303] Epoch: [1/5][0/10]	Loss: 0.000127
[INFO][10:47:01]: [Client #15] Epoch: [1/5][0/10]	Loss: 0.000434
[INFO][10:47:01]: [Client #272] Epoch: [2/5][0/10]	Loss: 0.001716
[INFO][10:47:01]: [Client #303] Epoch: [2/5][0/10]	Loss: 0.000660
[INFO][10:47:01]: [Client #15] Epoch: [2/5][0/10]	Loss: 0.000258
[INFO][10:47:01]: [Client #15] Epoch: [3/5][0/10]	Loss: 0.000387
[INFO][10:47:01]: [Client #303] Epoch: [3/5][0/10]	Loss: 0.000029
[INFO][10:47:01]: [Client #272] Epoch: [3/5][0/10]	Loss: 0.000053
[INFO][10:47:01]: [Client #15] Epoch: [4/5][0/10]	Loss: 0.000009
[INFO][10:47:01]: [Client #272] Epoch: [4/5][0/10]	Loss: 0.000192
[INFO][10:47:01]: [Client #303] Epoch: [4/5][0/10]	Loss: 0.000610
[INFO][10:47:01]: [Client #15] Epoch: [5/5][0/10]	Loss: 0.000037
[INFO][10:47:01]: [Client #272] Epoch: [5/5][0/10]	Loss: 0.000048
[INFO][10:47:01]: [Client #15] Model saved to /data/ykang/plato/results/test/model/lenet5_15_1127979.pth.
[INFO][10:47:01]: [Client #303] Epoch: [5/5][0/10]	Loss: 0.001095
[INFO][10:47:01]: [Client #272] Model saved to /data/ykang/plato/results/test/model/lenet5_272_1127977.pth.
[INFO][10:47:01]: [Client #303] Model saved to /data/ykang/plato/results/test/model/lenet5_303_1127978.pth.
[INFO][10:47:02]: [Client #15] Loading a model from /data/ykang/plato/results/test/model/lenet5_15_1127979.pth.
[INFO][10:47:02]: [Client #15] Model trained.
[INFO][10:47:02]: [Client #15] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:47:02]: [Server #1127936] Received 0.24 MB of payload data from client #15 (simulated).
[INFO][10:47:02]: [Client #272] Loading a model from /data/ykang/plato/results/test/model/lenet5_272_1127977.pth.
[INFO][10:47:02]: [Client #272] Model trained.
[INFO][10:47:02]: [Client #272] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:47:02]: [Server #1127936] Received 0.24 MB of payload data from client #272 (simulated).
[INFO][10:47:02]: [Client #303] Loading a model from /data/ykang/plato/results/test/model/lenet5_303_1127978.pth.
[INFO][10:47:02]: [Client #303] Model trained.
[INFO][10:47:02]: [Client #303] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:47:02]: [Server #1127936] Received 0.24 MB of payload data from client #303 (simulated).
[INFO][10:47:02]: [Server #1127936] Selecting client #74 for training.
[INFO][10:47:02]: [Server #1127936] Sending the current model to client #74 (simulated).
[INFO][10:47:02]: [Server #1127936] Sending 0.24 MB of payload data to client #74 (simulated).
[INFO][10:47:02]: [Server #1127936] Selecting client #142 for training.
[INFO][10:47:02]: [Server #1127936] Sending the current model to client #142 (simulated).
[INFO][10:47:02]: [Server #1127936] Sending 0.24 MB of payload data to client #142 (simulated).
[INFO][10:47:02]: [Server #1127936] Selecting client #3 for training.
[INFO][10:47:02]: [Server #1127936] Sending the current model to client #3 (simulated).
[INFO][10:47:02]: [Client #74] Selected by the server.
[INFO][10:47:02]: [Client #74] Loading its data source...
[INFO][10:47:02]: [Client #74] Dataset size: 60000
[INFO][10:47:02]: [Client #74] Sampler: noniid
[INFO][10:47:02]: [Server #1127936] Sending 0.24 MB of payload data to client #3 (simulated).
[INFO][10:47:02]: [Client #142] Selected by the server.
[INFO][10:47:02]: [Client #3] Selected by the server.
[INFO][10:47:02]: [Client #142] Loading its data source...
[INFO][10:47:02]: [Client #3] Loading its data source...
[INFO][10:47:02]: [Client #142] Dataset size: 60000
[INFO][10:47:02]: [Client #3] Dataset size: 60000
[INFO][10:47:02]: [Client #142] Sampler: noniid
[INFO][10:47:02]: [Client #3] Sampler: noniid
[INFO][10:47:02]: [Client #74] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:47:02]: [Client #142] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:47:02]: [Client #3] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:47:02]: [93m[1m[Client #3] Started training in communication round #99.[0m
[INFO][10:47:02]: [93m[1m[Client #74] Started training in communication round #99.[0m
[INFO][10:47:02]: [93m[1m[Client #142] Started training in communication round #99.[0m
[INFO][10:47:05]: [Client #3] Loading the dataset.
[INFO][10:47:05]: [Client #142] Loading the dataset.
[INFO][10:47:05]: [Client #74] Loading the dataset.
[INFO][10:47:11]: [Client #3] Epoch: [1/5][0/10]	Loss: 0.012481
[INFO][10:47:11]: [Client #74] Epoch: [1/5][0/10]	Loss: 0.000425
[INFO][10:47:11]: [Client #142] Epoch: [1/5][0/10]	Loss: 0.000809
[INFO][10:47:11]: [Client #3] Epoch: [2/5][0/10]	Loss: 0.000071
[INFO][10:47:11]: [Client #3] Epoch: [3/5][0/10]	Loss: 0.015510
[INFO][10:47:11]: [Client #142] Epoch: [2/5][0/10]	Loss: 0.000838
[INFO][10:47:11]: [Client #74] Epoch: [2/5][0/10]	Loss: 0.000416
[INFO][10:47:11]: [Client #74] Epoch: [3/5][0/10]	Loss: 0.000078
[INFO][10:47:11]: [Client #3] Epoch: [4/5][0/10]	Loss: 0.000000
[INFO][10:47:11]: [Client #142] Epoch: [3/5][0/10]	Loss: 0.000047
[INFO][10:47:11]: [Client #74] Epoch: [4/5][0/10]	Loss: 0.000031
[INFO][10:47:11]: [Client #142] Epoch: [4/5][0/10]	Loss: 0.000078
[INFO][10:47:11]: [Client #3] Epoch: [5/5][0/10]	Loss: 0.000113
[INFO][10:47:11]: [Client #74] Epoch: [5/5][0/10]	Loss: 0.000080
[INFO][10:47:11]: [Client #142] Epoch: [5/5][0/10]	Loss: 0.000099
[INFO][10:47:11]: [Client #3] Model saved to /data/ykang/plato/results/test/model/lenet5_3_1127979.pth.
[INFO][10:47:11]: [Client #74] Model saved to /data/ykang/plato/results/test/model/lenet5_74_1127977.pth.
[INFO][10:47:11]: [Client #142] Model saved to /data/ykang/plato/results/test/model/lenet5_142_1127978.pth.
[INFO][10:47:12]: [Client #3] Loading a model from /data/ykang/plato/results/test/model/lenet5_3_1127979.pth.
[INFO][10:47:12]: [Client #3] Model trained.
[INFO][10:47:12]: [Client #3] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:47:12]: [Server #1127936] Received 0.24 MB of payload data from client #3 (simulated).
[INFO][10:47:12]: [Client #74] Loading a model from /data/ykang/plato/results/test/model/lenet5_74_1127977.pth.
[INFO][10:47:12]: [Client #74] Model trained.
[INFO][10:47:12]: [Client #74] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:47:12]: [Server #1127936] Received 0.24 MB of payload data from client #74 (simulated).
[INFO][10:47:12]: [Client #142] Loading a model from /data/ykang/plato/results/test/model/lenet5_142_1127978.pth.
[INFO][10:47:12]: [Client #142] Model trained.
[INFO][10:47:12]: [Client #142] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:47:12]: [Server #1127936] Received 0.24 MB of payload data from client #142 (simulated).
[INFO][10:47:12]: [Server #1127936] Selecting client #335 for training.
[INFO][10:47:12]: [Server #1127936] Sending the current model to client #335 (simulated).
[INFO][10:47:12]: [Server #1127936] Sending 0.24 MB of payload data to client #335 (simulated).
[INFO][10:47:12]: [Client #335] Selected by the server.
[INFO][10:47:12]: [Client #335] Loading its data source...
[INFO][10:47:12]: [Client #335] Dataset size: 60000
[INFO][10:47:12]: [Client #335] Sampler: noniid
[INFO][10:47:12]: [Client #335] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:47:12]: [93m[1m[Client #335] Started training in communication round #99.[0m
[INFO][10:47:14]: [Client #335] Loading the dataset.
[INFO][10:47:20]: [Client #335] Epoch: [1/5][0/10]	Loss: 0.000633
[INFO][10:47:20]: [Client #335] Epoch: [2/5][0/10]	Loss: 0.001643
[INFO][10:47:20]: [Client #335] Epoch: [3/5][0/10]	Loss: 0.000101
[INFO][10:47:20]: [Client #335] Epoch: [4/5][0/10]	Loss: 0.000008
[INFO][10:47:20]: [Client #335] Epoch: [5/5][0/10]	Loss: 0.000012
[INFO][10:47:20]: [Client #335] Model saved to /data/ykang/plato/results/test/model/lenet5_335_1127977.pth.
[INFO][10:47:21]: [Client #335] Loading a model from /data/ykang/plato/results/test/model/lenet5_335_1127977.pth.
[INFO][10:47:21]: [Client #335] Model trained.
[INFO][10:47:21]: [Client #335] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:47:21]: [Server #1127936] Received 0.24 MB of payload data from client #335 (simulated).
[INFO][10:47:21]: [Server #1127936] Adding client #333 to the list of clients for aggregation.
[INFO][10:47:21]: [Server #1127936] Adding client #500 to the list of clients for aggregation.
[INFO][10:47:21]: [Server #1127936] Adding client #249 to the list of clients for aggregation.
[INFO][10:47:21]: [Server #1127936] Adding client #109 to the list of clients for aggregation.
[INFO][10:47:21]: [Server #1127936] Adding client #299 to the list of clients for aggregation.
[INFO][10:47:21]: [Server #1127936] Adding client #359 to the list of clients for aggregation.
[INFO][10:47:21]: [Server #1127936] Adding client #454 to the list of clients for aggregation.
[INFO][10:47:21]: [Server #1127936] Adding client #3 to the list of clients for aggregation.
[INFO][10:47:21]: [Server #1127936] Adding client #15 to the list of clients for aggregation.
[INFO][10:47:21]: [Server #1127936] Adding client #471 to the list of clients for aggregation.
[INFO][10:47:21]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.03590558 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00151786 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01024322 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.0009285  0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01692786 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00411897 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00098722 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00686825 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00083551 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.0026383 ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 1. 1. 0. 0. 0. 1. 0. 0. 3. 1. 0. 0. 0. 1. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 2. 0. 1. 0.
 1. 1. 0. 0. 2. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 3. 1. 1. 0. 2. 1. 1. 0. 0.
 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 2. 1. 6. 0. 0. 2. 1. 0. 1. 1. 2. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 3. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 1. 6. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 2. 0. 0.
 1. 1. 1. 2. 0. 0. 1. 2. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1.
 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 4. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1.
 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 4. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1.
 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 6. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0.
 1. 0. 1. 1. 4. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 6. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 2. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0.
 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 2. 1. 1. 1. 0. 0. 0. 1. 0. 6. 1. 1.
 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 6. 0. 0. 1. 1. 0. 1. 1. 2. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 2. 2. 1. 2. 1.]
local_gradient_bounds:  [0.         0.         0.03590558 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00151786 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01024322 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.0009285  0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01692786 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00411897 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00098722 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00686825 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00083551 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.0026383 ]
!!!!!!The aggregation weights of this round are:  [INFO][10:47:23]: [Server #1127936] Global model accuracy: 96.05%

[INFO][10:47:23]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_99.pth.
[INFO][10:47:23]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_99.pth.
[INFO][10:47:23]: [93m[1m
[Server #1127936] Starting round 100/100.[0m
[0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.002 0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1
 0.002 0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.002
 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.002 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.002
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.002 0.002 0.1   0.1
 0.1   0.002 0.002 0.1   0.1   0.002 0.1   0.002 0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.002 0.1   0.002 0.002 0.1   0.1   0.1   0.002 0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.002 0.1
 0.1   0.1   0.002 0.1   0.1   0.1   0.1   0.1   0.002 0.1   0.1   0.1
 0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1  ]
Calculating selection probabitliy ... 
     pcost       dcost       gap    pres   dres
 0:  0.0000e+00  6.8876e+00  5e+02  1e+00  1e+00
 1:  6.8187e+00  5.8897e+00  6e+00  1e-02  1e-02
 2:  6.8872e+00  6.0548e+00  9e-01  6e-05  6e-05
 3:  6.8875e+00  6.8559e+00  3e-02  6e-07  6e-07
 4:  6.8876e+00  6.8871e+00  4e-04  7e-09  7e-09
 5:  6.8876e+00  6.8874e+00  1e-04  2e-09  2e-09
 6:  6.8875e+00  6.8874e+00  1e-04  9e-10  9e-11
 7:  6.8875e+00  6.8874e+00  8e-05  8e-09  8e-10
 8:  6.8875e+00  6.8875e+00  6e-05  1e-08  1e-09
 9:  6.8875e+00  6.8875e+00  1e-05  4e-08  4e-09
10:  6.8875e+00  6.8875e+00  2e-06  9e-09  1e-09
Optimal solution found.
The calculated probability is:  [5.04649807e-05 5.04649807e-05 5.04566209e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649658e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.80780244e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.10742168e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 9.75254581e-01
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.32808607e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.11132269e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 1.04513150e-04
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.10125591e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.04649807e-05 5.04649807e-05 5.04649807e-05
 5.04649807e-05 5.22342454e-05]
current clients pool:  [INFO][10:47:24]: [Server #1127936] Selected clients: [299 376 174  25  81 100 249 321 443 454]
[INFO][10:47:24]: [Server #1127936] Selecting client #299 for training.
[INFO][10:47:24]: [Server #1127936] Sending the current model to client #299 (simulated).
[INFO][10:47:24]: [Server #1127936] Sending 0.24 MB of payload data to client #299 (simulated).
[INFO][10:47:24]: [Server #1127936] Selecting client #376 for training.
[INFO][10:47:24]: [Server #1127936] Sending the current model to client #376 (simulated).
[INFO][10:47:24]: [Server #1127936] Sending 0.24 MB of payload data to client #376 (simulated).
[INFO][10:47:24]: [Server #1127936] Selecting client #174 for training.
[INFO][10:47:24]: [Server #1127936] Sending the current model to client #174 (simulated).
[INFO][10:47:24]: [Client #299] Selected by the server.
[INFO][10:47:24]: [Client #299] Loading its data source...
[INFO][10:47:24]: [Client #299] Dataset size: 60000
[INFO][10:47:24]: [Client #299] Sampler: noniid
[INFO][10:47:24]: [Server #1127936] Sending 0.24 MB of payload data to client #174 (simulated).
[INFO][10:47:24]: [Client #376] Selected by the server.
[INFO][10:47:24]: [Client #376] Loading its data source...
[INFO][10:47:24]: [Client #376] Dataset size: 60000
[INFO][10:47:24]: [Client #376] Sampler: noniid
[INFO][10:47:24]: [Client #299] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:47:24]: [Client #174] Selected by the server.
[INFO][10:47:24]: [Client #174] Loading its data source...
[INFO][10:47:24]: [Client #174] Dataset size: 60000
[INFO][10:47:24]: [Client #174] Sampler: noniid
[INFO][10:47:24]: [Client #376] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:47:24]: [Client #174] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:47:24]: [93m[1m[Client #174] Started training in communication round #100.[0m
[INFO][10:47:24]: [93m[1m[Client #376] Started training in communication round #100.[0m
[INFO][10:47:24]: [93m[1m[Client #299] Started training in communication round #100.[0m
[INFO][10:47:26]: [Client #376] Loading the dataset.
[INFO][10:47:26]: [Client #174] Loading the dataset.
[INFO][10:47:26]: [Client #299] Loading the dataset.
[INFO][10:47:32]: [Client #376] Epoch: [1/5][0/10]	Loss: 0.000114
[INFO][10:47:32]: [Client #174] Epoch: [1/5][0/10]	Loss: 0.000699
[INFO][10:47:32]: [Client #376] Epoch: [2/5][0/10]	Loss: 0.000478
[INFO][10:47:32]: [Client #299] Epoch: [1/5][0/10]	Loss: 0.001702
[INFO][10:47:32]: [Client #174] Epoch: [2/5][0/10]	Loss: 0.000002
[INFO][10:47:32]: [Client #376] Epoch: [3/5][0/10]	Loss: 0.000019
[INFO][10:47:32]: [Client #174] Epoch: [3/5][0/10]	Loss: 0.000306
[INFO][10:47:32]: [Client #299] Epoch: [2/5][0/10]	Loss: 0.000621
[INFO][10:47:32]: [Client #376] Epoch: [4/5][0/10]	Loss: 0.000063
[INFO][10:47:32]: [Client #174] Epoch: [4/5][0/10]	Loss: 0.000343
[INFO][10:47:32]: [Client #299] Epoch: [3/5][0/10]	Loss: 0.000060
[INFO][10:47:32]: [Client #376] Epoch: [5/5][0/10]	Loss: 0.000012
[INFO][10:47:32]: [Client #174] Epoch: [5/5][0/10]	Loss: 0.007146
[INFO][10:47:32]: [Client #299] Epoch: [4/5][0/10]	Loss: 0.000001
[INFO][10:47:32]: [Client #376] Model saved to /data/ykang/plato/results/test/model/lenet5_376_1127978.pth.
[INFO][10:47:32]: [Client #174] Model saved to /data/ykang/plato/results/test/model/lenet5_174_1127979.pth.
[INFO][10:47:32]: [Client #299] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:47:32]: [Client #299] Model saved to /data/ykang/plato/results/test/model/lenet5_299_1127977.pth.
[INFO][10:47:33]: [Client #376] Loading a model from /data/ykang/plato/results/test/model/lenet5_376_1127978.pth.
[INFO][10:47:33]: [Client #376] Model trained.
[INFO][10:47:33]: [Client #376] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:47:33]: [Server #1127936] Received 0.24 MB of payload data from client #376 (simulated).
[INFO][10:47:33]: [Client #174] Loading a model from /data/ykang/plato/results/test/model/lenet5_174_1127979.pth.
[INFO][10:47:33]: [Client #174] Model trained.
[INFO][10:47:33]: [Client #174] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:47:33]: [Server #1127936] Received 0.24 MB of payload data from client #174 (simulated).
[INFO][10:47:33]: [Client #299] Loading a model from /data/ykang/plato/results/test/model/lenet5_299_1127977.pth.
[INFO][10:47:33]: [Client #299] Model trained.
[INFO][10:47:33]: [Client #299] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:47:33]: [Server #1127936] Received 0.24 MB of payload data from client #299 (simulated).
[INFO][10:47:33]: [Server #1127936] Selecting client #25 for training.
[INFO][10:47:33]: [Server #1127936] Sending the current model to client #25 (simulated).
[INFO][10:47:33]: [Server #1127936] Sending 0.24 MB of payload data to client #25 (simulated).
[INFO][10:47:33]: [Server #1127936] Selecting client #81 for training.
[INFO][10:47:33]: [Server #1127936] Sending the current model to client #81 (simulated).
[INFO][10:47:33]: [Server #1127936] Sending 0.24 MB of payload data to client #81 (simulated).
[INFO][10:47:33]: [Server #1127936] Selecting client #100 for training.
[INFO][10:47:33]: [Server #1127936] Sending the current model to client #100 (simulated).
[INFO][10:47:33]: [Client #25] Selected by the server.
[INFO][10:47:33]: [Client #25] Loading its data source...
[INFO][10:47:33]: [Client #25] Dataset size: 60000
[INFO][10:47:33]: [Client #25] Sampler: noniid
[INFO][10:47:33]: [Server #1127936] Sending 0.24 MB of payload data to client #100 (simulated).
[INFO][10:47:33]: [Client #81] Selected by the server.
[INFO][10:47:33]: [Client #81] Loading its data source...
[INFO][10:47:33]: [Client #100] Selected by the server.
[INFO][10:47:33]: [Client #81] Dataset size: 60000
[INFO][10:47:33]: [Client #81] Sampler: noniid
[INFO][10:47:33]: [Client #100] Loading its data source...
[INFO][10:47:33]: [Client #100] Dataset size: 60000
[INFO][10:47:33]: [Client #100] Sampler: noniid
[INFO][10:47:33]: [Client #25] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:47:33]: [Client #81] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:47:33]: [Client #100] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:47:33]: [93m[1m[Client #25] Started training in communication round #100.[0m
[INFO][10:47:33]: [93m[1m[Client #81] Started training in communication round #100.[0m
[INFO][10:47:33]: [93m[1m[Client #100] Started training in communication round #100.[0m
[INFO][10:47:35]: [Client #25] Loading the dataset.
[INFO][10:47:35]: [Client #100] Loading the dataset.
[INFO][10:47:35]: [Client #81] Loading the dataset.
[INFO][10:47:41]: [Client #100] Epoch: [1/5][0/10]	Loss: 0.000979
[INFO][10:47:41]: [Client #25] Epoch: [1/5][0/10]	Loss: 0.000808
[INFO][10:47:41]: [Client #81] Epoch: [1/5][0/10]	Loss: 0.000087
[INFO][10:47:41]: [Client #100] Epoch: [2/5][0/10]	Loss: 0.000205
[INFO][10:47:41]: [Client #81] Epoch: [2/5][0/10]	Loss: 0.000057
[INFO][10:47:41]: [Client #25] Epoch: [2/5][0/10]	Loss: 0.000820
[INFO][10:47:42]: [Client #81] Epoch: [3/5][0/10]	Loss: 0.000030
[INFO][10:47:42]: [Client #100] Epoch: [3/5][0/10]	Loss: 0.000004
[INFO][10:47:42]: [Client #25] Epoch: [3/5][0/10]	Loss: 0.000019
[INFO][10:47:42]: [Client #25] Epoch: [4/5][0/10]	Loss: 0.000005
[INFO][10:47:42]: [Client #81] Epoch: [4/5][0/10]	Loss: 0.000202
[INFO][10:47:42]: [Client #100] Epoch: [4/5][0/10]	Loss: 0.000074
[INFO][10:47:42]: [Client #25] Epoch: [5/5][0/10]	Loss: 0.000477
[INFO][10:47:42]: [Client #81] Epoch: [5/5][0/10]	Loss: 0.000030
[INFO][10:47:42]: [Client #100] Epoch: [5/5][0/10]	Loss: 0.000000
[INFO][10:47:42]: [Client #25] Model saved to /data/ykang/plato/results/test/model/lenet5_25_1127977.pth.
[INFO][10:47:42]: [Client #81] Model saved to /data/ykang/plato/results/test/model/lenet5_81_1127978.pth.
[INFO][10:47:42]: [Client #100] Model saved to /data/ykang/plato/results/test/model/lenet5_100_1127979.pth.
[INFO][10:47:43]: [Client #100] Loading a model from /data/ykang/plato/results/test/model/lenet5_100_1127979.pth.
[INFO][10:47:43]: [Client #100] Model trained.
[INFO][10:47:43]: [Client #100] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:47:43]: [Server #1127936] Received 0.24 MB of payload data from client #100 (simulated).
[INFO][10:47:43]: [Client #25] Loading a model from /data/ykang/plato/results/test/model/lenet5_25_1127977.pth.
[INFO][10:47:43]: [Client #25] Model trained.
[INFO][10:47:43]: [Client #25] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:47:43]: [Server #1127936] Received 0.24 MB of payload data from client #25 (simulated).
[INFO][10:47:43]: [Client #81] Loading a model from /data/ykang/plato/results/test/model/lenet5_81_1127978.pth.
[INFO][10:47:43]: [Client #81] Model trained.
[INFO][10:47:43]: [Client #81] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:47:43]: [Server #1127936] Received 0.24 MB of payload data from client #81 (simulated).
[INFO][10:47:43]: [Server #1127936] Selecting client #249 for training.
[INFO][10:47:43]: [Server #1127936] Sending the current model to client #249 (simulated).
[INFO][10:47:43]: [Server #1127936] Sending 0.24 MB of payload data to client #249 (simulated).
[INFO][10:47:43]: [Server #1127936] Selecting client #321 for training.
[INFO][10:47:43]: [Server #1127936] Sending the current model to client #321 (simulated).
[INFO][10:47:43]: [Server #1127936] Sending 0.24 MB of payload data to client #321 (simulated).
[INFO][10:47:43]: [Server #1127936] Selecting client #443 for training.
[INFO][10:47:43]: [Server #1127936] Sending the current model to client #443 (simulated).
[INFO][10:47:43]: [Client #249] Selected by the server.
[INFO][10:47:43]: [Client #249] Loading its data source...
[INFO][10:47:43]: [Client #249] Dataset size: 60000
[INFO][10:47:43]: [Client #249] Sampler: noniid
[INFO][10:47:43]: [Server #1127936] Sending 0.24 MB of payload data to client #443 (simulated).
[INFO][10:47:43]: [Client #321] Selected by the server.
[INFO][10:47:43]: [Client #443] Selected by the server.
[INFO][10:47:43]: [Client #321] Loading its data source...
[INFO][10:47:43]: [Client #443] Loading its data source...
[INFO][10:47:43]: [Client #443] Dataset size: 60000
[INFO][10:47:43]: [Client #321] Dataset size: 60000
[INFO][10:47:43]: [Client #443] Sampler: noniid
[INFO][10:47:43]: [Client #321] Sampler: noniid
[INFO][10:47:43]: [Client #249] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:47:43]: [Client #443] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:47:43]: [Client #321] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:47:43]: [93m[1m[Client #443] Started training in communication round #100.[0m
[INFO][10:47:43]: [93m[1m[Client #249] Started training in communication round #100.[0m
[INFO][10:47:43]: [93m[1m[Client #321] Started training in communication round #100.[0m
[INFO][10:47:45]: [Client #443] Loading the dataset.
[INFO][10:47:45]: [Client #249] Loading the dataset.
[INFO][10:47:45]: [Client #321] Loading the dataset.
[INFO][10:47:51]: [Client #249] Epoch: [1/5][0/10]	Loss: 0.001680
[INFO][10:47:51]: [Client #443] Epoch: [1/5][0/10]	Loss: 0.000090
[INFO][10:47:51]: [Client #249] Epoch: [2/5][0/10]	Loss: 0.004000
[INFO][10:47:51]: [Client #321] Epoch: [1/5][0/10]	Loss: 0.000808
[INFO][10:47:51]: [Client #249] Epoch: [3/5][0/10]	Loss: 0.000027
[INFO][10:47:51]: [Client #443] Epoch: [2/5][0/10]	Loss: 0.000911
[INFO][10:47:51]: [Client #321] Epoch: [2/5][0/10]	Loss: 0.000993
[INFO][10:47:51]: [Client #249] Epoch: [4/5][0/10]	Loss: 0.000013
[INFO][10:47:51]: [Client #443] Epoch: [3/5][0/10]	Loss: 0.000446
[INFO][10:47:51]: [Client #321] Epoch: [3/5][0/10]	Loss: 0.000009
[INFO][10:47:52]: [Client #321] Epoch: [4/5][0/10]	Loss: 0.000003
[INFO][10:47:52]: [Client #443] Epoch: [4/5][0/10]	Loss: 0.000200
[INFO][10:47:52]: [Client #249] Epoch: [5/5][0/10]	Loss: 0.000005
[INFO][10:47:52]: [Client #443] Epoch: [5/5][0/10]	Loss: 0.000159
[INFO][10:47:52]: [Client #443] Model saved to /data/ykang/plato/results/test/model/lenet5_443_1127979.pth.
[INFO][10:47:52]: [Client #249] Model saved to /data/ykang/plato/results/test/model/lenet5_249_1127977.pth.
[INFO][10:47:52]: [Client #321] Epoch: [5/5][0/10]	Loss: 0.000078
[INFO][10:47:52]: [Client #321] Model saved to /data/ykang/plato/results/test/model/lenet5_321_1127978.pth.
[INFO][10:47:52]: [Client #249] Loading a model from /data/ykang/plato/results/test/model/lenet5_249_1127977.pth.
[INFO][10:47:52]: [Client #249] Model trained.
[INFO][10:47:52]: [Client #249] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:47:52]: [Server #1127936] Received 0.24 MB of payload data from client #249 (simulated).
[INFO][10:47:53]: [Client #321] Loading a model from /data/ykang/plato/results/test/model/lenet5_321_1127978.pth.
[INFO][10:47:53]: [Client #321] Model trained.
[INFO][10:47:53]: [Client #321] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:47:53]: [Server #1127936] Received 0.24 MB of payload data from client #321 (simulated).
[INFO][10:47:53]: [Client #443] Loading a model from /data/ykang/plato/results/test/model/lenet5_443_1127979.pth.
[INFO][10:47:53]: [Client #443] Model trained.
[INFO][10:47:53]: [Client #443] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:47:53]: [Server #1127936] Received 0.24 MB of payload data from client #443 (simulated).
[INFO][10:47:53]: [Server #1127936] Selecting client #454 for training.
[INFO][10:47:53]: [Server #1127936] Sending the current model to client #454 (simulated).
[INFO][10:47:53]: [Server #1127936] Sending 0.24 MB of payload data to client #454 (simulated).
[INFO][10:47:53]: [Client #454] Selected by the server.
[INFO][10:47:53]: [Client #454] Loading its data source...
[INFO][10:47:53]: [Client #454] Dataset size: 60000
[INFO][10:47:53]: [Client #454] Sampler: noniid
[INFO][10:47:53]: [Client #454] Received 0.24 MB of payload data from the server (simulated).
[INFO][10:47:53]: [93m[1m[Client #454] Started training in communication round #100.[0m
[INFO][10:47:55]: [Client #454] Loading the dataset.
[INFO][10:48:00]: [Client #454] Epoch: [1/5][0/10]	Loss: 0.000086
[INFO][10:48:00]: [Client #454] Epoch: [2/5][0/10]	Loss: 0.000632
[INFO][10:48:00]: [Client #454] Epoch: [3/5][0/10]	Loss: 0.000014
[INFO][10:48:00]: [Client #454] Epoch: [4/5][0/10]	Loss: 0.000029
[INFO][10:48:00]: [Client #454] Epoch: [5/5][0/10]	Loss: 0.000014
[INFO][10:48:00]: [Client #454] Model saved to /data/ykang/plato/results/test/model/lenet5_454_1127977.pth.
[INFO][10:48:01]: [Client #454] Loading a model from /data/ykang/plato/results/test/model/lenet5_454_1127977.pth.
[INFO][10:48:01]: [Client #454] Model trained.
[INFO][10:48:01]: [Client #454] Sent 0.24 MB of payload data to the server (simulated).
[INFO][10:48:01]: [Server #1127936] Received 0.24 MB of payload data from client #454 (simulated).
[INFO][10:48:01]: [Server #1127936] Adding client #142 to the list of clients for aggregation.
[INFO][10:48:01]: [Server #1127936] Adding client #272 to the list of clients for aggregation.
[INFO][10:48:01]: [Server #1127936] Adding client #57 to the list of clients for aggregation.
[INFO][10:48:01]: [Server #1127936] Adding client #366 to the list of clients for aggregation.
[INFO][10:48:01]: [Server #1127936] Adding client #303 to the list of clients for aggregation.
[INFO][10:48:01]: [Server #1127936] Adding client #335 to the list of clients for aggregation.
[INFO][10:48:01]: [Server #1127936] Adding client #74 to the list of clients for aggregation.
[INFO][10:48:01]: [Server #1127936] Adding client #22 to the list of clients for aggregation.
[INFO][10:48:01]: [Server #1127936] Adding client #376 to the list of clients for aggregation.
[INFO][10:48:01]: [Server #1127936] Adding client #174 to the list of clients for aggregation.
[INFO][10:48:01]: [Server #1127936] Aggregating 10 clients in total.
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
type of selected clients:  <class 'numpy.ndarray'>
start aggregating weights
!!!!!!The squared deltas bound of this round are:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00088774 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00426001 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00203928 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00350793 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.02395636
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00089657 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00104115 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00124193 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01717407
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00070584 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
start updating records...
!!!The staleness of this round are:  [0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 3. 1. 1.
 1. 1. 0. 0. 0. 1. 0. 0. 3. 1. 0. 0. 0. 1. 0. 1. 6. 1. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 2. 0. 1. 0.
 1. 1. 0. 0. 2. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 3. 1. 1. 0. 2. 1. 1. 0. 0.
 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 2. 1. 6. 0. 0. 2. 1. 0. 1. 1. 2. 1. 0.
 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 3. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0.
 0. 0. 0. 1. 1. 0. 0. 3. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.
 1. 0. 1. 6. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 2. 0. 0.
 1. 1. 1. 2. 0. 0. 1. 2. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 3. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1.
 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 4. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1.
 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 4. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1.
 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 6. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 1. 1. 1. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0.
 1. 0. 1. 1. 4. 1. 6. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 6. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 2. 1. 0. 1. 1. 2. 0. 0.
 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0.
 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 2. 1. 1. 1. 0. 0. 0. 1. 0. 6. 1. 1.
 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 6. 0. 0. 1. 1. 0. 1. 1. 2. 0. 1. 1. 0. 1.
 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 2. 2. 1. 2. 1.]
local_gradient_bounds:  [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00088774 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00426001 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00203928 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00350793 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.02395636
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00089657 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00104115 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00124193 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01717407
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00070584 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ]
!!!!!!The aggregation weights of this round are:  [INFO][10:48:03]: [Server #1127936] Global model accuracy: 96.63%

[INFO][10:48:03]: [Server #1127936] Saving the checkpoint to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_100.pth.
[INFO][10:48:03]: [Server #1127936] Model saved to /data/ykang/plato/results/test/checkpoint/checkpoint_lenet5_100.pth.
[INFO][10:48:03]: Target number of training rounds reached.
[INFO][10:48:03]: [Server #1127936] Training concluded.
[INFO][10:48:03]: [Server #1127936] Model saved to /data/ykang/plato/results/test/model/lenet5.pth.
[INFO][10:48:03]: Closing the connection to client #3.
[INFO][10:48:03]: Closing the connection to client #2.
[INFO][10:48:03]: Closing the connection to client #1.
[INFO][10:48:03]: [Client #3] The server disconnected the connection.
[INFO][10:48:03]: [Client #2] The server disconnected the connection.
[INFO][10:48:03]: [Client #1] The server disconnected the connection.
